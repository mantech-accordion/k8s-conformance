I0718 23:44:34.599535      21 e2e.go:126] Starting e2e run "cc259ba7-b4dd-4a59-af4b-b9b08bdad088" on Ginkgo node 1
Jul 18 23:44:34.613: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1689723874 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jul 18 23:44:34.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 18 23:44:34.712: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 18 23:44:34.722: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 18 23:44:34.752: INFO: The status of Pod ceph-pools-audit-28162050-6kbc6 is Succeeded, skipping waiting
Jul 18 23:44:34.752: INFO: The status of Pod ceph-pools-audit-28162055-nnlrf is Succeeded, skipping waiting
Jul 18 23:44:34.752: INFO: The status of Pod ceph-pools-audit-28162060-b5kj5 is Succeeded, skipping waiting
Jul 18 23:44:34.752: INFO: The status of Pod cephfs-storage-init-sf7s7 is Succeeded, skipping waiting
Jul 18 23:44:34.752: INFO: The status of Pod rbd-storage-init-9b656 is Succeeded, skipping waiting
Jul 18 23:44:34.752: INFO: 27 / 32 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 18 23:44:34.752: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Jul 18 23:44:34.752: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'cephfs-nodeplugin' (0 seconds elapsed)
Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'ic-nginx-ingress-ingress-nginx-controller' (0 seconds elapsed)
Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-cni-ds-amd64' (0 seconds elapsed)
Jul 18 23:44:34.758: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-device-plugin-amd64' (0 seconds elapsed)
Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'rbd-nodeplugin' (0 seconds elapsed)
Jul 18 23:44:34.758: INFO: e2e test version: v1.26.1
Jul 18 23:44:34.759: INFO: kube-apiserver version: v1.26.1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jul 18 23:44:34.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 18 23:44:34.761: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.050 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jul 18 23:44:34.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 18 23:44:34.712: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jul 18 23:44:34.722: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jul 18 23:44:34.752: INFO: The status of Pod ceph-pools-audit-28162050-6kbc6 is Succeeded, skipping waiting
    Jul 18 23:44:34.752: INFO: The status of Pod ceph-pools-audit-28162055-nnlrf is Succeeded, skipping waiting
    Jul 18 23:44:34.752: INFO: The status of Pod ceph-pools-audit-28162060-b5kj5 is Succeeded, skipping waiting
    Jul 18 23:44:34.752: INFO: The status of Pod cephfs-storage-init-sf7s7 is Succeeded, skipping waiting
    Jul 18 23:44:34.752: INFO: The status of Pod rbd-storage-init-9b656 is Succeeded, skipping waiting
    Jul 18 23:44:34.752: INFO: 27 / 32 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jul 18 23:44:34.752: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
    Jul 18 23:44:34.752: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'cephfs-nodeplugin' (0 seconds elapsed)
    Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'ic-nginx-ingress-ingress-nginx-controller' (0 seconds elapsed)
    Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-multus-ds-amd64' (0 seconds elapsed)
    Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-cni-ds-amd64' (0 seconds elapsed)
    Jul 18 23:44:34.758: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-sriov-device-plugin-amd64' (0 seconds elapsed)
    Jul 18 23:44:34.758: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'rbd-nodeplugin' (0 seconds elapsed)
    Jul 18 23:44:34.758: INFO: e2e test version: v1.26.1
    Jul 18 23:44:34.759: INFO: kube-apiserver version: v1.26.1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jul 18 23:44:34.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 18 23:44:34.761: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:44:34.783
Jul 18 23:44:34.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pod-network-test 07/18/23 23:44:34.784
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:44:34.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:44:34.793
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-9509 07/18/23 23:44:34.795
STEP: creating a selector 07/18/23 23:44:34.795
STEP: Creating the service pods in kubernetes 07/18/23 23:44:34.795
Jul 18 23:44:34.795: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 18 23:44:34.827: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9509" to be "running and ready"
Jul 18 23:44:34.831: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.32657ms
Jul 18 23:44:34.831: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 18 23:44:36.834: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007203175s
Jul 18 23:44:36.834: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 18 23:44:38.834: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007623909s
Jul 18 23:44:38.834: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 18 23:44:40.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008234033s
Jul 18 23:44:40.835: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:44:42.840: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013333169s
Jul 18 23:44:42.840: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:44:44.834: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007699179s
Jul 18 23:44:44.834: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:44:46.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008013557s
Jul 18 23:44:46.835: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:44:48.834: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007121553s
Jul 18 23:44:48.834: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:44:50.833: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00669897s
Jul 18 23:44:50.833: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:44:52.833: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.006613471s
Jul 18 23:44:52.833: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:44:54.834: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.007382604s
Jul 18 23:44:54.834: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:44:56.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008446952s
Jul 18 23:44:56.835: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jul 18 23:44:56.835: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jul 18 23:44:56.837: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9509" to be "running and ready"
Jul 18 23:44:56.838: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.481448ms
Jul 18 23:44:56.838: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jul 18 23:44:56.838: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 07/18/23 23:44:56.84
Jul 18 23:44:56.845: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9509" to be "running"
Jul 18 23:44:56.850: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.820243ms
Jul 18 23:44:58.852: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007458916s
Jul 18 23:44:58.852: INFO: Pod "test-container-pod" satisfied condition "running"
Jul 18 23:44:58.854: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9509" to be "running"
Jul 18 23:44:58.856: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.658336ms
Jul 18 23:44:58.856: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jul 18 23:44:58.858: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 18 23:44:58.858: INFO: Going to poll 172.16.192.90 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jul 18 23:44:58.859: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.192.90 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9509 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 18 23:44:58.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 18 23:44:58.860: INFO: ExecWithOptions: Clientset creation
Jul 18 23:44:58.860: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9509/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.192.90+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 18 23:44:59.918: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 18 23:44:59.918: INFO: Going to poll 172.16.166.154 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jul 18 23:44:59.924: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.166.154 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9509 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 18 23:44:59.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 18 23:44:59.924: INFO: ExecWithOptions: Clientset creation
Jul 18 23:44:59.924: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9509/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.166.154+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 18 23:45:00.966: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jul 18 23:45:00.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9509" for this suite. 07/18/23 23:45:00.969
------------------------------
• [SLOW TEST] [26.188 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:44:34.783
    Jul 18 23:44:34.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pod-network-test 07/18/23 23:44:34.784
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:44:34.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:44:34.793
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-9509 07/18/23 23:44:34.795
    STEP: creating a selector 07/18/23 23:44:34.795
    STEP: Creating the service pods in kubernetes 07/18/23 23:44:34.795
    Jul 18 23:44:34.795: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jul 18 23:44:34.827: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9509" to be "running and ready"
    Jul 18 23:44:34.831: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.32657ms
    Jul 18 23:44:34.831: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 18 23:44:36.834: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007203175s
    Jul 18 23:44:36.834: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 18 23:44:38.834: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007623909s
    Jul 18 23:44:38.834: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 18 23:44:40.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008234033s
    Jul 18 23:44:40.835: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:44:42.840: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013333169s
    Jul 18 23:44:42.840: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:44:44.834: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007699179s
    Jul 18 23:44:44.834: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:44:46.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008013557s
    Jul 18 23:44:46.835: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:44:48.834: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.007121553s
    Jul 18 23:44:48.834: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:44:50.833: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.00669897s
    Jul 18 23:44:50.833: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:44:52.833: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.006613471s
    Jul 18 23:44:52.833: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:44:54.834: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.007382604s
    Jul 18 23:44:54.834: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:44:56.835: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.008446952s
    Jul 18 23:44:56.835: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jul 18 23:44:56.835: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jul 18 23:44:56.837: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9509" to be "running and ready"
    Jul 18 23:44:56.838: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.481448ms
    Jul 18 23:44:56.838: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jul 18 23:44:56.838: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 07/18/23 23:44:56.84
    Jul 18 23:44:56.845: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9509" to be "running"
    Jul 18 23:44:56.850: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.820243ms
    Jul 18 23:44:58.852: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007458916s
    Jul 18 23:44:58.852: INFO: Pod "test-container-pod" satisfied condition "running"
    Jul 18 23:44:58.854: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9509" to be "running"
    Jul 18 23:44:58.856: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.658336ms
    Jul 18 23:44:58.856: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jul 18 23:44:58.858: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jul 18 23:44:58.858: INFO: Going to poll 172.16.192.90 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jul 18 23:44:58.859: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.192.90 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9509 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 18 23:44:58.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 18 23:44:58.860: INFO: ExecWithOptions: Clientset creation
    Jul 18 23:44:58.860: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9509/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.192.90+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 18 23:44:59.918: INFO: Found all 1 expected endpoints: [netserver-0]
    Jul 18 23:44:59.918: INFO: Going to poll 172.16.166.154 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jul 18 23:44:59.924: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.166.154 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9509 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 18 23:44:59.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 18 23:44:59.924: INFO: ExecWithOptions: Clientset creation
    Jul 18 23:44:59.924: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9509/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.166.154+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 18 23:45:00.966: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:45:00.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9509" for this suite. 07/18/23 23:45:00.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:45:00.973
Jul 18 23:45:00.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename subpath 07/18/23 23:45:00.974
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:00.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:00.985
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 07/18/23 23:45:00.986
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-lplx 07/18/23 23:45:00.99
STEP: Creating a pod to test atomic-volume-subpath 07/18/23 23:45:00.99
Jul 18 23:45:00.994: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lplx" in namespace "subpath-4630" to be "Succeeded or Failed"
Jul 18 23:45:00.998: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204775ms
Jul 18 23:45:03.000: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 2.006426548s
Jul 18 23:45:05.000: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 4.006443848s
Jul 18 23:45:07.002: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 6.008436087s
Jul 18 23:45:09.001: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 8.007321629s
Jul 18 23:45:11.055: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 10.061612334s
Jul 18 23:45:13.001: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 12.007415183s
Jul 18 23:45:15.001: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 14.00700049s
Jul 18 23:45:17.006: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 16.012433204s
Jul 18 23:45:19.002: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 18.007963474s
Jul 18 23:45:21.000: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 20.006639889s
Jul 18 23:45:23.001: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=false. Elapsed: 22.007178658s
Jul 18 23:45:25.000: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006598243s
STEP: Saw pod success 07/18/23 23:45:25
Jul 18 23:45:25.000: INFO: Pod "pod-subpath-test-secret-lplx" satisfied condition "Succeeded or Failed"
Jul 18 23:45:25.002: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-secret-lplx container test-container-subpath-secret-lplx: <nil>
STEP: delete the pod 07/18/23 23:45:25.013
Jul 18 23:45:25.022: INFO: Waiting for pod pod-subpath-test-secret-lplx to disappear
Jul 18 23:45:25.023: INFO: Pod pod-subpath-test-secret-lplx no longer exists
STEP: Deleting pod pod-subpath-test-secret-lplx 07/18/23 23:45:25.023
Jul 18 23:45:25.023: INFO: Deleting pod "pod-subpath-test-secret-lplx" in namespace "subpath-4630"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jul 18 23:45:25.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4630" for this suite. 07/18/23 23:45:25.027
------------------------------
• [SLOW TEST] [24.057 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:45:00.973
    Jul 18 23:45:00.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename subpath 07/18/23 23:45:00.974
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:00.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:00.985
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 07/18/23 23:45:00.986
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-lplx 07/18/23 23:45:00.99
    STEP: Creating a pod to test atomic-volume-subpath 07/18/23 23:45:00.99
    Jul 18 23:45:00.994: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-lplx" in namespace "subpath-4630" to be "Succeeded or Failed"
    Jul 18 23:45:00.998: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204775ms
    Jul 18 23:45:03.000: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 2.006426548s
    Jul 18 23:45:05.000: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 4.006443848s
    Jul 18 23:45:07.002: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 6.008436087s
    Jul 18 23:45:09.001: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 8.007321629s
    Jul 18 23:45:11.055: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 10.061612334s
    Jul 18 23:45:13.001: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 12.007415183s
    Jul 18 23:45:15.001: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 14.00700049s
    Jul 18 23:45:17.006: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 16.012433204s
    Jul 18 23:45:19.002: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 18.007963474s
    Jul 18 23:45:21.000: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=true. Elapsed: 20.006639889s
    Jul 18 23:45:23.001: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Running", Reason="", readiness=false. Elapsed: 22.007178658s
    Jul 18 23:45:25.000: INFO: Pod "pod-subpath-test-secret-lplx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006598243s
    STEP: Saw pod success 07/18/23 23:45:25
    Jul 18 23:45:25.000: INFO: Pod "pod-subpath-test-secret-lplx" satisfied condition "Succeeded or Failed"
    Jul 18 23:45:25.002: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-secret-lplx container test-container-subpath-secret-lplx: <nil>
    STEP: delete the pod 07/18/23 23:45:25.013
    Jul 18 23:45:25.022: INFO: Waiting for pod pod-subpath-test-secret-lplx to disappear
    Jul 18 23:45:25.023: INFO: Pod pod-subpath-test-secret-lplx no longer exists
    STEP: Deleting pod pod-subpath-test-secret-lplx 07/18/23 23:45:25.023
    Jul 18 23:45:25.023: INFO: Deleting pod "pod-subpath-test-secret-lplx" in namespace "subpath-4630"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:45:25.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4630" for this suite. 07/18/23 23:45:25.027
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:45:25.03
Jul 18 23:45:25.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/18/23 23:45:25.031
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:25.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:25.041
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 07/18/23 23:45:25.042
Jul 18 23:45:25.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35" in namespace "projected-3020" to be "Succeeded or Failed"
Jul 18 23:45:25.049: INFO: Pod "downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597209ms
Jul 18 23:45:27.051: INFO: Pod "downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004018666s
Jul 18 23:45:29.052: INFO: Pod "downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004332658s
STEP: Saw pod success 07/18/23 23:45:29.052
Jul 18 23:45:29.052: INFO: Pod "downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35" satisfied condition "Succeeded or Failed"
Jul 18 23:45:29.053: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35 container client-container: <nil>
STEP: delete the pod 07/18/23 23:45:29.057
Jul 18 23:45:29.065: INFO: Waiting for pod downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35 to disappear
Jul 18 23:45:29.067: INFO: Pod downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 18 23:45:29.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3020" for this suite. 07/18/23 23:45:29.069
------------------------------
• [4.042 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:45:25.03
    Jul 18 23:45:25.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/18/23 23:45:25.031
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:25.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:25.041
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 07/18/23 23:45:25.042
    Jul 18 23:45:25.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35" in namespace "projected-3020" to be "Succeeded or Failed"
    Jul 18 23:45:25.049: INFO: Pod "downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597209ms
    Jul 18 23:45:27.051: INFO: Pod "downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004018666s
    Jul 18 23:45:29.052: INFO: Pod "downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004332658s
    STEP: Saw pod success 07/18/23 23:45:29.052
    Jul 18 23:45:29.052: INFO: Pod "downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35" satisfied condition "Succeeded or Failed"
    Jul 18 23:45:29.053: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35 container client-container: <nil>
    STEP: delete the pod 07/18/23 23:45:29.057
    Jul 18 23:45:29.065: INFO: Waiting for pod downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35 to disappear
    Jul 18 23:45:29.067: INFO: Pod downwardapi-volume-0cc0a223-864e-4204-84b7-ccf4e3e22b35 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:45:29.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3020" for this suite. 07/18/23 23:45:29.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:45:29.073
Jul 18 23:45:29.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename watch 07/18/23 23:45:29.073
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:29.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:29.084
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 07/18/23 23:45:29.087
STEP: modifying the configmap once 07/18/23 23:45:29.089
STEP: modifying the configmap a second time 07/18/23 23:45:29.093
STEP: deleting the configmap 07/18/23 23:45:29.098
STEP: creating a watch on configmaps from the resource version returned by the first update 07/18/23 23:45:29.101
STEP: Expecting to observe notifications for all changes to the configmap after the first update 07/18/23 23:45:29.102
Jul 18 23:45:29.102: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4992  57c3a20b-b1cf-427a-9501-a0766aa3c8b8 34673 0 2023-07-18 23:45:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-18 23:45:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 18 23:45:29.102: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4992  57c3a20b-b1cf-427a-9501-a0766aa3c8b8 34674 0 2023-07-18 23:45:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-18 23:45:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jul 18 23:45:29.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4992" for this suite. 07/18/23 23:45:29.104
------------------------------
• [0.035 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:45:29.073
    Jul 18 23:45:29.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename watch 07/18/23 23:45:29.073
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:29.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:29.084
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 07/18/23 23:45:29.087
    STEP: modifying the configmap once 07/18/23 23:45:29.089
    STEP: modifying the configmap a second time 07/18/23 23:45:29.093
    STEP: deleting the configmap 07/18/23 23:45:29.098
    STEP: creating a watch on configmaps from the resource version returned by the first update 07/18/23 23:45:29.101
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 07/18/23 23:45:29.102
    Jul 18 23:45:29.102: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4992  57c3a20b-b1cf-427a-9501-a0766aa3c8b8 34673 0 2023-07-18 23:45:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-18 23:45:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 18 23:45:29.102: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4992  57c3a20b-b1cf-427a-9501-a0766aa3c8b8 34674 0 2023-07-18 23:45:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-18 23:45:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:45:29.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4992" for this suite. 07/18/23 23:45:29.104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:45:29.108
Jul 18 23:45:29.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/18/23 23:45:29.108
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:29.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:29.115
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 07/18/23 23:45:29.117
STEP: Getting a ResourceQuota 07/18/23 23:45:29.119
STEP: Listing all ResourceQuotas with LabelSelector 07/18/23 23:45:29.121
STEP: Patching the ResourceQuota 07/18/23 23:45:29.122
STEP: Deleting a Collection of ResourceQuotas 07/18/23 23:45:29.126
STEP: Verifying the deleted ResourceQuota 07/18/23 23:45:29.13
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 18 23:45:29.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8186" for this suite. 07/18/23 23:45:29.134
------------------------------
• [0.028 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:45:29.108
    Jul 18 23:45:29.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/18/23 23:45:29.108
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:29.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:29.115
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 07/18/23 23:45:29.117
    STEP: Getting a ResourceQuota 07/18/23 23:45:29.119
    STEP: Listing all ResourceQuotas with LabelSelector 07/18/23 23:45:29.121
    STEP: Patching the ResourceQuota 07/18/23 23:45:29.122
    STEP: Deleting a Collection of ResourceQuotas 07/18/23 23:45:29.126
    STEP: Verifying the deleted ResourceQuota 07/18/23 23:45:29.13
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:45:29.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8186" for this suite. 07/18/23 23:45:29.134
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:45:29.136
Jul 18 23:45:29.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/18/23 23:45:29.137
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:29.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:29.144
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 07/18/23 23:45:29.146
Jul 18 23:45:29.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe" in namespace "downward-api-8368" to be "Succeeded or Failed"
Jul 18 23:45:29.152: INFO: Pod "downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.747389ms
Jul 18 23:45:31.155: INFO: Pod "downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005019824s
Jul 18 23:45:33.154: INFO: Pod "downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004342176s
STEP: Saw pod success 07/18/23 23:45:33.154
Jul 18 23:45:33.154: INFO: Pod "downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe" satisfied condition "Succeeded or Failed"
Jul 18 23:45:33.156: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe container client-container: <nil>
STEP: delete the pod 07/18/23 23:45:33.16
Jul 18 23:45:33.166: INFO: Waiting for pod downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe to disappear
Jul 18 23:45:33.167: INFO: Pod downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 18 23:45:33.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8368" for this suite. 07/18/23 23:45:33.17
------------------------------
• [4.036 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:45:29.136
    Jul 18 23:45:29.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/18/23 23:45:29.137
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:29.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:29.144
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 07/18/23 23:45:29.146
    Jul 18 23:45:29.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe" in namespace "downward-api-8368" to be "Succeeded or Failed"
    Jul 18 23:45:29.152: INFO: Pod "downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 1.747389ms
    Jul 18 23:45:31.155: INFO: Pod "downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005019824s
    Jul 18 23:45:33.154: INFO: Pod "downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004342176s
    STEP: Saw pod success 07/18/23 23:45:33.154
    Jul 18 23:45:33.154: INFO: Pod "downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe" satisfied condition "Succeeded or Failed"
    Jul 18 23:45:33.156: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe container client-container: <nil>
    STEP: delete the pod 07/18/23 23:45:33.16
    Jul 18 23:45:33.166: INFO: Waiting for pod downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe to disappear
    Jul 18 23:45:33.167: INFO: Pod downwardapi-volume-2dbe6100-b62d-4f42-9c4d-e248efb4c5fe no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:45:33.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8368" for this suite. 07/18/23 23:45:33.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:45:33.173
Jul 18 23:45:33.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/18/23 23:45:33.174
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:33.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:33.18
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 07/18/23 23:45:33.182
Jul 18 23:45:33.186: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680" in namespace "projected-766" to be "Succeeded or Failed"
Jul 18 23:45:33.187: INFO: Pod "downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680": Phase="Pending", Reason="", readiness=false. Elapsed: 1.312108ms
Jul 18 23:45:35.190: INFO: Pod "downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004093123s
Jul 18 23:45:37.190: INFO: Pod "downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004385217s
STEP: Saw pod success 07/18/23 23:45:37.19
Jul 18 23:45:37.190: INFO: Pod "downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680" satisfied condition "Succeeded or Failed"
Jul 18 23:45:37.192: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680 container client-container: <nil>
STEP: delete the pod 07/18/23 23:45:37.195
Jul 18 23:45:37.201: INFO: Waiting for pod downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680 to disappear
Jul 18 23:45:37.203: INFO: Pod downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 18 23:45:37.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-766" for this suite. 07/18/23 23:45:37.205
------------------------------
• [4.034 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:45:33.173
    Jul 18 23:45:33.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/18/23 23:45:33.174
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:33.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:33.18
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 07/18/23 23:45:33.182
    Jul 18 23:45:33.186: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680" in namespace "projected-766" to be "Succeeded or Failed"
    Jul 18 23:45:33.187: INFO: Pod "downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680": Phase="Pending", Reason="", readiness=false. Elapsed: 1.312108ms
    Jul 18 23:45:35.190: INFO: Pod "downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004093123s
    Jul 18 23:45:37.190: INFO: Pod "downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004385217s
    STEP: Saw pod success 07/18/23 23:45:37.19
    Jul 18 23:45:37.190: INFO: Pod "downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680" satisfied condition "Succeeded or Failed"
    Jul 18 23:45:37.192: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680 container client-container: <nil>
    STEP: delete the pod 07/18/23 23:45:37.195
    Jul 18 23:45:37.201: INFO: Waiting for pod downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680 to disappear
    Jul 18 23:45:37.203: INFO: Pod downwardapi-volume-e08b2dc1-e90f-4c5c-b4e2-f028e39f2680 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:45:37.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-766" for this suite. 07/18/23 23:45:37.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:45:37.208
Jul 18 23:45:37.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/18/23 23:45:37.209
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:37.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:37.217
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 07/18/23 23:45:37.219
Jul 18 23:45:37.223: INFO: Waiting up to 5m0s for pod "labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f" in namespace "downward-api-5292" to be "running and ready"
Jul 18 23:45:37.227: INFO: Pod "labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.295881ms
Jul 18 23:45:37.227: INFO: The phase of Pod labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f is Pending, waiting for it to be Running (with Ready = true)
Jul 18 23:45:39.230: INFO: Pod "labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f": Phase="Running", Reason="", readiness=true. Elapsed: 2.00703015s
Jul 18 23:45:39.230: INFO: The phase of Pod labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f is Running (Ready = true)
Jul 18 23:45:39.230: INFO: Pod "labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f" satisfied condition "running and ready"
Jul 18 23:45:39.743: INFO: Successfully updated pod "labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 18 23:45:43.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5292" for this suite. 07/18/23 23:45:43.759
------------------------------
• [SLOW TEST] [6.554 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:45:37.208
    Jul 18 23:45:37.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/18/23 23:45:37.209
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:37.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:37.217
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 07/18/23 23:45:37.219
    Jul 18 23:45:37.223: INFO: Waiting up to 5m0s for pod "labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f" in namespace "downward-api-5292" to be "running and ready"
    Jul 18 23:45:37.227: INFO: Pod "labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.295881ms
    Jul 18 23:45:37.227: INFO: The phase of Pod labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f is Pending, waiting for it to be Running (with Ready = true)
    Jul 18 23:45:39.230: INFO: Pod "labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f": Phase="Running", Reason="", readiness=true. Elapsed: 2.00703015s
    Jul 18 23:45:39.230: INFO: The phase of Pod labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f is Running (Ready = true)
    Jul 18 23:45:39.230: INFO: Pod "labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f" satisfied condition "running and ready"
    Jul 18 23:45:39.743: INFO: Successfully updated pod "labelsupdate4e8a4a4e-6c13-482c-9634-a7266c5af09f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:45:43.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5292" for this suite. 07/18/23 23:45:43.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:45:43.762
Jul 18 23:45:43.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename cronjob 07/18/23 23:45:43.763
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:43.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:43.769
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 07/18/23 23:45:43.772
STEP: Ensuring a job is scheduled 07/18/23 23:45:43.774
STEP: Ensuring exactly one is scheduled 07/18/23 23:46:01.777
STEP: Ensuring exactly one running job exists by listing jobs explicitly 07/18/23 23:46:01.779
STEP: Ensuring no more jobs are scheduled 07/18/23 23:46:01.78
STEP: Removing cronjob 07/18/23 23:51:01.785
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:01.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5522" for this suite. 07/18/23 23:51:01.79
------------------------------
• [SLOW TEST] [318.031 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:45:43.762
    Jul 18 23:45:43.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename cronjob 07/18/23 23:45:43.763
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:45:43.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:45:43.769
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 07/18/23 23:45:43.772
    STEP: Ensuring a job is scheduled 07/18/23 23:45:43.774
    STEP: Ensuring exactly one is scheduled 07/18/23 23:46:01.777
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 07/18/23 23:46:01.779
    STEP: Ensuring no more jobs are scheduled 07/18/23 23:46:01.78
    STEP: Removing cronjob 07/18/23 23:51:01.785
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:01.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5522" for this suite. 07/18/23 23:51:01.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:01.793
Jul 18 23:51:01.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/18/23 23:51:01.794
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:01.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:01.8
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 07/18/23 23:51:01.802
STEP: Creating a ResourceQuota 07/18/23 23:51:06.804
STEP: Ensuring resource quota status is calculated 07/18/23 23:51:06.807
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:08.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7609" for this suite. 07/18/23 23:51:08.811
------------------------------
• [SLOW TEST] [7.021 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:01.793
    Jul 18 23:51:01.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/18/23 23:51:01.794
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:01.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:01.8
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 07/18/23 23:51:01.802
    STEP: Creating a ResourceQuota 07/18/23 23:51:06.804
    STEP: Ensuring resource quota status is calculated 07/18/23 23:51:06.807
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:08.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7609" for this suite. 07/18/23 23:51:08.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:08.815
Jul 18 23:51:08.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename disruption 07/18/23 23:51:08.815
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:08.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:08.823
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:08.825
Jul 18 23:51:08.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename disruption-2 07/18/23 23:51:08.825
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:08.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:08.832
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 07/18/23 23:51:08.836
STEP: Waiting for the pdb to be processed 07/18/23 23:51:10.842
STEP: Waiting for the pdb to be processed 07/18/23 23:51:12.851
STEP: listing a collection of PDBs across all namespaces 07/18/23 23:51:14.855
STEP: listing a collection of PDBs in namespace disruption-7465 07/18/23 23:51:14.857
STEP: deleting a collection of PDBs 07/18/23 23:51:14.858
STEP: Waiting for the PDB collection to be deleted 07/18/23 23:51:14.866
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:14.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:14.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-3967" for this suite. 07/18/23 23:51:14.871
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7465" for this suite. 07/18/23 23:51:14.874
------------------------------
• [SLOW TEST] [6.062 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:08.815
    Jul 18 23:51:08.815: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename disruption 07/18/23 23:51:08.815
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:08.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:08.823
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:08.825
    Jul 18 23:51:08.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename disruption-2 07/18/23 23:51:08.825
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:08.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:08.832
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 07/18/23 23:51:08.836
    STEP: Waiting for the pdb to be processed 07/18/23 23:51:10.842
    STEP: Waiting for the pdb to be processed 07/18/23 23:51:12.851
    STEP: listing a collection of PDBs across all namespaces 07/18/23 23:51:14.855
    STEP: listing a collection of PDBs in namespace disruption-7465 07/18/23 23:51:14.857
    STEP: deleting a collection of PDBs 07/18/23 23:51:14.858
    STEP: Waiting for the PDB collection to be deleted 07/18/23 23:51:14.866
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:14.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:14.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-3967" for this suite. 07/18/23 23:51:14.871
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7465" for this suite. 07/18/23 23:51:14.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:14.878
Jul 18 23:51:14.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename runtimeclass 07/18/23 23:51:14.879
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:14.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:14.886
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jul 18 23:51:14.895: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3267 to be scheduled
Jul 18 23:51:14.897: INFO: 1 pods are not scheduled: [runtimeclass-3267/test-runtimeclass-runtimeclass-3267-preconfigured-handler-w6t4r(91aea49a-188b-4de8-87cc-028dd1063b50)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:16.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3267" for this suite. 07/18/23 23:51:16.904
------------------------------
• [2.029 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:14.878
    Jul 18 23:51:14.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename runtimeclass 07/18/23 23:51:14.879
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:14.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:14.886
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jul 18 23:51:14.895: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3267 to be scheduled
    Jul 18 23:51:14.897: INFO: 1 pods are not scheduled: [runtimeclass-3267/test-runtimeclass-runtimeclass-3267-preconfigured-handler-w6t4r(91aea49a-188b-4de8-87cc-028dd1063b50)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:16.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3267" for this suite. 07/18/23 23:51:16.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:16.907
Jul 18 23:51:16.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename csiinlinevolumes 07/18/23 23:51:16.908
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:16.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:16.921
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 07/18/23 23:51:16.923
STEP: getting 07/18/23 23:51:16.934
STEP: listing 07/18/23 23:51:16.938
STEP: deleting 07/18/23 23:51:16.94
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:16.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-5531" for this suite. 07/18/23 23:51:16.95
------------------------------
• [0.045 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:16.907
    Jul 18 23:51:16.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename csiinlinevolumes 07/18/23 23:51:16.908
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:16.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:16.921
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 07/18/23 23:51:16.923
    STEP: getting 07/18/23 23:51:16.934
    STEP: listing 07/18/23 23:51:16.938
    STEP: deleting 07/18/23 23:51:16.94
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:16.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-5531" for this suite. 07/18/23 23:51:16.95
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:16.953
Jul 18 23:51:16.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename podtemplate 07/18/23 23:51:16.953
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:16.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:16.972
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:17.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-890" for this suite. 07/18/23 23:51:17.004
------------------------------
• [0.054 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:16.953
    Jul 18 23:51:16.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename podtemplate 07/18/23 23:51:16.953
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:16.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:16.972
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:17.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-890" for this suite. 07/18/23 23:51:17.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:17.008
Jul 18 23:51:17.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/18/23 23:51:17.009
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:17.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:17.022
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 07/18/23 23:51:17.024
STEP: Creating a ResourceQuota 07/18/23 23:51:22.026
STEP: Ensuring resource quota status is calculated 07/18/23 23:51:22.028
STEP: Creating a Pod that fits quota 07/18/23 23:51:24.031
STEP: Ensuring ResourceQuota status captures the pod usage 07/18/23 23:51:24.038
STEP: Not allowing a pod to be created that exceeds remaining quota 07/18/23 23:51:26.042
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 07/18/23 23:51:26.044
STEP: Ensuring a pod cannot update its resource requirements 07/18/23 23:51:26.045
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 07/18/23 23:51:26.048
STEP: Deleting the pod 07/18/23 23:51:28.05
STEP: Ensuring resource quota status released the pod usage 07/18/23 23:51:28.054
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:30.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4294" for this suite. 07/18/23 23:51:30.059
------------------------------
• [SLOW TEST] [13.053 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:17.008
    Jul 18 23:51:17.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/18/23 23:51:17.009
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:17.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:17.022
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 07/18/23 23:51:17.024
    STEP: Creating a ResourceQuota 07/18/23 23:51:22.026
    STEP: Ensuring resource quota status is calculated 07/18/23 23:51:22.028
    STEP: Creating a Pod that fits quota 07/18/23 23:51:24.031
    STEP: Ensuring ResourceQuota status captures the pod usage 07/18/23 23:51:24.038
    STEP: Not allowing a pod to be created that exceeds remaining quota 07/18/23 23:51:26.042
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 07/18/23 23:51:26.044
    STEP: Ensuring a pod cannot update its resource requirements 07/18/23 23:51:26.045
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 07/18/23 23:51:26.048
    STEP: Deleting the pod 07/18/23 23:51:28.05
    STEP: Ensuring resource quota status released the pod usage 07/18/23 23:51:28.054
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:30.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4294" for this suite. 07/18/23 23:51:30.059
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:30.062
Jul 18 23:51:30.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/18/23 23:51:30.063
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:30.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:30.07
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 07/18/23 23:51:30.072
Jul 18 23:51:30.075: INFO: Waiting up to 5m0s for pod "annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e" in namespace "projected-7594" to be "running and ready"
Jul 18 23:51:30.083: INFO: Pod "annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.244762ms
Jul 18 23:51:30.083: INFO: The phase of Pod annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e is Pending, waiting for it to be Running (with Ready = true)
Jul 18 23:51:32.085: INFO: Pod "annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009468022s
Jul 18 23:51:32.085: INFO: The phase of Pod annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e is Running (Ready = true)
Jul 18 23:51:32.085: INFO: Pod "annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e" satisfied condition "running and ready"
Jul 18 23:51:32.604: INFO: Successfully updated pod "annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:36.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7594" for this suite. 07/18/23 23:51:36.621
------------------------------
• [SLOW TEST] [6.563 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:30.062
    Jul 18 23:51:30.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/18/23 23:51:30.063
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:30.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:30.07
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 07/18/23 23:51:30.072
    Jul 18 23:51:30.075: INFO: Waiting up to 5m0s for pod "annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e" in namespace "projected-7594" to be "running and ready"
    Jul 18 23:51:30.083: INFO: Pod "annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.244762ms
    Jul 18 23:51:30.083: INFO: The phase of Pod annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e is Pending, waiting for it to be Running (with Ready = true)
    Jul 18 23:51:32.085: INFO: Pod "annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009468022s
    Jul 18 23:51:32.085: INFO: The phase of Pod annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e is Running (Ready = true)
    Jul 18 23:51:32.085: INFO: Pod "annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e" satisfied condition "running and ready"
    Jul 18 23:51:32.604: INFO: Successfully updated pod "annotationupdated1905df3-b66f-4721-9e09-b26befc3c45e"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:36.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7594" for this suite. 07/18/23 23:51:36.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:36.626
Jul 18 23:51:36.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename ephemeral-containers-test 07/18/23 23:51:36.626
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:36.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:36.634
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 07/18/23 23:51:36.636
Jul 18 23:51:36.639: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4201" to be "running and ready"
Jul 18 23:51:36.641: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.384178ms
Jul 18 23:51:36.641: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 18 23:51:38.643: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003988565s
Jul 18 23:51:38.643: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jul 18 23:51:38.643: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 07/18/23 23:51:38.645
Jul 18 23:51:38.655: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4201" to be "container debugger running"
Jul 18 23:51:38.657: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.423387ms
Jul 18 23:51:40.659: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003666969s
Jul 18 23:51:42.659: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004163509s
Jul 18 23:51:42.659: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 07/18/23 23:51:42.659
Jul 18 23:51:42.659: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4201 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 18 23:51:42.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 18 23:51:42.660: INFO: ExecWithOptions: Clientset creation
Jul 18 23:51:42.660: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4201/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jul 18 23:51:42.704: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:42.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-4201" for this suite. 07/18/23 23:51:42.71
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:36.626
    Jul 18 23:51:36.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename ephemeral-containers-test 07/18/23 23:51:36.626
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:36.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:36.634
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 07/18/23 23:51:36.636
    Jul 18 23:51:36.639: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4201" to be "running and ready"
    Jul 18 23:51:36.641: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.384178ms
    Jul 18 23:51:36.641: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jul 18 23:51:38.643: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003988565s
    Jul 18 23:51:38.643: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jul 18 23:51:38.643: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 07/18/23 23:51:38.645
    Jul 18 23:51:38.655: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4201" to be "container debugger running"
    Jul 18 23:51:38.657: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.423387ms
    Jul 18 23:51:40.659: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003666969s
    Jul 18 23:51:42.659: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004163509s
    Jul 18 23:51:42.659: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 07/18/23 23:51:42.659
    Jul 18 23:51:42.659: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4201 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 18 23:51:42.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 18 23:51:42.660: INFO: ExecWithOptions: Clientset creation
    Jul 18 23:51:42.660: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4201/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jul 18 23:51:42.704: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:42.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-4201" for this suite. 07/18/23 23:51:42.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:42.715
Jul 18 23:51:42.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename dns 07/18/23 23:51:42.716
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:42.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:42.726
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 07/18/23 23:51:42.728
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3842.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3842.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 07/18/23 23:51:42.731
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3842.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3842.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 07/18/23 23:51:42.731
STEP: creating a pod to probe DNS 07/18/23 23:51:42.731
STEP: submitting the pod to kubernetes 07/18/23 23:51:42.731
Jul 18 23:51:42.739: INFO: Waiting up to 15m0s for pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e" in namespace "dns-3842" to be "running"
Jul 18 23:51:42.740: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457913ms
Jul 18 23:51:44.743: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00469777s
Jul 18 23:51:46.742: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003493283s
Jul 18 23:51:48.743: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004079223s
Jul 18 23:51:50.744: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e": Phase="Running", Reason="", readiness=true. Elapsed: 8.005065591s
Jul 18 23:51:50.744: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e" satisfied condition "running"
STEP: retrieving the pod 07/18/23 23:51:50.744
STEP: looking for the results for each expected name from probers 07/18/23 23:51:50.746
Jul 18 23:51:50.754: INFO: DNS probes using dns-3842/dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e succeeded

STEP: deleting the pod 07/18/23 23:51:50.754
STEP: deleting the test headless service 07/18/23 23:51:50.762
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3842" for this suite. 07/18/23 23:51:50.771
------------------------------
• [SLOW TEST] [8.058 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:42.715
    Jul 18 23:51:42.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename dns 07/18/23 23:51:42.716
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:42.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:42.726
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 07/18/23 23:51:42.728
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3842.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3842.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     07/18/23 23:51:42.731
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3842.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3842.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     07/18/23 23:51:42.731
    STEP: creating a pod to probe DNS 07/18/23 23:51:42.731
    STEP: submitting the pod to kubernetes 07/18/23 23:51:42.731
    Jul 18 23:51:42.739: INFO: Waiting up to 15m0s for pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e" in namespace "dns-3842" to be "running"
    Jul 18 23:51:42.740: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457913ms
    Jul 18 23:51:44.743: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00469777s
    Jul 18 23:51:46.742: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003493283s
    Jul 18 23:51:48.743: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004079223s
    Jul 18 23:51:50.744: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e": Phase="Running", Reason="", readiness=true. Elapsed: 8.005065591s
    Jul 18 23:51:50.744: INFO: Pod "dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e" satisfied condition "running"
    STEP: retrieving the pod 07/18/23 23:51:50.744
    STEP: looking for the results for each expected name from probers 07/18/23 23:51:50.746
    Jul 18 23:51:50.754: INFO: DNS probes using dns-3842/dns-test-b92ab1a8-daa1-4a67-97c6-c761f01dac2e succeeded

    STEP: deleting the pod 07/18/23 23:51:50.754
    STEP: deleting the test headless service 07/18/23 23:51:50.762
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3842" for this suite. 07/18/23 23:51:50.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:50.774
Jul 18 23:51:50.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-webhook 07/18/23 23:51:50.775
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:50.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:50.783
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 07/18/23 23:51:50.785
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 07/18/23 23:51:51.153
STEP: Deploying the custom resource conversion webhook pod 07/18/23 23:51:51.156
STEP: Wait for the deployment to be ready 07/18/23 23:51:51.162
Jul 18 23:51:51.167: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 07/18/23 23:51:53.173
STEP: Verifying the service has paired with the endpoint 07/18/23 23:51:53.182
Jul 18 23:51:54.182: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jul 18 23:51:54.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Creating a v1 custom resource 07/18/23 23:51:56.79
STEP: Create a v2 custom resource 07/18/23 23:51:56.798
STEP: List CRs in v1 07/18/23 23:51:56.891
STEP: List CRs in v2 07/18/23 23:51:56.895
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 18 23:51:57.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-217" for this suite. 07/18/23 23:51:57.425
------------------------------
• [SLOW TEST] [6.653 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:50.774
    Jul 18 23:51:50.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-webhook 07/18/23 23:51:50.775
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:50.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:50.783
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 07/18/23 23:51:50.785
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 07/18/23 23:51:51.153
    STEP: Deploying the custom resource conversion webhook pod 07/18/23 23:51:51.156
    STEP: Wait for the deployment to be ready 07/18/23 23:51:51.162
    Jul 18 23:51:51.167: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 07/18/23 23:51:53.173
    STEP: Verifying the service has paired with the endpoint 07/18/23 23:51:53.182
    Jul 18 23:51:54.182: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jul 18 23:51:54.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Creating a v1 custom resource 07/18/23 23:51:56.79
    STEP: Create a v2 custom resource 07/18/23 23:51:56.798
    STEP: List CRs in v1 07/18/23 23:51:56.891
    STEP: List CRs in v2 07/18/23 23:51:56.895
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:51:57.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-217" for this suite. 07/18/23 23:51:57.425
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:51:57.428
Jul 18 23:51:57.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename controllerrevisions 07/18/23 23:51:57.428
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:57.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:57.439
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-254s4-daemon-set" 07/18/23 23:51:57.449
STEP: Check that daemon pods launch on every node of the cluster. 07/18/23 23:51:57.452
Jul 18 23:51:57.458: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
Jul 18 23:51:57.458: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 18 23:51:58.462: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
Jul 18 23:51:58.462: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 18 23:51:59.463: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
Jul 18 23:51:59.463: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 18 23:52:00.462: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
Jul 18 23:52:00.462: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 18 23:52:01.462: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
Jul 18 23:52:01.462: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 18 23:52:02.461: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 1
Jul 18 23:52:02.461: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 18 23:52:03.462: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 2
Jul 18 23:52:03.462: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-254s4-daemon-set
STEP: Confirm DaemonSet "e2e-254s4-daemon-set" successfully created with "daemonset-name=e2e-254s4-daemon-set" label 07/18/23 23:52:03.464
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-254s4-daemon-set" 07/18/23 23:52:03.468
Jul 18 23:52:03.470: INFO: Located ControllerRevision: "e2e-254s4-daemon-set-867c8d7d85"
STEP: Patching ControllerRevision "e2e-254s4-daemon-set-867c8d7d85" 07/18/23 23:52:03.471
Jul 18 23:52:03.475: INFO: e2e-254s4-daemon-set-867c8d7d85 has been patched
STEP: Create a new ControllerRevision 07/18/23 23:52:03.475
Jul 18 23:52:03.478: INFO: Created ControllerRevision: e2e-254s4-daemon-set-fd8b8d557
STEP: Confirm that there are two ControllerRevisions 07/18/23 23:52:03.478
Jul 18 23:52:03.478: INFO: Requesting list of ControllerRevisions to confirm quantity
Jul 18 23:52:03.480: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-254s4-daemon-set-867c8d7d85" 07/18/23 23:52:03.48
STEP: Confirm that there is only one ControllerRevision 07/18/23 23:52:03.482
Jul 18 23:52:03.482: INFO: Requesting list of ControllerRevisions to confirm quantity
Jul 18 23:52:03.483: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-254s4-daemon-set-fd8b8d557" 07/18/23 23:52:03.485
Jul 18 23:52:03.488: INFO: e2e-254s4-daemon-set-fd8b8d557 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 07/18/23 23:52:03.488
W0718 23:52:03.493547      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 07/18/23 23:52:03.493
Jul 18 23:52:03.493: INFO: Requesting list of ControllerRevisions to confirm quantity
Jul 18 23:52:04.498: INFO: Requesting list of ControllerRevisions to confirm quantity
Jul 18 23:52:04.500: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-254s4-daemon-set-fd8b8d557=updated" 07/18/23 23:52:04.5
STEP: Confirm that there is only one ControllerRevision 07/18/23 23:52:04.503
Jul 18 23:52:04.503: INFO: Requesting list of ControllerRevisions to confirm quantity
Jul 18 23:52:04.505: INFO: Found 1 ControllerRevisions
Jul 18 23:52:04.506: INFO: ControllerRevision "e2e-254s4-daemon-set-cfc9cc57b" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-254s4-daemon-set" 07/18/23 23:52:04.508
STEP: deleting DaemonSet.extensions e2e-254s4-daemon-set in namespace controllerrevisions-3626, will wait for the garbage collector to delete the pods 07/18/23 23:52:04.508
Jul 18 23:52:04.563: INFO: Deleting DaemonSet.extensions e2e-254s4-daemon-set took: 2.454062ms
Jul 18 23:52:04.664: INFO: Terminating DaemonSet.extensions e2e-254s4-daemon-set pods took: 100.981694ms
Jul 18 23:52:06.166: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
Jul 18 23:52:06.166: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-254s4-daemon-set
Jul 18 23:52:06.168: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37546"},"items":null}

Jul 18 23:52:06.169: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37546"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 18 23:52:06.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-3626" for this suite. 07/18/23 23:52:06.177
------------------------------
• [SLOW TEST] [8.752 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:51:57.428
    Jul 18 23:51:57.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename controllerrevisions 07/18/23 23:51:57.428
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:51:57.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:51:57.439
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-254s4-daemon-set" 07/18/23 23:51:57.449
    STEP: Check that daemon pods launch on every node of the cluster. 07/18/23 23:51:57.452
    Jul 18 23:51:57.458: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
    Jul 18 23:51:57.458: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 18 23:51:58.462: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
    Jul 18 23:51:58.462: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 18 23:51:59.463: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
    Jul 18 23:51:59.463: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 18 23:52:00.462: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
    Jul 18 23:52:00.462: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 18 23:52:01.462: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
    Jul 18 23:52:01.462: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 18 23:52:02.461: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 1
    Jul 18 23:52:02.461: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 18 23:52:03.462: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 2
    Jul 18 23:52:03.462: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-254s4-daemon-set
    STEP: Confirm DaemonSet "e2e-254s4-daemon-set" successfully created with "daemonset-name=e2e-254s4-daemon-set" label 07/18/23 23:52:03.464
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-254s4-daemon-set" 07/18/23 23:52:03.468
    Jul 18 23:52:03.470: INFO: Located ControllerRevision: "e2e-254s4-daemon-set-867c8d7d85"
    STEP: Patching ControllerRevision "e2e-254s4-daemon-set-867c8d7d85" 07/18/23 23:52:03.471
    Jul 18 23:52:03.475: INFO: e2e-254s4-daemon-set-867c8d7d85 has been patched
    STEP: Create a new ControllerRevision 07/18/23 23:52:03.475
    Jul 18 23:52:03.478: INFO: Created ControllerRevision: e2e-254s4-daemon-set-fd8b8d557
    STEP: Confirm that there are two ControllerRevisions 07/18/23 23:52:03.478
    Jul 18 23:52:03.478: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jul 18 23:52:03.480: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-254s4-daemon-set-867c8d7d85" 07/18/23 23:52:03.48
    STEP: Confirm that there is only one ControllerRevision 07/18/23 23:52:03.482
    Jul 18 23:52:03.482: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jul 18 23:52:03.483: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-254s4-daemon-set-fd8b8d557" 07/18/23 23:52:03.485
    Jul 18 23:52:03.488: INFO: e2e-254s4-daemon-set-fd8b8d557 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 07/18/23 23:52:03.488
    W0718 23:52:03.493547      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 07/18/23 23:52:03.493
    Jul 18 23:52:03.493: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jul 18 23:52:04.498: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jul 18 23:52:04.500: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-254s4-daemon-set-fd8b8d557=updated" 07/18/23 23:52:04.5
    STEP: Confirm that there is only one ControllerRevision 07/18/23 23:52:04.503
    Jul 18 23:52:04.503: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jul 18 23:52:04.505: INFO: Found 1 ControllerRevisions
    Jul 18 23:52:04.506: INFO: ControllerRevision "e2e-254s4-daemon-set-cfc9cc57b" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-254s4-daemon-set" 07/18/23 23:52:04.508
    STEP: deleting DaemonSet.extensions e2e-254s4-daemon-set in namespace controllerrevisions-3626, will wait for the garbage collector to delete the pods 07/18/23 23:52:04.508
    Jul 18 23:52:04.563: INFO: Deleting DaemonSet.extensions e2e-254s4-daemon-set took: 2.454062ms
    Jul 18 23:52:04.664: INFO: Terminating DaemonSet.extensions e2e-254s4-daemon-set pods took: 100.981694ms
    Jul 18 23:52:06.166: INFO: Number of nodes with available pods controlled by daemonset e2e-254s4-daemon-set: 0
    Jul 18 23:52:06.166: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-254s4-daemon-set
    Jul 18 23:52:06.168: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37546"},"items":null}

    Jul 18 23:52:06.169: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37546"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:52:06.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-3626" for this suite. 07/18/23 23:52:06.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:52:06.181
Jul 18 23:52:06.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/18/23 23:52:06.182
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:52:06.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:52:06.189
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 07/18/23 23:52:06.191
Jul 18 23:52:06.196: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762" in namespace "downward-api-3995" to be "Succeeded or Failed"
Jul 18 23:52:06.198: INFO: Pod "downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712709ms
Jul 18 23:52:08.201: INFO: Pod "downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005878424s
Jul 18 23:52:10.201: INFO: Pod "downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005648116s
STEP: Saw pod success 07/18/23 23:52:10.201
Jul 18 23:52:10.201: INFO: Pod "downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762" satisfied condition "Succeeded or Failed"
Jul 18 23:52:10.203: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762 container client-container: <nil>
STEP: delete the pod 07/18/23 23:52:10.207
Jul 18 23:52:10.213: INFO: Waiting for pod downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762 to disappear
Jul 18 23:52:10.215: INFO: Pod downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 18 23:52:10.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3995" for this suite. 07/18/23 23:52:10.217
------------------------------
• [4.039 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:52:06.181
    Jul 18 23:52:06.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/18/23 23:52:06.182
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:52:06.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:52:06.189
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 07/18/23 23:52:06.191
    Jul 18 23:52:06.196: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762" in namespace "downward-api-3995" to be "Succeeded or Failed"
    Jul 18 23:52:06.198: INFO: Pod "downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712709ms
    Jul 18 23:52:08.201: INFO: Pod "downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005878424s
    Jul 18 23:52:10.201: INFO: Pod "downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005648116s
    STEP: Saw pod success 07/18/23 23:52:10.201
    Jul 18 23:52:10.201: INFO: Pod "downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762" satisfied condition "Succeeded or Failed"
    Jul 18 23:52:10.203: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762 container client-container: <nil>
    STEP: delete the pod 07/18/23 23:52:10.207
    Jul 18 23:52:10.213: INFO: Waiting for pod downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762 to disappear
    Jul 18 23:52:10.215: INFO: Pod downwardapi-volume-6e7a706a-608c-42a4-b788-2d037f1e0762 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:52:10.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3995" for this suite. 07/18/23 23:52:10.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:52:10.221
Jul 18 23:52:10.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/18/23 23:52:10.222
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:52:10.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:52:10.229
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-d3738f93-2e58-42e9-95e8-645cf2b8c465 07/18/23 23:52:10.231
STEP: Creating a pod to test consume secrets 07/18/23 23:52:10.234
Jul 18 23:52:10.238: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df" in namespace "projected-1565" to be "Succeeded or Failed"
Jul 18 23:52:10.240: INFO: Pod "pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df": Phase="Pending", Reason="", readiness=false. Elapsed: 1.519615ms
Jul 18 23:52:12.242: INFO: Pod "pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003905492s
Jul 18 23:52:14.242: INFO: Pod "pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003922305s
STEP: Saw pod success 07/18/23 23:52:14.242
Jul 18 23:52:14.242: INFO: Pod "pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df" satisfied condition "Succeeded or Failed"
Jul 18 23:52:14.244: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df container projected-secret-volume-test: <nil>
STEP: delete the pod 07/18/23 23:52:14.248
Jul 18 23:52:14.255: INFO: Waiting for pod pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df to disappear
Jul 18 23:52:14.257: INFO: Pod pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jul 18 23:52:14.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1565" for this suite. 07/18/23 23:52:14.259
------------------------------
• [4.040 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:52:10.221
    Jul 18 23:52:10.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/18/23 23:52:10.222
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:52:10.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:52:10.229
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-d3738f93-2e58-42e9-95e8-645cf2b8c465 07/18/23 23:52:10.231
    STEP: Creating a pod to test consume secrets 07/18/23 23:52:10.234
    Jul 18 23:52:10.238: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df" in namespace "projected-1565" to be "Succeeded or Failed"
    Jul 18 23:52:10.240: INFO: Pod "pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df": Phase="Pending", Reason="", readiness=false. Elapsed: 1.519615ms
    Jul 18 23:52:12.242: INFO: Pod "pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003905492s
    Jul 18 23:52:14.242: INFO: Pod "pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003922305s
    STEP: Saw pod success 07/18/23 23:52:14.242
    Jul 18 23:52:14.242: INFO: Pod "pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df" satisfied condition "Succeeded or Failed"
    Jul 18 23:52:14.244: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df container projected-secret-volume-test: <nil>
    STEP: delete the pod 07/18/23 23:52:14.248
    Jul 18 23:52:14.255: INFO: Waiting for pod pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df to disappear
    Jul 18 23:52:14.257: INFO: Pod pod-projected-secrets-b15cb939-9ff5-4897-9240-f0296ff6a5df no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:52:14.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1565" for this suite. 07/18/23 23:52:14.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:52:14.262
Jul 18 23:52:14.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename gc 07/18/23 23:52:14.263
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:52:14.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:52:14.274
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 07/18/23 23:52:14.276
STEP: Wait for the Deployment to create new ReplicaSet 07/18/23 23:52:14.279
STEP: delete the deployment 07/18/23 23:52:14.816
STEP: wait for all rs to be garbage collected 07/18/23 23:52:14.827
STEP: expected 0 rs, got 1 rs 07/18/23 23:52:14.83
STEP: expected 0 pods, got 2 pods 07/18/23 23:52:14.832
STEP: Gathering metrics 07/18/23 23:52:15.338
Jul 18 23:52:15.351: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
Jul 18 23:52:15.353: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.448251ms
Jul 18 23:52:15.353: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
Jul 18 23:52:15.353: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
Jul 18 23:52:15.396: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jul 18 23:52:15.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6114" for this suite. 07/18/23 23:52:15.399
------------------------------
• [1.140 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:52:14.262
    Jul 18 23:52:14.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename gc 07/18/23 23:52:14.263
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:52:14.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:52:14.274
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 07/18/23 23:52:14.276
    STEP: Wait for the Deployment to create new ReplicaSet 07/18/23 23:52:14.279
    STEP: delete the deployment 07/18/23 23:52:14.816
    STEP: wait for all rs to be garbage collected 07/18/23 23:52:14.827
    STEP: expected 0 rs, got 1 rs 07/18/23 23:52:14.83
    STEP: expected 0 pods, got 2 pods 07/18/23 23:52:14.832
    STEP: Gathering metrics 07/18/23 23:52:15.338
    Jul 18 23:52:15.351: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
    Jul 18 23:52:15.353: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.448251ms
    Jul 18 23:52:15.353: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
    Jul 18 23:52:15.353: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
    Jul 18 23:52:15.396: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:52:15.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6114" for this suite. 07/18/23 23:52:15.399
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:52:15.402
Jul 18 23:52:15.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename namespaces 07/18/23 23:52:15.403
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:52:15.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:52:15.412
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-7wxtn" 07/18/23 23:52:15.414
Jul 18 23:52:15.419: INFO: Namespace "e2e-ns-7wxtn-3790" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-7wxtn-3790" 07/18/23 23:52:15.419
Jul 18 23:52:15.426: INFO: Namespace "e2e-ns-7wxtn-3790" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-7wxtn-3790" 07/18/23 23:52:15.426
Jul 18 23:52:15.430: INFO: Namespace "e2e-ns-7wxtn-3790" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 18 23:52:15.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3919" for this suite. 07/18/23 23:52:15.432
STEP: Destroying namespace "e2e-ns-7wxtn-3790" for this suite. 07/18/23 23:52:15.435
------------------------------
• [0.035 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:52:15.402
    Jul 18 23:52:15.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename namespaces 07/18/23 23:52:15.403
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:52:15.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:52:15.412
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-7wxtn" 07/18/23 23:52:15.414
    Jul 18 23:52:15.419: INFO: Namespace "e2e-ns-7wxtn-3790" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-7wxtn-3790" 07/18/23 23:52:15.419
    Jul 18 23:52:15.426: INFO: Namespace "e2e-ns-7wxtn-3790" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-7wxtn-3790" 07/18/23 23:52:15.426
    Jul 18 23:52:15.430: INFO: Namespace "e2e-ns-7wxtn-3790" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:52:15.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3919" for this suite. 07/18/23 23:52:15.432
    STEP: Destroying namespace "e2e-ns-7wxtn-3790" for this suite. 07/18/23 23:52:15.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:52:15.438
Jul 18 23:52:15.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename cronjob 07/18/23 23:52:15.439
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:52:15.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:52:15.445
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 07/18/23 23:52:15.447
STEP: Ensuring no jobs are scheduled 07/18/23 23:52:15.45
STEP: Ensuring no job exists by listing jobs explicitly 07/18/23 23:57:15.454
STEP: Removing cronjob 07/18/23 23:57:15.456
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jul 18 23:57:15.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7782" for this suite. 07/18/23 23:57:15.462
------------------------------
• [SLOW TEST] [300.026 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:52:15.438
    Jul 18 23:52:15.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename cronjob 07/18/23 23:52:15.439
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:52:15.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:52:15.445
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 07/18/23 23:52:15.447
    STEP: Ensuring no jobs are scheduled 07/18/23 23:52:15.45
    STEP: Ensuring no job exists by listing jobs explicitly 07/18/23 23:57:15.454
    STEP: Removing cronjob 07/18/23 23:57:15.456
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:57:15.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7782" for this suite. 07/18/23 23:57:15.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:57:15.465
Jul 18 23:57:15.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename namespaces 07/18/23 23:57:15.466
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:15.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:15.473
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 07/18/23 23:57:15.475
STEP: patching the Namespace 07/18/23 23:57:15.482
STEP: get the Namespace and ensuring it has the label 07/18/23 23:57:15.487
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 18 23:57:15.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4861" for this suite. 07/18/23 23:57:15.491
STEP: Destroying namespace "nspatchtest-c6a78de0-8f62-4c48-b8f3-0c7823fa4a6b-4952" for this suite. 07/18/23 23:57:15.493
------------------------------
• [0.030 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:57:15.465
    Jul 18 23:57:15.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename namespaces 07/18/23 23:57:15.466
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:15.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:15.473
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 07/18/23 23:57:15.475
    STEP: patching the Namespace 07/18/23 23:57:15.482
    STEP: get the Namespace and ensuring it has the label 07/18/23 23:57:15.487
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:57:15.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4861" for this suite. 07/18/23 23:57:15.491
    STEP: Destroying namespace "nspatchtest-c6a78de0-8f62-4c48-b8f3-0c7823fa4a6b-4952" for this suite. 07/18/23 23:57:15.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:57:15.496
Jul 18 23:57:15.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/18/23 23:57:15.497
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:15.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:15.504
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 07/18/23 23:57:15.505
STEP: Creating a ResourceQuota 07/18/23 23:57:20.507
STEP: Ensuring resource quota status is calculated 07/18/23 23:57:20.51
STEP: Creating a ReplicationController 07/18/23 23:57:22.512
STEP: Ensuring resource quota status captures replication controller creation 07/18/23 23:57:22.521
STEP: Deleting a ReplicationController 07/18/23 23:57:24.524
STEP: Ensuring resource quota status released usage 07/18/23 23:57:24.527
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 18 23:57:26.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9739" for this suite. 07/18/23 23:57:26.533
------------------------------
• [SLOW TEST] [11.040 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:57:15.496
    Jul 18 23:57:15.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/18/23 23:57:15.497
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:15.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:15.504
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 07/18/23 23:57:15.505
    STEP: Creating a ResourceQuota 07/18/23 23:57:20.507
    STEP: Ensuring resource quota status is calculated 07/18/23 23:57:20.51
    STEP: Creating a ReplicationController 07/18/23 23:57:22.512
    STEP: Ensuring resource quota status captures replication controller creation 07/18/23 23:57:22.521
    STEP: Deleting a ReplicationController 07/18/23 23:57:24.524
    STEP: Ensuring resource quota status released usage 07/18/23 23:57:24.527
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:57:26.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9739" for this suite. 07/18/23 23:57:26.533
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:57:26.536
Jul 18 23:57:26.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename statefulset 07/18/23 23:57:26.537
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:26.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:26.545
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9891 07/18/23 23:57:26.547
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 07/18/23 23:57:26.55
STEP: Creating pod with conflicting port in namespace statefulset-9891 07/18/23 23:57:26.552
STEP: Waiting until pod test-pod will start running in namespace statefulset-9891 07/18/23 23:57:26.556
Jul 18 23:57:26.556: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9891" to be "running"
Jul 18 23:57:26.561: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.229396ms
Jul 18 23:57:28.564: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007758578s
Jul 18 23:57:28.564: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-9891 07/18/23 23:57:28.564
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9891 07/18/23 23:57:28.567
Jul 18 23:57:28.580: INFO: Observed stateful pod in namespace: statefulset-9891, name: ss-0, uid: 5971588d-16cd-4a8f-8287-227fa03ae064, status phase: Pending. Waiting for statefulset controller to delete.
Jul 18 23:57:28.591: INFO: Observed stateful pod in namespace: statefulset-9891, name: ss-0, uid: 5971588d-16cd-4a8f-8287-227fa03ae064, status phase: Failed. Waiting for statefulset controller to delete.
Jul 18 23:57:28.596: INFO: Observed stateful pod in namespace: statefulset-9891, name: ss-0, uid: 5971588d-16cd-4a8f-8287-227fa03ae064, status phase: Failed. Waiting for statefulset controller to delete.
Jul 18 23:57:28.602: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9891
STEP: Removing pod with conflicting port in namespace statefulset-9891 07/18/23 23:57:28.602
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9891 and will be in running state 07/18/23 23:57:28.614
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jul 18 23:57:30.621: INFO: Deleting all statefulset in ns statefulset-9891
Jul 18 23:57:30.623: INFO: Scaling statefulset ss to 0
Jul 18 23:57:40.634: INFO: Waiting for statefulset status.replicas updated to 0
Jul 18 23:57:40.636: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jul 18 23:57:40.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9891" for this suite. 07/18/23 23:57:40.643
------------------------------
• [SLOW TEST] [14.110 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:57:26.536
    Jul 18 23:57:26.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename statefulset 07/18/23 23:57:26.537
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:26.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:26.545
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9891 07/18/23 23:57:26.547
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 07/18/23 23:57:26.55
    STEP: Creating pod with conflicting port in namespace statefulset-9891 07/18/23 23:57:26.552
    STEP: Waiting until pod test-pod will start running in namespace statefulset-9891 07/18/23 23:57:26.556
    Jul 18 23:57:26.556: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9891" to be "running"
    Jul 18 23:57:26.561: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.229396ms
    Jul 18 23:57:28.564: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007758578s
    Jul 18 23:57:28.564: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-9891 07/18/23 23:57:28.564
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9891 07/18/23 23:57:28.567
    Jul 18 23:57:28.580: INFO: Observed stateful pod in namespace: statefulset-9891, name: ss-0, uid: 5971588d-16cd-4a8f-8287-227fa03ae064, status phase: Pending. Waiting for statefulset controller to delete.
    Jul 18 23:57:28.591: INFO: Observed stateful pod in namespace: statefulset-9891, name: ss-0, uid: 5971588d-16cd-4a8f-8287-227fa03ae064, status phase: Failed. Waiting for statefulset controller to delete.
    Jul 18 23:57:28.596: INFO: Observed stateful pod in namespace: statefulset-9891, name: ss-0, uid: 5971588d-16cd-4a8f-8287-227fa03ae064, status phase: Failed. Waiting for statefulset controller to delete.
    Jul 18 23:57:28.602: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9891
    STEP: Removing pod with conflicting port in namespace statefulset-9891 07/18/23 23:57:28.602
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9891 and will be in running state 07/18/23 23:57:28.614
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jul 18 23:57:30.621: INFO: Deleting all statefulset in ns statefulset-9891
    Jul 18 23:57:30.623: INFO: Scaling statefulset ss to 0
    Jul 18 23:57:40.634: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 18 23:57:40.636: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:57:40.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9891" for this suite. 07/18/23 23:57:40.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:57:40.647
Jul 18 23:57:40.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename cronjob 07/18/23 23:57:40.647
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:40.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:40.655
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 07/18/23 23:57:40.658
STEP: creating 07/18/23 23:57:40.658
STEP: getting 07/18/23 23:57:40.66
STEP: listing 07/18/23 23:57:40.664
STEP: watching 07/18/23 23:57:40.666
Jul 18 23:57:40.666: INFO: starting watch
STEP: cluster-wide listing 07/18/23 23:57:40.666
STEP: cluster-wide watching 07/18/23 23:57:40.668
Jul 18 23:57:40.668: INFO: starting watch
STEP: patching 07/18/23 23:57:40.669
STEP: updating 07/18/23 23:57:40.672
Jul 18 23:57:40.676: INFO: waiting for watch events with expected annotations
Jul 18 23:57:40.676: INFO: saw patched and updated annotations
STEP: patching /status 07/18/23 23:57:40.676
STEP: updating /status 07/18/23 23:57:40.679
STEP: get /status 07/18/23 23:57:40.684
STEP: deleting 07/18/23 23:57:40.686
STEP: deleting a collection 07/18/23 23:57:40.691
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jul 18 23:57:40.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2255" for this suite. 07/18/23 23:57:40.7
------------------------------
• [0.056 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:57:40.647
    Jul 18 23:57:40.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename cronjob 07/18/23 23:57:40.647
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:40.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:40.655
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 07/18/23 23:57:40.658
    STEP: creating 07/18/23 23:57:40.658
    STEP: getting 07/18/23 23:57:40.66
    STEP: listing 07/18/23 23:57:40.664
    STEP: watching 07/18/23 23:57:40.666
    Jul 18 23:57:40.666: INFO: starting watch
    STEP: cluster-wide listing 07/18/23 23:57:40.666
    STEP: cluster-wide watching 07/18/23 23:57:40.668
    Jul 18 23:57:40.668: INFO: starting watch
    STEP: patching 07/18/23 23:57:40.669
    STEP: updating 07/18/23 23:57:40.672
    Jul 18 23:57:40.676: INFO: waiting for watch events with expected annotations
    Jul 18 23:57:40.676: INFO: saw patched and updated annotations
    STEP: patching /status 07/18/23 23:57:40.676
    STEP: updating /status 07/18/23 23:57:40.679
    STEP: get /status 07/18/23 23:57:40.684
    STEP: deleting 07/18/23 23:57:40.686
    STEP: deleting a collection 07/18/23 23:57:40.691
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:57:40.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2255" for this suite. 07/18/23 23:57:40.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:57:40.703
Jul 18 23:57:40.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename disruption 07/18/23 23:57:40.704
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:40.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:40.713
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 07/18/23 23:57:40.715
STEP: Waiting for the pdb to be processed 07/18/23 23:57:40.717
STEP: First trying to evict a pod which shouldn't be evictable 07/18/23 23:57:42.723
STEP: Waiting for all pods to be running 07/18/23 23:57:42.723
Jul 18 23:57:42.724: INFO: pods: 0 < 3
STEP: locating a running pod 07/18/23 23:57:44.727
STEP: Updating the pdb to allow a pod to be evicted 07/18/23 23:57:44.733
STEP: Waiting for the pdb to be processed 07/18/23 23:57:44.736
STEP: Trying to evict the same pod we tried earlier which should now be evictable 07/18/23 23:57:46.74
STEP: Waiting for all pods to be running 07/18/23 23:57:46.74
STEP: Waiting for the pdb to observed all healthy pods 07/18/23 23:57:46.742
STEP: Patching the pdb to disallow a pod to be evicted 07/18/23 23:57:46.757
STEP: Waiting for the pdb to be processed 07/18/23 23:57:46.773
STEP: Waiting for all pods to be running 07/18/23 23:57:46.776
Jul 18 23:57:46.778: INFO: running pods: 2 < 3
STEP: locating a running pod 07/18/23 23:57:48.781
STEP: Deleting the pdb to allow a pod to be evicted 07/18/23 23:57:48.786
STEP: Waiting for the pdb to be deleted 07/18/23 23:57:48.788
STEP: Trying to evict the same pod we tried earlier which should now be evictable 07/18/23 23:57:48.789
STEP: Waiting for all pods to be running 07/18/23 23:57:48.79
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jul 18 23:57:48.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2252" for this suite. 07/18/23 23:57:48.804
------------------------------
• [SLOW TEST] [8.107 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:57:40.703
    Jul 18 23:57:40.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename disruption 07/18/23 23:57:40.704
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:40.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:40.713
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 07/18/23 23:57:40.715
    STEP: Waiting for the pdb to be processed 07/18/23 23:57:40.717
    STEP: First trying to evict a pod which shouldn't be evictable 07/18/23 23:57:42.723
    STEP: Waiting for all pods to be running 07/18/23 23:57:42.723
    Jul 18 23:57:42.724: INFO: pods: 0 < 3
    STEP: locating a running pod 07/18/23 23:57:44.727
    STEP: Updating the pdb to allow a pod to be evicted 07/18/23 23:57:44.733
    STEP: Waiting for the pdb to be processed 07/18/23 23:57:44.736
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 07/18/23 23:57:46.74
    STEP: Waiting for all pods to be running 07/18/23 23:57:46.74
    STEP: Waiting for the pdb to observed all healthy pods 07/18/23 23:57:46.742
    STEP: Patching the pdb to disallow a pod to be evicted 07/18/23 23:57:46.757
    STEP: Waiting for the pdb to be processed 07/18/23 23:57:46.773
    STEP: Waiting for all pods to be running 07/18/23 23:57:46.776
    Jul 18 23:57:46.778: INFO: running pods: 2 < 3
    STEP: locating a running pod 07/18/23 23:57:48.781
    STEP: Deleting the pdb to allow a pod to be evicted 07/18/23 23:57:48.786
    STEP: Waiting for the pdb to be deleted 07/18/23 23:57:48.788
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 07/18/23 23:57:48.789
    STEP: Waiting for all pods to be running 07/18/23 23:57:48.79
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:57:48.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2252" for this suite. 07/18/23 23:57:48.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:57:48.811
Jul 18 23:57:48.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replicaset 07/18/23 23:57:48.812
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:48.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:48.819
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 07/18/23 23:57:48.821
STEP: Verify that the required pods have come up 07/18/23 23:57:48.823
Jul 18 23:57:48.826: INFO: Pod name sample-pod: Found 0 pods out of 3
Jul 18 23:57:53.831: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 07/18/23 23:57:53.831
Jul 18 23:57:53.832: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 07/18/23 23:57:53.832
STEP: DeleteCollection of the ReplicaSets 07/18/23 23:57:53.834
STEP: After DeleteCollection verify that ReplicaSets have been deleted 07/18/23 23:57:53.837
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jul 18 23:57:53.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2739" for this suite. 07/18/23 23:57:53.841
------------------------------
• [SLOW TEST] [5.044 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:57:48.811
    Jul 18 23:57:48.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replicaset 07/18/23 23:57:48.812
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:48.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:48.819
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 07/18/23 23:57:48.821
    STEP: Verify that the required pods have come up 07/18/23 23:57:48.823
    Jul 18 23:57:48.826: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jul 18 23:57:53.831: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 07/18/23 23:57:53.831
    Jul 18 23:57:53.832: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 07/18/23 23:57:53.832
    STEP: DeleteCollection of the ReplicaSets 07/18/23 23:57:53.834
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 07/18/23 23:57:53.837
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:57:53.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2739" for this suite. 07/18/23 23:57:53.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:57:53.856
Jul 18 23:57:53.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/18/23 23:57:53.856
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:53.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:53.868
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 07/18/23 23:57:53.87
Jul 18 23:57:53.870: INFO: namespace kubectl-5087
Jul 18 23:57:53.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5087 create -f -'
Jul 18 23:57:55.046: INFO: stderr: ""
Jul 18 23:57:55.046: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 07/18/23 23:57:55.046
Jul 18 23:57:56.048: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 18 23:57:56.048: INFO: Found 1 / 1
Jul 18 23:57:56.048: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 18 23:57:56.050: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 18 23:57:56.050: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 18 23:57:56.050: INFO: wait on agnhost-primary startup in kubectl-5087 
Jul 18 23:57:56.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5087 logs agnhost-primary-fm4gg agnhost-primary'
Jul 18 23:57:56.123: INFO: stderr: ""
Jul 18 23:57:56.123: INFO: stdout: "Paused\n"
STEP: exposing RC 07/18/23 23:57:56.123
Jul 18 23:57:56.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5087 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jul 18 23:57:56.195: INFO: stderr: ""
Jul 18 23:57:56.195: INFO: stdout: "service/rm2 exposed\n"
Jul 18 23:57:56.198: INFO: Service rm2 in namespace kubectl-5087 found.
STEP: exposing service 07/18/23 23:57:58.205
Jul 18 23:57:58.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5087 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jul 18 23:57:58.279: INFO: stderr: ""
Jul 18 23:57:58.279: INFO: stdout: "service/rm3 exposed\n"
Jul 18 23:57:58.281: INFO: Service rm3 in namespace kubectl-5087 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 18 23:58:00.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5087" for this suite. 07/18/23 23:58:00.288
------------------------------
• [SLOW TEST] [6.435 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:57:53.856
    Jul 18 23:57:53.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/18/23 23:57:53.856
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:57:53.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:57:53.868
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 07/18/23 23:57:53.87
    Jul 18 23:57:53.870: INFO: namespace kubectl-5087
    Jul 18 23:57:53.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5087 create -f -'
    Jul 18 23:57:55.046: INFO: stderr: ""
    Jul 18 23:57:55.046: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 07/18/23 23:57:55.046
    Jul 18 23:57:56.048: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 18 23:57:56.048: INFO: Found 1 / 1
    Jul 18 23:57:56.048: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jul 18 23:57:56.050: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 18 23:57:56.050: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jul 18 23:57:56.050: INFO: wait on agnhost-primary startup in kubectl-5087 
    Jul 18 23:57:56.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5087 logs agnhost-primary-fm4gg agnhost-primary'
    Jul 18 23:57:56.123: INFO: stderr: ""
    Jul 18 23:57:56.123: INFO: stdout: "Paused\n"
    STEP: exposing RC 07/18/23 23:57:56.123
    Jul 18 23:57:56.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5087 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jul 18 23:57:56.195: INFO: stderr: ""
    Jul 18 23:57:56.195: INFO: stdout: "service/rm2 exposed\n"
    Jul 18 23:57:56.198: INFO: Service rm2 in namespace kubectl-5087 found.
    STEP: exposing service 07/18/23 23:57:58.205
    Jul 18 23:57:58.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5087 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jul 18 23:57:58.279: INFO: stderr: ""
    Jul 18 23:57:58.279: INFO: stdout: "service/rm3 exposed\n"
    Jul 18 23:57:58.281: INFO: Service rm3 in namespace kubectl-5087 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:58:00.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5087" for this suite. 07/18/23 23:58:00.288
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:58:00.291
Jul 18 23:58:00.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/18/23 23:58:00.292
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:00.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:00.299
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 07/18/23 23:58:00.301
Jul 18 23:58:00.305: INFO: Waiting up to 5m0s for pod "pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446" in namespace "emptydir-2612" to be "Succeeded or Failed"
Jul 18 23:58:00.306: INFO: Pod "pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446": Phase="Pending", Reason="", readiness=false. Elapsed: 1.389892ms
Jul 18 23:58:02.308: INFO: Pod "pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003539787s
Jul 18 23:58:04.309: INFO: Pod "pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004144483s
STEP: Saw pod success 07/18/23 23:58:04.309
Jul 18 23:58:04.309: INFO: Pod "pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446" satisfied condition "Succeeded or Failed"
Jul 18 23:58:04.310: INFO: Trying to get logs from node controller-1 pod pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446 container test-container: <nil>
STEP: delete the pod 07/18/23 23:58:04.313
Jul 18 23:58:04.324: INFO: Waiting for pod pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446 to disappear
Jul 18 23:58:04.325: INFO: Pod pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 18 23:58:04.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2612" for this suite. 07/18/23 23:58:04.327
------------------------------
• [4.039 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:58:00.291
    Jul 18 23:58:00.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/18/23 23:58:00.292
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:00.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:00.299
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 07/18/23 23:58:00.301
    Jul 18 23:58:00.305: INFO: Waiting up to 5m0s for pod "pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446" in namespace "emptydir-2612" to be "Succeeded or Failed"
    Jul 18 23:58:00.306: INFO: Pod "pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446": Phase="Pending", Reason="", readiness=false. Elapsed: 1.389892ms
    Jul 18 23:58:02.308: INFO: Pod "pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003539787s
    Jul 18 23:58:04.309: INFO: Pod "pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004144483s
    STEP: Saw pod success 07/18/23 23:58:04.309
    Jul 18 23:58:04.309: INFO: Pod "pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446" satisfied condition "Succeeded or Failed"
    Jul 18 23:58:04.310: INFO: Trying to get logs from node controller-1 pod pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446 container test-container: <nil>
    STEP: delete the pod 07/18/23 23:58:04.313
    Jul 18 23:58:04.324: INFO: Waiting for pod pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446 to disappear
    Jul 18 23:58:04.325: INFO: Pod pod-df8fe818-eb84-4bad-9cc4-6cc6e4a01446 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:58:04.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2612" for this suite. 07/18/23 23:58:04.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:58:04.33
Jul 18 23:58:04.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replicaset 07/18/23 23:58:04.331
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:04.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:04.337
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jul 18 23:58:04.339: INFO: Creating ReplicaSet my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70
Jul 18 23:58:04.342: INFO: Pod name my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70: Found 0 pods out of 1
Jul 18 23:58:09.349: INFO: Pod name my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70: Found 1 pods out of 1
Jul 18 23:58:09.349: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70" is running
Jul 18 23:58:09.349: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz" in namespace "replicaset-8218" to be "running"
Jul 18 23:58:09.354: INFO: Pod "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz": Phase="Running", Reason="", readiness=true. Elapsed: 5.587388ms
Jul 18 23:58:09.354: INFO: Pod "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz" satisfied condition "running"
Jul 18 23:58:09.354: INFO: Pod "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-18 23:58:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-18 23:58:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-18 23:58:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-18 23:58:04 +0000 UTC Reason: Message:}])
Jul 18 23:58:09.354: INFO: Trying to dial the pod
Jul 18 23:58:14.362: INFO: Controller my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70: Got expected result from replica 1 [my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz]: "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jul 18 23:58:14.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-8218" for this suite. 07/18/23 23:58:14.364
------------------------------
• [SLOW TEST] [10.036 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:58:04.33
    Jul 18 23:58:04.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replicaset 07/18/23 23:58:04.331
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:04.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:04.337
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jul 18 23:58:04.339: INFO: Creating ReplicaSet my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70
    Jul 18 23:58:04.342: INFO: Pod name my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70: Found 0 pods out of 1
    Jul 18 23:58:09.349: INFO: Pod name my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70: Found 1 pods out of 1
    Jul 18 23:58:09.349: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70" is running
    Jul 18 23:58:09.349: INFO: Waiting up to 5m0s for pod "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz" in namespace "replicaset-8218" to be "running"
    Jul 18 23:58:09.354: INFO: Pod "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz": Phase="Running", Reason="", readiness=true. Elapsed: 5.587388ms
    Jul 18 23:58:09.354: INFO: Pod "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz" satisfied condition "running"
    Jul 18 23:58:09.354: INFO: Pod "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-18 23:58:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-18 23:58:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-18 23:58:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-18 23:58:04 +0000 UTC Reason: Message:}])
    Jul 18 23:58:09.354: INFO: Trying to dial the pod
    Jul 18 23:58:14.362: INFO: Controller my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70: Got expected result from replica 1 [my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz]: "my-hostname-basic-3647cca6-63e0-466e-9b0a-5a3ebff74c70-mj6zz", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:58:14.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-8218" for this suite. 07/18/23 23:58:14.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:58:14.367
Jul 18 23:58:14.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/18/23 23:58:14.367
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:14.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:14.375
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 07/18/23 23:58:14.377
Jul 18 23:58:14.380: INFO: Waiting up to 5m0s for pod "pod-klh8s" in namespace "pods-9237" to be "running"
Jul 18 23:58:14.383: INFO: Pod "pod-klh8s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921829ms
Jul 18 23:58:16.389: INFO: Pod "pod-klh8s": Phase="Running", Reason="", readiness=true. Elapsed: 2.009086734s
Jul 18 23:58:16.389: INFO: Pod "pod-klh8s" satisfied condition "running"
STEP: patching /status 07/18/23 23:58:16.389
Jul 18 23:58:16.394: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jul 18 23:58:16.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9237" for this suite. 07/18/23 23:58:16.398
------------------------------
• [2.033 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:58:14.367
    Jul 18 23:58:14.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/18/23 23:58:14.367
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:14.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:14.375
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 07/18/23 23:58:14.377
    Jul 18 23:58:14.380: INFO: Waiting up to 5m0s for pod "pod-klh8s" in namespace "pods-9237" to be "running"
    Jul 18 23:58:14.383: INFO: Pod "pod-klh8s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921829ms
    Jul 18 23:58:16.389: INFO: Pod "pod-klh8s": Phase="Running", Reason="", readiness=true. Elapsed: 2.009086734s
    Jul 18 23:58:16.389: INFO: Pod "pod-klh8s" satisfied condition "running"
    STEP: patching /status 07/18/23 23:58:16.389
    Jul 18 23:58:16.394: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:58:16.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9237" for this suite. 07/18/23 23:58:16.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:58:16.4
Jul 18 23:58:16.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pod-network-test 07/18/23 23:58:16.401
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:16.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:16.41
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6460 07/18/23 23:58:16.411
STEP: creating a selector 07/18/23 23:58:16.411
STEP: Creating the service pods in kubernetes 07/18/23 23:58:16.412
Jul 18 23:58:16.412: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 18 23:58:16.423: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6460" to be "running and ready"
Jul 18 23:58:16.425: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395367ms
Jul 18 23:58:16.425: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 18 23:58:18.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005569208s
Jul 18 23:58:18.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:58:20.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007607712s
Jul 18 23:58:20.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:58:22.428: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005120564s
Jul 18 23:58:22.428: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:58:24.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005865004s
Jul 18 23:58:24.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:58:26.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005708689s
Jul 18 23:58:26.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:58:28.428: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005476866s
Jul 18 23:58:28.428: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:58:30.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006109856s
Jul 18 23:58:30.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:58:32.428: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004768942s
Jul 18 23:58:32.428: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:58:34.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005620282s
Jul 18 23:58:34.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:58:36.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.005950893s
Jul 18 23:58:36.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 18 23:58:38.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005625384s
Jul 18 23:58:38.429: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jul 18 23:58:38.429: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jul 18 23:58:38.430: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6460" to be "running and ready"
Jul 18 23:58:38.432: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.066235ms
Jul 18 23:58:38.432: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jul 18 23:58:38.432: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 07/18/23 23:58:38.435
Jul 18 23:58:38.442: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6460" to be "running"
Jul 18 23:58:38.445: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.489508ms
Jul 18 23:58:40.448: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006283127s
Jul 18 23:58:40.448: INFO: Pod "test-container-pod" satisfied condition "running"
Jul 18 23:58:40.450: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 18 23:58:40.450: INFO: Breadth first check of 172.16.192.98 on host 192.168.206.2...
Jul 18 23:58:40.451: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.182:9080/dial?request=hostname&protocol=udp&host=172.16.192.98&port=8081&tries=1'] Namespace:pod-network-test-6460 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 18 23:58:40.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 18 23:58:40.452: INFO: ExecWithOptions: Clientset creation
Jul 18 23:58:40.452: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6460/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.166.182%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.192.98%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul 18 23:58:40.522: INFO: Waiting for responses: map[]
Jul 18 23:58:40.522: INFO: reached 172.16.192.98 after 0/1 tries
Jul 18 23:58:40.522: INFO: Breadth first check of 172.16.166.181 on host 192.168.206.3...
Jul 18 23:58:40.523: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.182:9080/dial?request=hostname&protocol=udp&host=172.16.166.181&port=8081&tries=1'] Namespace:pod-network-test-6460 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 18 23:58:40.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 18 23:58:40.524: INFO: ExecWithOptions: Clientset creation
Jul 18 23:58:40.524: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6460/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.166.182%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.166.181%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul 18 23:58:40.572: INFO: Waiting for responses: map[]
Jul 18 23:58:40.572: INFO: reached 172.16.166.181 after 0/1 tries
Jul 18 23:58:40.572: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jul 18 23:58:40.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-6460" for this suite. 07/18/23 23:58:40.576
------------------------------
• [SLOW TEST] [24.178 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:58:16.4
    Jul 18 23:58:16.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pod-network-test 07/18/23 23:58:16.401
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:16.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:16.41
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6460 07/18/23 23:58:16.411
    STEP: creating a selector 07/18/23 23:58:16.411
    STEP: Creating the service pods in kubernetes 07/18/23 23:58:16.412
    Jul 18 23:58:16.412: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jul 18 23:58:16.423: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6460" to be "running and ready"
    Jul 18 23:58:16.425: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395367ms
    Jul 18 23:58:16.425: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 18 23:58:18.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.005569208s
    Jul 18 23:58:18.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:58:20.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007607712s
    Jul 18 23:58:20.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:58:22.428: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005120564s
    Jul 18 23:58:22.428: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:58:24.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005865004s
    Jul 18 23:58:24.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:58:26.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005708689s
    Jul 18 23:58:26.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:58:28.428: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005476866s
    Jul 18 23:58:28.428: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:58:30.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.006109856s
    Jul 18 23:58:30.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:58:32.428: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.004768942s
    Jul 18 23:58:32.428: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:58:34.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005620282s
    Jul 18 23:58:34.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:58:36.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.005950893s
    Jul 18 23:58:36.429: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 18 23:58:38.429: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005625384s
    Jul 18 23:58:38.429: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jul 18 23:58:38.429: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jul 18 23:58:38.430: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6460" to be "running and ready"
    Jul 18 23:58:38.432: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.066235ms
    Jul 18 23:58:38.432: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jul 18 23:58:38.432: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 07/18/23 23:58:38.435
    Jul 18 23:58:38.442: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6460" to be "running"
    Jul 18 23:58:38.445: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.489508ms
    Jul 18 23:58:40.448: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006283127s
    Jul 18 23:58:40.448: INFO: Pod "test-container-pod" satisfied condition "running"
    Jul 18 23:58:40.450: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jul 18 23:58:40.450: INFO: Breadth first check of 172.16.192.98 on host 192.168.206.2...
    Jul 18 23:58:40.451: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.182:9080/dial?request=hostname&protocol=udp&host=172.16.192.98&port=8081&tries=1'] Namespace:pod-network-test-6460 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 18 23:58:40.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 18 23:58:40.452: INFO: ExecWithOptions: Clientset creation
    Jul 18 23:58:40.452: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6460/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.166.182%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.192.98%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jul 18 23:58:40.522: INFO: Waiting for responses: map[]
    Jul 18 23:58:40.522: INFO: reached 172.16.192.98 after 0/1 tries
    Jul 18 23:58:40.522: INFO: Breadth first check of 172.16.166.181 on host 192.168.206.3...
    Jul 18 23:58:40.523: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.182:9080/dial?request=hostname&protocol=udp&host=172.16.166.181&port=8081&tries=1'] Namespace:pod-network-test-6460 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 18 23:58:40.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 18 23:58:40.524: INFO: ExecWithOptions: Clientset creation
    Jul 18 23:58:40.524: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6460/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.166.182%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.166.181%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jul 18 23:58:40.572: INFO: Waiting for responses: map[]
    Jul 18 23:58:40.572: INFO: reached 172.16.166.181 after 0/1 tries
    Jul 18 23:58:40.572: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:58:40.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-6460" for this suite. 07/18/23 23:58:40.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:58:40.579
Jul 18 23:58:40.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/18/23 23:58:40.58
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:40.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:40.587
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 07/18/23 23:58:40.589
Jul 18 23:58:40.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61889064-447b-487f-842f-c46fd7810450" in namespace "projected-8288" to be "Succeeded or Failed"
Jul 18 23:58:40.594: INFO: Pod "downwardapi-volume-61889064-447b-487f-842f-c46fd7810450": Phase="Pending", Reason="", readiness=false. Elapsed: 1.583499ms
Jul 18 23:58:42.597: INFO: Pod "downwardapi-volume-61889064-447b-487f-842f-c46fd7810450": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004191991s
Jul 18 23:58:44.597: INFO: Pod "downwardapi-volume-61889064-447b-487f-842f-c46fd7810450": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004474915s
STEP: Saw pod success 07/18/23 23:58:44.597
Jul 18 23:58:44.597: INFO: Pod "downwardapi-volume-61889064-447b-487f-842f-c46fd7810450" satisfied condition "Succeeded or Failed"
Jul 18 23:58:44.599: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-61889064-447b-487f-842f-c46fd7810450 container client-container: <nil>
STEP: delete the pod 07/18/23 23:58:44.602
Jul 18 23:58:44.610: INFO: Waiting for pod downwardapi-volume-61889064-447b-487f-842f-c46fd7810450 to disappear
Jul 18 23:58:44.612: INFO: Pod downwardapi-volume-61889064-447b-487f-842f-c46fd7810450 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 18 23:58:44.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8288" for this suite. 07/18/23 23:58:44.615
------------------------------
• [4.038 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:58:40.579
    Jul 18 23:58:40.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/18/23 23:58:40.58
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:40.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:40.587
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 07/18/23 23:58:40.589
    Jul 18 23:58:40.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61889064-447b-487f-842f-c46fd7810450" in namespace "projected-8288" to be "Succeeded or Failed"
    Jul 18 23:58:40.594: INFO: Pod "downwardapi-volume-61889064-447b-487f-842f-c46fd7810450": Phase="Pending", Reason="", readiness=false. Elapsed: 1.583499ms
    Jul 18 23:58:42.597: INFO: Pod "downwardapi-volume-61889064-447b-487f-842f-c46fd7810450": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004191991s
    Jul 18 23:58:44.597: INFO: Pod "downwardapi-volume-61889064-447b-487f-842f-c46fd7810450": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004474915s
    STEP: Saw pod success 07/18/23 23:58:44.597
    Jul 18 23:58:44.597: INFO: Pod "downwardapi-volume-61889064-447b-487f-842f-c46fd7810450" satisfied condition "Succeeded or Failed"
    Jul 18 23:58:44.599: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-61889064-447b-487f-842f-c46fd7810450 container client-container: <nil>
    STEP: delete the pod 07/18/23 23:58:44.602
    Jul 18 23:58:44.610: INFO: Waiting for pod downwardapi-volume-61889064-447b-487f-842f-c46fd7810450 to disappear
    Jul 18 23:58:44.612: INFO: Pod downwardapi-volume-61889064-447b-487f-842f-c46fd7810450 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:58:44.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8288" for this suite. 07/18/23 23:58:44.615
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:58:44.618
Jul 18 23:58:44.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/18/23 23:58:44.618
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:44.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:44.629
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 07/18/23 23:58:44.631
STEP: Getting a ResourceQuota 07/18/23 23:58:44.633
STEP: Updating a ResourceQuota 07/18/23 23:58:44.635
STEP: Verifying a ResourceQuota was modified 07/18/23 23:58:44.637
STEP: Deleting a ResourceQuota 07/18/23 23:58:44.641
STEP: Verifying the deleted ResourceQuota 07/18/23 23:58:44.644
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 18 23:58:44.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9732" for this suite. 07/18/23 23:58:44.649
------------------------------
• [0.034 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:58:44.618
    Jul 18 23:58:44.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/18/23 23:58:44.618
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:44.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:44.629
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 07/18/23 23:58:44.631
    STEP: Getting a ResourceQuota 07/18/23 23:58:44.633
    STEP: Updating a ResourceQuota 07/18/23 23:58:44.635
    STEP: Verifying a ResourceQuota was modified 07/18/23 23:58:44.637
    STEP: Deleting a ResourceQuota 07/18/23 23:58:44.641
    STEP: Verifying the deleted ResourceQuota 07/18/23 23:58:44.644
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:58:44.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9732" for this suite. 07/18/23 23:58:44.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:58:44.652
Jul 18 23:58:44.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/18/23 23:58:44.653
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:44.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:44.659
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 07/18/23 23:58:44.661
Jul 18 23:58:44.665: INFO: Waiting up to 5m0s for pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c" in namespace "downward-api-584" to be "Succeeded or Failed"
Jul 18 23:58:44.669: INFO: Pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.724428ms
Jul 18 23:58:46.671: INFO: Pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006300871s
Jul 18 23:58:48.672: INFO: Pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006684249s
Jul 18 23:58:50.672: INFO: Pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006676285s
STEP: Saw pod success 07/18/23 23:58:50.672
Jul 18 23:58:50.672: INFO: Pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c" satisfied condition "Succeeded or Failed"
Jul 18 23:58:50.674: INFO: Trying to get logs from node controller-0 pod downward-api-4692a10e-8a80-4e69-b00c-07069b14515c container dapi-container: <nil>
STEP: delete the pod 07/18/23 23:58:50.685
Jul 18 23:58:50.692: INFO: Waiting for pod downward-api-4692a10e-8a80-4e69-b00c-07069b14515c to disappear
Jul 18 23:58:50.694: INFO: Pod downward-api-4692a10e-8a80-4e69-b00c-07069b14515c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jul 18 23:58:50.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-584" for this suite. 07/18/23 23:58:50.696
------------------------------
• [SLOW TEST] [6.047 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:58:44.652
    Jul 18 23:58:44.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/18/23 23:58:44.653
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:44.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:44.659
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 07/18/23 23:58:44.661
    Jul 18 23:58:44.665: INFO: Waiting up to 5m0s for pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c" in namespace "downward-api-584" to be "Succeeded or Failed"
    Jul 18 23:58:44.669: INFO: Pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.724428ms
    Jul 18 23:58:46.671: INFO: Pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006300871s
    Jul 18 23:58:48.672: INFO: Pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006684249s
    Jul 18 23:58:50.672: INFO: Pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006676285s
    STEP: Saw pod success 07/18/23 23:58:50.672
    Jul 18 23:58:50.672: INFO: Pod "downward-api-4692a10e-8a80-4e69-b00c-07069b14515c" satisfied condition "Succeeded or Failed"
    Jul 18 23:58:50.674: INFO: Trying to get logs from node controller-0 pod downward-api-4692a10e-8a80-4e69-b00c-07069b14515c container dapi-container: <nil>
    STEP: delete the pod 07/18/23 23:58:50.685
    Jul 18 23:58:50.692: INFO: Waiting for pod downward-api-4692a10e-8a80-4e69-b00c-07069b14515c to disappear
    Jul 18 23:58:50.694: INFO: Pod downward-api-4692a10e-8a80-4e69-b00c-07069b14515c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:58:50.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-584" for this suite. 07/18/23 23:58:50.696
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:58:50.699
Jul 18 23:58:50.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/18/23 23:58:50.7
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:50.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:50.706
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 07/18/23 23:58:50.708
STEP: Ensuring ResourceQuota status is calculated 07/18/23 23:58:50.71
STEP: Creating a ResourceQuota with not terminating scope 07/18/23 23:58:52.712
STEP: Ensuring ResourceQuota status is calculated 07/18/23 23:58:52.714
STEP: Creating a long running pod 07/18/23 23:58:54.718
STEP: Ensuring resource quota with not terminating scope captures the pod usage 07/18/23 23:58:54.725
STEP: Ensuring resource quota with terminating scope ignored the pod usage 07/18/23 23:58:56.728
STEP: Deleting the pod 07/18/23 23:58:58.73
STEP: Ensuring resource quota status released the pod usage 07/18/23 23:58:58.736
STEP: Creating a terminating pod 07/18/23 23:59:00.738
STEP: Ensuring resource quota with terminating scope captures the pod usage 07/18/23 23:59:00.743
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 07/18/23 23:59:02.746
STEP: Deleting the pod 07/18/23 23:59:04.749
STEP: Ensuring resource quota status released the pod usage 07/18/23 23:59:04.755
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 18 23:59:06.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5465" for this suite. 07/18/23 23:59:06.761
------------------------------
• [SLOW TEST] [16.065 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:58:50.699
    Jul 18 23:58:50.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/18/23 23:58:50.7
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:58:50.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:58:50.706
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 07/18/23 23:58:50.708
    STEP: Ensuring ResourceQuota status is calculated 07/18/23 23:58:50.71
    STEP: Creating a ResourceQuota with not terminating scope 07/18/23 23:58:52.712
    STEP: Ensuring ResourceQuota status is calculated 07/18/23 23:58:52.714
    STEP: Creating a long running pod 07/18/23 23:58:54.718
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 07/18/23 23:58:54.725
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 07/18/23 23:58:56.728
    STEP: Deleting the pod 07/18/23 23:58:58.73
    STEP: Ensuring resource quota status released the pod usage 07/18/23 23:58:58.736
    STEP: Creating a terminating pod 07/18/23 23:59:00.738
    STEP: Ensuring resource quota with terminating scope captures the pod usage 07/18/23 23:59:00.743
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 07/18/23 23:59:02.746
    STEP: Deleting the pod 07/18/23 23:59:04.749
    STEP: Ensuring resource quota status released the pod usage 07/18/23 23:59:04.755
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:59:06.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5465" for this suite. 07/18/23 23:59:06.761
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:59:06.764
Jul 18 23:59:06.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/18/23 23:59:06.765
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:59:06.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:59:06.773
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 07/18/23 23:59:06.775
Jul 18 23:59:06.779: INFO: Waiting up to 5m0s for pod "pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468" in namespace "pods-3784" to be "running and ready"
Jul 18 23:59:06.781: INFO: Pod "pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468": Phase="Pending", Reason="", readiness=false. Elapsed: 1.604938ms
Jul 18 23:59:06.781: INFO: The phase of Pod pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468 is Pending, waiting for it to be Running (with Ready = true)
Jul 18 23:59:08.784: INFO: Pod "pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468": Phase="Running", Reason="", readiness=true. Elapsed: 2.004763384s
Jul 18 23:59:08.784: INFO: The phase of Pod pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468 is Running (Ready = true)
Jul 18 23:59:08.784: INFO: Pod "pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468" satisfied condition "running and ready"
Jul 18 23:59:08.787: INFO: Pod pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468 has hostIP: 192.168.206.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jul 18 23:59:08.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3784" for this suite. 07/18/23 23:59:08.789
------------------------------
• [2.027 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:59:06.764
    Jul 18 23:59:06.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/18/23 23:59:06.765
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:59:06.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:59:06.773
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 07/18/23 23:59:06.775
    Jul 18 23:59:06.779: INFO: Waiting up to 5m0s for pod "pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468" in namespace "pods-3784" to be "running and ready"
    Jul 18 23:59:06.781: INFO: Pod "pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468": Phase="Pending", Reason="", readiness=false. Elapsed: 1.604938ms
    Jul 18 23:59:06.781: INFO: The phase of Pod pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468 is Pending, waiting for it to be Running (with Ready = true)
    Jul 18 23:59:08.784: INFO: Pod "pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468": Phase="Running", Reason="", readiness=true. Elapsed: 2.004763384s
    Jul 18 23:59:08.784: INFO: The phase of Pod pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468 is Running (Ready = true)
    Jul 18 23:59:08.784: INFO: Pod "pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468" satisfied condition "running and ready"
    Jul 18 23:59:08.787: INFO: Pod pod-hostip-43b95270-c99e-49b6-9996-b5e22098c468 has hostIP: 192.168.206.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:59:08.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3784" for this suite. 07/18/23 23:59:08.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:59:08.793
Jul 18 23:59:08.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/18/23 23:59:08.794
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:59:08.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:59:08.801
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 07/18/23 23:59:08.804
STEP: Ensuring ResourceQuota status is calculated 07/18/23 23:59:08.806
STEP: Creating a ResourceQuota with not best effort scope 07/18/23 23:59:10.808
STEP: Ensuring ResourceQuota status is calculated 07/18/23 23:59:10.81
STEP: Creating a best-effort pod 07/18/23 23:59:12.813
STEP: Ensuring resource quota with best effort scope captures the pod usage 07/18/23 23:59:12.821
STEP: Ensuring resource quota with not best effort ignored the pod usage 07/18/23 23:59:14.823
STEP: Deleting the pod 07/18/23 23:59:16.826
STEP: Ensuring resource quota status released the pod usage 07/18/23 23:59:16.83
STEP: Creating a not best-effort pod 07/18/23 23:59:18.833
STEP: Ensuring resource quota with not best effort scope captures the pod usage 07/18/23 23:59:18.837
STEP: Ensuring resource quota with best effort scope ignored the pod usage 07/18/23 23:59:20.84
STEP: Deleting the pod 07/18/23 23:59:22.842
STEP: Ensuring resource quota status released the pod usage 07/18/23 23:59:22.846
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 18 23:59:24.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7883" for this suite. 07/18/23 23:59:24.851
------------------------------
• [SLOW TEST] [16.061 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:59:08.793
    Jul 18 23:59:08.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/18/23 23:59:08.794
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:59:08.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:59:08.801
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 07/18/23 23:59:08.804
    STEP: Ensuring ResourceQuota status is calculated 07/18/23 23:59:08.806
    STEP: Creating a ResourceQuota with not best effort scope 07/18/23 23:59:10.808
    STEP: Ensuring ResourceQuota status is calculated 07/18/23 23:59:10.81
    STEP: Creating a best-effort pod 07/18/23 23:59:12.813
    STEP: Ensuring resource quota with best effort scope captures the pod usage 07/18/23 23:59:12.821
    STEP: Ensuring resource quota with not best effort ignored the pod usage 07/18/23 23:59:14.823
    STEP: Deleting the pod 07/18/23 23:59:16.826
    STEP: Ensuring resource quota status released the pod usage 07/18/23 23:59:16.83
    STEP: Creating a not best-effort pod 07/18/23 23:59:18.833
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 07/18/23 23:59:18.837
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 07/18/23 23:59:20.84
    STEP: Deleting the pod 07/18/23 23:59:22.842
    STEP: Ensuring resource quota status released the pod usage 07/18/23 23:59:22.846
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 18 23:59:24.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7883" for this suite. 07/18/23 23:59:24.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/18/23 23:59:24.856
Jul 18 23:59:24.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/18/23 23:59:24.857
STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:59:24.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:59:24.867
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-jf2d7" 07/18/23 23:59:24.873
Jul 18 23:59:24.877: INFO: Resource quota "e2e-rq-status-jf2d7" reports spec: hard cpu limit of 500m
Jul 18 23:59:24.877: INFO: Resource quota "e2e-rq-status-jf2d7" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-jf2d7" /status 07/18/23 23:59:24.877
STEP: Confirm /status for "e2e-rq-status-jf2d7" resourceQuota via watch 07/18/23 23:59:24.883
Jul 18 23:59:24.885: INFO: observed resourceQuota "e2e-rq-status-jf2d7" in namespace "resourcequota-6199" with hard status: v1.ResourceList(nil)
Jul 18 23:59:24.885: INFO: Found resourceQuota "e2e-rq-status-jf2d7" in namespace "resourcequota-6199" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jul 18 23:59:24.885: INFO: ResourceQuota "e2e-rq-status-jf2d7" /status was updated
STEP: Patching hard spec values for cpu & memory 07/18/23 23:59:24.886
Jul 18 23:59:24.889: INFO: Resource quota "e2e-rq-status-jf2d7" reports spec: hard cpu limit of 1
Jul 18 23:59:24.889: INFO: Resource quota "e2e-rq-status-jf2d7" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-jf2d7" /status 07/18/23 23:59:24.889
STEP: Confirm /status for "e2e-rq-status-jf2d7" resourceQuota via watch 07/18/23 23:59:24.893
Jul 18 23:59:24.894: INFO: observed resourceQuota "e2e-rq-status-jf2d7" in namespace "resourcequota-6199" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jul 18 23:59:24.894: INFO: Found resourceQuota "e2e-rq-status-jf2d7" in namespace "resourcequota-6199" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jul 18 23:59:24.894: INFO: ResourceQuota "e2e-rq-status-jf2d7" /status was patched
STEP: Get "e2e-rq-status-jf2d7" /status 07/18/23 23:59:24.894
Jul 18 23:59:24.896: INFO: Resourcequota "e2e-rq-status-jf2d7" reports status: hard cpu of 1
Jul 18 23:59:24.896: INFO: Resourcequota "e2e-rq-status-jf2d7" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-jf2d7" /status before checking Spec is unchanged 07/18/23 23:59:24.898
Jul 18 23:59:24.900: INFO: Resourcequota "e2e-rq-status-jf2d7" reports status: hard cpu of 2
Jul 18 23:59:24.900: INFO: Resourcequota "e2e-rq-status-jf2d7" reports status: hard memory of 2Gi
Jul 18 23:59:24.901: INFO: Found resourceQuota "e2e-rq-status-jf2d7" in namespace "resourcequota-6199" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jul 19 00:00:09.907: INFO: ResourceQuota "e2e-rq-status-jf2d7" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 19 00:00:09.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6199" for this suite. 07/19/23 00:00:09.909
------------------------------
• [SLOW TEST] [45.056 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/18/23 23:59:24.856
    Jul 18 23:59:24.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/18/23 23:59:24.857
    STEP: Waiting for a default service account to be provisioned in namespace 07/18/23 23:59:24.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/18/23 23:59:24.867
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-jf2d7" 07/18/23 23:59:24.873
    Jul 18 23:59:24.877: INFO: Resource quota "e2e-rq-status-jf2d7" reports spec: hard cpu limit of 500m
    Jul 18 23:59:24.877: INFO: Resource quota "e2e-rq-status-jf2d7" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-jf2d7" /status 07/18/23 23:59:24.877
    STEP: Confirm /status for "e2e-rq-status-jf2d7" resourceQuota via watch 07/18/23 23:59:24.883
    Jul 18 23:59:24.885: INFO: observed resourceQuota "e2e-rq-status-jf2d7" in namespace "resourcequota-6199" with hard status: v1.ResourceList(nil)
    Jul 18 23:59:24.885: INFO: Found resourceQuota "e2e-rq-status-jf2d7" in namespace "resourcequota-6199" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jul 18 23:59:24.885: INFO: ResourceQuota "e2e-rq-status-jf2d7" /status was updated
    STEP: Patching hard spec values for cpu & memory 07/18/23 23:59:24.886
    Jul 18 23:59:24.889: INFO: Resource quota "e2e-rq-status-jf2d7" reports spec: hard cpu limit of 1
    Jul 18 23:59:24.889: INFO: Resource quota "e2e-rq-status-jf2d7" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-jf2d7" /status 07/18/23 23:59:24.889
    STEP: Confirm /status for "e2e-rq-status-jf2d7" resourceQuota via watch 07/18/23 23:59:24.893
    Jul 18 23:59:24.894: INFO: observed resourceQuota "e2e-rq-status-jf2d7" in namespace "resourcequota-6199" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jul 18 23:59:24.894: INFO: Found resourceQuota "e2e-rq-status-jf2d7" in namespace "resourcequota-6199" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jul 18 23:59:24.894: INFO: ResourceQuota "e2e-rq-status-jf2d7" /status was patched
    STEP: Get "e2e-rq-status-jf2d7" /status 07/18/23 23:59:24.894
    Jul 18 23:59:24.896: INFO: Resourcequota "e2e-rq-status-jf2d7" reports status: hard cpu of 1
    Jul 18 23:59:24.896: INFO: Resourcequota "e2e-rq-status-jf2d7" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-jf2d7" /status before checking Spec is unchanged 07/18/23 23:59:24.898
    Jul 18 23:59:24.900: INFO: Resourcequota "e2e-rq-status-jf2d7" reports status: hard cpu of 2
    Jul 18 23:59:24.900: INFO: Resourcequota "e2e-rq-status-jf2d7" reports status: hard memory of 2Gi
    Jul 18 23:59:24.901: INFO: Found resourceQuota "e2e-rq-status-jf2d7" in namespace "resourcequota-6199" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jul 19 00:00:09.907: INFO: ResourceQuota "e2e-rq-status-jf2d7" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:00:09.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6199" for this suite. 07/19/23 00:00:09.909
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:00:09.912
Jul 19 00:00:09.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename init-container 07/19/23 00:00:09.913
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:00:09.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:00:09.92
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 07/19/23 00:00:09.922
Jul 19 00:00:09.922: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:00:15.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5586" for this suite. 07/19/23 00:00:15.105
------------------------------
• [SLOW TEST] [5.196 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:00:09.912
    Jul 19 00:00:09.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename init-container 07/19/23 00:00:09.913
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:00:09.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:00:09.92
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 07/19/23 00:00:09.922
    Jul 19 00:00:09.922: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:00:15.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5586" for this suite. 07/19/23 00:00:15.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:00:15.108
Jul 19 00:00:15.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename taint-single-pod 07/19/23 00:00:15.109
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:00:15.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:00:15.116
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jul 19 00:00:15.118: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 00:01:15.147: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jul 19 00:01:15.149: INFO: Starting informer...
STEP: Starting pod... 07/19/23 00:01:15.149
Jul 19 00:01:15.357: INFO: Pod is running on controller-1. Tainting Node
STEP: Trying to apply a taint on the Node 07/19/23 00:01:15.357
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/19/23 00:01:15.365
STEP: Waiting short time to make sure Pod is queued for deletion 07/19/23 00:01:15.369
Jul 19 00:01:15.369: INFO: Pod wasn't evicted. Proceeding
Jul 19 00:01:15.369: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/19/23 00:01:15.379
STEP: Waiting some time to make sure that toleration time passed. 07/19/23 00:01:15.387
Jul 19 00:02:30.388: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:02:30.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-6516" for this suite. 07/19/23 00:02:30.391
------------------------------
• [SLOW TEST] [135.285 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:00:15.108
    Jul 19 00:00:15.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename taint-single-pod 07/19/23 00:00:15.109
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:00:15.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:00:15.116
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jul 19 00:00:15.118: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 19 00:01:15.147: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jul 19 00:01:15.149: INFO: Starting informer...
    STEP: Starting pod... 07/19/23 00:01:15.149
    Jul 19 00:01:15.357: INFO: Pod is running on controller-1. Tainting Node
    STEP: Trying to apply a taint on the Node 07/19/23 00:01:15.357
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/19/23 00:01:15.365
    STEP: Waiting short time to make sure Pod is queued for deletion 07/19/23 00:01:15.369
    Jul 19 00:01:15.369: INFO: Pod wasn't evicted. Proceeding
    Jul 19 00:01:15.369: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/19/23 00:01:15.379
    STEP: Waiting some time to make sure that toleration time passed. 07/19/23 00:01:15.387
    Jul 19 00:02:30.388: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:02:30.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-6516" for this suite. 07/19/23 00:02:30.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:02:30.394
Jul 19 00:02:30.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/19/23 00:02:30.395
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:02:30.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:02:30.406
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 07/19/23 00:02:47.412
STEP: Creating a ResourceQuota 07/19/23 00:02:52.414
STEP: Ensuring resource quota status is calculated 07/19/23 00:02:52.416
STEP: Creating a ConfigMap 07/19/23 00:02:54.419
STEP: Ensuring resource quota status captures configMap creation 07/19/23 00:02:54.445
STEP: Deleting a ConfigMap 07/19/23 00:02:56.448
STEP: Ensuring resource quota status released usage 07/19/23 00:02:56.452
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 19 00:02:58.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4077" for this suite. 07/19/23 00:02:58.457
------------------------------
• [SLOW TEST] [28.066 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:02:30.394
    Jul 19 00:02:30.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/19/23 00:02:30.395
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:02:30.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:02:30.406
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 07/19/23 00:02:47.412
    STEP: Creating a ResourceQuota 07/19/23 00:02:52.414
    STEP: Ensuring resource quota status is calculated 07/19/23 00:02:52.416
    STEP: Creating a ConfigMap 07/19/23 00:02:54.419
    STEP: Ensuring resource quota status captures configMap creation 07/19/23 00:02:54.445
    STEP: Deleting a ConfigMap 07/19/23 00:02:56.448
    STEP: Ensuring resource quota status released usage 07/19/23 00:02:56.452
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:02:58.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4077" for this suite. 07/19/23 00:02:58.457
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:02:58.46
Jul 19 00:02:58.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename security-context 07/19/23 00:02:58.462
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:02:58.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:02:58.471
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 07/19/23 00:02:58.473
Jul 19 00:02:58.476: INFO: Waiting up to 5m0s for pod "security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6" in namespace "security-context-5351" to be "Succeeded or Failed"
Jul 19 00:02:58.478: INFO: Pod "security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.448695ms
Jul 19 00:03:00.480: INFO: Pod "security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004005393s
Jul 19 00:03:02.481: INFO: Pod "security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00495024s
STEP: Saw pod success 07/19/23 00:03:02.481
Jul 19 00:03:02.481: INFO: Pod "security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6" satisfied condition "Succeeded or Failed"
Jul 19 00:03:02.483: INFO: Trying to get logs from node controller-1 pod security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6 container test-container: <nil>
STEP: delete the pod 07/19/23 00:03:02.494
Jul 19 00:03:02.499: INFO: Waiting for pod security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6 to disappear
Jul 19 00:03:02.501: INFO: Pod security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jul 19 00:03:02.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5351" for this suite. 07/19/23 00:03:02.503
------------------------------
• [4.048 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:02:58.46
    Jul 19 00:02:58.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename security-context 07/19/23 00:02:58.462
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:02:58.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:02:58.471
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 07/19/23 00:02:58.473
    Jul 19 00:02:58.476: INFO: Waiting up to 5m0s for pod "security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6" in namespace "security-context-5351" to be "Succeeded or Failed"
    Jul 19 00:02:58.478: INFO: Pod "security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.448695ms
    Jul 19 00:03:00.480: INFO: Pod "security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004005393s
    Jul 19 00:03:02.481: INFO: Pod "security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00495024s
    STEP: Saw pod success 07/19/23 00:03:02.481
    Jul 19 00:03:02.481: INFO: Pod "security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6" satisfied condition "Succeeded or Failed"
    Jul 19 00:03:02.483: INFO: Trying to get logs from node controller-1 pod security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6 container test-container: <nil>
    STEP: delete the pod 07/19/23 00:03:02.494
    Jul 19 00:03:02.499: INFO: Waiting for pod security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6 to disappear
    Jul 19 00:03:02.501: INFO: Pod security-context-1e0d0e66-50dd-4454-a3d0-70050aa828b6 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:03:02.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5351" for this suite. 07/19/23 00:03:02.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:03:02.509
Jul 19 00:03:02.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:03:02.51
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:03:02.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:03:02.52
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-7190 07/19/23 00:03:02.522
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7190 to expose endpoints map[] 07/19/23 00:03:02.53
Jul 19 00:03:02.532: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jul 19 00:03:03.536: INFO: successfully validated that service multi-endpoint-test in namespace services-7190 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7190 07/19/23 00:03:03.536
Jul 19 00:03:03.541: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7190" to be "running and ready"
Jul 19 00:03:03.544: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.720411ms
Jul 19 00:03:03.544: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:03:05.547: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00627452s
Jul 19 00:03:05.547: INFO: The phase of Pod pod1 is Running (Ready = true)
Jul 19 00:03:05.547: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7190 to expose endpoints map[pod1:[100]] 07/19/23 00:03:05.549
Jul 19 00:03:05.554: INFO: successfully validated that service multi-endpoint-test in namespace services-7190 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7190 07/19/23 00:03:05.554
Jul 19 00:03:05.557: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7190" to be "running and ready"
Jul 19 00:03:05.560: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.870139ms
Jul 19 00:03:05.560: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:03:07.563: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006204401s
Jul 19 00:03:07.563: INFO: The phase of Pod pod2 is Running (Ready = true)
Jul 19 00:03:07.563: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7190 to expose endpoints map[pod1:[100] pod2:[101]] 07/19/23 00:03:07.564
Jul 19 00:03:07.571: INFO: successfully validated that service multi-endpoint-test in namespace services-7190 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 07/19/23 00:03:07.571
Jul 19 00:03:07.571: INFO: Creating new exec pod
Jul 19 00:03:07.573: INFO: Waiting up to 5m0s for pod "execpodpbc2j" in namespace "services-7190" to be "running"
Jul 19 00:03:07.575: INFO: Pod "execpodpbc2j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618132ms
Jul 19 00:03:09.577: INFO: Pod "execpodpbc2j": Phase="Running", Reason="", readiness=true. Elapsed: 2.004087942s
Jul 19 00:03:09.577: INFO: Pod "execpodpbc2j" satisfied condition "running"
Jul 19 00:03:10.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7190 exec execpodpbc2j -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jul 19 00:03:10.679: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jul 19 00:03:10.679: INFO: stdout: ""
Jul 19 00:03:10.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7190 exec execpodpbc2j -- /bin/sh -x -c nc -v -z -w 2 10.106.192.184 80'
Jul 19 00:03:10.780: INFO: stderr: "+ nc -v -z -w 2 10.106.192.184 80\nConnection to 10.106.192.184 80 port [tcp/http] succeeded!\n"
Jul 19 00:03:10.780: INFO: stdout: ""
Jul 19 00:03:10.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7190 exec execpodpbc2j -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jul 19 00:03:10.880: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jul 19 00:03:10.880: INFO: stdout: ""
Jul 19 00:03:10.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7190 exec execpodpbc2j -- /bin/sh -x -c nc -v -z -w 2 10.106.192.184 81'
Jul 19 00:03:10.985: INFO: stderr: "+ nc -v -z -w 2 10.106.192.184 81\nConnection to 10.106.192.184 81 port [tcp/*] succeeded!\n"
Jul 19 00:03:10.985: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7190 07/19/23 00:03:10.985
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7190 to expose endpoints map[pod2:[101]] 07/19/23 00:03:10.991
Jul 19 00:03:10.999: INFO: successfully validated that service multi-endpoint-test in namespace services-7190 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7190 07/19/23 00:03:10.999
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7190 to expose endpoints map[] 07/19/23 00:03:11.007
Jul 19 00:03:11.013: INFO: successfully validated that service multi-endpoint-test in namespace services-7190 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:03:11.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7190" for this suite. 07/19/23 00:03:11.029
------------------------------
• [SLOW TEST] [8.522 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:03:02.509
    Jul 19 00:03:02.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:03:02.51
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:03:02.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:03:02.52
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-7190 07/19/23 00:03:02.522
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7190 to expose endpoints map[] 07/19/23 00:03:02.53
    Jul 19 00:03:02.532: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jul 19 00:03:03.536: INFO: successfully validated that service multi-endpoint-test in namespace services-7190 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7190 07/19/23 00:03:03.536
    Jul 19 00:03:03.541: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7190" to be "running and ready"
    Jul 19 00:03:03.544: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.720411ms
    Jul 19 00:03:03.544: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:03:05.547: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00627452s
    Jul 19 00:03:05.547: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jul 19 00:03:05.547: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7190 to expose endpoints map[pod1:[100]] 07/19/23 00:03:05.549
    Jul 19 00:03:05.554: INFO: successfully validated that service multi-endpoint-test in namespace services-7190 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7190 07/19/23 00:03:05.554
    Jul 19 00:03:05.557: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7190" to be "running and ready"
    Jul 19 00:03:05.560: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.870139ms
    Jul 19 00:03:05.560: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:03:07.563: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006204401s
    Jul 19 00:03:07.563: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jul 19 00:03:07.563: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7190 to expose endpoints map[pod1:[100] pod2:[101]] 07/19/23 00:03:07.564
    Jul 19 00:03:07.571: INFO: successfully validated that service multi-endpoint-test in namespace services-7190 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 07/19/23 00:03:07.571
    Jul 19 00:03:07.571: INFO: Creating new exec pod
    Jul 19 00:03:07.573: INFO: Waiting up to 5m0s for pod "execpodpbc2j" in namespace "services-7190" to be "running"
    Jul 19 00:03:07.575: INFO: Pod "execpodpbc2j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618132ms
    Jul 19 00:03:09.577: INFO: Pod "execpodpbc2j": Phase="Running", Reason="", readiness=true. Elapsed: 2.004087942s
    Jul 19 00:03:09.577: INFO: Pod "execpodpbc2j" satisfied condition "running"
    Jul 19 00:03:10.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7190 exec execpodpbc2j -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jul 19 00:03:10.679: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jul 19 00:03:10.679: INFO: stdout: ""
    Jul 19 00:03:10.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7190 exec execpodpbc2j -- /bin/sh -x -c nc -v -z -w 2 10.106.192.184 80'
    Jul 19 00:03:10.780: INFO: stderr: "+ nc -v -z -w 2 10.106.192.184 80\nConnection to 10.106.192.184 80 port [tcp/http] succeeded!\n"
    Jul 19 00:03:10.780: INFO: stdout: ""
    Jul 19 00:03:10.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7190 exec execpodpbc2j -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jul 19 00:03:10.880: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jul 19 00:03:10.880: INFO: stdout: ""
    Jul 19 00:03:10.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7190 exec execpodpbc2j -- /bin/sh -x -c nc -v -z -w 2 10.106.192.184 81'
    Jul 19 00:03:10.985: INFO: stderr: "+ nc -v -z -w 2 10.106.192.184 81\nConnection to 10.106.192.184 81 port [tcp/*] succeeded!\n"
    Jul 19 00:03:10.985: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7190 07/19/23 00:03:10.985
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7190 to expose endpoints map[pod2:[101]] 07/19/23 00:03:10.991
    Jul 19 00:03:10.999: INFO: successfully validated that service multi-endpoint-test in namespace services-7190 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7190 07/19/23 00:03:10.999
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7190 to expose endpoints map[] 07/19/23 00:03:11.007
    Jul 19 00:03:11.013: INFO: successfully validated that service multi-endpoint-test in namespace services-7190 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:03:11.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7190" for this suite. 07/19/23 00:03:11.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:03:11.032
Jul 19 00:03:11.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-watch 07/19/23 00:03:11.033
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:03:11.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:03:11.045
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jul 19 00:03:11.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Creating first CR  07/19/23 00:03:13.605
Jul 19 00:03:13.607: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:13Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:13Z]] name:name1 resourceVersion:43061 uid:26580c59-e14d-4c4e-93c7-49c51e0b8c48] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 07/19/23 00:03:23.61
Jul 19 00:03:23.613: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:23Z]] name:name2 resourceVersion:43144 uid:017dc150-81d6-4e46-bf13-42d163c130d3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 07/19/23 00:03:33.617
Jul 19 00:03:33.621: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:33Z]] name:name1 resourceVersion:43200 uid:26580c59-e14d-4c4e-93c7-49c51e0b8c48] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 07/19/23 00:03:43.624
Jul 19 00:03:43.628: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:43Z]] name:name2 resourceVersion:43260 uid:017dc150-81d6-4e46-bf13-42d163c130d3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 07/19/23 00:03:53.631
Jul 19 00:03:53.635: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:33Z]] name:name1 resourceVersion:43316 uid:26580c59-e14d-4c4e-93c7-49c51e0b8c48] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 07/19/23 00:04:03.639
Jul 19 00:04:03.643: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:43Z]] name:name2 resourceVersion:43373 uid:017dc150-81d6-4e46-bf13-42d163c130d3] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:04:14.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-7466" for this suite. 07/19/23 00:04:14.156
------------------------------
• [SLOW TEST] [63.127 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:03:11.032
    Jul 19 00:03:11.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-watch 07/19/23 00:03:11.033
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:03:11.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:03:11.045
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jul 19 00:03:11.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Creating first CR  07/19/23 00:03:13.605
    Jul 19 00:03:13.607: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:13Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:13Z]] name:name1 resourceVersion:43061 uid:26580c59-e14d-4c4e-93c7-49c51e0b8c48] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 07/19/23 00:03:23.61
    Jul 19 00:03:23.613: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:23Z]] name:name2 resourceVersion:43144 uid:017dc150-81d6-4e46-bf13-42d163c130d3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 07/19/23 00:03:33.617
    Jul 19 00:03:33.621: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:33Z]] name:name1 resourceVersion:43200 uid:26580c59-e14d-4c4e-93c7-49c51e0b8c48] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 07/19/23 00:03:43.624
    Jul 19 00:03:43.628: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:43Z]] name:name2 resourceVersion:43260 uid:017dc150-81d6-4e46-bf13-42d163c130d3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 07/19/23 00:03:53.631
    Jul 19 00:03:53.635: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:33Z]] name:name1 resourceVersion:43316 uid:26580c59-e14d-4c4e-93c7-49c51e0b8c48] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 07/19/23 00:04:03.639
    Jul 19 00:04:03.643: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-19T00:03:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-19T00:03:43Z]] name:name2 resourceVersion:43373 uid:017dc150-81d6-4e46-bf13-42d163c130d3] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:04:14.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-7466" for this suite. 07/19/23 00:04:14.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:04:14.159
Jul 19 00:04:14.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename gc 07/19/23 00:04:14.16
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:14.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:14.168
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 07/19/23 00:04:14.172
STEP: delete the rc 07/19/23 00:04:19.18
STEP: wait for the rc to be deleted 07/19/23 00:04:19.183
Jul 19 00:04:20.201: INFO: 80 pods remaining
Jul 19 00:04:20.201: INFO: 80 pods has nil DeletionTimestamp
Jul 19 00:04:20.201: INFO: 
Jul 19 00:04:21.313: INFO: 72 pods remaining
Jul 19 00:04:21.313: INFO: 69 pods has nil DeletionTimestamp
Jul 19 00:04:21.313: INFO: 
Jul 19 00:04:22.190: INFO: 60 pods remaining
Jul 19 00:04:22.190: INFO: 60 pods has nil DeletionTimestamp
Jul 19 00:04:22.190: INFO: 
Jul 19 00:04:23.190: INFO: 40 pods remaining
Jul 19 00:04:23.190: INFO: 40 pods has nil DeletionTimestamp
Jul 19 00:04:23.190: INFO: 
Jul 19 00:04:24.216: INFO: 31 pods remaining
Jul 19 00:04:24.216: INFO: 30 pods has nil DeletionTimestamp
Jul 19 00:04:24.216: INFO: 
Jul 19 00:04:25.190: INFO: 20 pods remaining
Jul 19 00:04:25.190: INFO: 20 pods has nil DeletionTimestamp
Jul 19 00:04:25.190: INFO: 
STEP: Gathering metrics 07/19/23 00:04:26.188
Jul 19 00:04:26.203: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
Jul 19 00:04:26.205: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.896165ms
Jul 19 00:04:26.205: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
Jul 19 00:04:26.205: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
Jul 19 00:04:28.417: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jul 19 00:04:28.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7881" for this suite. 07/19/23 00:04:28.438
------------------------------
• [SLOW TEST] [14.281 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:04:14.159
    Jul 19 00:04:14.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename gc 07/19/23 00:04:14.16
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:14.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:14.168
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 07/19/23 00:04:14.172
    STEP: delete the rc 07/19/23 00:04:19.18
    STEP: wait for the rc to be deleted 07/19/23 00:04:19.183
    Jul 19 00:04:20.201: INFO: 80 pods remaining
    Jul 19 00:04:20.201: INFO: 80 pods has nil DeletionTimestamp
    Jul 19 00:04:20.201: INFO: 
    Jul 19 00:04:21.313: INFO: 72 pods remaining
    Jul 19 00:04:21.313: INFO: 69 pods has nil DeletionTimestamp
    Jul 19 00:04:21.313: INFO: 
    Jul 19 00:04:22.190: INFO: 60 pods remaining
    Jul 19 00:04:22.190: INFO: 60 pods has nil DeletionTimestamp
    Jul 19 00:04:22.190: INFO: 
    Jul 19 00:04:23.190: INFO: 40 pods remaining
    Jul 19 00:04:23.190: INFO: 40 pods has nil DeletionTimestamp
    Jul 19 00:04:23.190: INFO: 
    Jul 19 00:04:24.216: INFO: 31 pods remaining
    Jul 19 00:04:24.216: INFO: 30 pods has nil DeletionTimestamp
    Jul 19 00:04:24.216: INFO: 
    Jul 19 00:04:25.190: INFO: 20 pods remaining
    Jul 19 00:04:25.190: INFO: 20 pods has nil DeletionTimestamp
    Jul 19 00:04:25.190: INFO: 
    STEP: Gathering metrics 07/19/23 00:04:26.188
    Jul 19 00:04:26.203: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
    Jul 19 00:04:26.205: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.896165ms
    Jul 19 00:04:26.205: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
    Jul 19 00:04:26.205: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
    Jul 19 00:04:28.417: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:04:28.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7881" for this suite. 07/19/23 00:04:28.438
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:04:28.441
Jul 19 00:04:28.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:04:28.442
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:28.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:28.452
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:04:28.466
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:04:28.954
STEP: Deploying the webhook pod 07/19/23 00:04:28.957
STEP: Wait for the deployment to be ready 07/19/23 00:04:28.963
Jul 19 00:04:28.973: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 19 00:04:30.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 00:04:32.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 00:04:34.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 00:04:36.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 07/19/23 00:04:38.981
STEP: Verifying the service has paired with the endpoint 07/19/23 00:04:38.989
Jul 19 00:04:39.989: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 07/19/23 00:04:39.991
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 07/19/23 00:04:40.002
STEP: Creating a dummy validating-webhook-configuration object 07/19/23 00:04:40.009
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 07/19/23 00:04:40.212
STEP: Creating a dummy mutating-webhook-configuration object 07/19/23 00:04:40.214
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 07/19/23 00:04:40.218
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:04:40.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3626" for this suite. 07/19/23 00:04:40.249
STEP: Destroying namespace "webhook-3626-markers" for this suite. 07/19/23 00:04:40.251
------------------------------
• [SLOW TEST] [11.816 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:04:28.441
    Jul 19 00:04:28.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:04:28.442
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:28.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:28.452
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:04:28.466
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:04:28.954
    STEP: Deploying the webhook pod 07/19/23 00:04:28.957
    STEP: Wait for the deployment to be ready 07/19/23 00:04:28.963
    Jul 19 00:04:28.973: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jul 19 00:04:30.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 00:04:32.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 00:04:34.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 00:04:36.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 4, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 07/19/23 00:04:38.981
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:04:38.989
    Jul 19 00:04:39.989: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 07/19/23 00:04:39.991
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 07/19/23 00:04:40.002
    STEP: Creating a dummy validating-webhook-configuration object 07/19/23 00:04:40.009
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 07/19/23 00:04:40.212
    STEP: Creating a dummy mutating-webhook-configuration object 07/19/23 00:04:40.214
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 07/19/23 00:04:40.218
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:04:40.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3626" for this suite. 07/19/23 00:04:40.249
    STEP: Destroying namespace "webhook-3626-markers" for this suite. 07/19/23 00:04:40.251
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:04:40.257
Jul 19 00:04:40.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename events 07/19/23 00:04:40.258
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:40.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:40.268
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 07/19/23 00:04:40.27
STEP: get a list of Events with a label in the current namespace 07/19/23 00:04:40.277
STEP: delete a list of events 07/19/23 00:04:40.278
Jul 19 00:04:40.278: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 07/19/23 00:04:40.292
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jul 19 00:04:40.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2215" for this suite. 07/19/23 00:04:40.299
------------------------------
• [0.044 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:04:40.257
    Jul 19 00:04:40.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename events 07/19/23 00:04:40.258
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:40.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:40.268
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 07/19/23 00:04:40.27
    STEP: get a list of Events with a label in the current namespace 07/19/23 00:04:40.277
    STEP: delete a list of events 07/19/23 00:04:40.278
    Jul 19 00:04:40.278: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 07/19/23 00:04:40.292
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:04:40.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2215" for this suite. 07/19/23 00:04:40.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:04:40.302
Jul 19 00:04:40.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 00:04:40.303
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:40.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:40.316
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 07/19/23 00:04:40.317
Jul 19 00:04:40.321: INFO: Waiting up to 5m0s for pod "downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1" in namespace "downward-api-1520" to be "Succeeded or Failed"
Jul 19 00:04:40.323: INFO: Pod "downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.445538ms
Jul 19 00:04:42.325: INFO: Pod "downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00354329s
Jul 19 00:04:44.325: INFO: Pod "downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003916805s
STEP: Saw pod success 07/19/23 00:04:44.325
Jul 19 00:04:44.325: INFO: Pod "downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1" satisfied condition "Succeeded or Failed"
Jul 19 00:04:44.327: INFO: Trying to get logs from node controller-1 pod downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1 container dapi-container: <nil>
STEP: delete the pod 07/19/23 00:04:44.331
Jul 19 00:04:44.337: INFO: Waiting for pod downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1 to disappear
Jul 19 00:04:44.339: INFO: Pod downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jul 19 00:04:44.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1520" for this suite. 07/19/23 00:04:44.341
------------------------------
• [4.041 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:04:40.302
    Jul 19 00:04:40.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 00:04:40.303
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:40.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:40.316
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 07/19/23 00:04:40.317
    Jul 19 00:04:40.321: INFO: Waiting up to 5m0s for pod "downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1" in namespace "downward-api-1520" to be "Succeeded or Failed"
    Jul 19 00:04:40.323: INFO: Pod "downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.445538ms
    Jul 19 00:04:42.325: INFO: Pod "downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00354329s
    Jul 19 00:04:44.325: INFO: Pod "downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003916805s
    STEP: Saw pod success 07/19/23 00:04:44.325
    Jul 19 00:04:44.325: INFO: Pod "downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1" satisfied condition "Succeeded or Failed"
    Jul 19 00:04:44.327: INFO: Trying to get logs from node controller-1 pod downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1 container dapi-container: <nil>
    STEP: delete the pod 07/19/23 00:04:44.331
    Jul 19 00:04:44.337: INFO: Waiting for pod downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1 to disappear
    Jul 19 00:04:44.339: INFO: Pod downward-api-9baa7e5d-3422-4775-b200-2fdb5accb3c1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:04:44.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1520" for this suite. 07/19/23 00:04:44.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:04:44.345
Jul 19 00:04:44.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename endpointslice 07/19/23 00:04:44.346
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:44.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:44.353
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 07/19/23 00:04:44.355
STEP: getting /apis/discovery.k8s.io 07/19/23 00:04:44.357
STEP: getting /apis/discovery.k8s.iov1 07/19/23 00:04:44.357
STEP: creating 07/19/23 00:04:44.358
STEP: getting 07/19/23 00:04:44.366
STEP: listing 07/19/23 00:04:44.367
STEP: watching 07/19/23 00:04:44.369
Jul 19 00:04:44.369: INFO: starting watch
STEP: cluster-wide listing 07/19/23 00:04:44.37
STEP: cluster-wide watching 07/19/23 00:04:44.372
Jul 19 00:04:44.372: INFO: starting watch
STEP: patching 07/19/23 00:04:44.373
STEP: updating 07/19/23 00:04:44.375
Jul 19 00:04:44.382: INFO: waiting for watch events with expected annotations
Jul 19 00:04:44.382: INFO: saw patched and updated annotations
STEP: deleting 07/19/23 00:04:44.382
STEP: deleting a collection 07/19/23 00:04:44.387
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jul 19 00:04:44.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5596" for this suite. 07/19/23 00:04:44.397
------------------------------
• [0.054 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:04:44.345
    Jul 19 00:04:44.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename endpointslice 07/19/23 00:04:44.346
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:44.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:44.353
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 07/19/23 00:04:44.355
    STEP: getting /apis/discovery.k8s.io 07/19/23 00:04:44.357
    STEP: getting /apis/discovery.k8s.iov1 07/19/23 00:04:44.357
    STEP: creating 07/19/23 00:04:44.358
    STEP: getting 07/19/23 00:04:44.366
    STEP: listing 07/19/23 00:04:44.367
    STEP: watching 07/19/23 00:04:44.369
    Jul 19 00:04:44.369: INFO: starting watch
    STEP: cluster-wide listing 07/19/23 00:04:44.37
    STEP: cluster-wide watching 07/19/23 00:04:44.372
    Jul 19 00:04:44.372: INFO: starting watch
    STEP: patching 07/19/23 00:04:44.373
    STEP: updating 07/19/23 00:04:44.375
    Jul 19 00:04:44.382: INFO: waiting for watch events with expected annotations
    Jul 19 00:04:44.382: INFO: saw patched and updated annotations
    STEP: deleting 07/19/23 00:04:44.382
    STEP: deleting a collection 07/19/23 00:04:44.387
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:04:44.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5596" for this suite. 07/19/23 00:04:44.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:04:44.4
Jul 19 00:04:44.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:04:44.4
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:44.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:44.407
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-dc5737dd-23f1-4202-8105-cbf9345497bf 07/19/23 00:04:44.409
STEP: Creating secret with name secret-projected-all-test-volume-d7eaa70d-48b6-42fe-9aed-3159aac242f7 07/19/23 00:04:44.411
STEP: Creating a pod to test Check all projections for projected volume plugin 07/19/23 00:04:44.413
Jul 19 00:04:44.417: INFO: Waiting up to 5m0s for pod "projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6" in namespace "projected-2855" to be "Succeeded or Failed"
Jul 19 00:04:44.419: INFO: Pod "projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.715179ms
Jul 19 00:04:46.422: INFO: Pod "projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004578318s
Jul 19 00:04:48.421: INFO: Pod "projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004292201s
STEP: Saw pod success 07/19/23 00:04:48.421
Jul 19 00:04:48.421: INFO: Pod "projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6" satisfied condition "Succeeded or Failed"
Jul 19 00:04:48.423: INFO: Trying to get logs from node controller-1 pod projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6 container projected-all-volume-test: <nil>
STEP: delete the pod 07/19/23 00:04:48.427
Jul 19 00:04:48.433: INFO: Waiting for pod projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6 to disappear
Jul 19 00:04:48.435: INFO: Pod projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jul 19 00:04:48.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2855" for this suite. 07/19/23 00:04:48.437
------------------------------
• [4.040 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:04:44.4
    Jul 19 00:04:44.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:04:44.4
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:44.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:44.407
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-dc5737dd-23f1-4202-8105-cbf9345497bf 07/19/23 00:04:44.409
    STEP: Creating secret with name secret-projected-all-test-volume-d7eaa70d-48b6-42fe-9aed-3159aac242f7 07/19/23 00:04:44.411
    STEP: Creating a pod to test Check all projections for projected volume plugin 07/19/23 00:04:44.413
    Jul 19 00:04:44.417: INFO: Waiting up to 5m0s for pod "projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6" in namespace "projected-2855" to be "Succeeded or Failed"
    Jul 19 00:04:44.419: INFO: Pod "projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.715179ms
    Jul 19 00:04:46.422: INFO: Pod "projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004578318s
    Jul 19 00:04:48.421: INFO: Pod "projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004292201s
    STEP: Saw pod success 07/19/23 00:04:48.421
    Jul 19 00:04:48.421: INFO: Pod "projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6" satisfied condition "Succeeded or Failed"
    Jul 19 00:04:48.423: INFO: Trying to get logs from node controller-1 pod projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6 container projected-all-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:04:48.427
    Jul 19 00:04:48.433: INFO: Waiting for pod projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6 to disappear
    Jul 19 00:04:48.435: INFO: Pod projected-volume-a46d3f55-0507-4ba0-a592-5b16c9daada6 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:04:48.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2855" for this suite. 07/19/23 00:04:48.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:04:48.442
Jul 19 00:04:48.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/19/23 00:04:48.443
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:48.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:48.45
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 07/19/23 00:04:48.451
Jul 19 00:04:48.455: INFO: created test-pod-1
Jul 19 00:04:48.459: INFO: created test-pod-2
Jul 19 00:04:48.462: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 07/19/23 00:04:48.462
Jul 19 00:04:48.462: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4622' to be running and ready
Jul 19 00:04:48.475: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul 19 00:04:48.475: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul 19 00:04:48.475: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jul 19 00:04:48.475: INFO: 0 / 3 pods in namespace 'pods-4622' are running and ready (0 seconds elapsed)
Jul 19 00:04:48.475: INFO: expected 0 pod replicas in namespace 'pods-4622', 0 are Running and Ready.
Jul 19 00:04:48.475: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
Jul 19 00:04:48.475: INFO: test-pod-1  controller-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC  }]
Jul 19 00:04:48.475: INFO: test-pod-2  controller-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC  }]
Jul 19 00:04:48.475: INFO: test-pod-3  controller-0  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC  }]
Jul 19 00:04:48.475: INFO: 
Jul 19 00:04:50.481: INFO: 3 / 3 pods in namespace 'pods-4622' are running and ready (2 seconds elapsed)
Jul 19 00:04:50.481: INFO: expected 0 pod replicas in namespace 'pods-4622', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 07/19/23 00:04:50.492
Jul 19 00:04:50.496: INFO: Pod quantity 3 is different from expected quantity 0
Jul 19 00:04:51.498: INFO: Pod quantity 3 is different from expected quantity 0
Jul 19 00:04:52.499: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jul 19 00:04:53.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4622" for this suite. 07/19/23 00:04:53.499
------------------------------
• [SLOW TEST] [5.059 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:04:48.442
    Jul 19 00:04:48.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/19/23 00:04:48.443
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:48.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:48.45
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 07/19/23 00:04:48.451
    Jul 19 00:04:48.455: INFO: created test-pod-1
    Jul 19 00:04:48.459: INFO: created test-pod-2
    Jul 19 00:04:48.462: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 07/19/23 00:04:48.462
    Jul 19 00:04:48.462: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4622' to be running and ready
    Jul 19 00:04:48.475: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jul 19 00:04:48.475: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jul 19 00:04:48.475: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jul 19 00:04:48.475: INFO: 0 / 3 pods in namespace 'pods-4622' are running and ready (0 seconds elapsed)
    Jul 19 00:04:48.475: INFO: expected 0 pod replicas in namespace 'pods-4622', 0 are Running and Ready.
    Jul 19 00:04:48.475: INFO: POD         NODE          PHASE    GRACE  CONDITIONS
    Jul 19 00:04:48.475: INFO: test-pod-1  controller-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC  }]
    Jul 19 00:04:48.475: INFO: test-pod-2  controller-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC  }]
    Jul 19 00:04:48.475: INFO: test-pod-3  controller-0  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:04:48 +0000 UTC  }]
    Jul 19 00:04:48.475: INFO: 
    Jul 19 00:04:50.481: INFO: 3 / 3 pods in namespace 'pods-4622' are running and ready (2 seconds elapsed)
    Jul 19 00:04:50.481: INFO: expected 0 pod replicas in namespace 'pods-4622', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 07/19/23 00:04:50.492
    Jul 19 00:04:50.496: INFO: Pod quantity 3 is different from expected quantity 0
    Jul 19 00:04:51.498: INFO: Pod quantity 3 is different from expected quantity 0
    Jul 19 00:04:52.499: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:04:53.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4622" for this suite. 07/19/23 00:04:53.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:04:53.502
Jul 19 00:04:53.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-probe 07/19/23 00:04:53.503
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:53.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:53.513
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233 in namespace container-probe-5431 07/19/23 00:04:53.515
Jul 19 00:04:53.518: INFO: Waiting up to 5m0s for pod "busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233" in namespace "container-probe-5431" to be "not pending"
Jul 19 00:04:53.520: INFO: Pod "busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233": Phase="Pending", Reason="", readiness=false. Elapsed: 1.512099ms
Jul 19 00:04:55.522: INFO: Pod "busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233": Phase="Running", Reason="", readiness=true. Elapsed: 2.004011618s
Jul 19 00:04:55.522: INFO: Pod "busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233" satisfied condition "not pending"
Jul 19 00:04:55.522: INFO: Started pod busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233 in namespace container-probe-5431
STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 00:04:55.522
Jul 19 00:04:55.524: INFO: Initial restart count of pod busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233 is 0
Jul 19 00:05:45.596: INFO: Restart count of pod container-probe-5431/busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233 is now 1 (50.071959506s elapsed)
STEP: deleting the pod 07/19/23 00:05:45.596
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jul 19 00:05:45.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5431" for this suite. 07/19/23 00:05:45.605
------------------------------
• [SLOW TEST] [52.105 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:04:53.502
    Jul 19 00:04:53.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-probe 07/19/23 00:04:53.503
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:04:53.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:04:53.513
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233 in namespace container-probe-5431 07/19/23 00:04:53.515
    Jul 19 00:04:53.518: INFO: Waiting up to 5m0s for pod "busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233" in namespace "container-probe-5431" to be "not pending"
    Jul 19 00:04:53.520: INFO: Pod "busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233": Phase="Pending", Reason="", readiness=false. Elapsed: 1.512099ms
    Jul 19 00:04:55.522: INFO: Pod "busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233": Phase="Running", Reason="", readiness=true. Elapsed: 2.004011618s
    Jul 19 00:04:55.522: INFO: Pod "busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233" satisfied condition "not pending"
    Jul 19 00:04:55.522: INFO: Started pod busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233 in namespace container-probe-5431
    STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 00:04:55.522
    Jul 19 00:04:55.524: INFO: Initial restart count of pod busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233 is 0
    Jul 19 00:05:45.596: INFO: Restart count of pod container-probe-5431/busybox-a8239a17-ba5b-4e9a-b6ee-17b2ade36233 is now 1 (50.071959506s elapsed)
    STEP: deleting the pod 07/19/23 00:05:45.596
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:05:45.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5431" for this suite. 07/19/23 00:05:45.605
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:05:45.607
Jul 19 00:05:45.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:05:45.608
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:05:45.615
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:05:45.617
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:05:45.625
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:05:46.437
STEP: Deploying the webhook pod 07/19/23 00:05:46.441
STEP: Wait for the deployment to be ready 07/19/23 00:05:46.447
Jul 19 00:05:46.454: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:05:48.461
STEP: Verifying the service has paired with the endpoint 07/19/23 00:05:48.47
Jul 19 00:05:49.470: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 07/19/23 00:05:49.472
STEP: Updating a mutating webhook configuration's rules to not include the create operation 07/19/23 00:05:49.483
STEP: Creating a configMap that should not be mutated 07/19/23 00:05:49.486
STEP: Patching a mutating webhook configuration's rules to include the create operation 07/19/23 00:05:49.49
STEP: Creating a configMap that should be mutated 07/19/23 00:05:49.494
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:05:49.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7100" for this suite. 07/19/23 00:05:49.527
STEP: Destroying namespace "webhook-7100-markers" for this suite. 07/19/23 00:05:49.529
------------------------------
• [3.926 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:05:45.607
    Jul 19 00:05:45.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:05:45.608
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:05:45.615
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:05:45.617
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:05:45.625
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:05:46.437
    STEP: Deploying the webhook pod 07/19/23 00:05:46.441
    STEP: Wait for the deployment to be ready 07/19/23 00:05:46.447
    Jul 19 00:05:46.454: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:05:48.461
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:05:48.47
    Jul 19 00:05:49.470: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 07/19/23 00:05:49.472
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 07/19/23 00:05:49.483
    STEP: Creating a configMap that should not be mutated 07/19/23 00:05:49.486
    STEP: Patching a mutating webhook configuration's rules to include the create operation 07/19/23 00:05:49.49
    STEP: Creating a configMap that should be mutated 07/19/23 00:05:49.494
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:05:49.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7100" for this suite. 07/19/23 00:05:49.527
    STEP: Destroying namespace "webhook-7100-markers" for this suite. 07/19/23 00:05:49.529
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:05:49.534
Jul 19 00:05:49.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replication-controller 07/19/23 00:05:49.535
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:05:49.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:05:49.542
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jul 19 00:05:49.544: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 07/19/23 00:05:50.551
STEP: Checking rc "condition-test" has the desired failure condition set 07/19/23 00:05:50.554
STEP: Scaling down rc "condition-test" to satisfy pod quota 07/19/23 00:05:51.557
Jul 19 00:05:51.561: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 07/19/23 00:05:51.561
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jul 19 00:05:52.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-436" for this suite. 07/19/23 00:05:52.569
------------------------------
• [3.037 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:05:49.534
    Jul 19 00:05:49.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replication-controller 07/19/23 00:05:49.535
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:05:49.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:05:49.542
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jul 19 00:05:49.544: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 07/19/23 00:05:50.551
    STEP: Checking rc "condition-test" has the desired failure condition set 07/19/23 00:05:50.554
    STEP: Scaling down rc "condition-test" to satisfy pod quota 07/19/23 00:05:51.557
    Jul 19 00:05:51.561: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 07/19/23 00:05:51.561
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:05:52.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-436" for this suite. 07/19/23 00:05:52.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:05:52.572
Jul 19 00:05:52.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sched-preemption 07/19/23 00:05:52.572
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:05:52.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:05:52.58
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jul 19 00:05:52.587: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 00:06:52.616: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:06:52.618
Jul 19 00:06:52.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sched-preemption-path 07/19/23 00:06:52.619
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:06:52.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:06:52.626
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 07/19/23 00:06:52.628
STEP: Trying to launch a pod without a label to get a node which can launch it. 07/19/23 00:06:52.628
Jul 19 00:06:52.632: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3695" to be "running"
Jul 19 00:06:52.633: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.354912ms
Jul 19 00:06:54.635: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003561912s
Jul 19 00:06:54.635: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 07/19/23 00:06:54.637
Jul 19 00:06:54.643: INFO: found a healthy node: controller-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Jul 19 00:07:00.681: INFO: pods created so far: [1 1 1]
Jul 19 00:07:00.681: INFO: length of pods created so far: 3
Jul 19 00:07:02.687: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jul 19 00:07:09.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:07:09.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-3695" for this suite. 07/19/23 00:07:09.727
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-2754" for this suite. 07/19/23 00:07:09.73
------------------------------
• [SLOW TEST] [77.161 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:05:52.572
    Jul 19 00:05:52.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sched-preemption 07/19/23 00:05:52.572
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:05:52.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:05:52.58
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jul 19 00:05:52.587: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 19 00:06:52.616: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:06:52.618
    Jul 19 00:06:52.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sched-preemption-path 07/19/23 00:06:52.619
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:06:52.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:06:52.626
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 07/19/23 00:06:52.628
    STEP: Trying to launch a pod without a label to get a node which can launch it. 07/19/23 00:06:52.628
    Jul 19 00:06:52.632: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3695" to be "running"
    Jul 19 00:06:52.633: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.354912ms
    Jul 19 00:06:54.635: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003561912s
    Jul 19 00:06:54.635: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 07/19/23 00:06:54.637
    Jul 19 00:06:54.643: INFO: found a healthy node: controller-1
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Jul 19 00:07:00.681: INFO: pods created so far: [1 1 1]
    Jul 19 00:07:00.681: INFO: length of pods created so far: 3
    Jul 19 00:07:02.687: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:07:09.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:07:09.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-3695" for this suite. 07/19/23 00:07:09.727
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-2754" for this suite. 07/19/23 00:07:09.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:07:09.734
Jul 19 00:07:09.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename runtimeclass 07/19/23 00:07:09.735
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:09.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:09.744
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jul 19 00:07:09.751: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8559 to be scheduled
Jul 19 00:07:09.754: INFO: 1 pods are not scheduled: [runtimeclass-8559/test-runtimeclass-runtimeclass-8559-preconfigured-handler-dzwvt(314efe2e-58e6-4b74-b40f-aa26f2e127ce)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jul 19 00:07:11.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8559" for this suite. 07/19/23 00:07:11.762
------------------------------
• [2.033 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:07:09.734
    Jul 19 00:07:09.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename runtimeclass 07/19/23 00:07:09.735
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:09.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:09.744
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jul 19 00:07:09.751: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8559 to be scheduled
    Jul 19 00:07:09.754: INFO: 1 pods are not scheduled: [runtimeclass-8559/test-runtimeclass-runtimeclass-8559-preconfigured-handler-dzwvt(314efe2e-58e6-4b74-b40f-aa26f2e127ce)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:07:11.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8559" for this suite. 07/19/23 00:07:11.762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:07:11.767
Jul 19 00:07:11.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename deployment 07/19/23 00:07:11.767
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:11.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:11.774
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jul 19 00:07:11.776: INFO: Creating deployment "webserver-deployment"
Jul 19 00:07:11.778: INFO: Waiting for observed generation 1
Jul 19 00:07:13.781: INFO: Waiting for all required pods to come up
Jul 19 00:07:13.784: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 07/19/23 00:07:13.784
Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fm97t" in namespace "deployment-2408" to be "running"
Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bscl5" in namespace "deployment-2408" to be "running"
Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-csmqq" in namespace "deployment-2408" to be "running"
Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mvvgx" in namespace "deployment-2408" to be "running"
Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-n4dc6" in namespace "deployment-2408" to be "running"
Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k84fg" in namespace "deployment-2408" to be "running"
Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-snhf6" in namespace "deployment-2408" to be "running"
Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-f6vbj" in namespace "deployment-2408" to be "running"
Jul 19 00:07:13.786: INFO: Pod "webserver-deployment-7f5969cbc7-f6vbj": Phase="Pending", Reason="", readiness=false. Elapsed: 1.917892ms
Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-mvvgx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225829ms
Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-snhf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144584ms
Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-fm97t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.321452ms
Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-csmqq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.376008ms
Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-k84fg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349803ms
Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-bscl5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.491294ms
Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-n4dc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.490548ms
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-n4dc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004304727s
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-n4dc6" satisfied condition "running"
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-snhf6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004729741s
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-snhf6" satisfied condition "running"
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-bscl5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004910098s
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-bscl5" satisfied condition "running"
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-fm97t": Phase="Running", Reason="", readiness=true. Elapsed: 2.004980805s
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-fm97t" satisfied condition "running"
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-f6vbj": Phase="Running", Reason="", readiness=true. Elapsed: 2.004762806s
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-csmqq": Phase="Running", Reason="", readiness=true. Elapsed: 2.004960522s
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-csmqq" satisfied condition "running"
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-mvvgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.00494125s
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-mvvgx" satisfied condition "running"
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-f6vbj" satisfied condition "running"
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-k84fg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004997268s
Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-k84fg" satisfied condition "running"
Jul 19 00:07:15.789: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 19 00:07:15.792: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 19 00:07:15.796: INFO: Updating deployment webserver-deployment
Jul 19 00:07:15.796: INFO: Waiting for observed generation 2
Jul 19 00:07:17.802: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 19 00:07:17.807: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 19 00:07:17.808: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 19 00:07:17.812: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 19 00:07:17.812: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 19 00:07:17.814: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 19 00:07:17.817: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 19 00:07:17.817: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 19 00:07:17.821: INFO: Updating deployment webserver-deployment
Jul 19 00:07:17.821: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 19 00:07:17.824: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 19 00:07:17.828: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 19 00:07:17.836: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2408  f11ff8e9-9bfe-4a76-b8a6-5092aee0e026 47168 3 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e05648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-19 00:07:14 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-07-19 00:07:15 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 19 00:07:17.844: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2408  7f34bd5b-c419-4460-8cd0-4f92046ae2fc 47172 3 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f11ff8e9-9bfe-4a76-b8a6-5092aee0e026 0xc0045f4477 0xc0045f4478}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f11ff8e9-9bfe-4a76-b8a6-5092aee0e026\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f4518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 00:07:17.844: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 19 00:07:17.845: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2408  8ba113de-452f-4de9-a15e-356b5c50b694 47169 3 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f11ff8e9-9bfe-4a76-b8a6-5092aee0e026 0xc0045f4387 0xc0045f4388}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f11ff8e9-9bfe-4a76-b8a6-5092aee0e026\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f4418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 19 00:07:17.857: INFO: Pod "webserver-deployment-7f5969cbc7-67c75" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-67c75 webserver-deployment-7f5969cbc7- deployment-2408  d9dbe8df-cfa2-4012-b26e-5acbe31685e8 47198 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f49f7 0xc0045f49f8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z6rj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6rj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.857: INFO: Pod "webserver-deployment-7f5969cbc7-b68cz" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b68cz webserver-deployment-7f5969cbc7- deployment-2408  e563a79e-2ab5-4dee-b19b-008008955a24 47199 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f4b37 0xc0045f4b38}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hkvs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hkvs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.857: INFO: Pod "webserver-deployment-7f5969cbc7-bdznp" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bdznp webserver-deployment-7f5969cbc7- deployment-2408  b4120362-5bb2-4cfc-aa25-af33cef530b1 47207 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f4ca0 0xc0045f4ca1}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-282mq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-282mq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-csmqq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-csmqq webserver-deployment-7f5969cbc7- deployment-2408  0fefa58f-b105-41b5-822f-57778bd089ac 46945 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3ac385d56b3ff8862b8e5b20df650ecdbbee2f149ae4ada15096ba23cd2a52eb cni.projectcalico.org/podIP:172.16.166.140/32 cni.projectcalico.org/podIPs:172.16.166.140/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.140"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.140"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f4dd7 0xc0045f4dd8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8j8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8j8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.140,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d9a8af62e630d06def9166b25f9d291a814f882e16d8a34f6b4aacc5d07b0431,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-dw8lq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dw8lq webserver-deployment-7f5969cbc7- deployment-2408  c1afe0f9-fa2c-4ae2-a8a6-554e9c40358a 47196 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5027 0xc0045f5028}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-f6vbj" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-f6vbj webserver-deployment-7f5969cbc7- deployment-2408  c1e5d322-e5be-47d6-a860-a38287b12b6c 46954 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:53955507e7f4a40374572cecb6061c1dd9eaf8bb07eb8f259ea6dac9b2772d2e cni.projectcalico.org/podIP:172.16.192.91/32 cni.projectcalico.org/podIPs:172.16.192.91/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.91"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.91"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f51b7 0xc0045f51b8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pp5zt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pp5zt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.91,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1778c5f2ce8e3ac59d47b2ffe36f3566fb5a4eedd42eaa8737626d89612a63ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-fm97t" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fm97t webserver-deployment-7f5969cbc7- deployment-2408  30b56684-303d-4e5d-aa0f-4119d3d21338 46937 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c1a9df822e3a9a0e2c6e0f64304ba35a3211d46751f5b929e3a2ff364a4bf751 cni.projectcalico.org/podIP:172.16.192.68/32 cni.projectcalico.org/podIPs:172.16.192.68/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.68"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.68"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5427 0xc0045f5428}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7qgnb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7qgnb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.68,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a47012d5c511e26b69143d3888e2e2cb0e607101353f8f807b516f6c0c1cbb04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-h9565" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h9565 webserver-deployment-7f5969cbc7- deployment-2408  de99f6c8-fe05-42fb-be79-56a11f60aff3 47203 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f56a7 0xc0045f56a8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-spwsg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-spwsg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-h9g4g" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h9g4g webserver-deployment-7f5969cbc7- deployment-2408  91e5fa96-e98a-4319-b464-df7e97669838 47192 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f57e7 0xc0045f57e8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7fwfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7fwfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-jmjhc" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jmjhc webserver-deployment-7f5969cbc7- deployment-2408  b754f866-fbd2-41c4-9771-191358f61d13 47191 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5950 0xc0045f5951}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2pfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2pfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-k84fg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k84fg webserver-deployment-7f5969cbc7- deployment-2408  039bae90-d5e4-4d5d-a205-cd7ea42b2421 46935 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e348fd32d147ef8dda612fdabce0018939f28c1ad38ccbd7427075a2564e69e6 cni.projectcalico.org/podIP:172.16.166.139/32 cni.projectcalico.org/podIPs:172.16.166.139/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.139"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.139"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5ab0 0xc0045f5ab1}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5l6nx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5l6nx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.139,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://435a0d41f69ba1dd90ebff53fdc60f1804aec0d0497fac095ac1edcb150da736,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-lrmr6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lrmr6 webserver-deployment-7f5969cbc7- deployment-2408  2287e91e-3ddd-48d7-862f-98248930a77c 47175 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5cd7 0xc0045f5cd8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87zhb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87zhb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-mbjjg" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mbjjg webserver-deployment-7f5969cbc7- deployment-2408  26981ff4-1542-474b-9b17-e398b48567a4 47200 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5e40 0xc0045f5e41}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nk7fv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nk7fv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-mv92d" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mv92d webserver-deployment-7f5969cbc7- deployment-2408  c8ef5e90-efd0-4fdd-8283-4276e3a38d39 46914 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ea08a7224d0ed31378584bfd3fd4d8200b5364a6cf0b314e1bf773221531518e cni.projectcalico.org/podIP:172.16.192.73/32 cni.projectcalico.org/podIPs:172.16.192.73/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.73"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.73"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5f97 0xc0045f5f98}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lwb7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lwb7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.73,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f4f834b23f18319dc7832a4b0330e43932c6814ac089150474d90125f13e6e1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-mvglh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mvglh webserver-deployment-7f5969cbc7- deployment-2408  998a3b7e-a2a6-4793-87f8-597603f643e2 47183 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d101c7 0xc004d101c8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9lf7k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9lf7k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-mvvgx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mvvgx webserver-deployment-7f5969cbc7- deployment-2408  9a7625f9-d004-4c16-80ba-92aa405fcdc9 46947 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:939c5694cfebcfdadd326a0f7c9868a72f9183428be1392fa4821bc0e11387b9 cni.projectcalico.org/podIP:172.16.192.92/32 cni.projectcalico.org/podIPs:172.16.192.92/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.92"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.92"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d10350 0xc004d10351}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fwht2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fwht2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.92,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e23f2fd368b5e5a6a2dcc0791b0d434b175b0e18c0d2503f0c9a7364f3b21d54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-snhf6" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-snhf6 webserver-deployment-7f5969cbc7- deployment-2408  5e87e191-cdc5-4224-a6ce-22170a0ce4c9 47015 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:60bdfe93360bfd7e0266b0b0dedcbe3c6e2ab5a697cbb7c93092750a3b4175e5 cni.projectcalico.org/podIP:172.16.166.160/32 cni.projectcalico.org/podIPs:172.16.166.160/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.160"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.160"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d10577 0xc004d10578}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tl2h6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tl2h6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.160,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://49c4d2c025b79e798b6b34002984d7d29296bab59fab054aa162313160d4b7c9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-tzjmc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tzjmc webserver-deployment-7f5969cbc7- deployment-2408  f0a09e34-beaf-45d2-b213-9eda6e809672 46925 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ca6f5299eca6962ba06f196c4e13bc80c828f812cbbcd7c372cd89af6a96eae4 cni.projectcalico.org/podIP:172.16.166.141/32 cni.projectcalico.org/podIPs:172.16.166.141/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.141"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.141"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d107a7 0xc004d107a8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47jd6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47jd6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.141,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a103ef35499383449dc6e1e770faa243b8285ef27937486a894c67cc812fd1dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-x6nsh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-x6nsh webserver-deployment-7f5969cbc7- deployment-2408  9d302b56-2939-4bca-bebe-a61f74b0044b 47193 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d109d7 0xc004d109d8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k9p9t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9p9t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-7f5969cbc7-xzv62" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xzv62 webserver-deployment-7f5969cbc7- deployment-2408  5ae16467-076e-4402-8097-253e5f3444fe 47184 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d10b40 0xc004d10b41}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-46vlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-46vlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-674zx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-674zx webserver-deployment-d9f79cb5- deployment-2408  016dca0b-ae50-402c-a619-768d3ae77d50 47167 0 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d53b59c641da6858e24f7de20608c89b7b4dfb6ca4e8c1faea6de0bd9d37673e cni.projectcalico.org/podIP:172.16.192.93/32 cni.projectcalico.org/podIPs:172.16.192.93/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.93"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.93"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d10c8f 0xc004d10cc0}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fq4qf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fq4qf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.93,StartTime:2023-07-19 00:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-7jmf6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7jmf6 webserver-deployment-d9f79cb5- deployment-2408  524036d1-2284-4db6-ad94-02e112396d4d 47194 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d10f17 0xc004d10f18}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbrvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbrvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-88p7w" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-88p7w webserver-deployment-d9f79cb5- deployment-2408  d523cb17-d622-4ffd-9877-117fee0d1d4c 47204 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11067 0xc004d11068}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4sch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4sch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-8jhbp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8jhbp webserver-deployment-d9f79cb5- deployment-2408  f62c0970-7331-40b0-92d2-9d339445b8b5 47186 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d111b7 0xc004d111b8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvggk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvggk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-bngdr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bngdr webserver-deployment-d9f79cb5- deployment-2408  adb3ccbc-4fdb-47ec-8762-2e3fc629374c 47205 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d1131f 0xc004d11330}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxl2k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxl2k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-cpb6r" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cpb6r webserver-deployment-d9f79cb5- deployment-2408  007e55e2-9f29-4295-a180-ec42c4401769 47197 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11477 0xc004d11478}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5n6fp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5n6fp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-lc7kj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lc7kj webserver-deployment-d9f79cb5- deployment-2408  c753139a-9e7d-43e9-b5e1-90f516b84189 47149 0 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:09f339ca57f93d9018a91fee8067e9d54a8ce94e4fe6e8cb7883b6ac91854b13 cni.projectcalico.org/podIP:172.16.192.96/32 cni.projectcalico.org/podIPs:172.16.192.96/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.96"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.96"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d115e7 0xc004d115e8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-twlwq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-twlwq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:,StartTime:2023-07-19 00:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.861: INFO: Pod "webserver-deployment-d9f79cb5-pvlkc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pvlkc webserver-deployment-d9f79cb5- deployment-2408  27b6fd9f-afa5-4823-ab6f-5f4a3b172301 47202 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11817 0xc004d11818}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q2cp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q2cp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.861: INFO: Pod "webserver-deployment-d9f79cb5-rpwgj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rpwgj webserver-deployment-d9f79cb5- deployment-2408  ab8e6888-184f-44d5-8991-8b740c098e20 47139 0 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:db2e59bb0ca5eae18b0d2b8993a43cc82ed0bb634748f3e6d10bbd7ac3d2cd32 cni.projectcalico.org/podIP:172.16.192.94/32 cni.projectcalico.org/podIPs:172.16.192.94/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.94"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.94"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d1197f 0xc004d119b0}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lgvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lgvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:,StartTime:2023-07-19 00:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.861: INFO: Pod "webserver-deployment-d9f79cb5-wtfmz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wtfmz webserver-deployment-d9f79cb5- deployment-2408  22cc1c8c-f45d-4138-a248-3d85261f67ff 47125 0 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f568fa86bfb81084b3b116429f5adb3b9e63584ed4a04faeca6273be3cfb81eb cni.projectcalico.org/podIP:172.16.166.171/32 cni.projectcalico.org/podIPs:172.16.166.171/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.171"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.171"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11bd7 0xc004d11bd8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mnmrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mnmrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.861: INFO: Pod "webserver-deployment-d9f79cb5-xw2dw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xw2dw webserver-deployment-d9f79cb5- deployment-2408  226fd240-31c0-4c01-9d69-38b5a1f9cd9d 47206 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11d8f 0xc004d11da0}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zxl42,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxl42,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:07:17.861: INFO: Pod "webserver-deployment-d9f79cb5-z7n99" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z7n99 webserver-deployment-d9f79cb5- deployment-2408  d46b7889-ae04-4e3e-9b66-1d5aab4ef10b 47166 0 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:5b500af4357cbd30744c41618f07dae93fc28716be0bce2fda1087a402393fd6 cni.projectcalico.org/podIP:172.16.166.154/32 cni.projectcalico.org/podIPs:172.16.166.154/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.154"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.154"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11f1f 0xc004d11f30}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6whxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6whxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:,StartTime:2023-07-19 00:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jul 19 00:07:17.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2408" for this suite. 07/19/23 00:07:17.869
------------------------------
• [SLOW TEST] [6.119 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:07:11.767
    Jul 19 00:07:11.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename deployment 07/19/23 00:07:11.767
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:11.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:11.774
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jul 19 00:07:11.776: INFO: Creating deployment "webserver-deployment"
    Jul 19 00:07:11.778: INFO: Waiting for observed generation 1
    Jul 19 00:07:13.781: INFO: Waiting for all required pods to come up
    Jul 19 00:07:13.784: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 07/19/23 00:07:13.784
    Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-fm97t" in namespace "deployment-2408" to be "running"
    Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bscl5" in namespace "deployment-2408" to be "running"
    Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-csmqq" in namespace "deployment-2408" to be "running"
    Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mvvgx" in namespace "deployment-2408" to be "running"
    Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-n4dc6" in namespace "deployment-2408" to be "running"
    Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k84fg" in namespace "deployment-2408" to be "running"
    Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-snhf6" in namespace "deployment-2408" to be "running"
    Jul 19 00:07:13.784: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-f6vbj" in namespace "deployment-2408" to be "running"
    Jul 19 00:07:13.786: INFO: Pod "webserver-deployment-7f5969cbc7-f6vbj": Phase="Pending", Reason="", readiness=false. Elapsed: 1.917892ms
    Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-mvvgx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225829ms
    Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-snhf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144584ms
    Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-fm97t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.321452ms
    Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-csmqq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.376008ms
    Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-k84fg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349803ms
    Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-bscl5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.491294ms
    Jul 19 00:07:13.787: INFO: Pod "webserver-deployment-7f5969cbc7-n4dc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.490548ms
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-n4dc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004304727s
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-n4dc6" satisfied condition "running"
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-snhf6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004729741s
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-snhf6" satisfied condition "running"
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-bscl5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004910098s
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-bscl5" satisfied condition "running"
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-fm97t": Phase="Running", Reason="", readiness=true. Elapsed: 2.004980805s
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-fm97t" satisfied condition "running"
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-f6vbj": Phase="Running", Reason="", readiness=true. Elapsed: 2.004762806s
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-csmqq": Phase="Running", Reason="", readiness=true. Elapsed: 2.004960522s
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-csmqq" satisfied condition "running"
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-mvvgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.00494125s
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-mvvgx" satisfied condition "running"
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-f6vbj" satisfied condition "running"
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-k84fg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004997268s
    Jul 19 00:07:15.789: INFO: Pod "webserver-deployment-7f5969cbc7-k84fg" satisfied condition "running"
    Jul 19 00:07:15.789: INFO: Waiting for deployment "webserver-deployment" to complete
    Jul 19 00:07:15.792: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jul 19 00:07:15.796: INFO: Updating deployment webserver-deployment
    Jul 19 00:07:15.796: INFO: Waiting for observed generation 2
    Jul 19 00:07:17.802: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jul 19 00:07:17.807: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jul 19 00:07:17.808: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jul 19 00:07:17.812: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jul 19 00:07:17.812: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jul 19 00:07:17.814: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jul 19 00:07:17.817: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jul 19 00:07:17.817: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jul 19 00:07:17.821: INFO: Updating deployment webserver-deployment
    Jul 19 00:07:17.821: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jul 19 00:07:17.824: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jul 19 00:07:17.828: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 19 00:07:17.836: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2408  f11ff8e9-9bfe-4a76-b8a6-5092aee0e026 47168 3 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e05648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-19 00:07:14 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-07-19 00:07:15 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jul 19 00:07:17.844: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2408  7f34bd5b-c419-4460-8cd0-4f92046ae2fc 47172 3 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f11ff8e9-9bfe-4a76-b8a6-5092aee0e026 0xc0045f4477 0xc0045f4478}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f11ff8e9-9bfe-4a76-b8a6-5092aee0e026\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f4518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 00:07:17.844: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jul 19 00:07:17.845: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2408  8ba113de-452f-4de9-a15e-356b5c50b694 47169 3 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f11ff8e9-9bfe-4a76-b8a6-5092aee0e026 0xc0045f4387 0xc0045f4388}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f11ff8e9-9bfe-4a76-b8a6-5092aee0e026\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045f4418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 00:07:17.857: INFO: Pod "webserver-deployment-7f5969cbc7-67c75" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-67c75 webserver-deployment-7f5969cbc7- deployment-2408  d9dbe8df-cfa2-4012-b26e-5acbe31685e8 47198 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f49f7 0xc0045f49f8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z6rj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6rj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.857: INFO: Pod "webserver-deployment-7f5969cbc7-b68cz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b68cz webserver-deployment-7f5969cbc7- deployment-2408  e563a79e-2ab5-4dee-b19b-008008955a24 47199 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f4b37 0xc0045f4b38}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hkvs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hkvs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.857: INFO: Pod "webserver-deployment-7f5969cbc7-bdznp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bdznp webserver-deployment-7f5969cbc7- deployment-2408  b4120362-5bb2-4cfc-aa25-af33cef530b1 47207 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f4ca0 0xc0045f4ca1}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-282mq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-282mq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-csmqq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-csmqq webserver-deployment-7f5969cbc7- deployment-2408  0fefa58f-b105-41b5-822f-57778bd089ac 46945 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3ac385d56b3ff8862b8e5b20df650ecdbbee2f149ae4ada15096ba23cd2a52eb cni.projectcalico.org/podIP:172.16.166.140/32 cni.projectcalico.org/podIPs:172.16.166.140/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.140"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.140"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f4dd7 0xc0045f4dd8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8j8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8j8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.140,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d9a8af62e630d06def9166b25f9d291a814f882e16d8a34f6b4aacc5d07b0431,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-dw8lq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dw8lq webserver-deployment-7f5969cbc7- deployment-2408  c1afe0f9-fa2c-4ae2-a8a6-554e9c40358a 47196 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5027 0xc0045f5028}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvdrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvdrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-f6vbj" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-f6vbj webserver-deployment-7f5969cbc7- deployment-2408  c1e5d322-e5be-47d6-a860-a38287b12b6c 46954 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:53955507e7f4a40374572cecb6061c1dd9eaf8bb07eb8f259ea6dac9b2772d2e cni.projectcalico.org/podIP:172.16.192.91/32 cni.projectcalico.org/podIPs:172.16.192.91/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.91"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.91"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f51b7 0xc0045f51b8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pp5zt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pp5zt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.91,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1778c5f2ce8e3ac59d47b2ffe36f3566fb5a4eedd42eaa8737626d89612a63ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-fm97t" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fm97t webserver-deployment-7f5969cbc7- deployment-2408  30b56684-303d-4e5d-aa0f-4119d3d21338 46937 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c1a9df822e3a9a0e2c6e0f64304ba35a3211d46751f5b929e3a2ff364a4bf751 cni.projectcalico.org/podIP:172.16.192.68/32 cni.projectcalico.org/podIPs:172.16.192.68/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.68"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.68"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5427 0xc0045f5428}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7qgnb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7qgnb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.68,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a47012d5c511e26b69143d3888e2e2cb0e607101353f8f807b516f6c0c1cbb04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-h9565" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h9565 webserver-deployment-7f5969cbc7- deployment-2408  de99f6c8-fe05-42fb-be79-56a11f60aff3 47203 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f56a7 0xc0045f56a8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-spwsg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-spwsg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-h9g4g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h9g4g webserver-deployment-7f5969cbc7- deployment-2408  91e5fa96-e98a-4319-b464-df7e97669838 47192 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f57e7 0xc0045f57e8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7fwfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7fwfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-jmjhc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jmjhc webserver-deployment-7f5969cbc7- deployment-2408  b754f866-fbd2-41c4-9771-191358f61d13 47191 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5950 0xc0045f5951}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2pfv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2pfv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.858: INFO: Pod "webserver-deployment-7f5969cbc7-k84fg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k84fg webserver-deployment-7f5969cbc7- deployment-2408  039bae90-d5e4-4d5d-a205-cd7ea42b2421 46935 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e348fd32d147ef8dda612fdabce0018939f28c1ad38ccbd7427075a2564e69e6 cni.projectcalico.org/podIP:172.16.166.139/32 cni.projectcalico.org/podIPs:172.16.166.139/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.139"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.139"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5ab0 0xc0045f5ab1}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5l6nx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5l6nx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.139,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://435a0d41f69ba1dd90ebff53fdc60f1804aec0d0497fac095ac1edcb150da736,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-lrmr6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lrmr6 webserver-deployment-7f5969cbc7- deployment-2408  2287e91e-3ddd-48d7-862f-98248930a77c 47175 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5cd7 0xc0045f5cd8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-87zhb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-87zhb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-mbjjg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mbjjg webserver-deployment-7f5969cbc7- deployment-2408  26981ff4-1542-474b-9b17-e398b48567a4 47200 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5e40 0xc0045f5e41}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nk7fv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nk7fv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-mv92d" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mv92d webserver-deployment-7f5969cbc7- deployment-2408  c8ef5e90-efd0-4fdd-8283-4276e3a38d39 46914 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ea08a7224d0ed31378584bfd3fd4d8200b5364a6cf0b314e1bf773221531518e cni.projectcalico.org/podIP:172.16.192.73/32 cni.projectcalico.org/podIPs:172.16.192.73/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.73"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.73"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc0045f5f97 0xc0045f5f98}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2lwb7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2lwb7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.73,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f4f834b23f18319dc7832a4b0330e43932c6814ac089150474d90125f13e6e1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-mvglh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mvglh webserver-deployment-7f5969cbc7- deployment-2408  998a3b7e-a2a6-4793-87f8-597603f643e2 47183 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d101c7 0xc004d101c8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9lf7k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9lf7k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-mvvgx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mvvgx webserver-deployment-7f5969cbc7- deployment-2408  9a7625f9-d004-4c16-80ba-92aa405fcdc9 46947 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:939c5694cfebcfdadd326a0f7c9868a72f9183428be1392fa4821bc0e11387b9 cni.projectcalico.org/podIP:172.16.192.92/32 cni.projectcalico.org/podIPs:172.16.192.92/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.92"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.92"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d10350 0xc004d10351}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fwht2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fwht2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.92,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e23f2fd368b5e5a6a2dcc0791b0d434b175b0e18c0d2503f0c9a7364f3b21d54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-snhf6" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-snhf6 webserver-deployment-7f5969cbc7- deployment-2408  5e87e191-cdc5-4224-a6ce-22170a0ce4c9 47015 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:60bdfe93360bfd7e0266b0b0dedcbe3c6e2ab5a697cbb7c93092750a3b4175e5 cni.projectcalico.org/podIP:172.16.166.160/32 cni.projectcalico.org/podIPs:172.16.166.160/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.160"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.160"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d10577 0xc004d10578}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tl2h6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tl2h6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.160,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://49c4d2c025b79e798b6b34002984d7d29296bab59fab054aa162313160d4b7c9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-tzjmc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tzjmc webserver-deployment-7f5969cbc7- deployment-2408  f0a09e34-beaf-45d2-b213-9eda6e809672 46925 0 2023-07-19 00:07:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ca6f5299eca6962ba06f196c4e13bc80c828f812cbbcd7c372cd89af6a96eae4 cni.projectcalico.org/podIP:172.16.166.141/32 cni.projectcalico.org/podIPs:172.16.166.141/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.141"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.141"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d107a7 0xc004d107a8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47jd6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47jd6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.141,StartTime:2023-07-19 00:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:07:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a103ef35499383449dc6e1e770faa243b8285ef27937486a894c67cc812fd1dd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.859: INFO: Pod "webserver-deployment-7f5969cbc7-x6nsh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-x6nsh webserver-deployment-7f5969cbc7- deployment-2408  9d302b56-2939-4bca-bebe-a61f74b0044b 47193 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d109d7 0xc004d109d8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k9p9t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9p9t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-7f5969cbc7-xzv62" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xzv62 webserver-deployment-7f5969cbc7- deployment-2408  5ae16467-076e-4402-8097-253e5f3444fe 47184 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 8ba113de-452f-4de9-a15e-356b5c50b694 0xc004d10b40 0xc004d10b41}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ba113de-452f-4de9-a15e-356b5c50b694\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-46vlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-46vlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-674zx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-674zx webserver-deployment-d9f79cb5- deployment-2408  016dca0b-ae50-402c-a619-768d3ae77d50 47167 0 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d53b59c641da6858e24f7de20608c89b7b4dfb6ca4e8c1faea6de0bd9d37673e cni.projectcalico.org/podIP:172.16.192.93/32 cni.projectcalico.org/podIPs:172.16.192.93/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.93"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.93"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d10c8f 0xc004d10cc0}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fq4qf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fq4qf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.93,StartTime:2023-07-19 00:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-7jmf6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7jmf6 webserver-deployment-d9f79cb5- deployment-2408  524036d1-2284-4db6-ad94-02e112396d4d 47194 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d10f17 0xc004d10f18}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbrvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbrvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-88p7w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-88p7w webserver-deployment-d9f79cb5- deployment-2408  d523cb17-d622-4ffd-9877-117fee0d1d4c 47204 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11067 0xc004d11068}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4sch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4sch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-8jhbp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8jhbp webserver-deployment-d9f79cb5- deployment-2408  f62c0970-7331-40b0-92d2-9d339445b8b5 47186 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d111b7 0xc004d111b8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fvggk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fvggk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-bngdr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bngdr webserver-deployment-d9f79cb5- deployment-2408  adb3ccbc-4fdb-47ec-8762-2e3fc629374c 47205 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d1131f 0xc004d11330}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxl2k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxl2k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-cpb6r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cpb6r webserver-deployment-d9f79cb5- deployment-2408  007e55e2-9f29-4295-a180-ec42c4401769 47197 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11477 0xc004d11478}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5n6fp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5n6fp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.860: INFO: Pod "webserver-deployment-d9f79cb5-lc7kj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lc7kj webserver-deployment-d9f79cb5- deployment-2408  c753139a-9e7d-43e9-b5e1-90f516b84189 47149 0 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:09f339ca57f93d9018a91fee8067e9d54a8ce94e4fe6e8cb7883b6ac91854b13 cni.projectcalico.org/podIP:172.16.192.96/32 cni.projectcalico.org/podIPs:172.16.192.96/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.96"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.96"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d115e7 0xc004d115e8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-twlwq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-twlwq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:,StartTime:2023-07-19 00:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.861: INFO: Pod "webserver-deployment-d9f79cb5-pvlkc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-pvlkc webserver-deployment-d9f79cb5- deployment-2408  27b6fd9f-afa5-4823-ab6f-5f4a3b172301 47202 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11817 0xc004d11818}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q2cp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q2cp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.861: INFO: Pod "webserver-deployment-d9f79cb5-rpwgj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rpwgj webserver-deployment-d9f79cb5- deployment-2408  ab8e6888-184f-44d5-8991-8b740c098e20 47139 0 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:db2e59bb0ca5eae18b0d2b8993a43cc82ed0bb634748f3e6d10bbd7ac3d2cd32 cni.projectcalico.org/podIP:172.16.192.94/32 cni.projectcalico.org/podIPs:172.16.192.94/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.94"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.94"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d1197f 0xc004d119b0}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lgvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lgvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:,StartTime:2023-07-19 00:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.861: INFO: Pod "webserver-deployment-d9f79cb5-wtfmz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wtfmz webserver-deployment-d9f79cb5- deployment-2408  22cc1c8c-f45d-4138-a248-3d85261f67ff 47125 0 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f568fa86bfb81084b3b116429f5adb3b9e63584ed4a04faeca6273be3cfb81eb cni.projectcalico.org/podIP:172.16.166.171/32 cni.projectcalico.org/podIPs:172.16.166.171/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.171"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.171"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11bd7 0xc004d11bd8}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mnmrr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mnmrr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.861: INFO: Pod "webserver-deployment-d9f79cb5-xw2dw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xw2dw webserver-deployment-d9f79cb5- deployment-2408  226fd240-31c0-4c01-9d69-38b5a1f9cd9d 47206 0 2023-07-19 00:07:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11d8f 0xc004d11da0}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zxl42,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxl42,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:07:17.861: INFO: Pod "webserver-deployment-d9f79cb5-z7n99" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z7n99 webserver-deployment-d9f79cb5- deployment-2408  d46b7889-ae04-4e3e-9b66-1d5aab4ef10b 47166 0 2023-07-19 00:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:5b500af4357cbd30744c41618f07dae93fc28716be0bce2fda1087a402393fd6 cni.projectcalico.org/podIP:172.16.166.154/32 cni.projectcalico.org/podIPs:172.16.166.154/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.154"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.154"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 7f34bd5b-c419-4460-8cd0-4f92046ae2fc 0xc004d11f1f 0xc004d11f30}] [] [{kube-controller-manager Update v1 2023-07-19 00:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f34bd5b-c419-4460-8cd0-4f92046ae2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:07:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6whxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6whxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:,StartTime:2023-07-19 00:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:07:17.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2408" for this suite. 07/19/23 00:07:17.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:07:17.887
Jul 19 00:07:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:07:17.888
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:17.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:17.903
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-5667 07/19/23 00:07:17.905
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5667 to expose endpoints map[] 07/19/23 00:07:17.926
Jul 19 00:07:17.931: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jul 19 00:07:18.936: INFO: successfully validated that service endpoint-test2 in namespace services-5667 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5667 07/19/23 00:07:18.936
Jul 19 00:07:18.940: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5667" to be "running and ready"
Jul 19 00:07:18.943: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.964162ms
Jul 19 00:07:18.943: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:07:20.945: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0050648s
Jul 19 00:07:20.945: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:07:22.949: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008980387s
Jul 19 00:07:22.949: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:07:24.945: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00536436s
Jul 19 00:07:24.945: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:07:26.946: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006349128s
Jul 19 00:07:26.946: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:07:28.945: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00506468s
Jul 19 00:07:28.945: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:07:30.946: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005934708s
Jul 19 00:07:30.946: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:07:32.946: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 14.005795064s
Jul 19 00:07:32.946: INFO: The phase of Pod pod1 is Running (Ready = true)
Jul 19 00:07:32.946: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5667 to expose endpoints map[pod1:[80]] 07/19/23 00:07:32.947
Jul 19 00:07:32.953: INFO: successfully validated that service endpoint-test2 in namespace services-5667 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 07/19/23 00:07:32.953
Jul 19 00:07:32.953: INFO: Creating new exec pod
Jul 19 00:07:32.955: INFO: Waiting up to 5m0s for pod "execpodjhsvc" in namespace "services-5667" to be "running"
Jul 19 00:07:32.959: INFO: Pod "execpodjhsvc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.965882ms
Jul 19 00:07:34.961: INFO: Pod "execpodjhsvc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006193194s
Jul 19 00:07:36.963: INFO: Pod "execpodjhsvc": Phase="Running", Reason="", readiness=true. Elapsed: 4.007521204s
Jul 19 00:07:36.963: INFO: Pod "execpodjhsvc" satisfied condition "running"
Jul 19 00:07:37.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jul 19 00:07:38.075: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul 19 00:07:38.075: INFO: stdout: ""
Jul 19 00:07:38.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 10.105.216.83 80'
Jul 19 00:07:38.184: INFO: stderr: "+ nc -v -z -w 2 10.105.216.83 80\nConnection to 10.105.216.83 80 port [tcp/http] succeeded!\n"
Jul 19 00:07:38.184: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-5667 07/19/23 00:07:38.184
Jul 19 00:07:38.187: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5667" to be "running and ready"
Jul 19 00:07:38.190: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252564ms
Jul 19 00:07:38.190: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:07:40.192: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00561157s
Jul 19 00:07:40.192: INFO: The phase of Pod pod2 is Running (Ready = true)
Jul 19 00:07:40.192: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5667 to expose endpoints map[pod1:[80] pod2:[80]] 07/19/23 00:07:40.194
Jul 19 00:07:40.201: INFO: successfully validated that service endpoint-test2 in namespace services-5667 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 07/19/23 00:07:40.201
Jul 19 00:07:41.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jul 19 00:07:41.316: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul 19 00:07:41.316: INFO: stdout: ""
Jul 19 00:07:41.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 10.105.216.83 80'
Jul 19 00:07:41.420: INFO: stderr: "+ nc -v -z -w 2 10.105.216.83 80\nConnection to 10.105.216.83 80 port [tcp/http] succeeded!\n"
Jul 19 00:07:41.420: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5667 07/19/23 00:07:41.42
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5667 to expose endpoints map[pod2:[80]] 07/19/23 00:07:41.427
Jul 19 00:07:41.434: INFO: successfully validated that service endpoint-test2 in namespace services-5667 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 07/19/23 00:07:41.434
Jul 19 00:07:42.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jul 19 00:07:42.548: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul 19 00:07:42.548: INFO: stdout: ""
Jul 19 00:07:42.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 10.105.216.83 80'
Jul 19 00:07:42.647: INFO: stderr: "+ nc -v -z -w 2 10.105.216.83 80\nConnection to 10.105.216.83 80 port [tcp/http] succeeded!\n"
Jul 19 00:07:42.647: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-5667 07/19/23 00:07:42.647
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5667 to expose endpoints map[] 07/19/23 00:07:42.656
Jul 19 00:07:42.665: INFO: successfully validated that service endpoint-test2 in namespace services-5667 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:07:42.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5667" for this suite. 07/19/23 00:07:42.693
------------------------------
• [SLOW TEST] [24.810 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:07:17.887
    Jul 19 00:07:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:07:17.888
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:17.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:17.903
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-5667 07/19/23 00:07:17.905
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5667 to expose endpoints map[] 07/19/23 00:07:17.926
    Jul 19 00:07:17.931: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jul 19 00:07:18.936: INFO: successfully validated that service endpoint-test2 in namespace services-5667 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5667 07/19/23 00:07:18.936
    Jul 19 00:07:18.940: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5667" to be "running and ready"
    Jul 19 00:07:18.943: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.964162ms
    Jul 19 00:07:18.943: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:07:20.945: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0050648s
    Jul 19 00:07:20.945: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:07:22.949: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008980387s
    Jul 19 00:07:22.949: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:07:24.945: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00536436s
    Jul 19 00:07:24.945: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:07:26.946: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006349128s
    Jul 19 00:07:26.946: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:07:28.945: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00506468s
    Jul 19 00:07:28.945: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:07:30.946: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005934708s
    Jul 19 00:07:30.946: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:07:32.946: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 14.005795064s
    Jul 19 00:07:32.946: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jul 19 00:07:32.946: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5667 to expose endpoints map[pod1:[80]] 07/19/23 00:07:32.947
    Jul 19 00:07:32.953: INFO: successfully validated that service endpoint-test2 in namespace services-5667 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 07/19/23 00:07:32.953
    Jul 19 00:07:32.953: INFO: Creating new exec pod
    Jul 19 00:07:32.955: INFO: Waiting up to 5m0s for pod "execpodjhsvc" in namespace "services-5667" to be "running"
    Jul 19 00:07:32.959: INFO: Pod "execpodjhsvc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.965882ms
    Jul 19 00:07:34.961: INFO: Pod "execpodjhsvc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006193194s
    Jul 19 00:07:36.963: INFO: Pod "execpodjhsvc": Phase="Running", Reason="", readiness=true. Elapsed: 4.007521204s
    Jul 19 00:07:36.963: INFO: Pod "execpodjhsvc" satisfied condition "running"
    Jul 19 00:07:37.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jul 19 00:07:38.075: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jul 19 00:07:38.075: INFO: stdout: ""
    Jul 19 00:07:38.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 10.105.216.83 80'
    Jul 19 00:07:38.184: INFO: stderr: "+ nc -v -z -w 2 10.105.216.83 80\nConnection to 10.105.216.83 80 port [tcp/http] succeeded!\n"
    Jul 19 00:07:38.184: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-5667 07/19/23 00:07:38.184
    Jul 19 00:07:38.187: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5667" to be "running and ready"
    Jul 19 00:07:38.190: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252564ms
    Jul 19 00:07:38.190: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:07:40.192: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00561157s
    Jul 19 00:07:40.192: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jul 19 00:07:40.192: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5667 to expose endpoints map[pod1:[80] pod2:[80]] 07/19/23 00:07:40.194
    Jul 19 00:07:40.201: INFO: successfully validated that service endpoint-test2 in namespace services-5667 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 07/19/23 00:07:40.201
    Jul 19 00:07:41.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jul 19 00:07:41.316: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jul 19 00:07:41.316: INFO: stdout: ""
    Jul 19 00:07:41.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 10.105.216.83 80'
    Jul 19 00:07:41.420: INFO: stderr: "+ nc -v -z -w 2 10.105.216.83 80\nConnection to 10.105.216.83 80 port [tcp/http] succeeded!\n"
    Jul 19 00:07:41.420: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5667 07/19/23 00:07:41.42
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5667 to expose endpoints map[pod2:[80]] 07/19/23 00:07:41.427
    Jul 19 00:07:41.434: INFO: successfully validated that service endpoint-test2 in namespace services-5667 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 07/19/23 00:07:41.434
    Jul 19 00:07:42.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jul 19 00:07:42.548: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jul 19 00:07:42.548: INFO: stdout: ""
    Jul 19 00:07:42.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5667 exec execpodjhsvc -- /bin/sh -x -c nc -v -z -w 2 10.105.216.83 80'
    Jul 19 00:07:42.647: INFO: stderr: "+ nc -v -z -w 2 10.105.216.83 80\nConnection to 10.105.216.83 80 port [tcp/http] succeeded!\n"
    Jul 19 00:07:42.647: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-5667 07/19/23 00:07:42.647
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5667 to expose endpoints map[] 07/19/23 00:07:42.656
    Jul 19 00:07:42.665: INFO: successfully validated that service endpoint-test2 in namespace services-5667 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:07:42.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5667" for this suite. 07/19/23 00:07:42.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:07:42.697
Jul 19 00:07:42.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 00:07:42.698
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:42.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:42.712
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-b6d1b88e-3837-41d4-ab4d-89810d50d18b 07/19/23 00:07:42.714
STEP: Creating a pod to test consume secrets 07/19/23 00:07:42.717
Jul 19 00:07:42.724: INFO: Waiting up to 5m0s for pod "pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38" in namespace "secrets-6121" to be "Succeeded or Failed"
Jul 19 00:07:42.728: INFO: Pod "pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.667497ms
Jul 19 00:07:44.731: INFO: Pod "pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006170385s
Jul 19 00:07:46.731: INFO: Pod "pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006821546s
STEP: Saw pod success 07/19/23 00:07:46.731
Jul 19 00:07:46.731: INFO: Pod "pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38" satisfied condition "Succeeded or Failed"
Jul 19 00:07:46.733: INFO: Trying to get logs from node controller-1 pod pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38 container secret-volume-test: <nil>
STEP: delete the pod 07/19/23 00:07:46.743
Jul 19 00:07:46.751: INFO: Waiting for pod pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38 to disappear
Jul 19 00:07:46.753: INFO: Pod pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 00:07:46.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6121" for this suite. 07/19/23 00:07:46.755
------------------------------
• [4.060 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:07:42.697
    Jul 19 00:07:42.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 00:07:42.698
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:42.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:42.712
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-b6d1b88e-3837-41d4-ab4d-89810d50d18b 07/19/23 00:07:42.714
    STEP: Creating a pod to test consume secrets 07/19/23 00:07:42.717
    Jul 19 00:07:42.724: INFO: Waiting up to 5m0s for pod "pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38" in namespace "secrets-6121" to be "Succeeded or Failed"
    Jul 19 00:07:42.728: INFO: Pod "pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.667497ms
    Jul 19 00:07:44.731: INFO: Pod "pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006170385s
    Jul 19 00:07:46.731: INFO: Pod "pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006821546s
    STEP: Saw pod success 07/19/23 00:07:46.731
    Jul 19 00:07:46.731: INFO: Pod "pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38" satisfied condition "Succeeded or Failed"
    Jul 19 00:07:46.733: INFO: Trying to get logs from node controller-1 pod pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38 container secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:07:46.743
    Jul 19 00:07:46.751: INFO: Waiting for pod pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38 to disappear
    Jul 19 00:07:46.753: INFO: Pod pod-secrets-5e35ebf0-50a6-4dbe-a792-f0a0c8151f38 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:07:46.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6121" for this suite. 07/19/23 00:07:46.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:07:46.758
Jul 19 00:07:46.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:07:46.759
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:46.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:46.768
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jul 19 00:07:46.777: INFO: created pod pod-service-account-defaultsa
Jul 19 00:07:46.777: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 19 00:07:46.780: INFO: created pod pod-service-account-mountsa
Jul 19 00:07:46.780: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 19 00:07:46.785: INFO: created pod pod-service-account-nomountsa
Jul 19 00:07:46.785: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 19 00:07:46.789: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 19 00:07:46.789: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 19 00:07:46.801: INFO: created pod pod-service-account-mountsa-mountspec
Jul 19 00:07:46.801: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 19 00:07:46.809: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 19 00:07:46.809: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 19 00:07:46.818: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 19 00:07:46.818: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 19 00:07:46.822: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 19 00:07:46.822: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 19 00:07:46.829: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 19 00:07:46.829: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jul 19 00:07:46.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7267" for this suite. 07/19/23 00:07:46.836
------------------------------
• [0.084 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:07:46.758
    Jul 19 00:07:46.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:07:46.759
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:46.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:46.768
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jul 19 00:07:46.777: INFO: created pod pod-service-account-defaultsa
    Jul 19 00:07:46.777: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jul 19 00:07:46.780: INFO: created pod pod-service-account-mountsa
    Jul 19 00:07:46.780: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jul 19 00:07:46.785: INFO: created pod pod-service-account-nomountsa
    Jul 19 00:07:46.785: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jul 19 00:07:46.789: INFO: created pod pod-service-account-defaultsa-mountspec
    Jul 19 00:07:46.789: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jul 19 00:07:46.801: INFO: created pod pod-service-account-mountsa-mountspec
    Jul 19 00:07:46.801: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jul 19 00:07:46.809: INFO: created pod pod-service-account-nomountsa-mountspec
    Jul 19 00:07:46.809: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jul 19 00:07:46.818: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jul 19 00:07:46.818: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jul 19 00:07:46.822: INFO: created pod pod-service-account-mountsa-nomountspec
    Jul 19 00:07:46.822: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jul 19 00:07:46.829: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jul 19 00:07:46.829: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:07:46.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7267" for this suite. 07/19/23 00:07:46.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:07:46.843
Jul 19 00:07:46.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:07:46.844
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:46.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:46.858
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-de454edb-1435-4ebc-8911-ef7fabb7221e 07/19/23 00:07:46.859
STEP: Creating a pod to test consume configMaps 07/19/23 00:07:46.87
Jul 19 00:07:46.877: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4" in namespace "projected-6546" to be "Succeeded or Failed"
Jul 19 00:07:46.878: INFO: Pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643209ms
Jul 19 00:07:48.881: INFO: Pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004370085s
Jul 19 00:07:50.881: INFO: Pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00438621s
Jul 19 00:07:52.881: INFO: Pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004530568s
STEP: Saw pod success 07/19/23 00:07:52.881
Jul 19 00:07:52.881: INFO: Pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4" satisfied condition "Succeeded or Failed"
Jul 19 00:07:52.883: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:07:52.893
Jul 19 00:07:52.900: INFO: Waiting for pod pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4 to disappear
Jul 19 00:07:52.901: INFO: Pod pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:07:52.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6546" for this suite. 07/19/23 00:07:52.903
------------------------------
• [SLOW TEST] [6.063 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:07:46.843
    Jul 19 00:07:46.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:07:46.844
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:46.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:46.858
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-de454edb-1435-4ebc-8911-ef7fabb7221e 07/19/23 00:07:46.859
    STEP: Creating a pod to test consume configMaps 07/19/23 00:07:46.87
    Jul 19 00:07:46.877: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4" in namespace "projected-6546" to be "Succeeded or Failed"
    Jul 19 00:07:46.878: INFO: Pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643209ms
    Jul 19 00:07:48.881: INFO: Pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004370085s
    Jul 19 00:07:50.881: INFO: Pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00438621s
    Jul 19 00:07:52.881: INFO: Pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.004530568s
    STEP: Saw pod success 07/19/23 00:07:52.881
    Jul 19 00:07:52.881: INFO: Pod "pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4" satisfied condition "Succeeded or Failed"
    Jul 19 00:07:52.883: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:07:52.893
    Jul 19 00:07:52.900: INFO: Waiting for pod pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4 to disappear
    Jul 19 00:07:52.901: INFO: Pod pod-projected-configmaps-ba466830-2cd8-4729-83ea-fd5842c9faa4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:07:52.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6546" for this suite. 07/19/23 00:07:52.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:07:52.906
Jul 19 00:07:52.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename statefulset 07/19/23 00:07:52.907
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:52.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:52.914
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-600 07/19/23 00:07:52.916
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-600 07/19/23 00:07:52.918
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-600 07/19/23 00:07:52.922
Jul 19 00:07:52.925: INFO: Found 0 stateful pods, waiting for 1
Jul 19 00:08:02.928: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 07/19/23 00:08:02.928
Jul 19 00:08:02.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 00:08:03.051: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 00:08:03.051: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 00:08:03.051: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 00:08:03.052: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 19 00:08:13.056: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 00:08:13.056: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 00:08:13.067: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 19 00:08:13.067: INFO: ss-0  controller-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  }]
Jul 19 00:08:13.067: INFO: 
Jul 19 00:08:13.067: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 19 00:08:14.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99482376s
Jul 19 00:08:15.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991286334s
Jul 19 00:08:16.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987935419s
Jul 19 00:08:17.080: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984916602s
Jul 19 00:08:18.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981949838s
Jul 19 00:08:19.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97922527s
Jul 19 00:08:20.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976141675s
Jul 19 00:08:21.092: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.972988592s
Jul 19 00:08:22.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 969.931683ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-600 07/19/23 00:08:23.096
Jul 19 00:08:23.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 00:08:23.215: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 00:08:23.215: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 00:08:23.215: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 00:08:23.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 00:08:23.315: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 19 00:08:23.315: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 00:08:23.315: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 00:08:23.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 00:08:23.421: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 19 00:08:23.421: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 00:08:23.421: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 00:08:23.424: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 00:08:23.424: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 00:08:23.424: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 07/19/23 00:08:23.424
Jul 19 00:08:23.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 00:08:23.527: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 00:08:23.527: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 00:08:23.527: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 00:08:23.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 00:08:23.637: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 00:08:23.637: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 00:08:23.637: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 00:08:23.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 00:08:23.740: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 00:08:23.740: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 00:08:23.740: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 00:08:23.740: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 00:08:23.742: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 19 00:08:33.750: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 00:08:33.750: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 00:08:33.750: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 00:08:33.756: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 19 00:08:33.756: INFO: ss-0  controller-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  }]
Jul 19 00:08:33.756: INFO: ss-1  controller-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  }]
Jul 19 00:08:33.756: INFO: ss-2  controller-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  }]
Jul 19 00:08:33.756: INFO: 
Jul 19 00:08:33.756: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 19 00:08:34.761: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Jul 19 00:08:34.761: INFO: ss-0  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  }]
Jul 19 00:08:34.761: INFO: ss-2  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  }]
Jul 19 00:08:34.761: INFO: 
Jul 19 00:08:34.761: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 19 00:08:35.764: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992097849s
Jul 19 00:08:36.767: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989140256s
Jul 19 00:08:37.770: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98617638s
Jul 19 00:08:38.773: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983158159s
Jul 19 00:08:39.776: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.980344401s
Jul 19 00:08:40.778: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.97818082s
Jul 19 00:08:41.780: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.976101386s
Jul 19 00:08:42.783: INFO: Verifying statefulset ss doesn't scale past 0 for another 973.13365ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-600 07/19/23 00:08:43.784
Jul 19 00:08:43.786: INFO: Scaling statefulset ss to 0
Jul 19 00:08:43.793: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jul 19 00:08:43.795: INFO: Deleting all statefulset in ns statefulset-600
Jul 19 00:08:43.796: INFO: Scaling statefulset ss to 0
Jul 19 00:08:43.802: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 00:08:43.803: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:08:43.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-600" for this suite. 07/19/23 00:08:43.813
------------------------------
• [SLOW TEST] [50.909 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:07:52.906
    Jul 19 00:07:52.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename statefulset 07/19/23 00:07:52.907
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:07:52.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:07:52.914
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-600 07/19/23 00:07:52.916
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-600 07/19/23 00:07:52.918
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-600 07/19/23 00:07:52.922
    Jul 19 00:07:52.925: INFO: Found 0 stateful pods, waiting for 1
    Jul 19 00:08:02.928: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 07/19/23 00:08:02.928
    Jul 19 00:08:02.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 19 00:08:03.051: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 19 00:08:03.051: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 19 00:08:03.051: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 19 00:08:03.052: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jul 19 00:08:13.056: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jul 19 00:08:13.056: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 19 00:08:13.067: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Jul 19 00:08:13.067: INFO: ss-0  controller-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  }]
    Jul 19 00:08:13.067: INFO: 
    Jul 19 00:08:13.067: INFO: StatefulSet ss has not reached scale 3, at 1
    Jul 19 00:08:14.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99482376s
    Jul 19 00:08:15.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991286334s
    Jul 19 00:08:16.077: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987935419s
    Jul 19 00:08:17.080: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984916602s
    Jul 19 00:08:18.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981949838s
    Jul 19 00:08:19.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97922527s
    Jul 19 00:08:20.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976141675s
    Jul 19 00:08:21.092: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.972988592s
    Jul 19 00:08:22.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 969.931683ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-600 07/19/23 00:08:23.096
    Jul 19 00:08:23.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 19 00:08:23.215: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 19 00:08:23.215: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 19 00:08:23.215: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 19 00:08:23.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 19 00:08:23.315: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jul 19 00:08:23.315: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 19 00:08:23.315: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 19 00:08:23.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 19 00:08:23.421: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jul 19 00:08:23.421: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 19 00:08:23.421: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 19 00:08:23.424: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 00:08:23.424: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 00:08:23.424: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 07/19/23 00:08:23.424
    Jul 19 00:08:23.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 19 00:08:23.527: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 19 00:08:23.527: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 19 00:08:23.527: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 19 00:08:23.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 19 00:08:23.637: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 19 00:08:23.637: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 19 00:08:23.637: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 19 00:08:23.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-600 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 19 00:08:23.740: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 19 00:08:23.740: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 19 00:08:23.740: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 19 00:08:23.740: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 19 00:08:23.742: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jul 19 00:08:33.750: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jul 19 00:08:33.750: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jul 19 00:08:33.750: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jul 19 00:08:33.756: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Jul 19 00:08:33.756: INFO: ss-0  controller-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  }]
    Jul 19 00:08:33.756: INFO: ss-1  controller-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  }]
    Jul 19 00:08:33.756: INFO: ss-2  controller-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  }]
    Jul 19 00:08:33.756: INFO: 
    Jul 19 00:08:33.756: INFO: StatefulSet ss has not reached scale 0, at 3
    Jul 19 00:08:34.761: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
    Jul 19 00:08:34.761: INFO: ss-0  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:07:52 +0000 UTC  }]
    Jul 19 00:08:34.761: INFO: ss-2  controller-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:24 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:08:13 +0000 UTC  }]
    Jul 19 00:08:34.761: INFO: 
    Jul 19 00:08:34.761: INFO: StatefulSet ss has not reached scale 0, at 2
    Jul 19 00:08:35.764: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992097849s
    Jul 19 00:08:36.767: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989140256s
    Jul 19 00:08:37.770: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98617638s
    Jul 19 00:08:38.773: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.983158159s
    Jul 19 00:08:39.776: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.980344401s
    Jul 19 00:08:40.778: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.97818082s
    Jul 19 00:08:41.780: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.976101386s
    Jul 19 00:08:42.783: INFO: Verifying statefulset ss doesn't scale past 0 for another 973.13365ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-600 07/19/23 00:08:43.784
    Jul 19 00:08:43.786: INFO: Scaling statefulset ss to 0
    Jul 19 00:08:43.793: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jul 19 00:08:43.795: INFO: Deleting all statefulset in ns statefulset-600
    Jul 19 00:08:43.796: INFO: Scaling statefulset ss to 0
    Jul 19 00:08:43.802: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 19 00:08:43.803: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:08:43.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-600" for this suite. 07/19/23 00:08:43.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:08:43.818
Jul 19 00:08:43.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename daemonsets 07/19/23 00:08:43.819
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:08:43.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:08:43.826
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 07/19/23 00:08:43.836
STEP: Check that daemon pods launch on every node of the cluster. 07/19/23 00:08:43.838
Jul 19 00:08:43.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:08:43.844: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:08:44.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 19 00:08:44.848: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:08:45.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 19 00:08:45.852: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 07/19/23 00:08:45.854
Jul 19 00:08:45.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 19 00:08:45.864: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 19 00:08:46.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 19 00:08:46.871: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 19 00:08:47.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 19 00:08:47.870: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 19 00:08:48.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 19 00:08:48.868: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:08:48.871
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2394, will wait for the garbage collector to delete the pods 07/19/23 00:08:48.871
Jul 19 00:08:48.929: INFO: Deleting DaemonSet.extensions daemon-set took: 6.197785ms
Jul 19 00:08:49.030: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.788139ms
Jul 19 00:08:51.833: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:08:51.833: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 19 00:08:51.835: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48589"},"items":null}

Jul 19 00:08:51.836: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48589"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:08:51.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2394" for this suite. 07/19/23 00:08:51.844
------------------------------
• [SLOW TEST] [8.028 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:08:43.818
    Jul 19 00:08:43.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename daemonsets 07/19/23 00:08:43.819
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:08:43.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:08:43.826
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 07/19/23 00:08:43.836
    STEP: Check that daemon pods launch on every node of the cluster. 07/19/23 00:08:43.838
    Jul 19 00:08:43.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:08:43.844: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:08:44.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 19 00:08:44.848: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:08:45.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 19 00:08:45.852: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 07/19/23 00:08:45.854
    Jul 19 00:08:45.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 19 00:08:45.864: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 19 00:08:46.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 19 00:08:46.871: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 19 00:08:47.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 19 00:08:47.870: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 19 00:08:48.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 19 00:08:48.868: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:08:48.871
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2394, will wait for the garbage collector to delete the pods 07/19/23 00:08:48.871
    Jul 19 00:08:48.929: INFO: Deleting DaemonSet.extensions daemon-set took: 6.197785ms
    Jul 19 00:08:49.030: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.788139ms
    Jul 19 00:08:51.833: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:08:51.833: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 19 00:08:51.835: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"48589"},"items":null}

    Jul 19 00:08:51.836: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"48589"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:08:51.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2394" for this suite. 07/19/23 00:08:51.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:08:51.846
Jul 19 00:08:51.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/19/23 00:08:51.847
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:08:51.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:08:51.854
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 07/19/23 00:08:51.856
STEP: submitting the pod to kubernetes 07/19/23 00:08:51.856
Jul 19 00:08:51.860: INFO: Waiting up to 5m0s for pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5" in namespace "pods-6458" to be "running and ready"
Jul 19 00:08:51.861: INFO: Pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.406387ms
Jul 19 00:08:51.861: INFO: The phase of Pod pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:08:53.865: INFO: Pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004852489s
Jul 19 00:08:53.865: INFO: The phase of Pod pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5 is Running (Ready = true)
Jul 19 00:08:53.865: INFO: Pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 07/19/23 00:08:53.866
STEP: updating the pod 07/19/23 00:08:53.868
Jul 19 00:08:54.374: INFO: Successfully updated pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5"
Jul 19 00:08:54.374: INFO: Waiting up to 5m0s for pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5" in namespace "pods-6458" to be "running"
Jul 19 00:08:54.377: INFO: Pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5": Phase="Running", Reason="", readiness=true. Elapsed: 3.295801ms
Jul 19 00:08:54.377: INFO: Pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 07/19/23 00:08:54.377
Jul 19 00:08:54.379: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jul 19 00:08:54.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6458" for this suite. 07/19/23 00:08:54.381
------------------------------
• [2.537 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:08:51.846
    Jul 19 00:08:51.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/19/23 00:08:51.847
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:08:51.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:08:51.854
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 07/19/23 00:08:51.856
    STEP: submitting the pod to kubernetes 07/19/23 00:08:51.856
    Jul 19 00:08:51.860: INFO: Waiting up to 5m0s for pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5" in namespace "pods-6458" to be "running and ready"
    Jul 19 00:08:51.861: INFO: Pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.406387ms
    Jul 19 00:08:51.861: INFO: The phase of Pod pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:08:53.865: INFO: Pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.004852489s
    Jul 19 00:08:53.865: INFO: The phase of Pod pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5 is Running (Ready = true)
    Jul 19 00:08:53.865: INFO: Pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 07/19/23 00:08:53.866
    STEP: updating the pod 07/19/23 00:08:53.868
    Jul 19 00:08:54.374: INFO: Successfully updated pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5"
    Jul 19 00:08:54.374: INFO: Waiting up to 5m0s for pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5" in namespace "pods-6458" to be "running"
    Jul 19 00:08:54.377: INFO: Pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5": Phase="Running", Reason="", readiness=true. Elapsed: 3.295801ms
    Jul 19 00:08:54.377: INFO: Pod "pod-update-05e7161a-5d12-46bf-b42f-fa5a3314c2f5" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 07/19/23 00:08:54.377
    Jul 19 00:08:54.379: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:08:54.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6458" for this suite. 07/19/23 00:08:54.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:08:54.386
Jul 19 00:08:54.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replication-controller 07/19/23 00:08:54.387
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:08:54.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:08:54.393
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 07/19/23 00:08:54.395
STEP: When the matched label of one of its pods change 07/19/23 00:08:54.397
Jul 19 00:08:54.399: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 19 00:08:59.404: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 07/19/23 00:08:59.409
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jul 19 00:09:00.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4579" for this suite. 07/19/23 00:09:00.416
------------------------------
• [SLOW TEST] [6.033 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:08:54.386
    Jul 19 00:08:54.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replication-controller 07/19/23 00:08:54.387
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:08:54.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:08:54.393
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 07/19/23 00:08:54.395
    STEP: When the matched label of one of its pods change 07/19/23 00:08:54.397
    Jul 19 00:08:54.399: INFO: Pod name pod-release: Found 0 pods out of 1
    Jul 19 00:08:59.404: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 07/19/23 00:08:59.409
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:09:00.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4579" for this suite. 07/19/23 00:09:00.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:09:00.419
Jul 19 00:09:00.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:09:00.42
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:00.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:00.43
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:09:00.437
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:09:00.869
STEP: Deploying the webhook pod 07/19/23 00:09:00.873
STEP: Wait for the deployment to be ready 07/19/23 00:09:00.879
Jul 19 00:09:00.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:09:02.894
STEP: Verifying the service has paired with the endpoint 07/19/23 00:09:02.904
Jul 19 00:09:03.904: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jul 19 00:09:03.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Registering the custom resource webhook via the AdmissionRegistration API 07/19/23 00:09:04.412
STEP: Creating a custom resource that should be denied by the webhook 07/19/23 00:09:04.421
STEP: Creating a custom resource whose deletion would be denied by the webhook 07/19/23 00:09:06.472
STEP: Updating the custom resource with disallowed data should be denied 07/19/23 00:09:06.677
STEP: Deleting the custom resource should be denied 07/19/23 00:09:06.682
STEP: Remove the offending key and value from the custom resource data 07/19/23 00:09:06.686
STEP: Deleting the updated custom resource should be successful 07/19/23 00:09:06.694
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:09:07.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9173" for this suite. 07/19/23 00:09:07.237
STEP: Destroying namespace "webhook-9173-markers" for this suite. 07/19/23 00:09:07.245
------------------------------
• [SLOW TEST] [6.835 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:09:00.419
    Jul 19 00:09:00.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:09:00.42
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:00.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:00.43
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:09:00.437
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:09:00.869
    STEP: Deploying the webhook pod 07/19/23 00:09:00.873
    STEP: Wait for the deployment to be ready 07/19/23 00:09:00.879
    Jul 19 00:09:00.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:09:02.894
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:09:02.904
    Jul 19 00:09:03.904: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jul 19 00:09:03.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 07/19/23 00:09:04.412
    STEP: Creating a custom resource that should be denied by the webhook 07/19/23 00:09:04.421
    STEP: Creating a custom resource whose deletion would be denied by the webhook 07/19/23 00:09:06.472
    STEP: Updating the custom resource with disallowed data should be denied 07/19/23 00:09:06.677
    STEP: Deleting the custom resource should be denied 07/19/23 00:09:06.682
    STEP: Remove the offending key and value from the custom resource data 07/19/23 00:09:06.686
    STEP: Deleting the updated custom resource should be successful 07/19/23 00:09:06.694
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:09:07.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9173" for this suite. 07/19/23 00:09:07.237
    STEP: Destroying namespace "webhook-9173-markers" for this suite. 07/19/23 00:09:07.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:09:07.254
Jul 19 00:09:07.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:09:07.255
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:07.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:07.263
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2162 07/19/23 00:09:07.265
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 07/19/23 00:09:07.274
STEP: creating service externalsvc in namespace services-2162 07/19/23 00:09:07.274
STEP: creating replication controller externalsvc in namespace services-2162 07/19/23 00:09:07.284
I0719 00:09:07.289207      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2162, replica count: 2
I0719 00:09:10.340416      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 07/19/23 00:09:10.342
Jul 19 00:09:10.356: INFO: Creating new exec pod
Jul 19 00:09:10.359: INFO: Waiting up to 5m0s for pod "execpod4sldg" in namespace "services-2162" to be "running"
Jul 19 00:09:10.361: INFO: Pod "execpod4sldg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.406837ms
Jul 19 00:09:12.363: INFO: Pod "execpod4sldg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004138435s
Jul 19 00:09:12.363: INFO: Pod "execpod4sldg" satisfied condition "running"
Jul 19 00:09:12.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-2162 exec execpod4sldg -- /bin/sh -x -c nslookup nodeport-service.services-2162.svc.cluster.local'
Jul 19 00:09:12.484: INFO: stderr: "+ nslookup nodeport-service.services-2162.svc.cluster.local\n"
Jul 19 00:09:12.484: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2162.svc.cluster.local\tcanonical name = externalsvc.services-2162.svc.cluster.local.\nName:\texternalsvc.services-2162.svc.cluster.local\nAddress: 10.109.152.136\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2162, will wait for the garbage collector to delete the pods 07/19/23 00:09:12.484
Jul 19 00:09:12.539: INFO: Deleting ReplicationController externalsvc took: 2.760597ms
Jul 19 00:09:12.639: INFO: Terminating ReplicationController externalsvc pods took: 100.643873ms
Jul 19 00:09:14.547: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:09:14.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2162" for this suite. 07/19/23 00:09:14.561
------------------------------
• [SLOW TEST] [7.309 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:09:07.254
    Jul 19 00:09:07.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:09:07.255
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:07.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:07.263
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-2162 07/19/23 00:09:07.265
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 07/19/23 00:09:07.274
    STEP: creating service externalsvc in namespace services-2162 07/19/23 00:09:07.274
    STEP: creating replication controller externalsvc in namespace services-2162 07/19/23 00:09:07.284
    I0719 00:09:07.289207      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2162, replica count: 2
    I0719 00:09:10.340416      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 07/19/23 00:09:10.342
    Jul 19 00:09:10.356: INFO: Creating new exec pod
    Jul 19 00:09:10.359: INFO: Waiting up to 5m0s for pod "execpod4sldg" in namespace "services-2162" to be "running"
    Jul 19 00:09:10.361: INFO: Pod "execpod4sldg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.406837ms
    Jul 19 00:09:12.363: INFO: Pod "execpod4sldg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004138435s
    Jul 19 00:09:12.363: INFO: Pod "execpod4sldg" satisfied condition "running"
    Jul 19 00:09:12.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-2162 exec execpod4sldg -- /bin/sh -x -c nslookup nodeport-service.services-2162.svc.cluster.local'
    Jul 19 00:09:12.484: INFO: stderr: "+ nslookup nodeport-service.services-2162.svc.cluster.local\n"
    Jul 19 00:09:12.484: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2162.svc.cluster.local\tcanonical name = externalsvc.services-2162.svc.cluster.local.\nName:\texternalsvc.services-2162.svc.cluster.local\nAddress: 10.109.152.136\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2162, will wait for the garbage collector to delete the pods 07/19/23 00:09:12.484
    Jul 19 00:09:12.539: INFO: Deleting ReplicationController externalsvc took: 2.760597ms
    Jul 19 00:09:12.639: INFO: Terminating ReplicationController externalsvc pods took: 100.643873ms
    Jul 19 00:09:14.547: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:09:14.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2162" for this suite. 07/19/23 00:09:14.561
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:09:14.564
Jul 19 00:09:14.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:09:14.564
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:14.57
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:14.572
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-8226/configmap-test-439a4428-f677-4ef3-b3f4-c9c9961608f2 07/19/23 00:09:14.574
STEP: Creating a pod to test consume configMaps 07/19/23 00:09:14.576
Jul 19 00:09:14.580: INFO: Waiting up to 5m0s for pod "pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71" in namespace "configmap-8226" to be "Succeeded or Failed"
Jul 19 00:09:14.582: INFO: Pod "pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71": Phase="Pending", Reason="", readiness=false. Elapsed: 1.579168ms
Jul 19 00:09:16.584: INFO: Pod "pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003648327s
Jul 19 00:09:18.584: INFO: Pod "pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003827704s
STEP: Saw pod success 07/19/23 00:09:18.584
Jul 19 00:09:18.584: INFO: Pod "pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71" satisfied condition "Succeeded or Failed"
Jul 19 00:09:18.586: INFO: Trying to get logs from node controller-0 pod pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71 container env-test: <nil>
STEP: delete the pod 07/19/23 00:09:18.59
Jul 19 00:09:18.596: INFO: Waiting for pod pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71 to disappear
Jul 19 00:09:18.598: INFO: Pod pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:09:18.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8226" for this suite. 07/19/23 00:09:18.6
------------------------------
• [4.039 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:09:14.564
    Jul 19 00:09:14.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:09:14.564
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:14.57
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:14.572
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-8226/configmap-test-439a4428-f677-4ef3-b3f4-c9c9961608f2 07/19/23 00:09:14.574
    STEP: Creating a pod to test consume configMaps 07/19/23 00:09:14.576
    Jul 19 00:09:14.580: INFO: Waiting up to 5m0s for pod "pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71" in namespace "configmap-8226" to be "Succeeded or Failed"
    Jul 19 00:09:14.582: INFO: Pod "pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71": Phase="Pending", Reason="", readiness=false. Elapsed: 1.579168ms
    Jul 19 00:09:16.584: INFO: Pod "pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003648327s
    Jul 19 00:09:18.584: INFO: Pod "pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003827704s
    STEP: Saw pod success 07/19/23 00:09:18.584
    Jul 19 00:09:18.584: INFO: Pod "pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71" satisfied condition "Succeeded or Failed"
    Jul 19 00:09:18.586: INFO: Trying to get logs from node controller-0 pod pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71 container env-test: <nil>
    STEP: delete the pod 07/19/23 00:09:18.59
    Jul 19 00:09:18.596: INFO: Waiting for pod pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71 to disappear
    Jul 19 00:09:18.598: INFO: Pod pod-configmaps-9c826d70-2b4e-49aa-8b17-2afa9ab9fc71 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:09:18.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8226" for this suite. 07/19/23 00:09:18.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:09:18.603
Jul 19 00:09:18.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename custom-resource-definition 07/19/23 00:09:18.603
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:18.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:18.61
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jul 19 00:09:18.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:09:24.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6785" for this suite. 07/19/23 00:09:24.822
------------------------------
• [SLOW TEST] [6.222 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:09:18.603
    Jul 19 00:09:18.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename custom-resource-definition 07/19/23 00:09:18.603
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:18.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:18.61
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jul 19 00:09:18.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:09:24.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6785" for this suite. 07/19/23 00:09:24.822
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:09:24.826
Jul 19 00:09:24.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename limitrange 07/19/23 00:09:24.826
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:24.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:24.836
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 07/19/23 00:09:24.838
STEP: Setting up watch 07/19/23 00:09:24.838
STEP: Submitting a LimitRange 07/19/23 00:09:24.94
STEP: Verifying LimitRange creation was observed 07/19/23 00:09:24.943
STEP: Fetching the LimitRange to ensure it has proper values 07/19/23 00:09:24.943
Jul 19 00:09:24.944: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 19 00:09:24.944: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 07/19/23 00:09:24.944
STEP: Ensuring Pod has resource requirements applied from LimitRange 07/19/23 00:09:24.947
Jul 19 00:09:24.950: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 19 00:09:24.950: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 07/19/23 00:09:24.95
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 07/19/23 00:09:24.954
Jul 19 00:09:24.955: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul 19 00:09:24.955: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 07/19/23 00:09:24.955
STEP: Failing to create a Pod with more than max resources 07/19/23 00:09:24.957
STEP: Updating a LimitRange 07/19/23 00:09:24.958
STEP: Verifying LimitRange updating is effective 07/19/23 00:09:24.961
STEP: Creating a Pod with less than former min resources 07/19/23 00:09:26.965
STEP: Failing to create a Pod with more than max resources 07/19/23 00:09:26.968
STEP: Deleting a LimitRange 07/19/23 00:09:26.969
STEP: Verifying the LimitRange was deleted 07/19/23 00:09:26.975
Jul 19 00:09:31.979: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 07/19/23 00:09:31.979
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jul 19 00:09:31.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-1306" for this suite. 07/19/23 00:09:31.985
------------------------------
• [SLOW TEST] [7.164 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:09:24.826
    Jul 19 00:09:24.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename limitrange 07/19/23 00:09:24.826
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:24.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:24.836
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 07/19/23 00:09:24.838
    STEP: Setting up watch 07/19/23 00:09:24.838
    STEP: Submitting a LimitRange 07/19/23 00:09:24.94
    STEP: Verifying LimitRange creation was observed 07/19/23 00:09:24.943
    STEP: Fetching the LimitRange to ensure it has proper values 07/19/23 00:09:24.943
    Jul 19 00:09:24.944: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jul 19 00:09:24.944: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 07/19/23 00:09:24.944
    STEP: Ensuring Pod has resource requirements applied from LimitRange 07/19/23 00:09:24.947
    Jul 19 00:09:24.950: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jul 19 00:09:24.950: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 07/19/23 00:09:24.95
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 07/19/23 00:09:24.954
    Jul 19 00:09:24.955: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jul 19 00:09:24.955: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 07/19/23 00:09:24.955
    STEP: Failing to create a Pod with more than max resources 07/19/23 00:09:24.957
    STEP: Updating a LimitRange 07/19/23 00:09:24.958
    STEP: Verifying LimitRange updating is effective 07/19/23 00:09:24.961
    STEP: Creating a Pod with less than former min resources 07/19/23 00:09:26.965
    STEP: Failing to create a Pod with more than max resources 07/19/23 00:09:26.968
    STEP: Deleting a LimitRange 07/19/23 00:09:26.969
    STEP: Verifying the LimitRange was deleted 07/19/23 00:09:26.975
    Jul 19 00:09:31.979: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 07/19/23 00:09:31.979
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:09:31.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-1306" for this suite. 07/19/23 00:09:31.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:09:31.991
Jul 19 00:09:31.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:09:31.992
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:32.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:32.003
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 07/19/23 00:09:32.005
Jul 19 00:09:32.008: INFO: Waiting up to 5m0s for pod "pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0" in namespace "emptydir-8347" to be "Succeeded or Failed"
Jul 19 00:09:32.010: INFO: Pod "pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.795258ms
Jul 19 00:09:34.013: INFO: Pod "pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004410756s
Jul 19 00:09:36.013: INFO: Pod "pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004410096s
STEP: Saw pod success 07/19/23 00:09:36.013
Jul 19 00:09:36.013: INFO: Pod "pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0" satisfied condition "Succeeded or Failed"
Jul 19 00:09:36.014: INFO: Trying to get logs from node controller-1 pod pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0 container test-container: <nil>
STEP: delete the pod 07/19/23 00:09:36.025
Jul 19 00:09:36.032: INFO: Waiting for pod pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0 to disappear
Jul 19 00:09:36.034: INFO: Pod pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:09:36.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8347" for this suite. 07/19/23 00:09:36.036
------------------------------
• [4.048 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:09:31.991
    Jul 19 00:09:31.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:09:31.992
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:32.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:32.003
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 07/19/23 00:09:32.005
    Jul 19 00:09:32.008: INFO: Waiting up to 5m0s for pod "pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0" in namespace "emptydir-8347" to be "Succeeded or Failed"
    Jul 19 00:09:32.010: INFO: Pod "pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.795258ms
    Jul 19 00:09:34.013: INFO: Pod "pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004410756s
    Jul 19 00:09:36.013: INFO: Pod "pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004410096s
    STEP: Saw pod success 07/19/23 00:09:36.013
    Jul 19 00:09:36.013: INFO: Pod "pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0" satisfied condition "Succeeded or Failed"
    Jul 19 00:09:36.014: INFO: Trying to get logs from node controller-1 pod pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0 container test-container: <nil>
    STEP: delete the pod 07/19/23 00:09:36.025
    Jul 19 00:09:36.032: INFO: Waiting for pod pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0 to disappear
    Jul 19 00:09:36.034: INFO: Pod pod-eba127ab-e6f8-4f7c-bde4-fba5daebcfd0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:09:36.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8347" for this suite. 07/19/23 00:09:36.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:09:36.039
Jul 19 00:09:36.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename watch 07/19/23 00:09:36.04
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:36.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:36.047
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 07/19/23 00:09:36.049
STEP: creating a new configmap 07/19/23 00:09:36.05
STEP: modifying the configmap once 07/19/23 00:09:36.052
STEP: changing the label value of the configmap 07/19/23 00:09:36.055
STEP: Expecting to observe a delete notification for the watched object 07/19/23 00:09:36.058
Jul 19 00:09:36.058: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49327 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:09:36.058: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49328 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:09:36.059: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49329 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 07/19/23 00:09:36.059
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 07/19/23 00:09:36.062
STEP: changing the label value of the configmap back 07/19/23 00:09:46.063
STEP: modifying the configmap a third time 07/19/23 00:09:46.068
STEP: deleting the configmap 07/19/23 00:09:46.072
STEP: Expecting to observe an add notification for the watched object when the label value was restored 07/19/23 00:09:46.078
Jul 19 00:09:46.078: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49413 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:09:46.078: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49414 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:09:46.078: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49415 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jul 19 00:09:46.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8892" for this suite. 07/19/23 00:09:46.081
------------------------------
• [SLOW TEST] [10.044 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:09:36.039
    Jul 19 00:09:36.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename watch 07/19/23 00:09:36.04
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:36.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:36.047
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 07/19/23 00:09:36.049
    STEP: creating a new configmap 07/19/23 00:09:36.05
    STEP: modifying the configmap once 07/19/23 00:09:36.052
    STEP: changing the label value of the configmap 07/19/23 00:09:36.055
    STEP: Expecting to observe a delete notification for the watched object 07/19/23 00:09:36.058
    Jul 19 00:09:36.058: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49327 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:09:36.058: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49328 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:09:36.059: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49329 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 07/19/23 00:09:36.059
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 07/19/23 00:09:36.062
    STEP: changing the label value of the configmap back 07/19/23 00:09:46.063
    STEP: modifying the configmap a third time 07/19/23 00:09:46.068
    STEP: deleting the configmap 07/19/23 00:09:46.072
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 07/19/23 00:09:46.078
    Jul 19 00:09:46.078: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49413 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:09:46.078: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49414 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:09:46.078: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8892  eba3e539-4651-42e3-8654-90f9212f6959 49415 0 2023-07-19 00:09:36 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-19 00:09:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:09:46.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8892" for this suite. 07/19/23 00:09:46.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:09:46.084
Jul 19 00:09:46.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sysctl 07/19/23 00:09:46.085
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:46.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:46.096
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 07/19/23 00:09:46.099
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:09:46.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-4942" for this suite. 07/19/23 00:09:46.103
------------------------------
• [0.021 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:09:46.084
    Jul 19 00:09:46.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sysctl 07/19/23 00:09:46.085
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:46.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:46.096
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 07/19/23 00:09:46.099
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:09:46.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-4942" for this suite. 07/19/23 00:09:46.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:09:46.107
Jul 19 00:09:46.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:09:46.108
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:46.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:46.115
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:09:46.123
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:09:46.501
STEP: Deploying the webhook pod 07/19/23 00:09:46.504
STEP: Wait for the deployment to be ready 07/19/23 00:09:46.512
Jul 19 00:09:46.519: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:09:48.525
STEP: Verifying the service has paired with the endpoint 07/19/23 00:09:48.535
Jul 19 00:09:49.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 07/19/23 00:09:49.538
STEP: Creating a custom resource definition that should be denied by the webhook 07/19/23 00:09:49.548
Jul 19 00:09:49.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:09:49.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6466" for this suite. 07/19/23 00:09:49.58
STEP: Destroying namespace "webhook-6466-markers" for this suite. 07/19/23 00:09:49.582
------------------------------
• [3.482 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:09:46.107
    Jul 19 00:09:46.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:09:46.108
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:46.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:46.115
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:09:46.123
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:09:46.501
    STEP: Deploying the webhook pod 07/19/23 00:09:46.504
    STEP: Wait for the deployment to be ready 07/19/23 00:09:46.512
    Jul 19 00:09:46.519: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:09:48.525
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:09:48.535
    Jul 19 00:09:49.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 07/19/23 00:09:49.538
    STEP: Creating a custom resource definition that should be denied by the webhook 07/19/23 00:09:49.548
    Jul 19 00:09:49.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:09:49.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6466" for this suite. 07/19/23 00:09:49.58
    STEP: Destroying namespace "webhook-6466-markers" for this suite. 07/19/23 00:09:49.582
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:09:49.59
Jul 19 00:09:49.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename job 07/19/23 00:09:49.59
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:49.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:49.597
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 07/19/23 00:09:49.599
STEP: Ensuring active pods == parallelism 07/19/23 00:09:49.601
STEP: delete a job 07/19/23 00:09:51.604
STEP: deleting Job.batch foo in namespace job-908, will wait for the garbage collector to delete the pods 07/19/23 00:09:51.604
Jul 19 00:09:51.660: INFO: Deleting Job.batch foo took: 3.727191ms
Jul 19 00:09:51.760: INFO: Terminating Job.batch foo pods took: 100.4024ms
STEP: Ensuring job was deleted 07/19/23 00:10:24.161
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jul 19 00:10:24.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-908" for this suite. 07/19/23 00:10:24.165
------------------------------
• [SLOW TEST] [34.579 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:09:49.59
    Jul 19 00:09:49.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename job 07/19/23 00:09:49.59
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:09:49.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:09:49.597
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 07/19/23 00:09:49.599
    STEP: Ensuring active pods == parallelism 07/19/23 00:09:49.601
    STEP: delete a job 07/19/23 00:09:51.604
    STEP: deleting Job.batch foo in namespace job-908, will wait for the garbage collector to delete the pods 07/19/23 00:09:51.604
    Jul 19 00:09:51.660: INFO: Deleting Job.batch foo took: 3.727191ms
    Jul 19 00:09:51.760: INFO: Terminating Job.batch foo pods took: 100.4024ms
    STEP: Ensuring job was deleted 07/19/23 00:10:24.161
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:10:24.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-908" for this suite. 07/19/23 00:10:24.165
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:10:24.169
Jul 19 00:10:24.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 00:10:24.169
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:24.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:24.177
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-cbe9c489-4a16-4d70-b5bc-4ecba71fc823 07/19/23 00:10:24.179
STEP: Creating a pod to test consume secrets 07/19/23 00:10:24.181
Jul 19 00:10:24.185: INFO: Waiting up to 5m0s for pod "pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3" in namespace "secrets-1945" to be "Succeeded or Failed"
Jul 19 00:10:24.189: INFO: Pod "pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.535441ms
Jul 19 00:10:26.191: INFO: Pod "pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006135515s
Jul 19 00:10:28.192: INFO: Pod "pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007044806s
STEP: Saw pod success 07/19/23 00:10:28.192
Jul 19 00:10:28.192: INFO: Pod "pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3" satisfied condition "Succeeded or Failed"
Jul 19 00:10:28.194: INFO: Trying to get logs from node controller-1 pod pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3 container secret-volume-test: <nil>
STEP: delete the pod 07/19/23 00:10:28.198
Jul 19 00:10:28.203: INFO: Waiting for pod pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3 to disappear
Jul 19 00:10:28.205: INFO: Pod pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 00:10:28.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1945" for this suite. 07/19/23 00:10:28.207
------------------------------
• [4.042 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:10:24.169
    Jul 19 00:10:24.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 00:10:24.169
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:24.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:24.177
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-cbe9c489-4a16-4d70-b5bc-4ecba71fc823 07/19/23 00:10:24.179
    STEP: Creating a pod to test consume secrets 07/19/23 00:10:24.181
    Jul 19 00:10:24.185: INFO: Waiting up to 5m0s for pod "pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3" in namespace "secrets-1945" to be "Succeeded or Failed"
    Jul 19 00:10:24.189: INFO: Pod "pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.535441ms
    Jul 19 00:10:26.191: INFO: Pod "pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006135515s
    Jul 19 00:10:28.192: INFO: Pod "pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007044806s
    STEP: Saw pod success 07/19/23 00:10:28.192
    Jul 19 00:10:28.192: INFO: Pod "pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3" satisfied condition "Succeeded or Failed"
    Jul 19 00:10:28.194: INFO: Trying to get logs from node controller-1 pod pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3 container secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:10:28.198
    Jul 19 00:10:28.203: INFO: Waiting for pod pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3 to disappear
    Jul 19 00:10:28.205: INFO: Pod pod-secrets-b8c6cff6-085a-4187-bf0f-cd77b7c00dc3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:10:28.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1945" for this suite. 07/19/23 00:10:28.207
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:10:28.21
Jul 19 00:10:28.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename job 07/19/23 00:10:28.211
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:28.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:28.22
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 07/19/23 00:10:28.222
STEP: Ensuring job reaches completions 07/19/23 00:10:28.225
STEP: Ensuring pods with index for job exist 07/19/23 00:10:36.228
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jul 19 00:10:36.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1738" for this suite. 07/19/23 00:10:36.235
------------------------------
• [SLOW TEST] [8.027 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:10:28.21
    Jul 19 00:10:28.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename job 07/19/23 00:10:28.211
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:28.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:28.22
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 07/19/23 00:10:28.222
    STEP: Ensuring job reaches completions 07/19/23 00:10:28.225
    STEP: Ensuring pods with index for job exist 07/19/23 00:10:36.228
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:10:36.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1738" for this suite. 07/19/23 00:10:36.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:10:36.239
Jul 19 00:10:36.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:10:36.239
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:36.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:36.25
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 07/19/23 00:10:36.252
Jul 19 00:10:36.252: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2047 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 07/19/23 00:10:36.298
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:10:36.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2047" for this suite. 07/19/23 00:10:36.307
------------------------------
• [0.070 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:10:36.239
    Jul 19 00:10:36.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:10:36.239
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:36.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:36.25
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 07/19/23 00:10:36.252
    Jul 19 00:10:36.252: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2047 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 07/19/23 00:10:36.298
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:10:36.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2047" for this suite. 07/19/23 00:10:36.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:10:36.31
Jul 19 00:10:36.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:10:36.311
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:36.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:36.317
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jul 19 00:10:36.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5876 version'
Jul 19 00:10:36.377: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jul 19 00:10:36.377: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:51:25Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:10:36.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5876" for this suite. 07/19/23 00:10:36.379
------------------------------
• [0.072 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:10:36.31
    Jul 19 00:10:36.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:10:36.311
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:36.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:36.317
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jul 19 00:10:36.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5876 version'
    Jul 19 00:10:36.377: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jul 19 00:10:36.377: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:58:16Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.1\", GitCommit:\"8f94681cd294aa8cfd3407b8191f6c70214973a4\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T15:51:25Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:10:36.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5876" for this suite. 07/19/23 00:10:36.379
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:10:36.382
Jul 19 00:10:36.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:10:36.382
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:36.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:36.389
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 07/19/23 00:10:36.391
Jul 19 00:10:36.394: INFO: Waiting up to 5m0s for pod "pod-c94a5eff-fcfd-4520-b884-5a665d649197" in namespace "emptydir-4367" to be "Succeeded or Failed"
Jul 19 00:10:36.398: INFO: Pod "pod-c94a5eff-fcfd-4520-b884-5a665d649197": Phase="Pending", Reason="", readiness=false. Elapsed: 3.117299ms
Jul 19 00:10:38.400: INFO: Pod "pod-c94a5eff-fcfd-4520-b884-5a665d649197": Phase="Running", Reason="", readiness=false. Elapsed: 2.005309431s
Jul 19 00:10:40.399: INFO: Pod "pod-c94a5eff-fcfd-4520-b884-5a665d649197": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004876723s
STEP: Saw pod success 07/19/23 00:10:40.399
Jul 19 00:10:40.399: INFO: Pod "pod-c94a5eff-fcfd-4520-b884-5a665d649197" satisfied condition "Succeeded or Failed"
Jul 19 00:10:40.401: INFO: Trying to get logs from node controller-1 pod pod-c94a5eff-fcfd-4520-b884-5a665d649197 container test-container: <nil>
STEP: delete the pod 07/19/23 00:10:40.405
Jul 19 00:10:40.412: INFO: Waiting for pod pod-c94a5eff-fcfd-4520-b884-5a665d649197 to disappear
Jul 19 00:10:40.413: INFO: Pod pod-c94a5eff-fcfd-4520-b884-5a665d649197 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:10:40.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4367" for this suite. 07/19/23 00:10:40.416
------------------------------
• [4.036 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:10:36.382
    Jul 19 00:10:36.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:10:36.382
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:36.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:36.389
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 07/19/23 00:10:36.391
    Jul 19 00:10:36.394: INFO: Waiting up to 5m0s for pod "pod-c94a5eff-fcfd-4520-b884-5a665d649197" in namespace "emptydir-4367" to be "Succeeded or Failed"
    Jul 19 00:10:36.398: INFO: Pod "pod-c94a5eff-fcfd-4520-b884-5a665d649197": Phase="Pending", Reason="", readiness=false. Elapsed: 3.117299ms
    Jul 19 00:10:38.400: INFO: Pod "pod-c94a5eff-fcfd-4520-b884-5a665d649197": Phase="Running", Reason="", readiness=false. Elapsed: 2.005309431s
    Jul 19 00:10:40.399: INFO: Pod "pod-c94a5eff-fcfd-4520-b884-5a665d649197": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004876723s
    STEP: Saw pod success 07/19/23 00:10:40.399
    Jul 19 00:10:40.399: INFO: Pod "pod-c94a5eff-fcfd-4520-b884-5a665d649197" satisfied condition "Succeeded or Failed"
    Jul 19 00:10:40.401: INFO: Trying to get logs from node controller-1 pod pod-c94a5eff-fcfd-4520-b884-5a665d649197 container test-container: <nil>
    STEP: delete the pod 07/19/23 00:10:40.405
    Jul 19 00:10:40.412: INFO: Waiting for pod pod-c94a5eff-fcfd-4520-b884-5a665d649197 to disappear
    Jul 19 00:10:40.413: INFO: Pod pod-c94a5eff-fcfd-4520-b884-5a665d649197 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:10:40.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4367" for this suite. 07/19/23 00:10:40.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:10:40.419
Jul 19 00:10:40.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename gc 07/19/23 00:10:40.419
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:40.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:40.427
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 07/19/23 00:10:40.431
STEP: create the rc2 07/19/23 00:10:40.434
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 07/19/23 00:10:45.442
STEP: delete the rc simpletest-rc-to-be-deleted 07/19/23 00:10:45.881
STEP: wait for the rc to be deleted 07/19/23 00:10:45.886
Jul 19 00:10:50.898: INFO: 67 pods remaining
Jul 19 00:10:50.898: INFO: 67 pods has nil DeletionTimestamp
Jul 19 00:10:50.898: INFO: 
STEP: Gathering metrics 07/19/23 00:10:55.895
Jul 19 00:10:55.906: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
Jul 19 00:10:55.908: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.818485ms
Jul 19 00:10:55.908: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
Jul 19 00:10:55.908: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
Jul 19 00:10:56.365: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 19 00:10:56.365: INFO: Deleting pod "simpletest-rc-to-be-deleted-2q5gz" in namespace "gc-3597"
Jul 19 00:10:56.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-2q8g9" in namespace "gc-3597"
Jul 19 00:10:56.378: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vx8t" in namespace "gc-3597"
Jul 19 00:10:56.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-4j4xx" in namespace "gc-3597"
Jul 19 00:10:56.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bpm2" in namespace "gc-3597"
Jul 19 00:10:56.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qwvd" in namespace "gc-3597"
Jul 19 00:10:56.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-64llr" in namespace "gc-3597"
Jul 19 00:10:56.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k5m8" in namespace "gc-3597"
Jul 19 00:10:56.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-6td46" in namespace "gc-3597"
Jul 19 00:10:56.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ttt9" in namespace "gc-3597"
Jul 19 00:10:56.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-75f9d" in namespace "gc-3597"
Jul 19 00:10:56.462: INFO: Deleting pod "simpletest-rc-to-be-deleted-77l4x" in namespace "gc-3597"
Jul 19 00:10:56.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lfpr" in namespace "gc-3597"
Jul 19 00:10:56.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n79k" in namespace "gc-3597"
Jul 19 00:10:56.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vcqd" in namespace "gc-3597"
Jul 19 00:10:56.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xcmq" in namespace "gc-3597"
Jul 19 00:10:56.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-82bf9" in namespace "gc-3597"
Jul 19 00:10:56.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-86kg4" in namespace "gc-3597"
Jul 19 00:10:56.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f49p" in namespace "gc-3597"
Jul 19 00:10:56.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-8schg" in namespace "gc-3597"
Jul 19 00:10:56.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-8spb9" in namespace "gc-3597"
Jul 19 00:10:56.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h8n2" in namespace "gc-3597"
Jul 19 00:10:56.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jc5d" in namespace "gc-3597"
Jul 19 00:10:56.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s9wf" in namespace "gc-3597"
Jul 19 00:10:56.597: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sjhr" in namespace "gc-3597"
Jul 19 00:10:56.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8wls" in namespace "gc-3597"
Jul 19 00:10:56.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfsw2" in namespace "gc-3597"
Jul 19 00:10:56.633: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfzp5" in namespace "gc-3597"
Jul 19 00:10:56.641: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm62p" in namespace "gc-3597"
Jul 19 00:10:56.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwvz2" in namespace "gc-3597"
Jul 19 00:10:56.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-cf852" in namespace "gc-3597"
Jul 19 00:10:56.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfl2z" in namespace "gc-3597"
Jul 19 00:10:56.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-chxbp" in namespace "gc-3597"
Jul 19 00:10:56.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4wjt" in namespace "gc-3597"
Jul 19 00:10:56.719: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbgvv" in namespace "gc-3597"
Jul 19 00:10:56.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc45g" in namespace "gc-3597"
Jul 19 00:10:56.780: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqrnq" in namespace "gc-3597"
Jul 19 00:10:56.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj55f" in namespace "gc-3597"
Jul 19 00:10:56.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjltz" in namespace "gc-3597"
Jul 19 00:10:56.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkx7r" in namespace "gc-3597"
Jul 19 00:10:56.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkxm5" in namespace "gc-3597"
Jul 19 00:10:56.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftjl5" in namespace "gc-3597"
Jul 19 00:10:56.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-fz2xn" in namespace "gc-3597"
Jul 19 00:10:56.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-gm4zb" in namespace "gc-3597"
Jul 19 00:10:56.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmqs6" in namespace "gc-3597"
Jul 19 00:10:56.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-gn6kf" in namespace "gc-3597"
Jul 19 00:10:57.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-grrm4" in namespace "gc-3597"
Jul 19 00:10:57.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2zmg" in namespace "gc-3597"
Jul 19 00:10:57.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-h869t" in namespace "gc-3597"
Jul 19 00:10:57.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-hggwq" in namespace "gc-3597"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jul 19 00:10:57.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3597" for this suite. 07/19/23 00:10:57.047
------------------------------
• [SLOW TEST] [16.642 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:10:40.419
    Jul 19 00:10:40.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename gc 07/19/23 00:10:40.419
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:40.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:40.427
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 07/19/23 00:10:40.431
    STEP: create the rc2 07/19/23 00:10:40.434
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 07/19/23 00:10:45.442
    STEP: delete the rc simpletest-rc-to-be-deleted 07/19/23 00:10:45.881
    STEP: wait for the rc to be deleted 07/19/23 00:10:45.886
    Jul 19 00:10:50.898: INFO: 67 pods remaining
    Jul 19 00:10:50.898: INFO: 67 pods has nil DeletionTimestamp
    Jul 19 00:10:50.898: INFO: 
    STEP: Gathering metrics 07/19/23 00:10:55.895
    Jul 19 00:10:55.906: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
    Jul 19 00:10:55.908: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.818485ms
    Jul 19 00:10:55.908: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
    Jul 19 00:10:55.908: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
    Jul 19 00:10:56.365: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jul 19 00:10:56.365: INFO: Deleting pod "simpletest-rc-to-be-deleted-2q5gz" in namespace "gc-3597"
    Jul 19 00:10:56.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-2q8g9" in namespace "gc-3597"
    Jul 19 00:10:56.378: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vx8t" in namespace "gc-3597"
    Jul 19 00:10:56.384: INFO: Deleting pod "simpletest-rc-to-be-deleted-4j4xx" in namespace "gc-3597"
    Jul 19 00:10:56.393: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bpm2" in namespace "gc-3597"
    Jul 19 00:10:56.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qwvd" in namespace "gc-3597"
    Jul 19 00:10:56.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-64llr" in namespace "gc-3597"
    Jul 19 00:10:56.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k5m8" in namespace "gc-3597"
    Jul 19 00:10:56.433: INFO: Deleting pod "simpletest-rc-to-be-deleted-6td46" in namespace "gc-3597"
    Jul 19 00:10:56.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ttt9" in namespace "gc-3597"
    Jul 19 00:10:56.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-75f9d" in namespace "gc-3597"
    Jul 19 00:10:56.462: INFO: Deleting pod "simpletest-rc-to-be-deleted-77l4x" in namespace "gc-3597"
    Jul 19 00:10:56.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lfpr" in namespace "gc-3597"
    Jul 19 00:10:56.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n79k" in namespace "gc-3597"
    Jul 19 00:10:56.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vcqd" in namespace "gc-3597"
    Jul 19 00:10:56.506: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xcmq" in namespace "gc-3597"
    Jul 19 00:10:56.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-82bf9" in namespace "gc-3597"
    Jul 19 00:10:56.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-86kg4" in namespace "gc-3597"
    Jul 19 00:10:56.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f49p" in namespace "gc-3597"
    Jul 19 00:10:56.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-8schg" in namespace "gc-3597"
    Jul 19 00:10:56.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-8spb9" in namespace "gc-3597"
    Jul 19 00:10:56.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h8n2" in namespace "gc-3597"
    Jul 19 00:10:56.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jc5d" in namespace "gc-3597"
    Jul 19 00:10:56.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s9wf" in namespace "gc-3597"
    Jul 19 00:10:56.597: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sjhr" in namespace "gc-3597"
    Jul 19 00:10:56.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8wls" in namespace "gc-3597"
    Jul 19 00:10:56.626: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfsw2" in namespace "gc-3597"
    Jul 19 00:10:56.633: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfzp5" in namespace "gc-3597"
    Jul 19 00:10:56.641: INFO: Deleting pod "simpletest-rc-to-be-deleted-bm62p" in namespace "gc-3597"
    Jul 19 00:10:56.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-bwvz2" in namespace "gc-3597"
    Jul 19 00:10:56.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-cf852" in namespace "gc-3597"
    Jul 19 00:10:56.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-cfl2z" in namespace "gc-3597"
    Jul 19 00:10:56.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-chxbp" in namespace "gc-3597"
    Jul 19 00:10:56.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4wjt" in namespace "gc-3597"
    Jul 19 00:10:56.719: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbgvv" in namespace "gc-3597"
    Jul 19 00:10:56.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc45g" in namespace "gc-3597"
    Jul 19 00:10:56.780: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqrnq" in namespace "gc-3597"
    Jul 19 00:10:56.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-fj55f" in namespace "gc-3597"
    Jul 19 00:10:56.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjltz" in namespace "gc-3597"
    Jul 19 00:10:56.870: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkx7r" in namespace "gc-3597"
    Jul 19 00:10:56.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkxm5" in namespace "gc-3597"
    Jul 19 00:10:56.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftjl5" in namespace "gc-3597"
    Jul 19 00:10:56.915: INFO: Deleting pod "simpletest-rc-to-be-deleted-fz2xn" in namespace "gc-3597"
    Jul 19 00:10:56.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-gm4zb" in namespace "gc-3597"
    Jul 19 00:10:56.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmqs6" in namespace "gc-3597"
    Jul 19 00:10:56.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-gn6kf" in namespace "gc-3597"
    Jul 19 00:10:57.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-grrm4" in namespace "gc-3597"
    Jul 19 00:10:57.023: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2zmg" in namespace "gc-3597"
    Jul 19 00:10:57.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-h869t" in namespace "gc-3597"
    Jul 19 00:10:57.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-hggwq" in namespace "gc-3597"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:10:57.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3597" for this suite. 07/19/23 00:10:57.047
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:10:57.061
Jul 19 00:10:57.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-probe 07/19/23 00:10:57.062
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:57.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:57.071
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf in namespace container-probe-8469 07/19/23 00:10:57.073
Jul 19 00:10:57.080: INFO: Waiting up to 5m0s for pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf" in namespace "container-probe-8469" to be "not pending"
Jul 19 00:10:57.086: INFO: Pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.745671ms
Jul 19 00:10:59.091: INFO: Pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010223235s
Jul 19 00:11:01.089: INFO: Pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008685049s
Jul 19 00:11:03.089: INFO: Pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf": Phase="Running", Reason="", readiness=true. Elapsed: 6.009101065s
Jul 19 00:11:03.089: INFO: Pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf" satisfied condition "not pending"
Jul 19 00:11:03.089: INFO: Started pod busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf in namespace container-probe-8469
STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 00:11:03.089
Jul 19 00:11:03.091: INFO: Initial restart count of pod busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf is 0
STEP: deleting the pod 07/19/23 00:15:03.579
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jul 19 00:15:03.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8469" for this suite. 07/19/23 00:15:03.592
------------------------------
• [SLOW TEST] [246.534 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:10:57.061
    Jul 19 00:10:57.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-probe 07/19/23 00:10:57.062
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:10:57.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:10:57.071
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf in namespace container-probe-8469 07/19/23 00:10:57.073
    Jul 19 00:10:57.080: INFO: Waiting up to 5m0s for pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf" in namespace "container-probe-8469" to be "not pending"
    Jul 19 00:10:57.086: INFO: Pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.745671ms
    Jul 19 00:10:59.091: INFO: Pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010223235s
    Jul 19 00:11:01.089: INFO: Pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008685049s
    Jul 19 00:11:03.089: INFO: Pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf": Phase="Running", Reason="", readiness=true. Elapsed: 6.009101065s
    Jul 19 00:11:03.089: INFO: Pod "busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf" satisfied condition "not pending"
    Jul 19 00:11:03.089: INFO: Started pod busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf in namespace container-probe-8469
    STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 00:11:03.089
    Jul 19 00:11:03.091: INFO: Initial restart count of pod busybox-37b88cd5-2163-483e-ae68-c71e5f57e7cf is 0
    STEP: deleting the pod 07/19/23 00:15:03.579
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:15:03.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8469" for this suite. 07/19/23 00:15:03.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:15:03.595
Jul 19 00:15:03.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-probe 07/19/23 00:15:03.596
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:15:03.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:15:03.604
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jul 19 00:15:03.609: INFO: Waiting up to 5m0s for pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb" in namespace "container-probe-4329" to be "running and ready"
Jul 19 00:15:03.610: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.348078ms
Jul 19 00:15:03.610: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:15:05.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 2.004018368s
Jul 19 00:15:05.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
Jul 19 00:15:07.614: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 4.005128946s
Jul 19 00:15:07.614: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
Jul 19 00:15:09.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 6.00422428s
Jul 19 00:15:09.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
Jul 19 00:15:11.614: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 8.005047215s
Jul 19 00:15:11.614: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
Jul 19 00:15:13.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 10.004050561s
Jul 19 00:15:13.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
Jul 19 00:15:15.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 12.003968983s
Jul 19 00:15:15.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
Jul 19 00:15:17.614: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 14.004480279s
Jul 19 00:15:17.614: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
Jul 19 00:15:19.614: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 16.004938205s
Jul 19 00:15:19.614: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
Jul 19 00:15:21.614: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 18.0046901s
Jul 19 00:15:21.614: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
Jul 19 00:15:23.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 20.00432144s
Jul 19 00:15:23.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
Jul 19 00:15:25.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=true. Elapsed: 22.003775288s
Jul 19 00:15:25.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = true)
Jul 19 00:15:25.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb" satisfied condition "running and ready"
Jul 19 00:15:25.614: INFO: Container started at 2023-07-19 00:15:04 +0000 UTC, pod became ready at 2023-07-19 00:15:23 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jul 19 00:15:25.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4329" for this suite. 07/19/23 00:15:25.616
------------------------------
• [SLOW TEST] [22.024 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:15:03.595
    Jul 19 00:15:03.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-probe 07/19/23 00:15:03.596
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:15:03.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:15:03.604
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jul 19 00:15:03.609: INFO: Waiting up to 5m0s for pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb" in namespace "container-probe-4329" to be "running and ready"
    Jul 19 00:15:03.610: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.348078ms
    Jul 19 00:15:03.610: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:15:05.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 2.004018368s
    Jul 19 00:15:05.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
    Jul 19 00:15:07.614: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 4.005128946s
    Jul 19 00:15:07.614: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
    Jul 19 00:15:09.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 6.00422428s
    Jul 19 00:15:09.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
    Jul 19 00:15:11.614: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 8.005047215s
    Jul 19 00:15:11.614: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
    Jul 19 00:15:13.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 10.004050561s
    Jul 19 00:15:13.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
    Jul 19 00:15:15.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 12.003968983s
    Jul 19 00:15:15.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
    Jul 19 00:15:17.614: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 14.004480279s
    Jul 19 00:15:17.614: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
    Jul 19 00:15:19.614: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 16.004938205s
    Jul 19 00:15:19.614: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
    Jul 19 00:15:21.614: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 18.0046901s
    Jul 19 00:15:21.614: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
    Jul 19 00:15:23.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=false. Elapsed: 20.00432144s
    Jul 19 00:15:23.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = false)
    Jul 19 00:15:25.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb": Phase="Running", Reason="", readiness=true. Elapsed: 22.003775288s
    Jul 19 00:15:25.613: INFO: The phase of Pod test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb is Running (Ready = true)
    Jul 19 00:15:25.613: INFO: Pod "test-webserver-9649af01-c744-4ce8-8cfa-200832cea5fb" satisfied condition "running and ready"
    Jul 19 00:15:25.614: INFO: Container started at 2023-07-19 00:15:04 +0000 UTC, pod became ready at 2023-07-19 00:15:23 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:15:25.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4329" for this suite. 07/19/23 00:15:25.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:15:25.62
Jul 19 00:15:25.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:15:25.62
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:15:25.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:15:25.628
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-f2ee2843-394f-434e-966e-acb072697029 07/19/23 00:15:25.63
STEP: Creating a pod to test consume configMaps 07/19/23 00:15:25.632
Jul 19 00:15:25.635: INFO: Waiting up to 5m0s for pod "pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4" in namespace "configmap-3454" to be "Succeeded or Failed"
Jul 19 00:15:25.638: INFO: Pod "pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692439ms
Jul 19 00:15:27.640: INFO: Pod "pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004976451s
Jul 19 00:15:29.641: INFO: Pod "pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005370936s
STEP: Saw pod success 07/19/23 00:15:29.641
Jul 19 00:15:29.641: INFO: Pod "pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4" satisfied condition "Succeeded or Failed"
Jul 19 00:15:29.642: INFO: Trying to get logs from node controller-1 pod pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:15:29.653
Jul 19 00:15:29.659: INFO: Waiting for pod pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4 to disappear
Jul 19 00:15:29.661: INFO: Pod pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:15:29.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3454" for this suite. 07/19/23 00:15:29.663
------------------------------
• [4.046 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:15:25.62
    Jul 19 00:15:25.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:15:25.62
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:15:25.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:15:25.628
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-f2ee2843-394f-434e-966e-acb072697029 07/19/23 00:15:25.63
    STEP: Creating a pod to test consume configMaps 07/19/23 00:15:25.632
    Jul 19 00:15:25.635: INFO: Waiting up to 5m0s for pod "pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4" in namespace "configmap-3454" to be "Succeeded or Failed"
    Jul 19 00:15:25.638: INFO: Pod "pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.692439ms
    Jul 19 00:15:27.640: INFO: Pod "pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004976451s
    Jul 19 00:15:29.641: INFO: Pod "pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005370936s
    STEP: Saw pod success 07/19/23 00:15:29.641
    Jul 19 00:15:29.641: INFO: Pod "pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4" satisfied condition "Succeeded or Failed"
    Jul 19 00:15:29.642: INFO: Trying to get logs from node controller-1 pod pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:15:29.653
    Jul 19 00:15:29.659: INFO: Waiting for pod pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4 to disappear
    Jul 19 00:15:29.661: INFO: Pod pod-configmaps-5de468eb-dccf-48c9-999e-044f81798dd4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:15:29.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3454" for this suite. 07/19/23 00:15:29.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:15:29.666
Jul 19 00:15:29.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename endpointslice 07/19/23 00:15:29.667
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:15:29.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:15:29.675
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 07/19/23 00:15:34.747
STEP: referencing matching pods with named port 07/19/23 00:15:39.752
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 07/19/23 00:15:44.756
STEP: recreating EndpointSlices after they've been deleted 07/19/23 00:15:49.761
Jul 19 00:15:49.769: INFO: EndpointSlice for Service endpointslice-900/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jul 19 00:15:59.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-900" for this suite. 07/19/23 00:15:59.779
------------------------------
• [SLOW TEST] [30.116 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:15:29.666
    Jul 19 00:15:29.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename endpointslice 07/19/23 00:15:29.667
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:15:29.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:15:29.675
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 07/19/23 00:15:34.747
    STEP: referencing matching pods with named port 07/19/23 00:15:39.752
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 07/19/23 00:15:44.756
    STEP: recreating EndpointSlices after they've been deleted 07/19/23 00:15:49.761
    Jul 19 00:15:49.769: INFO: EndpointSlice for Service endpointslice-900/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:15:59.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-900" for this suite. 07/19/23 00:15:59.779
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:15:59.782
Jul 19 00:15:59.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 07/19/23 00:15:59.783
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:15:59.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:15:59.791
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 07/19/23 00:15:59.794
STEP: Creating hostNetwork=false pod 07/19/23 00:15:59.794
Jul 19 00:15:59.797: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-418" to be "running and ready"
Jul 19 00:15:59.801: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.565382ms
Jul 19 00:15:59.801: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:16:01.803: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006047074s
Jul 19 00:16:01.803: INFO: The phase of Pod test-pod is Running (Ready = true)
Jul 19 00:16:01.803: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 07/19/23 00:16:01.805
Jul 19 00:16:01.808: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-418" to be "running and ready"
Jul 19 00:16:01.809: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.268232ms
Jul 19 00:16:01.809: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:16:03.813: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004644231s
Jul 19 00:16:03.813: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jul 19 00:16:03.813: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 07/19/23 00:16:03.814
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 07/19/23 00:16:03.814
Jul 19 00:16:03.814: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:03.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:03.815: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:03.815: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul 19 00:16:03.863: INFO: Exec stderr: ""
Jul 19 00:16:03.863: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:03.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:03.864: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:03.864: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul 19 00:16:03.906: INFO: Exec stderr: ""
Jul 19 00:16:03.906: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:03.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:03.907: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:03.907: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul 19 00:16:03.949: INFO: Exec stderr: ""
Jul 19 00:16:03.949: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:03.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:03.950: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:03.950: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul 19 00:16:03.994: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 07/19/23 00:16:03.994
Jul 19 00:16:03.994: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:03.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:03.995: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:03.995: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jul 19 00:16:04.039: INFO: Exec stderr: ""
Jul 19 00:16:04.039: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:04.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:04.039: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:04.040: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jul 19 00:16:04.084: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 07/19/23 00:16:04.084
Jul 19 00:16:04.084: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:04.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:04.085: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:04.085: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul 19 00:16:04.139: INFO: Exec stderr: ""
Jul 19 00:16:04.139: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:04.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:04.139: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:04.139: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jul 19 00:16:04.179: INFO: Exec stderr: ""
Jul 19 00:16:04.179: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:04.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:04.179: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:04.179: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul 19 00:16:04.217: INFO: Exec stderr: ""
Jul 19 00:16:04.217: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:04.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:04.217: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:04.218: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jul 19 00:16:04.256: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jul 19 00:16:04.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-418" for this suite. 07/19/23 00:16:04.259
------------------------------
• [4.479 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:15:59.782
    Jul 19 00:15:59.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 07/19/23 00:15:59.783
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:15:59.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:15:59.791
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 07/19/23 00:15:59.794
    STEP: Creating hostNetwork=false pod 07/19/23 00:15:59.794
    Jul 19 00:15:59.797: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-418" to be "running and ready"
    Jul 19 00:15:59.801: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.565382ms
    Jul 19 00:15:59.801: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:16:01.803: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006047074s
    Jul 19 00:16:01.803: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jul 19 00:16:01.803: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 07/19/23 00:16:01.805
    Jul 19 00:16:01.808: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-418" to be "running and ready"
    Jul 19 00:16:01.809: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.268232ms
    Jul 19 00:16:01.809: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:16:03.813: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004644231s
    Jul 19 00:16:03.813: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jul 19 00:16:03.813: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 07/19/23 00:16:03.814
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 07/19/23 00:16:03.814
    Jul 19 00:16:03.814: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:03.814: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:03.815: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:03.815: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jul 19 00:16:03.863: INFO: Exec stderr: ""
    Jul 19 00:16:03.863: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:03.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:03.864: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:03.864: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jul 19 00:16:03.906: INFO: Exec stderr: ""
    Jul 19 00:16:03.906: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:03.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:03.907: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:03.907: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jul 19 00:16:03.949: INFO: Exec stderr: ""
    Jul 19 00:16:03.949: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:03.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:03.950: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:03.950: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jul 19 00:16:03.994: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 07/19/23 00:16:03.994
    Jul 19 00:16:03.994: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:03.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:03.995: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:03.995: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jul 19 00:16:04.039: INFO: Exec stderr: ""
    Jul 19 00:16:04.039: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:04.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:04.039: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:04.040: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jul 19 00:16:04.084: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 07/19/23 00:16:04.084
    Jul 19 00:16:04.084: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:04.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:04.085: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:04.085: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jul 19 00:16:04.139: INFO: Exec stderr: ""
    Jul 19 00:16:04.139: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:04.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:04.139: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:04.139: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jul 19 00:16:04.179: INFO: Exec stderr: ""
    Jul 19 00:16:04.179: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:04.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:04.179: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:04.179: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jul 19 00:16:04.217: INFO: Exec stderr: ""
    Jul 19 00:16:04.217: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-418 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:04.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:04.217: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:04.218: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-418/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jul 19 00:16:04.256: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:16:04.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-418" for this suite. 07/19/23 00:16:04.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:16:04.262
Jul 19 00:16:04.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename daemonsets 07/19/23 00:16:04.263
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:04.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:04.269
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jul 19 00:16:04.278: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 07/19/23 00:16:04.28
Jul 19 00:16:04.286: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:16:04.286: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:16:05.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:16:05.290: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:16:06.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 19 00:16:06.291: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 07/19/23 00:16:06.298
STEP: Check that daemon pods images are updated. 07/19/23 00:16:06.303
Jul 19 00:16:06.305: INFO: Wrong image for pod: daemon-set-5qcsh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jul 19 00:16:06.305: INFO: Wrong image for pod: daemon-set-c85mv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jul 19 00:16:07.314: INFO: Wrong image for pod: daemon-set-c85mv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jul 19 00:16:08.313: INFO: Wrong image for pod: daemon-set-c85mv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jul 19 00:16:09.314: INFO: Wrong image for pod: daemon-set-c85mv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jul 19 00:16:09.314: INFO: Pod daemon-set-xnvqt is not available
Jul 19 00:16:11.313: INFO: Pod daemon-set-kkzgj is not available
STEP: Check that daemon pods are still running on every node of the cluster. 07/19/23 00:16:11.315
Jul 19 00:16:11.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 19 00:16:11.319: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 19 00:16:12.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 19 00:16:12.323: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:16:12.332
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-790, will wait for the garbage collector to delete the pods 07/19/23 00:16:12.332
Jul 19 00:16:12.386: INFO: Deleting DaemonSet.extensions daemon-set took: 2.285867ms
Jul 19 00:16:12.487: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.012555ms
Jul 19 00:16:14.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:16:14.690: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 19 00:16:14.691: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54521"},"items":null}

Jul 19 00:16:14.693: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54521"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:16:14.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-790" for this suite. 07/19/23 00:16:14.702
------------------------------
• [SLOW TEST] [10.443 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:16:04.262
    Jul 19 00:16:04.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename daemonsets 07/19/23 00:16:04.263
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:04.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:04.269
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jul 19 00:16:04.278: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 07/19/23 00:16:04.28
    Jul 19 00:16:04.286: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:16:04.286: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:16:05.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:16:05.290: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:16:06.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 19 00:16:06.291: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 07/19/23 00:16:06.298
    STEP: Check that daemon pods images are updated. 07/19/23 00:16:06.303
    Jul 19 00:16:06.305: INFO: Wrong image for pod: daemon-set-5qcsh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jul 19 00:16:06.305: INFO: Wrong image for pod: daemon-set-c85mv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jul 19 00:16:07.314: INFO: Wrong image for pod: daemon-set-c85mv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jul 19 00:16:08.313: INFO: Wrong image for pod: daemon-set-c85mv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jul 19 00:16:09.314: INFO: Wrong image for pod: daemon-set-c85mv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jul 19 00:16:09.314: INFO: Pod daemon-set-xnvqt is not available
    Jul 19 00:16:11.313: INFO: Pod daemon-set-kkzgj is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 07/19/23 00:16:11.315
    Jul 19 00:16:11.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 19 00:16:11.319: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 19 00:16:12.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 19 00:16:12.323: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:16:12.332
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-790, will wait for the garbage collector to delete the pods 07/19/23 00:16:12.332
    Jul 19 00:16:12.386: INFO: Deleting DaemonSet.extensions daemon-set took: 2.285867ms
    Jul 19 00:16:12.487: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.012555ms
    Jul 19 00:16:14.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:16:14.690: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 19 00:16:14.691: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"54521"},"items":null}

    Jul 19 00:16:14.693: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"54521"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:16:14.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-790" for this suite. 07/19/23 00:16:14.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:16:14.705
Jul 19 00:16:14.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:16:14.706
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:14.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:14.712
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-29196a38-2227-45fc-9bbe-8dcb9fb92273 07/19/23 00:16:14.715
STEP: Creating a pod to test consume configMaps 07/19/23 00:16:14.717
Jul 19 00:16:14.722: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f" in namespace "configmap-2899" to be "Succeeded or Failed"
Jul 19 00:16:14.724: INFO: Pod "pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482435ms
Jul 19 00:16:16.726: INFO: Pod "pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004179968s
Jul 19 00:16:18.727: INFO: Pod "pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005259909s
STEP: Saw pod success 07/19/23 00:16:18.727
Jul 19 00:16:18.727: INFO: Pod "pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f" satisfied condition "Succeeded or Failed"
Jul 19 00:16:18.729: INFO: Trying to get logs from node controller-1 pod pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:16:18.733
Jul 19 00:16:18.739: INFO: Waiting for pod pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f to disappear
Jul 19 00:16:18.740: INFO: Pod pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:16:18.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2899" for this suite. 07/19/23 00:16:18.743
------------------------------
• [4.040 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:16:14.705
    Jul 19 00:16:14.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:16:14.706
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:14.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:14.712
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-29196a38-2227-45fc-9bbe-8dcb9fb92273 07/19/23 00:16:14.715
    STEP: Creating a pod to test consume configMaps 07/19/23 00:16:14.717
    Jul 19 00:16:14.722: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f" in namespace "configmap-2899" to be "Succeeded or Failed"
    Jul 19 00:16:14.724: INFO: Pod "pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482435ms
    Jul 19 00:16:16.726: INFO: Pod "pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004179968s
    Jul 19 00:16:18.727: INFO: Pod "pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005259909s
    STEP: Saw pod success 07/19/23 00:16:18.727
    Jul 19 00:16:18.727: INFO: Pod "pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f" satisfied condition "Succeeded or Failed"
    Jul 19 00:16:18.729: INFO: Trying to get logs from node controller-1 pod pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:16:18.733
    Jul 19 00:16:18.739: INFO: Waiting for pod pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f to disappear
    Jul 19 00:16:18.740: INFO: Pod pod-configmaps-5ecd9b6d-8294-421d-87bb-0967f0049a1f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:16:18.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2899" for this suite. 07/19/23 00:16:18.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:16:18.746
Jul 19 00:16:18.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename job 07/19/23 00:16:18.746
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:18.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:18.754
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 07/19/23 00:16:18.758
STEP: Patching the Job 07/19/23 00:16:18.76
STEP: Watching for Job to be patched 07/19/23 00:16:18.772
Jul 19 00:16:18.773: INFO: Event ADDED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl] and annotations: map[batch.kubernetes.io/job-tracking:]
Jul 19 00:16:18.773: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl] and annotations: map[batch.kubernetes.io/job-tracking:]
Jul 19 00:16:18.773: INFO: Event MODIFIED found for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 07/19/23 00:16:18.773
STEP: Watching for Job to be updated 07/19/23 00:16:18.779
Jul 19 00:16:18.780: INFO: Event MODIFIED found for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 19 00:16:18.780: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 07/19/23 00:16:18.78
Jul 19 00:16:18.782: INFO: Job: e2e-klxhl as labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched]
STEP: Waiting for job to complete 07/19/23 00:16:18.782
STEP: Delete a job collection with a labelselector 07/19/23 00:16:26.785
STEP: Watching for Job to be deleted 07/19/23 00:16:26.792
Jul 19 00:16:26.793: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 19 00:16:26.793: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 19 00:16:26.793: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 19 00:16:26.793: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 19 00:16:26.793: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jul 19 00:16:26.793: INFO: Event DELETED found for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 07/19/23 00:16:26.793
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jul 19 00:16:26.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-475" for this suite. 07/19/23 00:16:26.801
------------------------------
• [SLOW TEST] [8.060 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:16:18.746
    Jul 19 00:16:18.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename job 07/19/23 00:16:18.746
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:18.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:18.754
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 07/19/23 00:16:18.758
    STEP: Patching the Job 07/19/23 00:16:18.76
    STEP: Watching for Job to be patched 07/19/23 00:16:18.772
    Jul 19 00:16:18.773: INFO: Event ADDED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jul 19 00:16:18.773: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jul 19 00:16:18.773: INFO: Event MODIFIED found for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 07/19/23 00:16:18.773
    STEP: Watching for Job to be updated 07/19/23 00:16:18.779
    Jul 19 00:16:18.780: INFO: Event MODIFIED found for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 19 00:16:18.780: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 07/19/23 00:16:18.78
    Jul 19 00:16:18.782: INFO: Job: e2e-klxhl as labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched]
    STEP: Waiting for job to complete 07/19/23 00:16:18.782
    STEP: Delete a job collection with a labelselector 07/19/23 00:16:26.785
    STEP: Watching for Job to be deleted 07/19/23 00:16:26.792
    Jul 19 00:16:26.793: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 19 00:16:26.793: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 19 00:16:26.793: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 19 00:16:26.793: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 19 00:16:26.793: INFO: Event MODIFIED observed for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jul 19 00:16:26.793: INFO: Event DELETED found for Job e2e-klxhl in namespace job-475 with labels: map[e2e-job-label:e2e-klxhl e2e-klxhl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 07/19/23 00:16:26.793
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:16:26.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-475" for this suite. 07/19/23 00:16:26.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:16:26.812
Jul 19 00:16:26.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:16:26.814
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:26.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:26.821
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:16:26.823
Jul 19 00:16:26.827: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f" in namespace "projected-1495" to be "Succeeded or Failed"
Jul 19 00:16:26.829: INFO: Pod "downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.442653ms
Jul 19 00:16:28.839: INFO: Pod "downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011601786s
Jul 19 00:16:30.832: INFO: Pod "downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004871502s
STEP: Saw pod success 07/19/23 00:16:30.832
Jul 19 00:16:30.832: INFO: Pod "downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f" satisfied condition "Succeeded or Failed"
Jul 19 00:16:30.834: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f container client-container: <nil>
STEP: delete the pod 07/19/23 00:16:30.843
Jul 19 00:16:30.849: INFO: Waiting for pod downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f to disappear
Jul 19 00:16:30.853: INFO: Pod downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 19 00:16:30.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1495" for this suite. 07/19/23 00:16:30.855
------------------------------
• [4.045 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:16:26.812
    Jul 19 00:16:26.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:16:26.814
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:26.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:26.821
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:16:26.823
    Jul 19 00:16:26.827: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f" in namespace "projected-1495" to be "Succeeded or Failed"
    Jul 19 00:16:26.829: INFO: Pod "downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.442653ms
    Jul 19 00:16:28.839: INFO: Pod "downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011601786s
    Jul 19 00:16:30.832: INFO: Pod "downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004871502s
    STEP: Saw pod success 07/19/23 00:16:30.832
    Jul 19 00:16:30.832: INFO: Pod "downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f" satisfied condition "Succeeded or Failed"
    Jul 19 00:16:30.834: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f container client-container: <nil>
    STEP: delete the pod 07/19/23 00:16:30.843
    Jul 19 00:16:30.849: INFO: Waiting for pod downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f to disappear
    Jul 19 00:16:30.853: INFO: Pod downwardapi-volume-a5241a80-dc1f-470e-817f-0783744bd69f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:16:30.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1495" for this suite. 07/19/23 00:16:30.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:16:30.858
Jul 19 00:16:30.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename dns 07/19/23 00:16:30.859
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:30.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:30.866
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 07/19/23 00:16:30.868
Jul 19 00:16:30.871: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6320  d3eff6d3-925c-474b-8335-7ff52a85f08d 54819 0 2023-07-19 00:16:30 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-19 00:16:30 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pznsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pznsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:16:30.871: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6320" to be "running and ready"
Jul 19 00:16:30.873: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482759ms
Jul 19 00:16:30.873: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:16:32.875: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.003742692s
Jul 19 00:16:32.875: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jul 19 00:16:32.875: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 07/19/23 00:16:32.875
Jul 19 00:16:32.875: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6320 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:32.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:32.875: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:32.875: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6320/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 07/19/23 00:16:32.945
Jul 19 00:16:32.945: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6320 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:16:32.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:16:32.945: INFO: ExecWithOptions: Clientset creation
Jul 19 00:16:32.946: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6320/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 19 00:16:33.000: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jul 19 00:16:33.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6320" for this suite. 07/19/23 00:16:33.008
------------------------------
• [2.152 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:16:30.858
    Jul 19 00:16:30.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename dns 07/19/23 00:16:30.859
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:30.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:30.866
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 07/19/23 00:16:30.868
    Jul 19 00:16:30.871: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6320  d3eff6d3-925c-474b-8335-7ff52a85f08d 54819 0 2023-07-19 00:16:30 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-19 00:16:30 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pznsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pznsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:16:30.871: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6320" to be "running and ready"
    Jul 19 00:16:30.873: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.482759ms
    Jul 19 00:16:30.873: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:16:32.875: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.003742692s
    Jul 19 00:16:32.875: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jul 19 00:16:32.875: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 07/19/23 00:16:32.875
    Jul 19 00:16:32.875: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6320 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:32.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:32.875: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:32.875: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6320/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 07/19/23 00:16:32.945
    Jul 19 00:16:32.945: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6320 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:16:32.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:16:32.945: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:16:32.946: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-6320/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 19 00:16:33.000: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:16:33.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6320" for this suite. 07/19/23 00:16:33.008
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:16:33.011
Jul 19 00:16:33.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename security-context-test 07/19/23 00:16:33.011
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:33.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:33.018
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jul 19 00:16:33.023: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27" in namespace "security-context-test-486" to be "Succeeded or Failed"
Jul 19 00:16:33.027: INFO: Pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416108ms
Jul 19 00:16:35.029: INFO: Pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006033389s
Jul 19 00:16:37.030: INFO: Pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006667784s
Jul 19 00:16:39.030: INFO: Pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00645782s
Jul 19 00:16:39.030: INFO: Pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jul 19 00:16:39.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-486" for this suite. 07/19/23 00:16:39.035
------------------------------
• [SLOW TEST] [6.027 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:16:33.011
    Jul 19 00:16:33.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename security-context-test 07/19/23 00:16:33.011
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:33.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:33.018
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jul 19 00:16:33.023: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27" in namespace "security-context-test-486" to be "Succeeded or Failed"
    Jul 19 00:16:33.027: INFO: Pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416108ms
    Jul 19 00:16:35.029: INFO: Pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006033389s
    Jul 19 00:16:37.030: INFO: Pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006667784s
    Jul 19 00:16:39.030: INFO: Pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00645782s
    Jul 19 00:16:39.030: INFO: Pod "alpine-nnp-false-7c8bd0c4-3d7f-47ea-ad0e-15edfa660e27" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:16:39.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-486" for this suite. 07/19/23 00:16:39.035
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:16:39.038
Jul 19 00:16:39.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-lifecycle-hook 07/19/23 00:16:39.039
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:39.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:39.049
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 07/19/23 00:16:39.053
Jul 19 00:16:39.057: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1888" to be "running and ready"
Jul 19 00:16:39.059: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.506598ms
Jul 19 00:16:39.059: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:16:41.062: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005531147s
Jul 19 00:16:41.062: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jul 19 00:16:41.062: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 07/19/23 00:16:41.064
Jul 19 00:16:41.067: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1888" to be "running and ready"
Jul 19 00:16:41.070: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.537629ms
Jul 19 00:16:41.070: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:16:43.072: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005738661s
Jul 19 00:16:43.073: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jul 19 00:16:43.073: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 07/19/23 00:16:43.074
STEP: delete the pod with lifecycle hook 07/19/23 00:16:43.079
Jul 19 00:16:43.082: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 00:16:43.086: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 19 00:16:45.087: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 19 00:16:45.089: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jul 19 00:16:45.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1888" for this suite. 07/19/23 00:16:45.091
------------------------------
• [SLOW TEST] [6.055 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:16:39.038
    Jul 19 00:16:39.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-lifecycle-hook 07/19/23 00:16:39.039
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:39.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:39.049
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 07/19/23 00:16:39.053
    Jul 19 00:16:39.057: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1888" to be "running and ready"
    Jul 19 00:16:39.059: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.506598ms
    Jul 19 00:16:39.059: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:16:41.062: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005531147s
    Jul 19 00:16:41.062: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jul 19 00:16:41.062: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 07/19/23 00:16:41.064
    Jul 19 00:16:41.067: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1888" to be "running and ready"
    Jul 19 00:16:41.070: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.537629ms
    Jul 19 00:16:41.070: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:16:43.072: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005738661s
    Jul 19 00:16:43.073: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jul 19 00:16:43.073: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 07/19/23 00:16:43.074
    STEP: delete the pod with lifecycle hook 07/19/23 00:16:43.079
    Jul 19 00:16:43.082: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jul 19 00:16:43.086: INFO: Pod pod-with-poststart-exec-hook still exists
    Jul 19 00:16:45.087: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jul 19 00:16:45.089: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:16:45.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1888" for this suite. 07/19/23 00:16:45.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:16:45.094
Jul 19 00:16:45.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:16:45.095
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:45.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:45.105
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 07/19/23 00:16:45.107
Jul 19 00:16:45.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: rename a version 07/19/23 00:16:49.464
STEP: check the new version name is served 07/19/23 00:16:49.474
STEP: check the old version name is removed 07/19/23 00:16:51.444
STEP: check the other version is not changed 07/19/23 00:16:52.202
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:16:55.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1681" for this suite. 07/19/23 00:16:55.671
------------------------------
• [SLOW TEST] [10.580 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:16:45.094
    Jul 19 00:16:45.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:16:45.095
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:45.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:45.105
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 07/19/23 00:16:45.107
    Jul 19 00:16:45.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: rename a version 07/19/23 00:16:49.464
    STEP: check the new version name is served 07/19/23 00:16:49.474
    STEP: check the old version name is removed 07/19/23 00:16:51.444
    STEP: check the other version is not changed 07/19/23 00:16:52.202
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:16:55.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1681" for this suite. 07/19/23 00:16:55.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:16:55.675
Jul 19 00:16:55.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:16:55.676
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:55.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:55.684
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 07/19/23 00:16:55.686
Jul 19 00:16:55.689: INFO: Waiting up to 5m0s for pod "pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60" in namespace "emptydir-4053" to be "Succeeded or Failed"
Jul 19 00:16:55.691: INFO: Pod "pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60": Phase="Pending", Reason="", readiness=false. Elapsed: 1.510574ms
Jul 19 00:16:57.695: INFO: Pod "pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004997419s
Jul 19 00:16:59.693: INFO: Pod "pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003835235s
STEP: Saw pod success 07/19/23 00:16:59.693
Jul 19 00:16:59.694: INFO: Pod "pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60" satisfied condition "Succeeded or Failed"
Jul 19 00:16:59.695: INFO: Trying to get logs from node controller-1 pod pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60 container test-container: <nil>
STEP: delete the pod 07/19/23 00:16:59.698
Jul 19 00:16:59.704: INFO: Waiting for pod pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60 to disappear
Jul 19 00:16:59.705: INFO: Pod pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:16:59.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4053" for this suite. 07/19/23 00:16:59.708
------------------------------
• [4.035 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:16:55.675
    Jul 19 00:16:55.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:16:55.676
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:55.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:55.684
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 07/19/23 00:16:55.686
    Jul 19 00:16:55.689: INFO: Waiting up to 5m0s for pod "pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60" in namespace "emptydir-4053" to be "Succeeded or Failed"
    Jul 19 00:16:55.691: INFO: Pod "pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60": Phase="Pending", Reason="", readiness=false. Elapsed: 1.510574ms
    Jul 19 00:16:57.695: INFO: Pod "pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004997419s
    Jul 19 00:16:59.693: INFO: Pod "pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003835235s
    STEP: Saw pod success 07/19/23 00:16:59.693
    Jul 19 00:16:59.694: INFO: Pod "pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60" satisfied condition "Succeeded or Failed"
    Jul 19 00:16:59.695: INFO: Trying to get logs from node controller-1 pod pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60 container test-container: <nil>
    STEP: delete the pod 07/19/23 00:16:59.698
    Jul 19 00:16:59.704: INFO: Waiting for pod pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60 to disappear
    Jul 19 00:16:59.705: INFO: Pod pod-f68d35eb-4c44-4b59-88eb-8d57141a4e60 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:16:59.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4053" for this suite. 07/19/23 00:16:59.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:16:59.711
Jul 19 00:16:59.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename var-expansion 07/19/23 00:16:59.712
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:59.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:59.719
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jul 19 00:16:59.725: INFO: Waiting up to 2m0s for pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b" in namespace "var-expansion-9512" to be "container 0 failed with reason CreateContainerConfigError"
Jul 19 00:16:59.726: INFO: Pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.511217ms
Jul 19 00:17:01.729: INFO: Pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004659734s
Jul 19 00:17:01.729: INFO: Pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jul 19 00:17:01.729: INFO: Deleting pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b" in namespace "var-expansion-9512"
Jul 19 00:17:01.732: INFO: Wait up to 5m0s for pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:05.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9512" for this suite. 07/19/23 00:17:05.739
------------------------------
• [SLOW TEST] [6.030 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:16:59.711
    Jul 19 00:16:59.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename var-expansion 07/19/23 00:16:59.712
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:16:59.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:16:59.719
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jul 19 00:16:59.725: INFO: Waiting up to 2m0s for pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b" in namespace "var-expansion-9512" to be "container 0 failed with reason CreateContainerConfigError"
    Jul 19 00:16:59.726: INFO: Pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.511217ms
    Jul 19 00:17:01.729: INFO: Pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004659734s
    Jul 19 00:17:01.729: INFO: Pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jul 19 00:17:01.729: INFO: Deleting pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b" in namespace "var-expansion-9512"
    Jul 19 00:17:01.732: INFO: Wait up to 5m0s for pod "var-expansion-1cfd91f1-3536-4735-88b5-e51de7e17e8b" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:05.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9512" for this suite. 07/19/23 00:17:05.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:05.742
Jul 19 00:17:05.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:17:05.743
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:05.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:05.751
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-bm9xw"  07/19/23 00:17:05.753
Jul 19 00:17:05.755: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-bm9xw"  07/19/23 00:17:05.755
Jul 19 00:17:05.758: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:05.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6522" for this suite. 07/19/23 00:17:05.76
------------------------------
• [0.020 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:05.742
    Jul 19 00:17:05.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:17:05.743
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:05.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:05.751
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-bm9xw"  07/19/23 00:17:05.753
    Jul 19 00:17:05.755: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-bm9xw"  07/19/23 00:17:05.755
    Jul 19 00:17:05.758: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:05.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6522" for this suite. 07/19/23 00:17:05.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:05.763
Jul 19 00:17:05.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:17:05.764
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:05.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:05.771
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 07/19/23 00:17:05.773
Jul 19 00:17:05.776: INFO: Waiting up to 5m0s for pod "pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1" in namespace "emptydir-5636" to be "Succeeded or Failed"
Jul 19 00:17:05.778: INFO: Pod "pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.277878ms
Jul 19 00:17:07.780: INFO: Pod "pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003724921s
Jul 19 00:17:09.781: INFO: Pod "pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004865575s
STEP: Saw pod success 07/19/23 00:17:09.781
Jul 19 00:17:09.781: INFO: Pod "pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1" satisfied condition "Succeeded or Failed"
Jul 19 00:17:09.783: INFO: Trying to get logs from node controller-1 pod pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1 container test-container: <nil>
STEP: delete the pod 07/19/23 00:17:09.786
Jul 19 00:17:09.794: INFO: Waiting for pod pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1 to disappear
Jul 19 00:17:09.795: INFO: Pod pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:09.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5636" for this suite. 07/19/23 00:17:09.797
------------------------------
• [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:05.763
    Jul 19 00:17:05.763: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:17:05.764
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:05.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:05.771
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 07/19/23 00:17:05.773
    Jul 19 00:17:05.776: INFO: Waiting up to 5m0s for pod "pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1" in namespace "emptydir-5636" to be "Succeeded or Failed"
    Jul 19 00:17:05.778: INFO: Pod "pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.277878ms
    Jul 19 00:17:07.780: INFO: Pod "pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003724921s
    Jul 19 00:17:09.781: INFO: Pod "pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004865575s
    STEP: Saw pod success 07/19/23 00:17:09.781
    Jul 19 00:17:09.781: INFO: Pod "pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1" satisfied condition "Succeeded or Failed"
    Jul 19 00:17:09.783: INFO: Trying to get logs from node controller-1 pod pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1 container test-container: <nil>
    STEP: delete the pod 07/19/23 00:17:09.786
    Jul 19 00:17:09.794: INFO: Waiting for pod pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1 to disappear
    Jul 19 00:17:09.795: INFO: Pod pod-a2643b51-45b9-403e-83e9-1fb5f1e82bf1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:09.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5636" for this suite. 07/19/23 00:17:09.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:09.8
Jul 19 00:17:09.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:17:09.801
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:09.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:09.811
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-b9bdb39b-8249-47a3-aed7-235891461ecc 07/19/23 00:17:09.813
STEP: Creating a pod to test consume configMaps 07/19/23 00:17:09.815
Jul 19 00:17:09.818: INFO: Waiting up to 5m0s for pod "pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083" in namespace "configmap-8742" to be "Succeeded or Failed"
Jul 19 00:17:09.821: INFO: Pod "pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083": Phase="Pending", Reason="", readiness=false. Elapsed: 2.715264ms
Jul 19 00:17:11.823: INFO: Pod "pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004823886s
Jul 19 00:17:13.823: INFO: Pod "pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005309067s
STEP: Saw pod success 07/19/23 00:17:13.823
Jul 19 00:17:13.823: INFO: Pod "pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083" satisfied condition "Succeeded or Failed"
Jul 19 00:17:13.825: INFO: Trying to get logs from node controller-1 pod pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:17:13.829
Jul 19 00:17:13.837: INFO: Waiting for pod pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083 to disappear
Jul 19 00:17:13.839: INFO: Pod pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:13.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8742" for this suite. 07/19/23 00:17:13.841
------------------------------
• [4.044 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:09.8
    Jul 19 00:17:09.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:17:09.801
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:09.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:09.811
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-b9bdb39b-8249-47a3-aed7-235891461ecc 07/19/23 00:17:09.813
    STEP: Creating a pod to test consume configMaps 07/19/23 00:17:09.815
    Jul 19 00:17:09.818: INFO: Waiting up to 5m0s for pod "pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083" in namespace "configmap-8742" to be "Succeeded or Failed"
    Jul 19 00:17:09.821: INFO: Pod "pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083": Phase="Pending", Reason="", readiness=false. Elapsed: 2.715264ms
    Jul 19 00:17:11.823: INFO: Pod "pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004823886s
    Jul 19 00:17:13.823: INFO: Pod "pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005309067s
    STEP: Saw pod success 07/19/23 00:17:13.823
    Jul 19 00:17:13.823: INFO: Pod "pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083" satisfied condition "Succeeded or Failed"
    Jul 19 00:17:13.825: INFO: Trying to get logs from node controller-1 pod pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:17:13.829
    Jul 19 00:17:13.837: INFO: Waiting for pod pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083 to disappear
    Jul 19 00:17:13.839: INFO: Pod pod-configmaps-a44a317e-24cc-4aed-b50d-4ddeb7cd5083 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:13.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8742" for this suite. 07/19/23 00:17:13.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:13.845
Jul 19 00:17:13.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 00:17:13.845
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:13.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:13.855
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:17:13.857
Jul 19 00:17:13.861: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f" in namespace "downward-api-3847" to be "Succeeded or Failed"
Jul 19 00:17:13.864: INFO: Pod "downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.262465ms
Jul 19 00:17:15.867: INFO: Pod "downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005935526s
Jul 19 00:17:17.867: INFO: Pod "downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006016508s
STEP: Saw pod success 07/19/23 00:17:17.867
Jul 19 00:17:17.867: INFO: Pod "downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f" satisfied condition "Succeeded or Failed"
Jul 19 00:17:17.869: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f container client-container: <nil>
STEP: delete the pod 07/19/23 00:17:17.872
Jul 19 00:17:17.880: INFO: Waiting for pod downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f to disappear
Jul 19 00:17:17.881: INFO: Pod downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:17.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3847" for this suite. 07/19/23 00:17:17.883
------------------------------
• [4.041 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:13.845
    Jul 19 00:17:13.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 00:17:13.845
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:13.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:13.855
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:17:13.857
    Jul 19 00:17:13.861: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f" in namespace "downward-api-3847" to be "Succeeded or Failed"
    Jul 19 00:17:13.864: INFO: Pod "downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.262465ms
    Jul 19 00:17:15.867: INFO: Pod "downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005935526s
    Jul 19 00:17:17.867: INFO: Pod "downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006016508s
    STEP: Saw pod success 07/19/23 00:17:17.867
    Jul 19 00:17:17.867: INFO: Pod "downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f" satisfied condition "Succeeded or Failed"
    Jul 19 00:17:17.869: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f container client-container: <nil>
    STEP: delete the pod 07/19/23 00:17:17.872
    Jul 19 00:17:17.880: INFO: Waiting for pod downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f to disappear
    Jul 19 00:17:17.881: INFO: Pod downwardapi-volume-7f3c4caa-8888-4c26-b962-e8dccb99215f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:17.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3847" for this suite. 07/19/23 00:17:17.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:17.887
Jul 19 00:17:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:17:17.888
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:17.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:17.895
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jul 19 00:17:17.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 create -f -'
Jul 19 00:17:18.545: INFO: stderr: ""
Jul 19 00:17:18.545: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul 19 00:17:18.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 create -f -'
Jul 19 00:17:19.319: INFO: stderr: ""
Jul 19 00:17:19.319: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 07/19/23 00:17:19.319
Jul 19 00:17:20.322: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 00:17:20.322: INFO: Found 1 / 1
Jul 19 00:17:20.322: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 19 00:17:20.324: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 00:17:20.324: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 19 00:17:20.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 describe pod agnhost-primary-ptv6b'
Jul 19 00:17:20.389: INFO: stderr: ""
Jul 19 00:17:20.389: INFO: stdout: "Name:             agnhost-primary-ptv6b\nNamespace:        kubectl-6657\nPriority:         0\nService Account:  default\nNode:             controller-1/192.168.206.3\nStart Time:       Wed, 19 Jul 2023 00:17:18 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 33c7e3888e63278b5bfc30c9dcffc3565ef71328f2b60106d56f26a545a293dc\n                  cni.projectcalico.org/podIP: 172.16.166.160/32\n                  cni.projectcalico.org/podIPs: 172.16.166.160/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"chain\",\n                        \"ips\": [\n                            \"172.16.166.160\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"chain\",\n                        \"ips\": [\n                            \"172.16.166.160\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\nStatus:           Running\nIP:               172.16.166.160\nIPs:\n  IP:           172.16.166.160\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://b61d1995470fd8568402d2480b3a597c11cb5510eeb0ddd082c2fdde7def15d8\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 19 Jul 2023 00:17:19 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tl8qj (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-tl8qj:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 30s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 30s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-6657/agnhost-primary-ptv6b to controller-1\n  Normal  AddedInterface  1s    multus             Add eth0 [172.16.166.160/32] from chain\n  Normal  Pulled          1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Jul 19 00:17:20.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 describe rc agnhost-primary'
Jul 19 00:17:20.457: INFO: stderr: ""
Jul 19 00:17:20.457: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6657\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-ptv6b\n"
Jul 19 00:17:20.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 describe service agnhost-primary'
Jul 19 00:17:20.523: INFO: stderr: ""
Jul 19 00:17:20.523: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6657\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.101.148.245\nIPs:               10.101.148.245\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.16.166.160:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 19 00:17:20.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 describe node controller-0'
Jul 19 00:17:20.640: INFO: stderr: ""
Jul 19 00:17:20.640: INFO: stdout: "Name:               controller-0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=controller-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cephfs.csi.ceph.com\":\"controller-0\",\"rbd.csi.ceph.com\":\"controller-0\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.206.2/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.16.192.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 18 Jul 2023 21:43:30 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  controller-0\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 19 Jul 2023 00:17:18 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 18 Jul 2023 22:04:53 +0000   Tue, 18 Jul 2023 22:04:53 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 19 Jul 2023 00:17:20 +0000   Tue, 18 Jul 2023 21:43:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 19 Jul 2023 00:17:20 +0000   Tue, 18 Jul 2023 21:43:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 19 Jul 2023 00:17:20 +0000   Tue, 18 Jul 2023 21:43:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 19 Jul 2023 00:17:20 +0000   Tue, 18 Jul 2023 21:44:16 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.206.2\n  Hostname:    controller-0\nCapacity:\n  cpu:                16\n  ephemeral-storage:  10218772Ki\n  hugepages-1Gi:      40Gi\n  hugepages-2Mi:      0\n  memory:             63395392Ki\n  pods:               110\nAllocatable:\n  cpu:                12\n  ephemeral-storage:  9417620260\n  hugepages-1Gi:      40Gi\n  hugepages-2Mi:      0\n  memory:             13157952Ki\n  pods:               110\nSystem Info:\n  Machine ID:                  f3e31f1e858440e5bb8043418c541e93\n  System UUID:                 00000000-0000-0000-0000-0cc47a971c5c\n  Boot ID:                     1be28b2d-7181-43df-a4be-7d244301e283\n  Kernel Version:              5.10.0-6-amd64\n  OS Image:                    Debian GNU/Linux 11 (bullseye)\n  Operating System:            linux\n  Architecture:                amd64\n  Container Runtime Version:   containerd://1.6.21\n  Kubelet Version:             v1.26.1\n  Kube-Proxy Version:          v1.26.1\nPodCIDR:                       172.16.0.0/24\nPodCIDRs:                      172.16.0.0/24\nNon-terminated Pods:           (22 in total)\n  Namespace                    Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                    ----                                                       ------------  ----------  ---------------  -------------  ---\n  cert-manager                 cm-cert-manager-7fb65857f5-5mnw8                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         146m\n  cert-manager                 cm-cert-manager-cainjector-86b69d7d69-6czpr                0 (0%)        0 (0%)      0 (0%)           0 (0%)         146m\n  cert-manager                 cm-cert-manager-webhook-98ddcd5cb-2q6p9                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         146m\n  flux-helm                    helm-controller-5fb8ccb85d-nl9lf                           0 (0%)        1 (8%)      64Mi (0%)        1Gi (7%)       149m\n  flux-helm                    source-controller-69b5d8f7d8-66tt6                         0 (0%)        1 (8%)      64Mi (0%)        1Gi (7%)       149m\n  kube-system                  calico-kube-controllers-7f5cd5f684-5jblg                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  calico-node-hmrnj                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  cephfs-nodeplugin-n2vfj                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  kube-system                  cephfs-provisioner-5f69fbf97-q8lg6                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  kube-system                  coredns-66856967f4-whd5z                                   0 (0%)        0 (0%)      70Mi (0%)        170Mi (1%)     153m\n  kube-system                  ic-nginx-ingress-ingress-nginx-controller-6hpm5            0 (0%)        0 (0%)      90Mi (0%)        0 (0%)         148m\n  kube-system                  kube-apiserver-controller-0                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  kube-controller-manager-controller-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  kube-multus-ds-amd64-bcmw4                                 0 (0%)        0 (0%)      50Mi (0%)        50Mi (0%)      153m\n  kube-system                  kube-proxy-kxjqb                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  kube-scheduler-controller-0                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  kube-sriov-cni-ds-amd64-69r58                              0 (0%)        0 (0%)      50Mi (0%)        50Mi (0%)      153m\n  kube-system                  rbd-nodeplugin-j7slt                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  kube-system                  rbd-provisioner-54c6c894f7-sggw9                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  platform-deployment-manager  dm-monitor-84b75cf89c-vnb6c                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         144m\n  platform-deployment-manager  platform-deployment-manager-7ff76b89d-qg7g9                0 (0%)        0 (0%)      0 (0%)           0 (0%)         145m\n  sonobuoy                     sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                0 (0%)      2 (16%)\n  memory             388Mi (3%)  2318Mi (18%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                  From             Message\n  ----     ------                   ----                 ----             -------\n  Normal   Starting                 132m                 kube-proxy       \n  Normal   Starting                 153m                 kube-proxy       \n  Normal   Starting                 153m                 kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      153m                 kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  153m                 kubelet          Node controller-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    153m                 kubelet          Node controller-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     153m                 kubelet          Node controller-0 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  153m                 kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           153m                 node-controller  Node controller-0 event: Registered Node controller-0 in Controller\n  Normal   NodeReady                153m                 kubelet          Node controller-0 status is now: NodeReady\n  Normal   NodeHasSufficientMemory  153m                 kubelet          Node controller-0 status is now: NodeHasSufficientMemory\n  Warning  InvalidDiskCapacity      153m                 kubelet          invalid capacity 0 on image filesystem\n  Normal   Starting                 153m                 kubelet          Starting kubelet.\n  Normal   NodeHasNoDiskPressure    153m                 kubelet          Node controller-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     153m                 kubelet          Node controller-0 status is now: NodeHasSufficientPID\n  Normal   NodeNotReady             153m                 kubelet          Node controller-0 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  153m                 kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                153m                 kubelet          Node controller-0 status is now: NodeReady\n  Normal   Starting                 133m                 kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      133m                 kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  133m (x8 over 133m)  kubelet          Node controller-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    133m (x7 over 133m)  kubelet          Node controller-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     133m (x7 over 133m)  kubelet          Node controller-0 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  133m                 kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           132m                 node-controller  Node controller-0 event: Registered Node controller-0 in Controller\n"
Jul 19 00:17:20.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 describe namespace kubectl-6657'
Jul 19 00:17:20.706: INFO: stderr: ""
Jul 19 00:17:20.706: INFO: stdout: "Name:         kubectl-6657\nLabels:       e2e-framework=kubectl\n              e2e-run=cc259ba7-b4dd-4a59-af4b-b9b08bdad088\n              kubernetes.io/metadata.name=kubectl-6657\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:20.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6657" for this suite. 07/19/23 00:17:20.708
------------------------------
• [2.823 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:17.887
    Jul 19 00:17:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:17:17.888
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:17.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:17.895
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jul 19 00:17:17.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 create -f -'
    Jul 19 00:17:18.545: INFO: stderr: ""
    Jul 19 00:17:18.545: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jul 19 00:17:18.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 create -f -'
    Jul 19 00:17:19.319: INFO: stderr: ""
    Jul 19 00:17:19.319: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 07/19/23 00:17:19.319
    Jul 19 00:17:20.322: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 19 00:17:20.322: INFO: Found 1 / 1
    Jul 19 00:17:20.322: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jul 19 00:17:20.324: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 19 00:17:20.324: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jul 19 00:17:20.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 describe pod agnhost-primary-ptv6b'
    Jul 19 00:17:20.389: INFO: stderr: ""
    Jul 19 00:17:20.389: INFO: stdout: "Name:             agnhost-primary-ptv6b\nNamespace:        kubectl-6657\nPriority:         0\nService Account:  default\nNode:             controller-1/192.168.206.3\nStart Time:       Wed, 19 Jul 2023 00:17:18 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 33c7e3888e63278b5bfc30c9dcffc3565ef71328f2b60106d56f26a545a293dc\n                  cni.projectcalico.org/podIP: 172.16.166.160/32\n                  cni.projectcalico.org/podIPs: 172.16.166.160/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"chain\",\n                        \"ips\": [\n                            \"172.16.166.160\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"chain\",\n                        \"ips\": [\n                            \"172.16.166.160\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\nStatus:           Running\nIP:               172.16.166.160\nIPs:\n  IP:           172.16.166.160\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://b61d1995470fd8568402d2480b3a597c11cb5510eeb0ddd082c2fdde7def15d8\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 19 Jul 2023 00:17:19 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tl8qj (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-tl8qj:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 30s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 30s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-6657/agnhost-primary-ptv6b to controller-1\n  Normal  AddedInterface  1s    multus             Add eth0 [172.16.166.160/32] from chain\n  Normal  Pulled          1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
    Jul 19 00:17:20.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 describe rc agnhost-primary'
    Jul 19 00:17:20.457: INFO: stderr: ""
    Jul 19 00:17:20.457: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6657\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-ptv6b\n"
    Jul 19 00:17:20.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 describe service agnhost-primary'
    Jul 19 00:17:20.523: INFO: stderr: ""
    Jul 19 00:17:20.523: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6657\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.101.148.245\nIPs:               10.101.148.245\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.16.166.160:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jul 19 00:17:20.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 describe node controller-0'
    Jul 19 00:17:20.640: INFO: stderr: ""
    Jul 19 00:17:20.640: INFO: stdout: "Name:               controller-0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=controller-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cephfs.csi.ceph.com\":\"controller-0\",\"rbd.csi.ceph.com\":\"controller-0\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.206.2/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.16.192.64\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 18 Jul 2023 21:43:30 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  controller-0\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 19 Jul 2023 00:17:18 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 18 Jul 2023 22:04:53 +0000   Tue, 18 Jul 2023 22:04:53 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 19 Jul 2023 00:17:20 +0000   Tue, 18 Jul 2023 21:43:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 19 Jul 2023 00:17:20 +0000   Tue, 18 Jul 2023 21:43:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 19 Jul 2023 00:17:20 +0000   Tue, 18 Jul 2023 21:43:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 19 Jul 2023 00:17:20 +0000   Tue, 18 Jul 2023 21:44:16 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.206.2\n  Hostname:    controller-0\nCapacity:\n  cpu:                16\n  ephemeral-storage:  10218772Ki\n  hugepages-1Gi:      40Gi\n  hugepages-2Mi:      0\n  memory:             63395392Ki\n  pods:               110\nAllocatable:\n  cpu:                12\n  ephemeral-storage:  9417620260\n  hugepages-1Gi:      40Gi\n  hugepages-2Mi:      0\n  memory:             13157952Ki\n  pods:               110\nSystem Info:\n  Machine ID:                  f3e31f1e858440e5bb8043418c541e93\n  System UUID:                 00000000-0000-0000-0000-0cc47a971c5c\n  Boot ID:                     1be28b2d-7181-43df-a4be-7d244301e283\n  Kernel Version:              5.10.0-6-amd64\n  OS Image:                    Debian GNU/Linux 11 (bullseye)\n  Operating System:            linux\n  Architecture:                amd64\n  Container Runtime Version:   containerd://1.6.21\n  Kubelet Version:             v1.26.1\n  Kube-Proxy Version:          v1.26.1\nPodCIDR:                       172.16.0.0/24\nPodCIDRs:                      172.16.0.0/24\nNon-terminated Pods:           (22 in total)\n  Namespace                    Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                    ----                                                       ------------  ----------  ---------------  -------------  ---\n  cert-manager                 cm-cert-manager-7fb65857f5-5mnw8                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         146m\n  cert-manager                 cm-cert-manager-cainjector-86b69d7d69-6czpr                0 (0%)        0 (0%)      0 (0%)           0 (0%)         146m\n  cert-manager                 cm-cert-manager-webhook-98ddcd5cb-2q6p9                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         146m\n  flux-helm                    helm-controller-5fb8ccb85d-nl9lf                           0 (0%)        1 (8%)      64Mi (0%)        1Gi (7%)       149m\n  flux-helm                    source-controller-69b5d8f7d8-66tt6                         0 (0%)        1 (8%)      64Mi (0%)        1Gi (7%)       149m\n  kube-system                  calico-kube-controllers-7f5cd5f684-5jblg                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  calico-node-hmrnj                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  cephfs-nodeplugin-n2vfj                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  kube-system                  cephfs-provisioner-5f69fbf97-q8lg6                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  kube-system                  coredns-66856967f4-whd5z                                   0 (0%)        0 (0%)      70Mi (0%)        170Mi (1%)     153m\n  kube-system                  ic-nginx-ingress-ingress-nginx-controller-6hpm5            0 (0%)        0 (0%)      90Mi (0%)        0 (0%)         148m\n  kube-system                  kube-apiserver-controller-0                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  kube-controller-manager-controller-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  kube-multus-ds-amd64-bcmw4                                 0 (0%)        0 (0%)      50Mi (0%)        50Mi (0%)      153m\n  kube-system                  kube-proxy-kxjqb                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  kube-scheduler-controller-0                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         153m\n  kube-system                  kube-sriov-cni-ds-amd64-69r58                              0 (0%)        0 (0%)      50Mi (0%)        50Mi (0%)      153m\n  kube-system                  rbd-nodeplugin-j7slt                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  kube-system                  rbd-provisioner-54c6c894f7-sggw9                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  platform-deployment-manager  dm-monitor-84b75cf89c-vnb6c                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         144m\n  platform-deployment-manager  platform-deployment-manager-7ff76b89d-qg7g9                0 (0%)        0 (0%)      0 (0%)           0 (0%)         145m\n  sonobuoy                     sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                0 (0%)      2 (16%)\n  memory             388Mi (3%)  2318Mi (18%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                  From             Message\n  ----     ------                   ----                 ----             -------\n  Normal   Starting                 132m                 kube-proxy       \n  Normal   Starting                 153m                 kube-proxy       \n  Normal   Starting                 153m                 kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      153m                 kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  153m                 kubelet          Node controller-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    153m                 kubelet          Node controller-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     153m                 kubelet          Node controller-0 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  153m                 kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           153m                 node-controller  Node controller-0 event: Registered Node controller-0 in Controller\n  Normal   NodeReady                153m                 kubelet          Node controller-0 status is now: NodeReady\n  Normal   NodeHasSufficientMemory  153m                 kubelet          Node controller-0 status is now: NodeHasSufficientMemory\n  Warning  InvalidDiskCapacity      153m                 kubelet          invalid capacity 0 on image filesystem\n  Normal   Starting                 153m                 kubelet          Starting kubelet.\n  Normal   NodeHasNoDiskPressure    153m                 kubelet          Node controller-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     153m                 kubelet          Node controller-0 status is now: NodeHasSufficientPID\n  Normal   NodeNotReady             153m                 kubelet          Node controller-0 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  153m                 kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                153m                 kubelet          Node controller-0 status is now: NodeReady\n  Normal   Starting                 133m                 kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      133m                 kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  133m (x8 over 133m)  kubelet          Node controller-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    133m (x7 over 133m)  kubelet          Node controller-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     133m (x7 over 133m)  kubelet          Node controller-0 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  133m                 kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           132m                 node-controller  Node controller-0 event: Registered Node controller-0 in Controller\n"
    Jul 19 00:17:20.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6657 describe namespace kubectl-6657'
    Jul 19 00:17:20.706: INFO: stderr: ""
    Jul 19 00:17:20.706: INFO: stdout: "Name:         kubectl-6657\nLabels:       e2e-framework=kubectl\n              e2e-run=cc259ba7-b4dd-4a59-af4b-b9b08bdad088\n              kubernetes.io/metadata.name=kubectl-6657\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:20.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6657" for this suite. 07/19/23 00:17:20.708
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:20.71
Jul 19 00:17:20.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename containers 07/19/23 00:17:20.711
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:20.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:20.717
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 07/19/23 00:17:20.719
Jul 19 00:17:20.722: INFO: Waiting up to 5m0s for pod "client-containers-50912c02-d330-414e-98b3-666e5825c75b" in namespace "containers-781" to be "Succeeded or Failed"
Jul 19 00:17:20.726: INFO: Pod "client-containers-50912c02-d330-414e-98b3-666e5825c75b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.558887ms
Jul 19 00:17:22.729: INFO: Pod "client-containers-50912c02-d330-414e-98b3-666e5825c75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006162957s
Jul 19 00:17:24.729: INFO: Pod "client-containers-50912c02-d330-414e-98b3-666e5825c75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006630355s
STEP: Saw pod success 07/19/23 00:17:24.729
Jul 19 00:17:24.729: INFO: Pod "client-containers-50912c02-d330-414e-98b3-666e5825c75b" satisfied condition "Succeeded or Failed"
Jul 19 00:17:24.731: INFO: Trying to get logs from node controller-1 pod client-containers-50912c02-d330-414e-98b3-666e5825c75b container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:17:24.734
Jul 19 00:17:24.740: INFO: Waiting for pod client-containers-50912c02-d330-414e-98b3-666e5825c75b to disappear
Jul 19 00:17:24.741: INFO: Pod client-containers-50912c02-d330-414e-98b3-666e5825c75b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:24.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-781" for this suite. 07/19/23 00:17:24.744
------------------------------
• [4.036 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:20.71
    Jul 19 00:17:20.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename containers 07/19/23 00:17:20.711
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:20.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:20.717
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 07/19/23 00:17:20.719
    Jul 19 00:17:20.722: INFO: Waiting up to 5m0s for pod "client-containers-50912c02-d330-414e-98b3-666e5825c75b" in namespace "containers-781" to be "Succeeded or Failed"
    Jul 19 00:17:20.726: INFO: Pod "client-containers-50912c02-d330-414e-98b3-666e5825c75b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.558887ms
    Jul 19 00:17:22.729: INFO: Pod "client-containers-50912c02-d330-414e-98b3-666e5825c75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006162957s
    Jul 19 00:17:24.729: INFO: Pod "client-containers-50912c02-d330-414e-98b3-666e5825c75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006630355s
    STEP: Saw pod success 07/19/23 00:17:24.729
    Jul 19 00:17:24.729: INFO: Pod "client-containers-50912c02-d330-414e-98b3-666e5825c75b" satisfied condition "Succeeded or Failed"
    Jul 19 00:17:24.731: INFO: Trying to get logs from node controller-1 pod client-containers-50912c02-d330-414e-98b3-666e5825c75b container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:17:24.734
    Jul 19 00:17:24.740: INFO: Waiting for pod client-containers-50912c02-d330-414e-98b3-666e5825c75b to disappear
    Jul 19 00:17:24.741: INFO: Pod client-containers-50912c02-d330-414e-98b3-666e5825c75b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:24.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-781" for this suite. 07/19/23 00:17:24.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:24.746
Jul 19 00:17:24.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename disruption 07/19/23 00:17:24.747
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:24.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:24.753
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 07/19/23 00:17:24.758
STEP: Waiting for all pods to be running 07/19/23 00:17:26.771
Jul 19 00:17:26.777: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:28.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8707" for this suite. 07/19/23 00:17:28.783
------------------------------
• [4.039 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:24.746
    Jul 19 00:17:24.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename disruption 07/19/23 00:17:24.747
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:24.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:24.753
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 07/19/23 00:17:24.758
    STEP: Waiting for all pods to be running 07/19/23 00:17:26.771
    Jul 19 00:17:26.777: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:28.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8707" for this suite. 07/19/23 00:17:28.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:28.786
Jul 19 00:17:28.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename endpointslice 07/19/23 00:17:28.786
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:28.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:28.796
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jul 19 00:17:28.803: INFO: Endpoints addresses: [192.168.206.2 192.168.206.3] , ports: [6443]
Jul 19 00:17:28.803: INFO: EndpointSlices addresses: [192.168.206.2 192.168.206.3] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:28.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6468" for this suite. 07/19/23 00:17:28.805
------------------------------
• [0.022 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:28.786
    Jul 19 00:17:28.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename endpointslice 07/19/23 00:17:28.786
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:28.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:28.796
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jul 19 00:17:28.803: INFO: Endpoints addresses: [192.168.206.2 192.168.206.3] , ports: [6443]
    Jul 19 00:17:28.803: INFO: EndpointSlices addresses: [192.168.206.2 192.168.206.3] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:28.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6468" for this suite. 07/19/23 00:17:28.805
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:28.808
Jul 19 00:17:28.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename runtimeclass 07/19/23 00:17:28.808
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:28.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:28.818
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 07/19/23 00:17:28.82
STEP: getting /apis/node.k8s.io 07/19/23 00:17:28.822
STEP: getting /apis/node.k8s.io/v1 07/19/23 00:17:28.823
STEP: creating 07/19/23 00:17:28.824
STEP: watching 07/19/23 00:17:28.83
Jul 19 00:17:28.830: INFO: starting watch
STEP: getting 07/19/23 00:17:28.833
STEP: listing 07/19/23 00:17:28.834
STEP: patching 07/19/23 00:17:28.836
STEP: updating 07/19/23 00:17:28.838
Jul 19 00:17:28.841: INFO: waiting for watch events with expected annotations
STEP: deleting 07/19/23 00:17:28.841
STEP: deleting a collection 07/19/23 00:17:28.846
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:28.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9778" for this suite. 07/19/23 00:17:28.855
------------------------------
• [0.050 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:28.808
    Jul 19 00:17:28.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename runtimeclass 07/19/23 00:17:28.808
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:28.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:28.818
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 07/19/23 00:17:28.82
    STEP: getting /apis/node.k8s.io 07/19/23 00:17:28.822
    STEP: getting /apis/node.k8s.io/v1 07/19/23 00:17:28.823
    STEP: creating 07/19/23 00:17:28.824
    STEP: watching 07/19/23 00:17:28.83
    Jul 19 00:17:28.830: INFO: starting watch
    STEP: getting 07/19/23 00:17:28.833
    STEP: listing 07/19/23 00:17:28.834
    STEP: patching 07/19/23 00:17:28.836
    STEP: updating 07/19/23 00:17:28.838
    Jul 19 00:17:28.841: INFO: waiting for watch events with expected annotations
    STEP: deleting 07/19/23 00:17:28.841
    STEP: deleting a collection 07/19/23 00:17:28.846
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:28.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9778" for this suite. 07/19/23 00:17:28.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:28.858
Jul 19 00:17:28.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename var-expansion 07/19/23 00:17:28.859
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:28.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:28.866
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 07/19/23 00:17:28.868
Jul 19 00:17:28.872: INFO: Waiting up to 5m0s for pod "var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2" in namespace "var-expansion-8777" to be "Succeeded or Failed"
Jul 19 00:17:28.873: INFO: Pod "var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.510238ms
Jul 19 00:17:30.876: INFO: Pod "var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004218089s
Jul 19 00:17:32.877: INFO: Pod "var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005340115s
STEP: Saw pod success 07/19/23 00:17:32.877
Jul 19 00:17:32.877: INFO: Pod "var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2" satisfied condition "Succeeded or Failed"
Jul 19 00:17:32.879: INFO: Trying to get logs from node controller-1 pod var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2 container dapi-container: <nil>
STEP: delete the pod 07/19/23 00:17:32.883
Jul 19 00:17:32.889: INFO: Waiting for pod var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2 to disappear
Jul 19 00:17:32.890: INFO: Pod var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:32.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8777" for this suite. 07/19/23 00:17:32.893
------------------------------
• [4.036 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:28.858
    Jul 19 00:17:28.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename var-expansion 07/19/23 00:17:28.859
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:28.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:28.866
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 07/19/23 00:17:28.868
    Jul 19 00:17:28.872: INFO: Waiting up to 5m0s for pod "var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2" in namespace "var-expansion-8777" to be "Succeeded or Failed"
    Jul 19 00:17:28.873: INFO: Pod "var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.510238ms
    Jul 19 00:17:30.876: INFO: Pod "var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004218089s
    Jul 19 00:17:32.877: INFO: Pod "var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005340115s
    STEP: Saw pod success 07/19/23 00:17:32.877
    Jul 19 00:17:32.877: INFO: Pod "var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2" satisfied condition "Succeeded or Failed"
    Jul 19 00:17:32.879: INFO: Trying to get logs from node controller-1 pod var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2 container dapi-container: <nil>
    STEP: delete the pod 07/19/23 00:17:32.883
    Jul 19 00:17:32.889: INFO: Waiting for pod var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2 to disappear
    Jul 19 00:17:32.890: INFO: Pod var-expansion-dd22e811-c03e-47e6-86d4-5edfd4f54df2 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:32.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8777" for this suite. 07/19/23 00:17:32.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:32.896
Jul 19 00:17:32.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pod-network-test 07/19/23 00:17:32.897
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:32.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:32.904
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-9926 07/19/23 00:17:32.906
STEP: creating a selector 07/19/23 00:17:32.906
STEP: Creating the service pods in kubernetes 07/19/23 00:17:32.906
Jul 19 00:17:32.906: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 19 00:17:32.919: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9926" to be "running and ready"
Jul 19 00:17:32.924: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.603254ms
Jul 19 00:17:32.924: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:17:34.926: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006488079s
Jul 19 00:17:34.926: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 19 00:17:36.928: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.0085408s
Jul 19 00:17:36.928: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 19 00:17:38.927: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007616792s
Jul 19 00:17:38.927: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 19 00:17:40.927: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007219142s
Jul 19 00:17:40.927: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 19 00:17:42.927: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007973512s
Jul 19 00:17:42.927: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 19 00:17:44.927: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.007592508s
Jul 19 00:17:44.927: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jul 19 00:17:44.927: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jul 19 00:17:44.928: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9926" to be "running and ready"
Jul 19 00:17:44.930: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.477059ms
Jul 19 00:17:44.930: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jul 19 00:17:44.930: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 07/19/23 00:17:44.931
Jul 19 00:17:44.936: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9926" to be "running"
Jul 19 00:17:44.940: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.626012ms
Jul 19 00:17:46.947: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01081263s
Jul 19 00:17:46.947: INFO: Pod "test-container-pod" satisfied condition "running"
Jul 19 00:17:46.949: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9926" to be "running"
Jul 19 00:17:46.954: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.972573ms
Jul 19 00:17:46.954: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jul 19 00:17:46.958: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 19 00:17:46.958: INFO: Going to poll 172.16.192.113 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jul 19 00:17:46.960: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.192.113:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9926 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:17:46.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:17:46.960: INFO: ExecWithOptions: Clientset creation
Jul 19 00:17:46.960: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9926/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.192.113%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 19 00:17:47.017: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 19 00:17:47.017: INFO: Going to poll 172.16.166.129 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jul 19 00:17:47.019: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.166.129:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9926 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:17:47.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:17:47.020: INFO: ExecWithOptions: Clientset creation
Jul 19 00:17:47.020: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9926/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.166.129%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jul 19 00:17:47.061: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:47.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9926" for this suite. 07/19/23 00:17:47.063
------------------------------
• [SLOW TEST] [14.170 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:32.896
    Jul 19 00:17:32.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pod-network-test 07/19/23 00:17:32.897
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:32.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:32.904
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-9926 07/19/23 00:17:32.906
    STEP: creating a selector 07/19/23 00:17:32.906
    STEP: Creating the service pods in kubernetes 07/19/23 00:17:32.906
    Jul 19 00:17:32.906: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jul 19 00:17:32.919: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9926" to be "running and ready"
    Jul 19 00:17:32.924: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.603254ms
    Jul 19 00:17:32.924: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:17:34.926: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006488079s
    Jul 19 00:17:34.926: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 19 00:17:36.928: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.0085408s
    Jul 19 00:17:36.928: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 19 00:17:38.927: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007616792s
    Jul 19 00:17:38.927: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 19 00:17:40.927: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007219142s
    Jul 19 00:17:40.927: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 19 00:17:42.927: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007973512s
    Jul 19 00:17:42.927: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 19 00:17:44.927: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.007592508s
    Jul 19 00:17:44.927: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jul 19 00:17:44.927: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jul 19 00:17:44.928: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9926" to be "running and ready"
    Jul 19 00:17:44.930: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.477059ms
    Jul 19 00:17:44.930: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jul 19 00:17:44.930: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 07/19/23 00:17:44.931
    Jul 19 00:17:44.936: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9926" to be "running"
    Jul 19 00:17:44.940: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.626012ms
    Jul 19 00:17:46.947: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01081263s
    Jul 19 00:17:46.947: INFO: Pod "test-container-pod" satisfied condition "running"
    Jul 19 00:17:46.949: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9926" to be "running"
    Jul 19 00:17:46.954: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.972573ms
    Jul 19 00:17:46.954: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jul 19 00:17:46.958: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jul 19 00:17:46.958: INFO: Going to poll 172.16.192.113 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jul 19 00:17:46.960: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.192.113:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9926 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:17:46.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:17:46.960: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:17:46.960: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9926/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.192.113%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 19 00:17:47.017: INFO: Found all 1 expected endpoints: [netserver-0]
    Jul 19 00:17:47.017: INFO: Going to poll 172.16.166.129 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jul 19 00:17:47.019: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.166.129:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9926 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:17:47.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:17:47.020: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:17:47.020: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9926/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.166.129%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jul 19 00:17:47.061: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:47.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9926" for this suite. 07/19/23 00:17:47.063
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:47.066
Jul 19 00:17:47.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubelet-test 07/19/23 00:17:47.067
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:47.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:47.076
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:47.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2632" for this suite. 07/19/23 00:17:47.093
------------------------------
• [0.029 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:47.066
    Jul 19 00:17:47.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubelet-test 07/19/23 00:17:47.067
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:47.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:47.076
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:47.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2632" for this suite. 07/19/23 00:17:47.093
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:47.096
Jul 19 00:17:47.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/19/23 00:17:47.097
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:47.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:47.103
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 07/19/23 00:17:47.105
STEP: submitting the pod to kubernetes 07/19/23 00:17:47.105
STEP: verifying QOS class is set on the pod 07/19/23 00:17:47.109
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:47.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8817" for this suite. 07/19/23 00:17:47.117
------------------------------
• [0.024 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:47.096
    Jul 19 00:17:47.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/19/23 00:17:47.097
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:47.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:47.103
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 07/19/23 00:17:47.105
    STEP: submitting the pod to kubernetes 07/19/23 00:17:47.105
    STEP: verifying QOS class is set on the pod 07/19/23 00:17:47.109
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:47.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8817" for this suite. 07/19/23 00:17:47.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:47.121
Jul 19 00:17:47.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 00:17:47.122
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:47.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:47.138
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:17:47.14
Jul 19 00:17:47.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda" in namespace "downward-api-164" to be "Succeeded or Failed"
Jul 19 00:17:47.145: INFO: Pod "downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda": Phase="Pending", Reason="", readiness=false. Elapsed: 1.296002ms
Jul 19 00:17:49.148: INFO: Pod "downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003931117s
Jul 19 00:17:51.147: INFO: Pod "downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003350417s
STEP: Saw pod success 07/19/23 00:17:51.147
Jul 19 00:17:51.147: INFO: Pod "downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda" satisfied condition "Succeeded or Failed"
Jul 19 00:17:51.149: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda container client-container: <nil>
STEP: delete the pod 07/19/23 00:17:51.152
Jul 19 00:17:51.159: INFO: Waiting for pod downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda to disappear
Jul 19 00:17:51.160: INFO: Pod downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:51.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-164" for this suite. 07/19/23 00:17:51.163
------------------------------
• [4.044 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:47.121
    Jul 19 00:17:47.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 00:17:47.122
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:47.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:47.138
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:17:47.14
    Jul 19 00:17:47.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda" in namespace "downward-api-164" to be "Succeeded or Failed"
    Jul 19 00:17:47.145: INFO: Pod "downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda": Phase="Pending", Reason="", readiness=false. Elapsed: 1.296002ms
    Jul 19 00:17:49.148: INFO: Pod "downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003931117s
    Jul 19 00:17:51.147: INFO: Pod "downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003350417s
    STEP: Saw pod success 07/19/23 00:17:51.147
    Jul 19 00:17:51.147: INFO: Pod "downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda" satisfied condition "Succeeded or Failed"
    Jul 19 00:17:51.149: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda container client-container: <nil>
    STEP: delete the pod 07/19/23 00:17:51.152
    Jul 19 00:17:51.159: INFO: Waiting for pod downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda to disappear
    Jul 19 00:17:51.160: INFO: Pod downwardapi-volume-c21fe5b7-a9a3-40c0-9068-d0bd16646eda no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:51.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-164" for this suite. 07/19/23 00:17:51.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:51.168
Jul 19 00:17:51.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename gc 07/19/23 00:17:51.169
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:51.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:51.179
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jul 19 00:17:51.198: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d170ec1d-cf34-4581-a267-9a37ca4d7a77", Controller:(*bool)(0xc0062f4736), BlockOwnerDeletion:(*bool)(0xc0062f4737)}}
Jul 19 00:17:51.208: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a6d7d37f-15a0-457d-b738-a02ed40199d8", Controller:(*bool)(0xc0062f4976), BlockOwnerDeletion:(*bool)(0xc0062f4977)}}
Jul 19 00:17:51.211: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d66c7c1b-79b5-4540-9392-de7c71eae310", Controller:(*bool)(0xc0051dc80e), BlockOwnerDeletion:(*bool)(0xc0051dc80f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jul 19 00:17:56.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1791" for this suite. 07/19/23 00:17:56.218
------------------------------
• [SLOW TEST] [5.053 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:51.168
    Jul 19 00:17:51.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename gc 07/19/23 00:17:51.169
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:51.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:51.179
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jul 19 00:17:51.198: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d170ec1d-cf34-4581-a267-9a37ca4d7a77", Controller:(*bool)(0xc0062f4736), BlockOwnerDeletion:(*bool)(0xc0062f4737)}}
    Jul 19 00:17:51.208: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a6d7d37f-15a0-457d-b738-a02ed40199d8", Controller:(*bool)(0xc0062f4976), BlockOwnerDeletion:(*bool)(0xc0062f4977)}}
    Jul 19 00:17:51.211: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d66c7c1b-79b5-4540-9392-de7c71eae310", Controller:(*bool)(0xc0051dc80e), BlockOwnerDeletion:(*bool)(0xc0051dc80f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:17:56.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1791" for this suite. 07/19/23 00:17:56.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:17:56.221
Jul 19 00:17:56.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename statefulset 07/19/23 00:17:56.222
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:56.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:56.23
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8761 07/19/23 00:17:56.232
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 07/19/23 00:17:56.235
Jul 19 00:17:56.244: INFO: Found 0 stateful pods, waiting for 3
Jul 19 00:18:06.247: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 00:18:06.247: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 00:18:06.247: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 07/19/23 00:18:06.252
Jul 19 00:18:06.267: INFO: Updating stateful set ss2
STEP: Creating a new revision 07/19/23 00:18:06.267
STEP: Not applying an update when the partition is greater than the number of replicas 07/19/23 00:18:16.277
STEP: Performing a canary update 07/19/23 00:18:16.278
Jul 19 00:18:16.293: INFO: Updating stateful set ss2
Jul 19 00:18:16.300: INFO: Waiting for Pod statefulset-8761/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 07/19/23 00:18:26.305
Jul 19 00:18:26.329: INFO: Found 2 stateful pods, waiting for 3
Jul 19 00:18:36.333: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 00:18:36.333: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 00:18:36.333: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 07/19/23 00:18:36.336
Jul 19 00:18:36.351: INFO: Updating stateful set ss2
Jul 19 00:18:36.357: INFO: Waiting for Pod statefulset-8761/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jul 19 00:18:46.376: INFO: Updating stateful set ss2
Jul 19 00:18:46.382: INFO: Waiting for StatefulSet statefulset-8761/ss2 to complete update
Jul 19 00:18:46.382: INFO: Waiting for Pod statefulset-8761/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jul 19 00:18:56.387: INFO: Deleting all statefulset in ns statefulset-8761
Jul 19 00:18:56.388: INFO: Scaling statefulset ss2 to 0
Jul 19 00:19:06.415: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 00:19:06.416: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:19:06.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8761" for this suite. 07/19/23 00:19:06.425
------------------------------
• [SLOW TEST] [70.207 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:17:56.221
    Jul 19 00:17:56.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename statefulset 07/19/23 00:17:56.222
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:17:56.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:17:56.23
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8761 07/19/23 00:17:56.232
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 07/19/23 00:17:56.235
    Jul 19 00:17:56.244: INFO: Found 0 stateful pods, waiting for 3
    Jul 19 00:18:06.247: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 00:18:06.247: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 00:18:06.247: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 07/19/23 00:18:06.252
    Jul 19 00:18:06.267: INFO: Updating stateful set ss2
    STEP: Creating a new revision 07/19/23 00:18:06.267
    STEP: Not applying an update when the partition is greater than the number of replicas 07/19/23 00:18:16.277
    STEP: Performing a canary update 07/19/23 00:18:16.278
    Jul 19 00:18:16.293: INFO: Updating stateful set ss2
    Jul 19 00:18:16.300: INFO: Waiting for Pod statefulset-8761/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 07/19/23 00:18:26.305
    Jul 19 00:18:26.329: INFO: Found 2 stateful pods, waiting for 3
    Jul 19 00:18:36.333: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 00:18:36.333: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 00:18:36.333: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 07/19/23 00:18:36.336
    Jul 19 00:18:36.351: INFO: Updating stateful set ss2
    Jul 19 00:18:36.357: INFO: Waiting for Pod statefulset-8761/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jul 19 00:18:46.376: INFO: Updating stateful set ss2
    Jul 19 00:18:46.382: INFO: Waiting for StatefulSet statefulset-8761/ss2 to complete update
    Jul 19 00:18:46.382: INFO: Waiting for Pod statefulset-8761/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jul 19 00:18:56.387: INFO: Deleting all statefulset in ns statefulset-8761
    Jul 19 00:18:56.388: INFO: Scaling statefulset ss2 to 0
    Jul 19 00:19:06.415: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 19 00:19:06.416: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:19:06.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8761" for this suite. 07/19/23 00:19:06.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:19:06.43
Jul 19 00:19:06.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename init-container 07/19/23 00:19:06.431
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:19:06.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:19:06.438
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 07/19/23 00:19:06.441
Jul 19 00:19:06.441: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:19:09.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2292" for this suite. 07/19/23 00:19:09.199
------------------------------
• [2.772 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:19:06.43
    Jul 19 00:19:06.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename init-container 07/19/23 00:19:06.431
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:19:06.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:19:06.438
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 07/19/23 00:19:06.441
    Jul 19 00:19:06.441: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:19:09.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2292" for this suite. 07/19/23 00:19:09.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:19:09.202
Jul 19 00:19:09.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:19:09.203
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:19:09.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:19:09.21
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:19:09.218
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:19:09.685
STEP: Deploying the webhook pod 07/19/23 00:19:09.689
STEP: Wait for the deployment to be ready 07/19/23 00:19:09.695
Jul 19 00:19:09.701: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:19:11.706
STEP: Verifying the service has paired with the endpoint 07/19/23 00:19:11.714
Jul 19 00:19:12.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 07/19/23 00:19:12.717
STEP: Registering slow webhook via the AdmissionRegistration API 07/19/23 00:19:12.717
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 07/19/23 00:19:12.727
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 07/19/23 00:19:13.732
STEP: Registering slow webhook via the AdmissionRegistration API 07/19/23 00:19:13.732
STEP: Having no error when timeout is longer than webhook latency 07/19/23 00:19:14.987
STEP: Registering slow webhook via the AdmissionRegistration API 07/19/23 00:19:14.988
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 07/19/23 00:19:20.003
STEP: Registering slow webhook via the AdmissionRegistration API 07/19/23 00:19:20.003
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:19:25.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2922" for this suite. 07/19/23 00:19:25.044
STEP: Destroying namespace "webhook-2922-markers" for this suite. 07/19/23 00:19:25.05
------------------------------
• [SLOW TEST] [15.851 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:19:09.202
    Jul 19 00:19:09.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:19:09.203
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:19:09.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:19:09.21
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:19:09.218
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:19:09.685
    STEP: Deploying the webhook pod 07/19/23 00:19:09.689
    STEP: Wait for the deployment to be ready 07/19/23 00:19:09.695
    Jul 19 00:19:09.701: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:19:11.706
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:19:11.714
    Jul 19 00:19:12.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 07/19/23 00:19:12.717
    STEP: Registering slow webhook via the AdmissionRegistration API 07/19/23 00:19:12.717
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 07/19/23 00:19:12.727
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 07/19/23 00:19:13.732
    STEP: Registering slow webhook via the AdmissionRegistration API 07/19/23 00:19:13.732
    STEP: Having no error when timeout is longer than webhook latency 07/19/23 00:19:14.987
    STEP: Registering slow webhook via the AdmissionRegistration API 07/19/23 00:19:14.988
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 07/19/23 00:19:20.003
    STEP: Registering slow webhook via the AdmissionRegistration API 07/19/23 00:19:20.003
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:19:25.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2922" for this suite. 07/19/23 00:19:25.044
    STEP: Destroying namespace "webhook-2922-markers" for this suite. 07/19/23 00:19:25.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:19:25.054
Jul 19 00:19:25.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename podtemplate 07/19/23 00:19:25.055
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:19:25.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:19:25.061
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 07/19/23 00:19:25.063
STEP: Replace a pod template 07/19/23 00:19:25.066
Jul 19 00:19:25.070: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jul 19 00:19:25.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7630" for this suite. 07/19/23 00:19:25.072
------------------------------
• [0.020 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:19:25.054
    Jul 19 00:19:25.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename podtemplate 07/19/23 00:19:25.055
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:19:25.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:19:25.061
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 07/19/23 00:19:25.063
    STEP: Replace a pod template 07/19/23 00:19:25.066
    Jul 19 00:19:25.070: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:19:25.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7630" for this suite. 07/19/23 00:19:25.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:19:25.075
Jul 19 00:19:25.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename dns 07/19/23 00:19:25.075
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:19:25.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:19:25.083
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 07/19/23 00:19:25.085
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
 07/19/23 00:19:25.087
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
 07/19/23 00:19:25.087
STEP: creating a pod to probe DNS 07/19/23 00:19:25.087
STEP: submitting the pod to kubernetes 07/19/23 00:19:25.087
Jul 19 00:19:25.092: INFO: Waiting up to 15m0s for pod "dns-test-a607b24d-fcc8-4659-b985-c99f815ebeee" in namespace "dns-9938" to be "running"
Jul 19 00:19:25.094: INFO: Pod "dns-test-a607b24d-fcc8-4659-b985-c99f815ebeee": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548609ms
Jul 19 00:19:27.096: INFO: Pod "dns-test-a607b24d-fcc8-4659-b985-c99f815ebeee": Phase="Running", Reason="", readiness=true. Elapsed: 2.003967151s
Jul 19 00:19:27.096: INFO: Pod "dns-test-a607b24d-fcc8-4659-b985-c99f815ebeee" satisfied condition "running"
STEP: retrieving the pod 07/19/23 00:19:27.096
STEP: looking for the results for each expected name from probers 07/19/23 00:19:27.098
Jul 19 00:19:27.103: INFO: DNS probes using dns-test-a607b24d-fcc8-4659-b985-c99f815ebeee succeeded

STEP: deleting the pod 07/19/23 00:19:27.103
STEP: changing the externalName to bar.example.com 07/19/23 00:19:27.11
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
 07/19/23 00:19:27.114
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
 07/19/23 00:19:27.114
STEP: creating a second pod to probe DNS 07/19/23 00:19:27.114
STEP: submitting the pod to kubernetes 07/19/23 00:19:27.114
Jul 19 00:19:27.117: INFO: Waiting up to 15m0s for pod "dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd" in namespace "dns-9938" to be "running"
Jul 19 00:19:27.120: INFO: Pod "dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079779ms
Jul 19 00:19:29.123: INFO: Pod "dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.006061081s
Jul 19 00:19:29.123: INFO: Pod "dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd" satisfied condition "running"
STEP: retrieving the pod 07/19/23 00:19:29.123
STEP: looking for the results for each expected name from probers 07/19/23 00:19:29.125
Jul 19 00:19:29.128: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:29.130: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:29.130: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

Jul 19 00:19:34.135: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:34.137: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:34.137: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

Jul 19 00:19:39.135: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:39.137: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:39.137: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

Jul 19 00:19:44.133: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:44.135: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:44.135: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

Jul 19 00:19:49.136: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:49.138: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:49.138: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

Jul 19 00:19:54.133: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:54.136: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 19 00:19:54.136: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

Jul 19 00:19:59.138: INFO: DNS probes using dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd succeeded

STEP: deleting the pod 07/19/23 00:19:59.138
STEP: changing the service to type=ClusterIP 07/19/23 00:19:59.145
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
 07/19/23 00:19:59.159
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
 07/19/23 00:19:59.159
STEP: creating a third pod to probe DNS 07/19/23 00:19:59.159
STEP: submitting the pod to kubernetes 07/19/23 00:19:59.161
Jul 19 00:19:59.165: INFO: Waiting up to 15m0s for pod "dns-test-f5ca7c70-4be3-4d95-a8a1-f798d63035af" in namespace "dns-9938" to be "running"
Jul 19 00:19:59.170: INFO: Pod "dns-test-f5ca7c70-4be3-4d95-a8a1-f798d63035af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.831946ms
Jul 19 00:20:01.172: INFO: Pod "dns-test-f5ca7c70-4be3-4d95-a8a1-f798d63035af": Phase="Running", Reason="", readiness=true. Elapsed: 2.00705202s
Jul 19 00:20:01.172: INFO: Pod "dns-test-f5ca7c70-4be3-4d95-a8a1-f798d63035af" satisfied condition "running"
STEP: retrieving the pod 07/19/23 00:20:01.172
STEP: looking for the results for each expected name from probers 07/19/23 00:20:01.174
Jul 19 00:20:01.178: INFO: DNS probes using dns-test-f5ca7c70-4be3-4d95-a8a1-f798d63035af succeeded

STEP: deleting the pod 07/19/23 00:20:01.178
STEP: deleting the test externalName service 07/19/23 00:20:01.186
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:01.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9938" for this suite. 07/19/23 00:20:01.196
------------------------------
• [SLOW TEST] [36.124 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:19:25.075
    Jul 19 00:19:25.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename dns 07/19/23 00:19:25.075
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:19:25.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:19:25.083
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 07/19/23 00:19:25.085
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
     07/19/23 00:19:25.087
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
     07/19/23 00:19:25.087
    STEP: creating a pod to probe DNS 07/19/23 00:19:25.087
    STEP: submitting the pod to kubernetes 07/19/23 00:19:25.087
    Jul 19 00:19:25.092: INFO: Waiting up to 15m0s for pod "dns-test-a607b24d-fcc8-4659-b985-c99f815ebeee" in namespace "dns-9938" to be "running"
    Jul 19 00:19:25.094: INFO: Pod "dns-test-a607b24d-fcc8-4659-b985-c99f815ebeee": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548609ms
    Jul 19 00:19:27.096: INFO: Pod "dns-test-a607b24d-fcc8-4659-b985-c99f815ebeee": Phase="Running", Reason="", readiness=true. Elapsed: 2.003967151s
    Jul 19 00:19:27.096: INFO: Pod "dns-test-a607b24d-fcc8-4659-b985-c99f815ebeee" satisfied condition "running"
    STEP: retrieving the pod 07/19/23 00:19:27.096
    STEP: looking for the results for each expected name from probers 07/19/23 00:19:27.098
    Jul 19 00:19:27.103: INFO: DNS probes using dns-test-a607b24d-fcc8-4659-b985-c99f815ebeee succeeded

    STEP: deleting the pod 07/19/23 00:19:27.103
    STEP: changing the externalName to bar.example.com 07/19/23 00:19:27.11
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
     07/19/23 00:19:27.114
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
     07/19/23 00:19:27.114
    STEP: creating a second pod to probe DNS 07/19/23 00:19:27.114
    STEP: submitting the pod to kubernetes 07/19/23 00:19:27.114
    Jul 19 00:19:27.117: INFO: Waiting up to 15m0s for pod "dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd" in namespace "dns-9938" to be "running"
    Jul 19 00:19:27.120: INFO: Pod "dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079779ms
    Jul 19 00:19:29.123: INFO: Pod "dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd": Phase="Running", Reason="", readiness=true. Elapsed: 2.006061081s
    Jul 19 00:19:29.123: INFO: Pod "dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd" satisfied condition "running"
    STEP: retrieving the pod 07/19/23 00:19:29.123
    STEP: looking for the results for each expected name from probers 07/19/23 00:19:29.125
    Jul 19 00:19:29.128: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:29.130: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:29.130: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

    Jul 19 00:19:34.135: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:34.137: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:34.137: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

    Jul 19 00:19:39.135: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:39.137: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:39.137: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

    Jul 19 00:19:44.133: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:44.135: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:44.135: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

    Jul 19 00:19:49.136: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:49.138: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:49.138: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

    Jul 19 00:19:54.133: INFO: File wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:54.136: INFO: File jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local from pod  dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jul 19 00:19:54.136: INFO: Lookups using dns-9938/dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd failed for: [wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local]

    Jul 19 00:19:59.138: INFO: DNS probes using dns-test-3a3aee9b-12c7-4991-97fa-944b04d7b8fd succeeded

    STEP: deleting the pod 07/19/23 00:19:59.138
    STEP: changing the service to type=ClusterIP 07/19/23 00:19:59.145
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
     07/19/23 00:19:59.159
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9938.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9938.svc.cluster.local; sleep 1; done
     07/19/23 00:19:59.159
    STEP: creating a third pod to probe DNS 07/19/23 00:19:59.159
    STEP: submitting the pod to kubernetes 07/19/23 00:19:59.161
    Jul 19 00:19:59.165: INFO: Waiting up to 15m0s for pod "dns-test-f5ca7c70-4be3-4d95-a8a1-f798d63035af" in namespace "dns-9938" to be "running"
    Jul 19 00:19:59.170: INFO: Pod "dns-test-f5ca7c70-4be3-4d95-a8a1-f798d63035af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.831946ms
    Jul 19 00:20:01.172: INFO: Pod "dns-test-f5ca7c70-4be3-4d95-a8a1-f798d63035af": Phase="Running", Reason="", readiness=true. Elapsed: 2.00705202s
    Jul 19 00:20:01.172: INFO: Pod "dns-test-f5ca7c70-4be3-4d95-a8a1-f798d63035af" satisfied condition "running"
    STEP: retrieving the pod 07/19/23 00:20:01.172
    STEP: looking for the results for each expected name from probers 07/19/23 00:20:01.174
    Jul 19 00:20:01.178: INFO: DNS probes using dns-test-f5ca7c70-4be3-4d95-a8a1-f798d63035af succeeded

    STEP: deleting the pod 07/19/23 00:20:01.178
    STEP: deleting the test externalName service 07/19/23 00:20:01.186
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:01.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9938" for this suite. 07/19/23 00:20:01.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:01.199
Jul 19 00:20:01.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/19/23 00:20:01.201
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:01.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:01.209
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 07/19/23 00:20:01.21
STEP: Creating a ResourceQuota 07/19/23 00:20:06.212
STEP: Ensuring resource quota status is calculated 07/19/23 00:20:06.215
STEP: Creating a ReplicaSet 07/19/23 00:20:08.217
STEP: Ensuring resource quota status captures replicaset creation 07/19/23 00:20:08.225
STEP: Deleting a ReplicaSet 07/19/23 00:20:10.228
STEP: Ensuring resource quota status released usage 07/19/23 00:20:10.231
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:12.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7918" for this suite. 07/19/23 00:20:12.236
------------------------------
• [SLOW TEST] [11.039 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:01.199
    Jul 19 00:20:01.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/19/23 00:20:01.201
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:01.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:01.209
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 07/19/23 00:20:01.21
    STEP: Creating a ResourceQuota 07/19/23 00:20:06.212
    STEP: Ensuring resource quota status is calculated 07/19/23 00:20:06.215
    STEP: Creating a ReplicaSet 07/19/23 00:20:08.217
    STEP: Ensuring resource quota status captures replicaset creation 07/19/23 00:20:08.225
    STEP: Deleting a ReplicaSet 07/19/23 00:20:10.228
    STEP: Ensuring resource quota status released usage 07/19/23 00:20:10.231
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:12.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7918" for this suite. 07/19/23 00:20:12.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:12.239
Jul 19 00:20:12.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-runtime 07/19/23 00:20:12.24
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:12.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:12.247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 07/19/23 00:20:12.249
STEP: wait for the container to reach Succeeded 07/19/23 00:20:12.29
STEP: get the container status 07/19/23 00:20:16.303
STEP: the container should be terminated 07/19/23 00:20:16.305
STEP: the termination message should be set 07/19/23 00:20:16.305
Jul 19 00:20:16.305: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 07/19/23 00:20:16.305
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:16.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3341" for this suite. 07/19/23 00:20:16.317
------------------------------
• [4.080 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:12.239
    Jul 19 00:20:12.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-runtime 07/19/23 00:20:12.24
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:12.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:12.247
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 07/19/23 00:20:12.249
    STEP: wait for the container to reach Succeeded 07/19/23 00:20:12.29
    STEP: get the container status 07/19/23 00:20:16.303
    STEP: the container should be terminated 07/19/23 00:20:16.305
    STEP: the termination message should be set 07/19/23 00:20:16.305
    Jul 19 00:20:16.305: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 07/19/23 00:20:16.305
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:16.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3341" for this suite. 07/19/23 00:20:16.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:16.32
Jul 19 00:20:16.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename security-context-test 07/19/23 00:20:16.32
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:16.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:16.328
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jul 19 00:20:16.333: INFO: Waiting up to 5m0s for pod "busybox-user-65534-016d3d49-2a5f-4cf3-8b5d-625cafe50920" in namespace "security-context-test-4330" to be "Succeeded or Failed"
Jul 19 00:20:16.336: INFO: Pod "busybox-user-65534-016d3d49-2a5f-4cf3-8b5d-625cafe50920": Phase="Pending", Reason="", readiness=false. Elapsed: 3.53111ms
Jul 19 00:20:18.339: INFO: Pod "busybox-user-65534-016d3d49-2a5f-4cf3-8b5d-625cafe50920": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005590402s
Jul 19 00:20:20.339: INFO: Pod "busybox-user-65534-016d3d49-2a5f-4cf3-8b5d-625cafe50920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00655027s
Jul 19 00:20:20.339: INFO: Pod "busybox-user-65534-016d3d49-2a5f-4cf3-8b5d-625cafe50920" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:20.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4330" for this suite. 07/19/23 00:20:20.342
------------------------------
• [4.025 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:16.32
    Jul 19 00:20:16.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename security-context-test 07/19/23 00:20:16.32
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:16.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:16.328
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jul 19 00:20:16.333: INFO: Waiting up to 5m0s for pod "busybox-user-65534-016d3d49-2a5f-4cf3-8b5d-625cafe50920" in namespace "security-context-test-4330" to be "Succeeded or Failed"
    Jul 19 00:20:16.336: INFO: Pod "busybox-user-65534-016d3d49-2a5f-4cf3-8b5d-625cafe50920": Phase="Pending", Reason="", readiness=false. Elapsed: 3.53111ms
    Jul 19 00:20:18.339: INFO: Pod "busybox-user-65534-016d3d49-2a5f-4cf3-8b5d-625cafe50920": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005590402s
    Jul 19 00:20:20.339: INFO: Pod "busybox-user-65534-016d3d49-2a5f-4cf3-8b5d-625cafe50920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00655027s
    Jul 19 00:20:20.339: INFO: Pod "busybox-user-65534-016d3d49-2a5f-4cf3-8b5d-625cafe50920" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:20.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4330" for this suite. 07/19/23 00:20:20.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:20.345
Jul 19 00:20:20.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:20:20.345
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:20.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:20.352
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-d374f50b-4e94-4bad-81c1-19ac08989509 07/19/23 00:20:20.356
STEP: Creating configMap with name cm-test-opt-upd-3aa70f19-67b6-40a5-afb4-196c42276ab8 07/19/23 00:20:20.358
STEP: Creating the pod 07/19/23 00:20:20.36
Jul 19 00:20:20.364: INFO: Waiting up to 5m0s for pod "pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6" in namespace "configmap-7081" to be "running and ready"
Jul 19 00:20:20.367: INFO: Pod "pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369217ms
Jul 19 00:20:20.367: INFO: The phase of Pod pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:20:22.369: INFO: Pod "pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005597984s
Jul 19 00:20:22.369: INFO: The phase of Pod pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6 is Running (Ready = true)
Jul 19 00:20:22.369: INFO: Pod "pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-d374f50b-4e94-4bad-81c1-19ac08989509 07/19/23 00:20:22.386
STEP: Updating configmap cm-test-opt-upd-3aa70f19-67b6-40a5-afb4-196c42276ab8 07/19/23 00:20:22.388
STEP: Creating configMap with name cm-test-opt-create-b8bd4bde-ead4-4557-b8f7-93da81b8cb97 07/19/23 00:20:22.392
STEP: waiting to observe update in volume 07/19/23 00:20:22.393
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:24.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7081" for this suite. 07/19/23 00:20:24.411
------------------------------
• [4.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:20.345
    Jul 19 00:20:20.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:20:20.345
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:20.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:20.352
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-d374f50b-4e94-4bad-81c1-19ac08989509 07/19/23 00:20:20.356
    STEP: Creating configMap with name cm-test-opt-upd-3aa70f19-67b6-40a5-afb4-196c42276ab8 07/19/23 00:20:20.358
    STEP: Creating the pod 07/19/23 00:20:20.36
    Jul 19 00:20:20.364: INFO: Waiting up to 5m0s for pod "pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6" in namespace "configmap-7081" to be "running and ready"
    Jul 19 00:20:20.367: INFO: Pod "pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369217ms
    Jul 19 00:20:20.367: INFO: The phase of Pod pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:20:22.369: INFO: Pod "pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.005597984s
    Jul 19 00:20:22.369: INFO: The phase of Pod pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6 is Running (Ready = true)
    Jul 19 00:20:22.369: INFO: Pod "pod-configmaps-083439cd-08b4-4e9f-aa00-173cacd984b6" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-d374f50b-4e94-4bad-81c1-19ac08989509 07/19/23 00:20:22.386
    STEP: Updating configmap cm-test-opt-upd-3aa70f19-67b6-40a5-afb4-196c42276ab8 07/19/23 00:20:22.388
    STEP: Creating configMap with name cm-test-opt-create-b8bd4bde-ead4-4557-b8f7-93da81b8cb97 07/19/23 00:20:22.392
    STEP: waiting to observe update in volume 07/19/23 00:20:22.393
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:24.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7081" for this suite. 07/19/23 00:20:24.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:24.414
Jul 19 00:20:24.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename containers 07/19/23 00:20:24.415
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:24.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:24.442
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jul 19 00:20:24.447: INFO: Waiting up to 5m0s for pod "client-containers-aad7a5b6-6492-4723-8b71-956064090542" in namespace "containers-7274" to be "running"
Jul 19 00:20:24.449: INFO: Pod "client-containers-aad7a5b6-6492-4723-8b71-956064090542": Phase="Pending", Reason="", readiness=false. Elapsed: 1.454258ms
Jul 19 00:20:26.451: INFO: Pod "client-containers-aad7a5b6-6492-4723-8b71-956064090542": Phase="Running", Reason="", readiness=true. Elapsed: 2.00394506s
Jul 19 00:20:26.451: INFO: Pod "client-containers-aad7a5b6-6492-4723-8b71-956064090542" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:26.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7274" for this suite. 07/19/23 00:20:26.464
------------------------------
• [2.053 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:24.414
    Jul 19 00:20:24.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename containers 07/19/23 00:20:24.415
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:24.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:24.442
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jul 19 00:20:24.447: INFO: Waiting up to 5m0s for pod "client-containers-aad7a5b6-6492-4723-8b71-956064090542" in namespace "containers-7274" to be "running"
    Jul 19 00:20:24.449: INFO: Pod "client-containers-aad7a5b6-6492-4723-8b71-956064090542": Phase="Pending", Reason="", readiness=false. Elapsed: 1.454258ms
    Jul 19 00:20:26.451: INFO: Pod "client-containers-aad7a5b6-6492-4723-8b71-956064090542": Phase="Running", Reason="", readiness=true. Elapsed: 2.00394506s
    Jul 19 00:20:26.451: INFO: Pod "client-containers-aad7a5b6-6492-4723-8b71-956064090542" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:26.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7274" for this suite. 07/19/23 00:20:26.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:26.471
Jul 19 00:20:26.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:20:26.472
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:26.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:26.48
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 07/19/23 00:20:26.482
Jul 19 00:20:26.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-9546 api-versions'
Jul 19 00:20:26.546: INFO: stderr: ""
Jul 19 00:20:26.546: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.toolkit.fluxcd.io/v2beta1\nk8s.cni.cncf.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsource.toolkit.fluxcd.io/v1beta1\nsource.toolkit.fluxcd.io/v1beta2\nstarlingx.windriver.com/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:26.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9546" for this suite. 07/19/23 00:20:26.548
------------------------------
• [0.080 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:26.471
    Jul 19 00:20:26.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:20:26.472
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:26.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:26.48
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 07/19/23 00:20:26.482
    Jul 19 00:20:26.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-9546 api-versions'
    Jul 19 00:20:26.546: INFO: stderr: ""
    Jul 19 00:20:26.546: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.toolkit.fluxcd.io/v2beta1\nk8s.cni.cncf.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsource.toolkit.fluxcd.io/v1beta1\nsource.toolkit.fluxcd.io/v1beta2\nstarlingx.windriver.com/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:26.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9546" for this suite. 07/19/23 00:20:26.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:26.552
Jul 19 00:20:26.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename gc 07/19/23 00:20:26.552
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:26.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:26.56
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 07/19/23 00:20:26.562
STEP: Wait for the Deployment to create new ReplicaSet 07/19/23 00:20:26.565
STEP: delete the deployment 07/19/23 00:20:27.071
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 07/19/23 00:20:27.077
STEP: Gathering metrics 07/19/23 00:20:27.587
Jul 19 00:20:27.604: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
Jul 19 00:20:27.605: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.83157ms
Jul 19 00:20:27.605: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
Jul 19 00:20:27.605: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
Jul 19 00:20:27.643: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:27.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4645" for this suite. 07/19/23 00:20:27.646
------------------------------
• [1.097 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:26.552
    Jul 19 00:20:26.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename gc 07/19/23 00:20:26.552
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:26.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:26.56
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 07/19/23 00:20:26.562
    STEP: Wait for the Deployment to create new ReplicaSet 07/19/23 00:20:26.565
    STEP: delete the deployment 07/19/23 00:20:27.071
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 07/19/23 00:20:27.077
    STEP: Gathering metrics 07/19/23 00:20:27.587
    Jul 19 00:20:27.604: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
    Jul 19 00:20:27.605: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.83157ms
    Jul 19 00:20:27.605: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
    Jul 19 00:20:27.605: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
    Jul 19 00:20:27.643: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:27.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4645" for this suite. 07/19/23 00:20:27.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:27.649
Jul 19 00:20:27.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-lifecycle-hook 07/19/23 00:20:27.65
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:27.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:27.659
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 07/19/23 00:20:27.663
Jul 19 00:20:27.667: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1793" to be "running and ready"
Jul 19 00:20:27.670: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.962895ms
Jul 19 00:20:27.670: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:20:29.672: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005216519s
Jul 19 00:20:29.673: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jul 19 00:20:29.673: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 07/19/23 00:20:29.674
Jul 19 00:20:29.676: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1793" to be "running and ready"
Jul 19 00:20:29.681: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.759087ms
Jul 19 00:20:29.681: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:20:31.683: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006752139s
Jul 19 00:20:31.683: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jul 19 00:20:31.683: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 07/19/23 00:20:31.685
Jul 19 00:20:31.688: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 00:20:31.693: INFO: Pod pod-with-prestop-http-hook still exists
Jul 19 00:20:33.694: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 19 00:20:33.696: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 07/19/23 00:20:33.696
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:33.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1793" for this suite. 07/19/23 00:20:33.705
------------------------------
• [SLOW TEST] [6.058 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:27.649
    Jul 19 00:20:27.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-lifecycle-hook 07/19/23 00:20:27.65
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:27.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:27.659
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 07/19/23 00:20:27.663
    Jul 19 00:20:27.667: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1793" to be "running and ready"
    Jul 19 00:20:27.670: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.962895ms
    Jul 19 00:20:27.670: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:20:29.672: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005216519s
    Jul 19 00:20:29.673: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jul 19 00:20:29.673: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 07/19/23 00:20:29.674
    Jul 19 00:20:29.676: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1793" to be "running and ready"
    Jul 19 00:20:29.681: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.759087ms
    Jul 19 00:20:29.681: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:20:31.683: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006752139s
    Jul 19 00:20:31.683: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jul 19 00:20:31.683: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 07/19/23 00:20:31.685
    Jul 19 00:20:31.688: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jul 19 00:20:31.693: INFO: Pod pod-with-prestop-http-hook still exists
    Jul 19 00:20:33.694: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jul 19 00:20:33.696: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 07/19/23 00:20:33.696
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:33.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1793" for this suite. 07/19/23 00:20:33.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:33.709
Jul 19 00:20:33.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:20:33.71
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:33.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:33.717
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-4211 07/19/23 00:20:33.719
STEP: creating service affinity-nodeport-transition in namespace services-4211 07/19/23 00:20:33.719
STEP: creating replication controller affinity-nodeport-transition in namespace services-4211 07/19/23 00:20:33.728
I0719 00:20:33.731413      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4211, replica count: 3
I0719 00:20:36.782067      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 00:20:36.787: INFO: Creating new exec pod
Jul 19 00:20:36.791: INFO: Waiting up to 5m0s for pod "execpod-affinitysl6ds" in namespace "services-4211" to be "running"
Jul 19 00:20:36.794: INFO: Pod "execpod-affinitysl6ds": Phase="Pending", Reason="", readiness=false. Elapsed: 3.609697ms
Jul 19 00:20:38.796: INFO: Pod "execpod-affinitysl6ds": Phase="Running", Reason="", readiness=true. Elapsed: 2.005895207s
Jul 19 00:20:38.796: INFO: Pod "execpod-affinitysl6ds" satisfied condition "running"
Jul 19 00:20:39.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jul 19 00:20:39.911: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul 19 00:20:39.911: INFO: stdout: ""
Jul 19 00:20:39.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c nc -v -z -w 2 10.104.245.38 80'
Jul 19 00:20:40.017: INFO: stderr: "+ nc -v -z -w 2 10.104.245.38 80\nConnection to 10.104.245.38 80 port [tcp/http] succeeded!\n"
Jul 19 00:20:40.017: INFO: stdout: ""
Jul 19 00:20:40.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c nc -v -z -w 2 192.168.206.2 32739'
Jul 19 00:20:40.125: INFO: stderr: "+ nc -v -z -w 2 192.168.206.2 32739\nConnection to 192.168.206.2 32739 port [tcp/*] succeeded!\n"
Jul 19 00:20:40.126: INFO: stdout: ""
Jul 19 00:20:40.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c nc -v -z -w 2 192.168.206.3 32739'
Jul 19 00:20:40.229: INFO: stderr: "+ nc -v -z -w 2 192.168.206.3 32739\nConnection to 192.168.206.3 32739 port [tcp/*] succeeded!\n"
Jul 19 00:20:40.229: INFO: stdout: ""
Jul 19 00:20:40.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.206.2:32739/ ; done'
Jul 19 00:20:40.403: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n"
Jul 19 00:20:40.404: INFO: stdout: "\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-2jzd6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-2jzd6\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-2jzd6"
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-2jzd6
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-2jzd6
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-2jzd6
Jul 19 00:20:40.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.206.2:32739/ ; done'
Jul 19 00:20:40.571: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n"
Jul 19 00:20:40.571: INFO: stdout: "\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw"
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
Jul 19 00:20:40.571: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4211, will wait for the garbage collector to delete the pods 07/19/23 00:20:40.577
Jul 19 00:20:40.632: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.226919ms
Jul 19 00:20:40.732: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.693516ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:42.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4211" for this suite. 07/19/23 00:20:42.548
------------------------------
• [SLOW TEST] [8.841 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:33.709
    Jul 19 00:20:33.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:20:33.71
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:33.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:33.717
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-4211 07/19/23 00:20:33.719
    STEP: creating service affinity-nodeport-transition in namespace services-4211 07/19/23 00:20:33.719
    STEP: creating replication controller affinity-nodeport-transition in namespace services-4211 07/19/23 00:20:33.728
    I0719 00:20:33.731413      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4211, replica count: 3
    I0719 00:20:36.782067      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 19 00:20:36.787: INFO: Creating new exec pod
    Jul 19 00:20:36.791: INFO: Waiting up to 5m0s for pod "execpod-affinitysl6ds" in namespace "services-4211" to be "running"
    Jul 19 00:20:36.794: INFO: Pod "execpod-affinitysl6ds": Phase="Pending", Reason="", readiness=false. Elapsed: 3.609697ms
    Jul 19 00:20:38.796: INFO: Pod "execpod-affinitysl6ds": Phase="Running", Reason="", readiness=true. Elapsed: 2.005895207s
    Jul 19 00:20:38.796: INFO: Pod "execpod-affinitysl6ds" satisfied condition "running"
    Jul 19 00:20:39.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jul 19 00:20:39.911: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jul 19 00:20:39.911: INFO: stdout: ""
    Jul 19 00:20:39.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c nc -v -z -w 2 10.104.245.38 80'
    Jul 19 00:20:40.017: INFO: stderr: "+ nc -v -z -w 2 10.104.245.38 80\nConnection to 10.104.245.38 80 port [tcp/http] succeeded!\n"
    Jul 19 00:20:40.017: INFO: stdout: ""
    Jul 19 00:20:40.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c nc -v -z -w 2 192.168.206.2 32739'
    Jul 19 00:20:40.125: INFO: stderr: "+ nc -v -z -w 2 192.168.206.2 32739\nConnection to 192.168.206.2 32739 port [tcp/*] succeeded!\n"
    Jul 19 00:20:40.126: INFO: stdout: ""
    Jul 19 00:20:40.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c nc -v -z -w 2 192.168.206.3 32739'
    Jul 19 00:20:40.229: INFO: stderr: "+ nc -v -z -w 2 192.168.206.3 32739\nConnection to 192.168.206.3 32739 port [tcp/*] succeeded!\n"
    Jul 19 00:20:40.229: INFO: stdout: ""
    Jul 19 00:20:40.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.206.2:32739/ ; done'
    Jul 19 00:20:40.403: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n"
    Jul 19 00:20:40.404: INFO: stdout: "\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-2jzd6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-2jzd6\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-5qmv6\naffinity-nodeport-transition-2jzd6"
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-2jzd6
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-2jzd6
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-5qmv6
    Jul 19 00:20:40.404: INFO: Received response from host: affinity-nodeport-transition-2jzd6
    Jul 19 00:20:40.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4211 exec execpod-affinitysl6ds -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.206.2:32739/ ; done'
    Jul 19 00:20:40.571: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32739/\n"
    Jul 19 00:20:40.571: INFO: stdout: "\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw\naffinity-nodeport-transition-gtrvw"
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Received response from host: affinity-nodeport-transition-gtrvw
    Jul 19 00:20:40.571: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4211, will wait for the garbage collector to delete the pods 07/19/23 00:20:40.577
    Jul 19 00:20:40.632: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.226919ms
    Jul 19 00:20:40.732: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.693516ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:42.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4211" for this suite. 07/19/23 00:20:42.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:42.551
Jul 19 00:20:42.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:20:42.552
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:42.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:42.566
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-5757/configmap-test-594a1ff2-2f62-4233-8da6-e03fdbc57812 07/19/23 00:20:42.568
STEP: Creating a pod to test consume configMaps 07/19/23 00:20:42.57
Jul 19 00:20:42.574: INFO: Waiting up to 5m0s for pod "pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd" in namespace "configmap-5757" to be "Succeeded or Failed"
Jul 19 00:20:42.577: INFO: Pod "pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.279898ms
Jul 19 00:20:44.579: INFO: Pod "pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00485132s
Jul 19 00:20:46.581: INFO: Pod "pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007275975s
STEP: Saw pod success 07/19/23 00:20:46.581
Jul 19 00:20:46.581: INFO: Pod "pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd" satisfied condition "Succeeded or Failed"
Jul 19 00:20:46.583: INFO: Trying to get logs from node controller-1 pod pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd container env-test: <nil>
STEP: delete the pod 07/19/23 00:20:46.586
Jul 19 00:20:46.593: INFO: Waiting for pod pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd to disappear
Jul 19 00:20:46.595: INFO: Pod pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:46.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5757" for this suite. 07/19/23 00:20:46.598
------------------------------
• [4.049 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:42.551
    Jul 19 00:20:42.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:20:42.552
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:42.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:42.566
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-5757/configmap-test-594a1ff2-2f62-4233-8da6-e03fdbc57812 07/19/23 00:20:42.568
    STEP: Creating a pod to test consume configMaps 07/19/23 00:20:42.57
    Jul 19 00:20:42.574: INFO: Waiting up to 5m0s for pod "pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd" in namespace "configmap-5757" to be "Succeeded or Failed"
    Jul 19 00:20:42.577: INFO: Pod "pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.279898ms
    Jul 19 00:20:44.579: INFO: Pod "pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00485132s
    Jul 19 00:20:46.581: INFO: Pod "pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007275975s
    STEP: Saw pod success 07/19/23 00:20:46.581
    Jul 19 00:20:46.581: INFO: Pod "pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd" satisfied condition "Succeeded or Failed"
    Jul 19 00:20:46.583: INFO: Trying to get logs from node controller-1 pod pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd container env-test: <nil>
    STEP: delete the pod 07/19/23 00:20:46.586
    Jul 19 00:20:46.593: INFO: Waiting for pod pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd to disappear
    Jul 19 00:20:46.595: INFO: Pod pod-configmaps-958f8641-af47-4931-9d9a-4bd257b7c6bd no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:46.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5757" for this suite. 07/19/23 00:20:46.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:46.602
Jul 19 00:20:46.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename watch 07/19/23 00:20:46.603
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:46.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:46.61
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 07/19/23 00:20:46.611
STEP: starting a background goroutine to produce watch events 07/19/23 00:20:46.613
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 07/19/23 00:20:46.613
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:49.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5885" for this suite. 07/19/23 00:20:49.456
------------------------------
• [2.903 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:46.602
    Jul 19 00:20:46.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename watch 07/19/23 00:20:46.603
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:46.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:46.61
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 07/19/23 00:20:46.611
    STEP: starting a background goroutine to produce watch events 07/19/23 00:20:46.613
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 07/19/23 00:20:46.613
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:49.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5885" for this suite. 07/19/23 00:20:49.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:49.506
Jul 19 00:20:49.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:20:49.507
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:49.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:49.514
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-d42c4f18-f5ff-480b-b0e4-a44e501fbac0 07/19/23 00:20:49.518
STEP: Creating configMap with name cm-test-opt-upd-a3fd5f3c-e577-4089-8f33-0886c73e8a55 07/19/23 00:20:49.52
STEP: Creating the pod 07/19/23 00:20:49.522
Jul 19 00:20:49.526: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b" in namespace "projected-1621" to be "running and ready"
Jul 19 00:20:49.532: INFO: Pod "pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.450176ms
Jul 19 00:20:49.532: INFO: The phase of Pod pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:20:51.534: INFO: Pod "pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00856011s
Jul 19 00:20:51.534: INFO: The phase of Pod pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b is Running (Ready = true)
Jul 19 00:20:51.534: INFO: Pod "pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-d42c4f18-f5ff-480b-b0e4-a44e501fbac0 07/19/23 00:20:51.545
STEP: Updating configmap cm-test-opt-upd-a3fd5f3c-e577-4089-8f33-0886c73e8a55 07/19/23 00:20:51.547
STEP: Creating configMap with name cm-test-opt-create-52aed9fd-926f-4a6f-9157-ada63f5092b4 07/19/23 00:20:51.549
STEP: waiting to observe update in volume 07/19/23 00:20:51.552
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:53.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1621" for this suite. 07/19/23 00:20:53.571
------------------------------
• [4.068 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:49.506
    Jul 19 00:20:49.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:20:49.507
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:49.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:49.514
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-d42c4f18-f5ff-480b-b0e4-a44e501fbac0 07/19/23 00:20:49.518
    STEP: Creating configMap with name cm-test-opt-upd-a3fd5f3c-e577-4089-8f33-0886c73e8a55 07/19/23 00:20:49.52
    STEP: Creating the pod 07/19/23 00:20:49.522
    Jul 19 00:20:49.526: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b" in namespace "projected-1621" to be "running and ready"
    Jul 19 00:20:49.532: INFO: Pod "pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.450176ms
    Jul 19 00:20:49.532: INFO: The phase of Pod pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:20:51.534: INFO: Pod "pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00856011s
    Jul 19 00:20:51.534: INFO: The phase of Pod pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b is Running (Ready = true)
    Jul 19 00:20:51.534: INFO: Pod "pod-projected-configmaps-22d4cc4e-91fb-45e2-9d66-a87a72d44f1b" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-d42c4f18-f5ff-480b-b0e4-a44e501fbac0 07/19/23 00:20:51.545
    STEP: Updating configmap cm-test-opt-upd-a3fd5f3c-e577-4089-8f33-0886c73e8a55 07/19/23 00:20:51.547
    STEP: Creating configMap with name cm-test-opt-create-52aed9fd-926f-4a6f-9157-ada63f5092b4 07/19/23 00:20:51.549
    STEP: waiting to observe update in volume 07/19/23 00:20:51.552
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:53.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1621" for this suite. 07/19/23 00:20:53.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:53.574
Jul 19 00:20:53.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename deployment 07/19/23 00:20:53.575
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:53.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:53.585
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jul 19 00:20:53.587: INFO: Creating simple deployment test-new-deployment
Jul 19 00:20:53.594: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 07/19/23 00:20:55.601
STEP: updating a scale subresource 07/19/23 00:20:55.602
STEP: verifying the deployment Spec.Replicas was modified 07/19/23 00:20:55.605
STEP: Patch a scale subresource 07/19/23 00:20:55.609
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 19 00:20:55.627: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3298  3163b882-9709-4f58-a306-80855774e2eb 58542 3 2023-07-19 00:20:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-19 00:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cd8e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-07-19 00:20:55 +0000 UTC,LastTransitionTime:2023-07-19 00:20:53 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-19 00:20:55 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 19 00:20:55.631: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-3298  6a8e941d-8a7b-44e6-a058-9066bfd96ea3 58548 3 2023-07-19 00:20:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3163b882-9709-4f58-a306-80855774e2eb 0xc004e4d217 0xc004e4d218}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3163b882-9709-4f58-a306-80855774e2eb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e4d2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 19 00:20:55.635: INFO: Pod "test-new-deployment-7f5969cbc7-djzpq" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-djzpq test-new-deployment-7f5969cbc7- deployment-3298  850c3be5-a05e-4063-b58c-c572dcf88f84 58551 0 2023-07-19 00:20:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 6a8e941d-8a7b-44e6-a058-9066bfd96ea3 0xc004e4d657 0xc004e4d658}] [] [{kube-controller-manager Update v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a8e941d-8a7b-44e6-a058-9066bfd96ea3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gfm85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gfm85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:,StartTime:2023-07-19 00:20:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:20:55.635: INFO: Pod "test-new-deployment-7f5969cbc7-msssl" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-msssl test-new-deployment-7f5969cbc7- deployment-3298  fe4de3e8-4213-4771-8782-29444623d209 58549 0 2023-07-19 00:20:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 6a8e941d-8a7b-44e6-a058-9066bfd96ea3 0xc004e4d827 0xc004e4d828}] [] [{kube-controller-manager Update v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a8e941d-8a7b-44e6-a058-9066bfd96ea3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2c8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2c8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 19 00:20:55.635: INFO: Pod "test-new-deployment-7f5969cbc7-stkkk" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-stkkk test-new-deployment-7f5969cbc7- deployment-3298  492bc91f-a400-4196-9a79-72ac4041e7dc 58534 0 2023-07-19 00:20:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3b0ded6c5b340b246401b80f39917aa781d41c24982e2bda294f0c2f1d060145 cni.projectcalico.org/podIP:172.16.192.110/32 cni.projectcalico.org/podIPs:172.16.192.110/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.110"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.110"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 6a8e941d-8a7b-44e6-a058-9066bfd96ea3 0xc004e4d967 0xc004e4d968}] [] [{kube-controller-manager Update v1 2023-07-19 00:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a8e941d-8a7b-44e6-a058-9066bfd96ea3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:20:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:20:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tktkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tktkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.110,StartTime:2023-07-19 00:20:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:20:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8ec26541b9c02663f64973e6ed21ecbf4c74d1b7e114c195e5a51b4604aaeea4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:55.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3298" for this suite. 07/19/23 00:20:55.641
------------------------------
• [2.075 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:53.574
    Jul 19 00:20:53.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename deployment 07/19/23 00:20:53.575
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:53.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:53.585
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jul 19 00:20:53.587: INFO: Creating simple deployment test-new-deployment
    Jul 19 00:20:53.594: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 07/19/23 00:20:55.601
    STEP: updating a scale subresource 07/19/23 00:20:55.602
    STEP: verifying the deployment Spec.Replicas was modified 07/19/23 00:20:55.605
    STEP: Patch a scale subresource 07/19/23 00:20:55.609
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 19 00:20:55.627: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-3298  3163b882-9709-4f58-a306-80855774e2eb 58542 3 2023-07-19 00:20:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-19 00:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cd8e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-07-19 00:20:55 +0000 UTC,LastTransitionTime:2023-07-19 00:20:53 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-19 00:20:55 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jul 19 00:20:55.631: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-3298  6a8e941d-8a7b-44e6-a058-9066bfd96ea3 58548 3 2023-07-19 00:20:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3163b882-9709-4f58-a306-80855774e2eb 0xc004e4d217 0xc004e4d218}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3163b882-9709-4f58-a306-80855774e2eb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e4d2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 00:20:55.635: INFO: Pod "test-new-deployment-7f5969cbc7-djzpq" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-djzpq test-new-deployment-7f5969cbc7- deployment-3298  850c3be5-a05e-4063-b58c-c572dcf88f84 58551 0 2023-07-19 00:20:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 6a8e941d-8a7b-44e6-a058-9066bfd96ea3 0xc004e4d657 0xc004e4d658}] [] [{kube-controller-manager Update v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a8e941d-8a7b-44e6-a058-9066bfd96ea3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gfm85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gfm85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:,StartTime:2023-07-19 00:20:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:20:55.635: INFO: Pod "test-new-deployment-7f5969cbc7-msssl" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-msssl test-new-deployment-7f5969cbc7- deployment-3298  fe4de3e8-4213-4771-8782-29444623d209 58549 0 2023-07-19 00:20:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 6a8e941d-8a7b-44e6-a058-9066bfd96ea3 0xc004e4d827 0xc004e4d828}] [] [{kube-controller-manager Update v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a8e941d-8a7b-44e6-a058-9066bfd96ea3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2c8c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2c8c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jul 19 00:20:55.635: INFO: Pod "test-new-deployment-7f5969cbc7-stkkk" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-stkkk test-new-deployment-7f5969cbc7- deployment-3298  492bc91f-a400-4196-9a79-72ac4041e7dc 58534 0 2023-07-19 00:20:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:3b0ded6c5b340b246401b80f39917aa781d41c24982e2bda294f0c2f1d060145 cni.projectcalico.org/podIP:172.16.192.110/32 cni.projectcalico.org/podIPs:172.16.192.110/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.110"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.110"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 6a8e941d-8a7b-44e6-a058-9066bfd96ea3 0xc004e4d967 0xc004e4d968}] [] [{kube-controller-manager Update v1 2023-07-19 00:20:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a8e941d-8a7b-44e6-a058-9066bfd96ea3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 00:20:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-07-19 00:20:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:20:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tktkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tktkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:20:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.110,StartTime:2023-07-19 00:20:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:20:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8ec26541b9c02663f64973e6ed21ecbf4c74d1b7e114c195e5a51b4604aaeea4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:55.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3298" for this suite. 07/19/23 00:20:55.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:55.65
Jul 19 00:20:55.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubelet-test 07/19/23 00:20:55.65
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:55.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:55.664
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:59.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2740" for this suite. 07/19/23 00:20:59.677
------------------------------
• [4.029 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:55.65
    Jul 19 00:20:55.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubelet-test 07/19/23 00:20:55.65
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:55.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:55.664
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:59.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2740" for this suite. 07/19/23 00:20:59.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:59.679
Jul 19 00:20:59.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:20:59.68
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:59.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:59.688
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 07/19/23 00:20:59.692
STEP: waiting for available Endpoint 07/19/23 00:20:59.694
STEP: listing all Endpoints 07/19/23 00:20:59.695
STEP: updating the Endpoint 07/19/23 00:20:59.697
STEP: fetching the Endpoint 07/19/23 00:20:59.7
STEP: patching the Endpoint 07/19/23 00:20:59.701
STEP: fetching the Endpoint 07/19/23 00:20:59.706
STEP: deleting the Endpoint by Collection 07/19/23 00:20:59.707
STEP: waiting for Endpoint deletion 07/19/23 00:20:59.71
STEP: fetching the Endpoint 07/19/23 00:20:59.711
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:59.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5299" for this suite. 07/19/23 00:20:59.714
------------------------------
• [0.037 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:59.679
    Jul 19 00:20:59.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:20:59.68
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:59.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:59.688
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 07/19/23 00:20:59.692
    STEP: waiting for available Endpoint 07/19/23 00:20:59.694
    STEP: listing all Endpoints 07/19/23 00:20:59.695
    STEP: updating the Endpoint 07/19/23 00:20:59.697
    STEP: fetching the Endpoint 07/19/23 00:20:59.7
    STEP: patching the Endpoint 07/19/23 00:20:59.701
    STEP: fetching the Endpoint 07/19/23 00:20:59.706
    STEP: deleting the Endpoint by Collection 07/19/23 00:20:59.707
    STEP: waiting for Endpoint deletion 07/19/23 00:20:59.71
    STEP: fetching the Endpoint 07/19/23 00:20:59.711
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:59.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5299" for this suite. 07/19/23 00:20:59.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:59.717
Jul 19 00:20:59.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename watch 07/19/23 00:20:59.718
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:59.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:59.725
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 07/19/23 00:20:59.727
STEP: creating a new configmap 07/19/23 00:20:59.728
STEP: modifying the configmap once 07/19/23 00:20:59.73
STEP: closing the watch once it receives two notifications 07/19/23 00:20:59.733
Jul 19 00:20:59.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4623  766d738d-a858-4692-8b35-58c217828b06 58645 0 2023-07-19 00:20:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-19 00:20:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:20:59.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4623  766d738d-a858-4692-8b35-58c217828b06 58646 0 2023-07-19 00:20:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-19 00:20:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 07/19/23 00:20:59.734
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 07/19/23 00:20:59.737
STEP: deleting the configmap 07/19/23 00:20:59.738
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 07/19/23 00:20:59.74
Jul 19 00:20:59.740: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4623  766d738d-a858-4692-8b35-58c217828b06 58647 0 2023-07-19 00:20:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-19 00:20:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:20:59.740: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4623  766d738d-a858-4692-8b35-58c217828b06 58648 0 2023-07-19 00:20:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-19 00:20:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jul 19 00:20:59.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4623" for this suite. 07/19/23 00:20:59.745
------------------------------
• [0.030 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:59.717
    Jul 19 00:20:59.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename watch 07/19/23 00:20:59.718
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:59.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:59.725
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 07/19/23 00:20:59.727
    STEP: creating a new configmap 07/19/23 00:20:59.728
    STEP: modifying the configmap once 07/19/23 00:20:59.73
    STEP: closing the watch once it receives two notifications 07/19/23 00:20:59.733
    Jul 19 00:20:59.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4623  766d738d-a858-4692-8b35-58c217828b06 58645 0 2023-07-19 00:20:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-19 00:20:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:20:59.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4623  766d738d-a858-4692-8b35-58c217828b06 58646 0 2023-07-19 00:20:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-19 00:20:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 07/19/23 00:20:59.734
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 07/19/23 00:20:59.737
    STEP: deleting the configmap 07/19/23 00:20:59.738
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 07/19/23 00:20:59.74
    Jul 19 00:20:59.740: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4623  766d738d-a858-4692-8b35-58c217828b06 58647 0 2023-07-19 00:20:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-19 00:20:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:20:59.740: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4623  766d738d-a858-4692-8b35-58c217828b06 58648 0 2023-07-19 00:20:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-19 00:20:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:20:59.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4623" for this suite. 07/19/23 00:20:59.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:20:59.747
Jul 19 00:20:59.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename subpath 07/19/23 00:20:59.748
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:59.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:59.754
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 07/19/23 00:20:59.756
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-gqmc 07/19/23 00:20:59.76
STEP: Creating a pod to test atomic-volume-subpath 07/19/23 00:20:59.76
Jul 19 00:20:59.763: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gqmc" in namespace "subpath-2713" to be "Succeeded or Failed"
Jul 19 00:20:59.765: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.576991ms
Jul 19 00:21:01.768: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004773756s
Jul 19 00:21:03.767: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 4.003891329s
Jul 19 00:21:05.767: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 6.004036178s
Jul 19 00:21:07.768: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 8.004746078s
Jul 19 00:21:09.769: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 10.005215677s
Jul 19 00:21:11.768: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 12.004775093s
Jul 19 00:21:13.767: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 14.003916758s
Jul 19 00:21:15.767: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 16.00379338s
Jul 19 00:21:17.768: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 18.004691595s
Jul 19 00:21:19.769: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 20.006036381s
Jul 19 00:21:21.768: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=false. Elapsed: 22.004568356s
Jul 19 00:21:23.767: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004100718s
STEP: Saw pod success 07/19/23 00:21:23.768
Jul 19 00:21:23.768: INFO: Pod "pod-subpath-test-configmap-gqmc" satisfied condition "Succeeded or Failed"
Jul 19 00:21:23.770: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-configmap-gqmc container test-container-subpath-configmap-gqmc: <nil>
STEP: delete the pod 07/19/23 00:21:23.774
Jul 19 00:21:23.781: INFO: Waiting for pod pod-subpath-test-configmap-gqmc to disappear
Jul 19 00:21:23.783: INFO: Pod pod-subpath-test-configmap-gqmc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gqmc 07/19/23 00:21:23.783
Jul 19 00:21:23.783: INFO: Deleting pod "pod-subpath-test-configmap-gqmc" in namespace "subpath-2713"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jul 19 00:21:23.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2713" for this suite. 07/19/23 00:21:23.786
------------------------------
• [SLOW TEST] [24.042 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:20:59.747
    Jul 19 00:20:59.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename subpath 07/19/23 00:20:59.748
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:20:59.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:20:59.754
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 07/19/23 00:20:59.756
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-gqmc 07/19/23 00:20:59.76
    STEP: Creating a pod to test atomic-volume-subpath 07/19/23 00:20:59.76
    Jul 19 00:20:59.763: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gqmc" in namespace "subpath-2713" to be "Succeeded or Failed"
    Jul 19 00:20:59.765: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.576991ms
    Jul 19 00:21:01.768: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004773756s
    Jul 19 00:21:03.767: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 4.003891329s
    Jul 19 00:21:05.767: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 6.004036178s
    Jul 19 00:21:07.768: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 8.004746078s
    Jul 19 00:21:09.769: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 10.005215677s
    Jul 19 00:21:11.768: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 12.004775093s
    Jul 19 00:21:13.767: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 14.003916758s
    Jul 19 00:21:15.767: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 16.00379338s
    Jul 19 00:21:17.768: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 18.004691595s
    Jul 19 00:21:19.769: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=true. Elapsed: 20.006036381s
    Jul 19 00:21:21.768: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Running", Reason="", readiness=false. Elapsed: 22.004568356s
    Jul 19 00:21:23.767: INFO: Pod "pod-subpath-test-configmap-gqmc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004100718s
    STEP: Saw pod success 07/19/23 00:21:23.768
    Jul 19 00:21:23.768: INFO: Pod "pod-subpath-test-configmap-gqmc" satisfied condition "Succeeded or Failed"
    Jul 19 00:21:23.770: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-configmap-gqmc container test-container-subpath-configmap-gqmc: <nil>
    STEP: delete the pod 07/19/23 00:21:23.774
    Jul 19 00:21:23.781: INFO: Waiting for pod pod-subpath-test-configmap-gqmc to disappear
    Jul 19 00:21:23.783: INFO: Pod pod-subpath-test-configmap-gqmc no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-gqmc 07/19/23 00:21:23.783
    Jul 19 00:21:23.783: INFO: Deleting pod "pod-subpath-test-configmap-gqmc" in namespace "subpath-2713"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:21:23.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2713" for this suite. 07/19/23 00:21:23.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:21:23.79
Jul 19 00:21:23.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename var-expansion 07/19/23 00:21:23.791
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:21:23.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:21:23.798
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 07/19/23 00:21:23.8
Jul 19 00:21:23.804: INFO: Waiting up to 2m0s for pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86" in namespace "var-expansion-7087" to be "running"
Jul 19 00:21:23.805: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347681ms
Jul 19 00:21:25.807: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003406547s
Jul 19 00:21:27.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004213883s
Jul 19 00:21:29.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004810043s
Jul 19 00:21:31.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004465314s
Jul 19 00:21:33.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003831117s
Jul 19 00:21:35.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004274373s
Jul 19 00:21:37.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004468958s
Jul 19 00:21:39.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004439207s
Jul 19 00:21:41.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005463976s
Jul 19 00:21:43.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004178238s
Jul 19 00:21:45.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00376513s
Jul 19 00:21:47.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004366679s
Jul 19 00:21:49.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004262341s
Jul 19 00:21:51.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005106978s
Jul 19 00:21:53.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004669382s
Jul 19 00:21:55.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004721717s
Jul 19 00:21:57.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004465684s
Jul 19 00:21:59.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004391605s
Jul 19 00:22:01.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005276668s
Jul 19 00:22:03.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005448401s
Jul 19 00:22:05.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004245099s
Jul 19 00:22:07.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005334142s
Jul 19 00:22:09.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004653351s
Jul 19 00:22:11.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004114631s
Jul 19 00:22:13.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004191444s
Jul 19 00:22:15.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004445121s
Jul 19 00:22:17.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005362537s
Jul 19 00:22:19.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 56.003994228s
Jul 19 00:22:21.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004820793s
Jul 19 00:22:23.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004347324s
Jul 19 00:22:25.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004336581s
Jul 19 00:22:27.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005362534s
Jul 19 00:22:29.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004733268s
Jul 19 00:22:31.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005047514s
Jul 19 00:22:33.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005262799s
Jul 19 00:22:35.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.003567409s
Jul 19 00:22:37.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004841468s
Jul 19 00:22:39.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004837843s
Jul 19 00:22:41.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004016606s
Jul 19 00:22:43.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004261205s
Jul 19 00:22:45.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004197286s
Jul 19 00:22:47.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005330881s
Jul 19 00:22:49.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004376072s
Jul 19 00:22:51.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005117728s
Jul 19 00:22:53.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004400388s
Jul 19 00:22:55.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004406765s
Jul 19 00:22:57.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004955712s
Jul 19 00:22:59.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004654798s
Jul 19 00:23:01.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005160106s
Jul 19 00:23:03.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005479993s
Jul 19 00:23:05.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004287975s
Jul 19 00:23:07.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005353769s
Jul 19 00:23:09.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.0049216s
Jul 19 00:23:11.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004298812s
Jul 19 00:23:13.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004064692s
Jul 19 00:23:15.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.003969776s
Jul 19 00:23:17.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.0041264s
Jul 19 00:23:19.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.003952472s
Jul 19 00:23:21.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004853082s
Jul 19 00:23:23.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004067547s
Jul 19 00:23:23.810: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.00619138s
STEP: updating the pod 07/19/23 00:23:23.81
Jul 19 00:23:24.318: INFO: Successfully updated pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86"
STEP: waiting for pod running 07/19/23 00:23:24.318
Jul 19 00:23:24.318: INFO: Waiting up to 2m0s for pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86" in namespace "var-expansion-7087" to be "running"
Jul 19 00:23:24.321: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.652391ms
Jul 19 00:23:26.323: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Running", Reason="", readiness=true. Elapsed: 2.00523623s
Jul 19 00:23:26.324: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86" satisfied condition "running"
STEP: deleting the pod gracefully 07/19/23 00:23:26.324
Jul 19 00:23:26.324: INFO: Deleting pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86" in namespace "var-expansion-7087"
Jul 19 00:23:26.326: INFO: Wait up to 5m0s for pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jul 19 00:23:58.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7087" for this suite. 07/19/23 00:23:58.335
------------------------------
• [SLOW TEST] [154.547 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:21:23.79
    Jul 19 00:21:23.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename var-expansion 07/19/23 00:21:23.791
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:21:23.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:21:23.798
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 07/19/23 00:21:23.8
    Jul 19 00:21:23.804: INFO: Waiting up to 2m0s for pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86" in namespace "var-expansion-7087" to be "running"
    Jul 19 00:21:23.805: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1.347681ms
    Jul 19 00:21:25.807: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003406547s
    Jul 19 00:21:27.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004213883s
    Jul 19 00:21:29.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004810043s
    Jul 19 00:21:31.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004465314s
    Jul 19 00:21:33.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003831117s
    Jul 19 00:21:35.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004274373s
    Jul 19 00:21:37.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004468958s
    Jul 19 00:21:39.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 16.004439207s
    Jul 19 00:21:41.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005463976s
    Jul 19 00:21:43.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004178238s
    Jul 19 00:21:45.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 22.00376513s
    Jul 19 00:21:47.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004366679s
    Jul 19 00:21:49.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004262341s
    Jul 19 00:21:51.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005106978s
    Jul 19 00:21:53.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004669382s
    Jul 19 00:21:55.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004721717s
    Jul 19 00:21:57.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004465684s
    Jul 19 00:21:59.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004391605s
    Jul 19 00:22:01.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 38.005276668s
    Jul 19 00:22:03.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005448401s
    Jul 19 00:22:05.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004245099s
    Jul 19 00:22:07.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005334142s
    Jul 19 00:22:09.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004653351s
    Jul 19 00:22:11.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004114631s
    Jul 19 00:22:13.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004191444s
    Jul 19 00:22:15.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004445121s
    Jul 19 00:22:17.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 54.005362537s
    Jul 19 00:22:19.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 56.003994228s
    Jul 19 00:22:21.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004820793s
    Jul 19 00:22:23.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.004347324s
    Jul 19 00:22:25.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004336581s
    Jul 19 00:22:27.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005362534s
    Jul 19 00:22:29.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004733268s
    Jul 19 00:22:31.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005047514s
    Jul 19 00:22:33.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005262799s
    Jul 19 00:22:35.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.003567409s
    Jul 19 00:22:37.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004841468s
    Jul 19 00:22:39.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004837843s
    Jul 19 00:22:41.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.004016606s
    Jul 19 00:22:43.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.004261205s
    Jul 19 00:22:45.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004197286s
    Jul 19 00:22:47.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005330881s
    Jul 19 00:22:49.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004376072s
    Jul 19 00:22:51.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.005117728s
    Jul 19 00:22:53.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004400388s
    Jul 19 00:22:55.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004406765s
    Jul 19 00:22:57.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004955712s
    Jul 19 00:22:59.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004654798s
    Jul 19 00:23:01.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005160106s
    Jul 19 00:23:03.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005479993s
    Jul 19 00:23:05.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.004287975s
    Jul 19 00:23:07.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005353769s
    Jul 19 00:23:09.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.0049216s
    Jul 19 00:23:11.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.004298812s
    Jul 19 00:23:13.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004064692s
    Jul 19 00:23:15.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.003969776s
    Jul 19 00:23:17.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.0041264s
    Jul 19 00:23:19.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.003952472s
    Jul 19 00:23:21.809: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004853082s
    Jul 19 00:23:23.808: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004067547s
    Jul 19 00:23:23.810: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.00619138s
    STEP: updating the pod 07/19/23 00:23:23.81
    Jul 19 00:23:24.318: INFO: Successfully updated pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86"
    STEP: waiting for pod running 07/19/23 00:23:24.318
    Jul 19 00:23:24.318: INFO: Waiting up to 2m0s for pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86" in namespace "var-expansion-7087" to be "running"
    Jul 19 00:23:24.321: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.652391ms
    Jul 19 00:23:26.323: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86": Phase="Running", Reason="", readiness=true. Elapsed: 2.00523623s
    Jul 19 00:23:26.324: INFO: Pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86" satisfied condition "running"
    STEP: deleting the pod gracefully 07/19/23 00:23:26.324
    Jul 19 00:23:26.324: INFO: Deleting pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86" in namespace "var-expansion-7087"
    Jul 19 00:23:26.326: INFO: Wait up to 5m0s for pod "var-expansion-1f5a74f7-f014-45eb-8a1e-053ea73ced86" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:23:58.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7087" for this suite. 07/19/23 00:23:58.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:23:58.338
Jul 19 00:23:58.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:23:58.339
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:23:58.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:23:58.35
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 07/19/23 00:23:58.352
Jul 19 00:23:58.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5584 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jul 19 00:23:58.417: INFO: stderr: ""
Jul 19 00:23:58.417: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 07/19/23 00:23:58.417
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jul 19 00:23:58.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5584 delete pods e2e-test-httpd-pod'
Jul 19 00:24:00.977: INFO: stderr: ""
Jul 19 00:24:00.977: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:00.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5584" for this suite. 07/19/23 00:24:00.979
------------------------------
• [2.643 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:23:58.338
    Jul 19 00:23:58.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:23:58.339
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:23:58.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:23:58.35
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 07/19/23 00:23:58.352
    Jul 19 00:23:58.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5584 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jul 19 00:23:58.417: INFO: stderr: ""
    Jul 19 00:23:58.417: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 07/19/23 00:23:58.417
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jul 19 00:23:58.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-5584 delete pods e2e-test-httpd-pod'
    Jul 19 00:24:00.977: INFO: stderr: ""
    Jul 19 00:24:00.977: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:00.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5584" for this suite. 07/19/23 00:24:00.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:00.983
Jul 19 00:24:00.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:24:00.984
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:00.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:00.991
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-50b2cb4a-7b15-48b9-aa6c-8fcc5047b562 07/19/23 00:24:00.993
STEP: Creating a pod to test consume configMaps 07/19/23 00:24:00.995
Jul 19 00:24:00.999: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940" in namespace "projected-5311" to be "Succeeded or Failed"
Jul 19 00:24:01.000: INFO: Pod "pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940": Phase="Pending", Reason="", readiness=false. Elapsed: 1.469509ms
Jul 19 00:24:03.003: INFO: Pod "pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004166298s
Jul 19 00:24:05.003: INFO: Pod "pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00450462s
STEP: Saw pod success 07/19/23 00:24:05.003
Jul 19 00:24:05.003: INFO: Pod "pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940" satisfied condition "Succeeded or Failed"
Jul 19 00:24:05.005: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940 container projected-configmap-volume-test: <nil>
STEP: delete the pod 07/19/23 00:24:05.014
Jul 19 00:24:05.020: INFO: Waiting for pod pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940 to disappear
Jul 19 00:24:05.021: INFO: Pod pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:05.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5311" for this suite. 07/19/23 00:24:05.023
------------------------------
• [4.043 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:00.983
    Jul 19 00:24:00.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:24:00.984
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:00.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:00.991
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-50b2cb4a-7b15-48b9-aa6c-8fcc5047b562 07/19/23 00:24:00.993
    STEP: Creating a pod to test consume configMaps 07/19/23 00:24:00.995
    Jul 19 00:24:00.999: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940" in namespace "projected-5311" to be "Succeeded or Failed"
    Jul 19 00:24:01.000: INFO: Pod "pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940": Phase="Pending", Reason="", readiness=false. Elapsed: 1.469509ms
    Jul 19 00:24:03.003: INFO: Pod "pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004166298s
    Jul 19 00:24:05.003: INFO: Pod "pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00450462s
    STEP: Saw pod success 07/19/23 00:24:05.003
    Jul 19 00:24:05.003: INFO: Pod "pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940" satisfied condition "Succeeded or Failed"
    Jul 19 00:24:05.005: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:24:05.014
    Jul 19 00:24:05.020: INFO: Waiting for pod pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940 to disappear
    Jul 19 00:24:05.021: INFO: Pod pod-projected-configmaps-40d8b24a-7469-4114-9825-fea04e505940 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:05.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5311" for this suite. 07/19/23 00:24:05.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:05.027
Jul 19 00:24:05.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sysctl 07/19/23 00:24:05.028
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:05.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:05.035
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 07/19/23 00:24:05.037
STEP: Watching for error events or started pod 07/19/23 00:24:05.04
STEP: Waiting for pod completion 07/19/23 00:24:07.045
Jul 19 00:24:07.045: INFO: Waiting up to 3m0s for pod "sysctl-51e1415a-04ed-4db0-b22e-e9f69a3a4e58" in namespace "sysctl-3517" to be "completed"
Jul 19 00:24:07.047: INFO: Pod "sysctl-51e1415a-04ed-4db0-b22e-e9f69a3a4e58": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574288ms
Jul 19 00:24:09.050: INFO: Pod "sysctl-51e1415a-04ed-4db0-b22e-e9f69a3a4e58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004663849s
Jul 19 00:24:09.050: INFO: Pod "sysctl-51e1415a-04ed-4db0-b22e-e9f69a3a4e58" satisfied condition "completed"
STEP: Checking that the pod succeeded 07/19/23 00:24:09.051
STEP: Getting logs from the pod 07/19/23 00:24:09.052
STEP: Checking that the sysctl is actually updated 07/19/23 00:24:09.055
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:09.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3517" for this suite. 07/19/23 00:24:09.058
------------------------------
• [4.033 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:05.027
    Jul 19 00:24:05.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sysctl 07/19/23 00:24:05.028
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:05.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:05.035
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 07/19/23 00:24:05.037
    STEP: Watching for error events or started pod 07/19/23 00:24:05.04
    STEP: Waiting for pod completion 07/19/23 00:24:07.045
    Jul 19 00:24:07.045: INFO: Waiting up to 3m0s for pod "sysctl-51e1415a-04ed-4db0-b22e-e9f69a3a4e58" in namespace "sysctl-3517" to be "completed"
    Jul 19 00:24:07.047: INFO: Pod "sysctl-51e1415a-04ed-4db0-b22e-e9f69a3a4e58": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574288ms
    Jul 19 00:24:09.050: INFO: Pod "sysctl-51e1415a-04ed-4db0-b22e-e9f69a3a4e58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004663849s
    Jul 19 00:24:09.050: INFO: Pod "sysctl-51e1415a-04ed-4db0-b22e-e9f69a3a4e58" satisfied condition "completed"
    STEP: Checking that the pod succeeded 07/19/23 00:24:09.051
    STEP: Getting logs from the pod 07/19/23 00:24:09.052
    STEP: Checking that the sysctl is actually updated 07/19/23 00:24:09.055
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:09.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3517" for this suite. 07/19/23 00:24:09.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:09.06
Jul 19 00:24:09.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename conformance-tests 07/19/23 00:24:09.061
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:09.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:09.068
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 07/19/23 00:24:09.07
Jul 19 00:24:09.070: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:09.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-3685" for this suite. 07/19/23 00:24:09.075
------------------------------
• [0.016 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:09.06
    Jul 19 00:24:09.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename conformance-tests 07/19/23 00:24:09.061
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:09.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:09.068
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 07/19/23 00:24:09.07
    Jul 19 00:24:09.070: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:09.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-3685" for this suite. 07/19/23 00:24:09.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:09.077
Jul 19 00:24:09.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename statefulset 07/19/23 00:24:09.078
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:09.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:09.086
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3629 07/19/23 00:24:09.088
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jul 19 00:24:09.104: INFO: Found 0 stateful pods, waiting for 1
Jul 19 00:24:19.109: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 07/19/23 00:24:19.113
W0719 00:24:19.120246      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jul 19 00:24:19.126: INFO: Found 1 stateful pods, waiting for 2
Jul 19 00:24:29.129: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 00:24:29.129: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 07/19/23 00:24:29.133
STEP: Delete all of the StatefulSets 07/19/23 00:24:29.134
STEP: Verify that StatefulSets have been deleted 07/19/23 00:24:29.137
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jul 19 00:24:29.139: INFO: Deleting all statefulset in ns statefulset-3629
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:29.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3629" for this suite. 07/19/23 00:24:29.145
------------------------------
• [SLOW TEST] [20.070 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:09.077
    Jul 19 00:24:09.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename statefulset 07/19/23 00:24:09.078
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:09.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:09.086
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3629 07/19/23 00:24:09.088
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jul 19 00:24:09.104: INFO: Found 0 stateful pods, waiting for 1
    Jul 19 00:24:19.109: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 07/19/23 00:24:19.113
    W0719 00:24:19.120246      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jul 19 00:24:19.126: INFO: Found 1 stateful pods, waiting for 2
    Jul 19 00:24:29.129: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 00:24:29.129: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 07/19/23 00:24:29.133
    STEP: Delete all of the StatefulSets 07/19/23 00:24:29.134
    STEP: Verify that StatefulSets have been deleted 07/19/23 00:24:29.137
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jul 19 00:24:29.139: INFO: Deleting all statefulset in ns statefulset-3629
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:29.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3629" for this suite. 07/19/23 00:24:29.145
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:29.148
Jul 19 00:24:29.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:24:29.148
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:29.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:29.161
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:24:29.169
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:24:29.462
STEP: Deploying the webhook pod 07/19/23 00:24:29.467
STEP: Wait for the deployment to be ready 07/19/23 00:24:29.477
Jul 19 00:24:29.483: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:24:31.488
STEP: Verifying the service has paired with the endpoint 07/19/23 00:24:31.496
Jul 19 00:24:32.497: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jul 19 00:24:32.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1708-crds.webhook.example.com via the AdmissionRegistration API 07/19/23 00:24:33.006
STEP: Creating a custom resource while v1 is storage version 07/19/23 00:24:33.015
STEP: Patching Custom Resource Definition to set v2 as storage 07/19/23 00:24:35.346
STEP: Patching the custom resource while v2 is storage version 07/19/23 00:24:35.349
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:35.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2039" for this suite. 07/19/23 00:24:35.987
STEP: Destroying namespace "webhook-2039-markers" for this suite. 07/19/23 00:24:35.989
------------------------------
• [SLOW TEST] [6.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:29.148
    Jul 19 00:24:29.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:24:29.148
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:29.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:29.161
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:24:29.169
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:24:29.462
    STEP: Deploying the webhook pod 07/19/23 00:24:29.467
    STEP: Wait for the deployment to be ready 07/19/23 00:24:29.477
    Jul 19 00:24:29.483: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:24:31.488
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:24:31.496
    Jul 19 00:24:32.497: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jul 19 00:24:32.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1708-crds.webhook.example.com via the AdmissionRegistration API 07/19/23 00:24:33.006
    STEP: Creating a custom resource while v1 is storage version 07/19/23 00:24:33.015
    STEP: Patching Custom Resource Definition to set v2 as storage 07/19/23 00:24:35.346
    STEP: Patching the custom resource while v2 is storage version 07/19/23 00:24:35.349
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:35.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2039" for this suite. 07/19/23 00:24:35.987
    STEP: Destroying namespace "webhook-2039-markers" for this suite. 07/19/23 00:24:35.989
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:35.994
Jul 19 00:24:35.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:24:35.994
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:36.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:36.009
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 07/19/23 00:24:36.01
Jul 19 00:24:36.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:24:39.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:47.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-868" for this suite. 07/19/23 00:24:47.118
------------------------------
• [SLOW TEST] [11.128 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:35.994
    Jul 19 00:24:35.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:24:35.994
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:36.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:36.009
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 07/19/23 00:24:36.01
    Jul 19 00:24:36.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:24:39.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:47.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-868" for this suite. 07/19/23 00:24:47.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:47.123
Jul 19 00:24:47.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename tables 07/19/23 00:24:47.124
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:47.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:47.136
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:47.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-594" for this suite. 07/19/23 00:24:47.141
------------------------------
• [0.019 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:47.123
    Jul 19 00:24:47.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename tables 07/19/23 00:24:47.124
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:47.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:47.136
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:47.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-594" for this suite. 07/19/23 00:24:47.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:47.145
Jul 19 00:24:47.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:24:47.145
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:47.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:47.156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 07/19/23 00:24:47.158
Jul 19 00:24:47.162: INFO: Waiting up to 5m0s for pod "pod-5b8a7d5b-65b5-4c16-972b-66149c647449" in namespace "emptydir-7411" to be "Succeeded or Failed"
Jul 19 00:24:47.165: INFO: Pod "pod-5b8a7d5b-65b5-4c16-972b-66149c647449": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428901ms
Jul 19 00:24:49.168: INFO: Pod "pod-5b8a7d5b-65b5-4c16-972b-66149c647449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005916857s
Jul 19 00:24:51.168: INFO: Pod "pod-5b8a7d5b-65b5-4c16-972b-66149c647449": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006216022s
STEP: Saw pod success 07/19/23 00:24:51.168
Jul 19 00:24:51.168: INFO: Pod "pod-5b8a7d5b-65b5-4c16-972b-66149c647449" satisfied condition "Succeeded or Failed"
Jul 19 00:24:51.170: INFO: Trying to get logs from node controller-1 pod pod-5b8a7d5b-65b5-4c16-972b-66149c647449 container test-container: <nil>
STEP: delete the pod 07/19/23 00:24:51.179
Jul 19 00:24:51.184: INFO: Waiting for pod pod-5b8a7d5b-65b5-4c16-972b-66149c647449 to disappear
Jul 19 00:24:51.186: INFO: Pod pod-5b8a7d5b-65b5-4c16-972b-66149c647449 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:51.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7411" for this suite. 07/19/23 00:24:51.188
------------------------------
• [4.046 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:47.145
    Jul 19 00:24:47.145: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:24:47.145
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:47.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:47.156
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 07/19/23 00:24:47.158
    Jul 19 00:24:47.162: INFO: Waiting up to 5m0s for pod "pod-5b8a7d5b-65b5-4c16-972b-66149c647449" in namespace "emptydir-7411" to be "Succeeded or Failed"
    Jul 19 00:24:47.165: INFO: Pod "pod-5b8a7d5b-65b5-4c16-972b-66149c647449": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428901ms
    Jul 19 00:24:49.168: INFO: Pod "pod-5b8a7d5b-65b5-4c16-972b-66149c647449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005916857s
    Jul 19 00:24:51.168: INFO: Pod "pod-5b8a7d5b-65b5-4c16-972b-66149c647449": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006216022s
    STEP: Saw pod success 07/19/23 00:24:51.168
    Jul 19 00:24:51.168: INFO: Pod "pod-5b8a7d5b-65b5-4c16-972b-66149c647449" satisfied condition "Succeeded or Failed"
    Jul 19 00:24:51.170: INFO: Trying to get logs from node controller-1 pod pod-5b8a7d5b-65b5-4c16-972b-66149c647449 container test-container: <nil>
    STEP: delete the pod 07/19/23 00:24:51.179
    Jul 19 00:24:51.184: INFO: Waiting for pod pod-5b8a7d5b-65b5-4c16-972b-66149c647449 to disappear
    Jul 19 00:24:51.186: INFO: Pod pod-5b8a7d5b-65b5-4c16-972b-66149c647449 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:51.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7411" for this suite. 07/19/23 00:24:51.188
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:51.191
Jul 19 00:24:51.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename security-context-test 07/19/23 00:24:51.191
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:51.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:51.2
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jul 19 00:24:51.208: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fa3b8ecd-680c-4247-8c17-6539d9d951bd" in namespace "security-context-test-962" to be "Succeeded or Failed"
Jul 19 00:24:51.209: INFO: Pod "busybox-readonly-false-fa3b8ecd-680c-4247-8c17-6539d9d951bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.293393ms
Jul 19 00:24:53.212: INFO: Pod "busybox-readonly-false-fa3b8ecd-680c-4247-8c17-6539d9d951bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003766665s
Jul 19 00:24:55.212: INFO: Pod "busybox-readonly-false-fa3b8ecd-680c-4247-8c17-6539d9d951bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004409449s
Jul 19 00:24:55.212: INFO: Pod "busybox-readonly-false-fa3b8ecd-680c-4247-8c17-6539d9d951bd" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:55.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-962" for this suite. 07/19/23 00:24:55.215
------------------------------
• [4.026 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:51.191
    Jul 19 00:24:51.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename security-context-test 07/19/23 00:24:51.191
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:51.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:51.2
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jul 19 00:24:51.208: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fa3b8ecd-680c-4247-8c17-6539d9d951bd" in namespace "security-context-test-962" to be "Succeeded or Failed"
    Jul 19 00:24:51.209: INFO: Pod "busybox-readonly-false-fa3b8ecd-680c-4247-8c17-6539d9d951bd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.293393ms
    Jul 19 00:24:53.212: INFO: Pod "busybox-readonly-false-fa3b8ecd-680c-4247-8c17-6539d9d951bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003766665s
    Jul 19 00:24:55.212: INFO: Pod "busybox-readonly-false-fa3b8ecd-680c-4247-8c17-6539d9d951bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004409449s
    Jul 19 00:24:55.212: INFO: Pod "busybox-readonly-false-fa3b8ecd-680c-4247-8c17-6539d9d951bd" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:55.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-962" for this suite. 07/19/23 00:24:55.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:55.218
Jul 19 00:24:55.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename ingress 07/19/23 00:24:55.219
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:55.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:55.229
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 07/19/23 00:24:55.231
STEP: getting /apis/networking.k8s.io 07/19/23 00:24:55.232
STEP: getting /apis/networking.k8s.iov1 07/19/23 00:24:55.233
STEP: creating 07/19/23 00:24:55.234
STEP: getting 07/19/23 00:24:55.247
STEP: listing 07/19/23 00:24:55.249
STEP: watching 07/19/23 00:24:55.251
Jul 19 00:24:55.251: INFO: starting watch
STEP: cluster-wide listing 07/19/23 00:24:55.251
STEP: cluster-wide watching 07/19/23 00:24:55.253
Jul 19 00:24:55.253: INFO: starting watch
STEP: patching 07/19/23 00:24:55.254
STEP: updating 07/19/23 00:24:55.257
Jul 19 00:24:55.262: INFO: waiting for watch events with expected annotations
Jul 19 00:24:55.262: INFO: saw patched and updated annotations
STEP: patching /status 07/19/23 00:24:55.262
STEP: updating /status 07/19/23 00:24:55.265
STEP: get /status 07/19/23 00:24:55.269
STEP: deleting 07/19/23 00:24:55.272
STEP: deleting a collection 07/19/23 00:24:55.277
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jul 19 00:24:55.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-4455" for this suite. 07/19/23 00:24:55.287
------------------------------
• [0.071 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:55.218
    Jul 19 00:24:55.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename ingress 07/19/23 00:24:55.219
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:55.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:55.229
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 07/19/23 00:24:55.231
    STEP: getting /apis/networking.k8s.io 07/19/23 00:24:55.232
    STEP: getting /apis/networking.k8s.iov1 07/19/23 00:24:55.233
    STEP: creating 07/19/23 00:24:55.234
    STEP: getting 07/19/23 00:24:55.247
    STEP: listing 07/19/23 00:24:55.249
    STEP: watching 07/19/23 00:24:55.251
    Jul 19 00:24:55.251: INFO: starting watch
    STEP: cluster-wide listing 07/19/23 00:24:55.251
    STEP: cluster-wide watching 07/19/23 00:24:55.253
    Jul 19 00:24:55.253: INFO: starting watch
    STEP: patching 07/19/23 00:24:55.254
    STEP: updating 07/19/23 00:24:55.257
    Jul 19 00:24:55.262: INFO: waiting for watch events with expected annotations
    Jul 19 00:24:55.262: INFO: saw patched and updated annotations
    STEP: patching /status 07/19/23 00:24:55.262
    STEP: updating /status 07/19/23 00:24:55.265
    STEP: get /status 07/19/23 00:24:55.269
    STEP: deleting 07/19/23 00:24:55.272
    STEP: deleting a collection 07/19/23 00:24:55.277
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:24:55.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-4455" for this suite. 07/19/23 00:24:55.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:24:55.291
Jul 19 00:24:55.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:24:55.291
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:55.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:55.299
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-5898 07/19/23 00:24:55.301
STEP: creating replication controller nodeport-test in namespace services-5898 07/19/23 00:24:55.312
I0719 00:24:55.316561      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5898, replica count: 2
I0719 00:24:58.368510      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 00:24:58.368: INFO: Creating new exec pod
Jul 19 00:24:58.372: INFO: Waiting up to 5m0s for pod "execpodqfrjr" in namespace "services-5898" to be "running"
Jul 19 00:24:58.374: INFO: Pod "execpodqfrjr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.514373ms
Jul 19 00:25:00.375: INFO: Pod "execpodqfrjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.003255938s
Jul 19 00:25:00.376: INFO: Pod "execpodqfrjr" satisfied condition "running"
Jul 19 00:25:01.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5898 exec execpodqfrjr -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jul 19 00:25:01.482: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 19 00:25:01.482: INFO: stdout: ""
Jul 19 00:25:01.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5898 exec execpodqfrjr -- /bin/sh -x -c nc -v -z -w 2 10.110.107.44 80'
Jul 19 00:25:01.586: INFO: stderr: "+ nc -v -z -w 2 10.110.107.44 80\nConnection to 10.110.107.44 80 port [tcp/http] succeeded!\n"
Jul 19 00:25:01.586: INFO: stdout: ""
Jul 19 00:25:01.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5898 exec execpodqfrjr -- /bin/sh -x -c nc -v -z -w 2 192.168.206.2 31110'
Jul 19 00:25:01.704: INFO: stderr: "+ nc -v -z -w 2 192.168.206.2 31110\nConnection to 192.168.206.2 31110 port [tcp/*] succeeded!\n"
Jul 19 00:25:01.704: INFO: stdout: ""
Jul 19 00:25:01.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5898 exec execpodqfrjr -- /bin/sh -x -c nc -v -z -w 2 192.168.206.3 31110'
Jul 19 00:25:01.841: INFO: stderr: "+ nc -v -z -w 2 192.168.206.3 31110\nConnection to 192.168.206.3 31110 port [tcp/*] succeeded!\n"
Jul 19 00:25:01.841: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:01.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5898" for this suite. 07/19/23 00:25:01.844
------------------------------
• [SLOW TEST] [6.556 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:24:55.291
    Jul 19 00:24:55.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:24:55.291
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:24:55.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:24:55.299
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-5898 07/19/23 00:24:55.301
    STEP: creating replication controller nodeport-test in namespace services-5898 07/19/23 00:24:55.312
    I0719 00:24:55.316561      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5898, replica count: 2
    I0719 00:24:58.368510      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 19 00:24:58.368: INFO: Creating new exec pod
    Jul 19 00:24:58.372: INFO: Waiting up to 5m0s for pod "execpodqfrjr" in namespace "services-5898" to be "running"
    Jul 19 00:24:58.374: INFO: Pod "execpodqfrjr": Phase="Pending", Reason="", readiness=false. Elapsed: 1.514373ms
    Jul 19 00:25:00.375: INFO: Pod "execpodqfrjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.003255938s
    Jul 19 00:25:00.376: INFO: Pod "execpodqfrjr" satisfied condition "running"
    Jul 19 00:25:01.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5898 exec execpodqfrjr -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jul 19 00:25:01.482: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jul 19 00:25:01.482: INFO: stdout: ""
    Jul 19 00:25:01.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5898 exec execpodqfrjr -- /bin/sh -x -c nc -v -z -w 2 10.110.107.44 80'
    Jul 19 00:25:01.586: INFO: stderr: "+ nc -v -z -w 2 10.110.107.44 80\nConnection to 10.110.107.44 80 port [tcp/http] succeeded!\n"
    Jul 19 00:25:01.586: INFO: stdout: ""
    Jul 19 00:25:01.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5898 exec execpodqfrjr -- /bin/sh -x -c nc -v -z -w 2 192.168.206.2 31110'
    Jul 19 00:25:01.704: INFO: stderr: "+ nc -v -z -w 2 192.168.206.2 31110\nConnection to 192.168.206.2 31110 port [tcp/*] succeeded!\n"
    Jul 19 00:25:01.704: INFO: stdout: ""
    Jul 19 00:25:01.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-5898 exec execpodqfrjr -- /bin/sh -x -c nc -v -z -w 2 192.168.206.3 31110'
    Jul 19 00:25:01.841: INFO: stderr: "+ nc -v -z -w 2 192.168.206.3 31110\nConnection to 192.168.206.3 31110 port [tcp/*] succeeded!\n"
    Jul 19 00:25:01.841: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:01.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5898" for this suite. 07/19/23 00:25:01.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:01.848
Jul 19 00:25:01.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/19/23 00:25:01.848
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:01.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:01.857
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 07/19/23 00:25:01.859
STEP: Counting existing ResourceQuota 07/19/23 00:25:06.862
STEP: Creating a ResourceQuota 07/19/23 00:25:11.864
STEP: Ensuring resource quota status is calculated 07/19/23 00:25:11.867
STEP: Creating a Secret 07/19/23 00:25:13.869
STEP: Ensuring resource quota status captures secret creation 07/19/23 00:25:13.877
STEP: Deleting a secret 07/19/23 00:25:15.88
STEP: Ensuring resource quota status released usage 07/19/23 00:25:15.883
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:17.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1270" for this suite. 07/19/23 00:25:17.888
------------------------------
• [SLOW TEST] [16.057 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:01.848
    Jul 19 00:25:01.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/19/23 00:25:01.848
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:01.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:01.857
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 07/19/23 00:25:01.859
    STEP: Counting existing ResourceQuota 07/19/23 00:25:06.862
    STEP: Creating a ResourceQuota 07/19/23 00:25:11.864
    STEP: Ensuring resource quota status is calculated 07/19/23 00:25:11.867
    STEP: Creating a Secret 07/19/23 00:25:13.869
    STEP: Ensuring resource quota status captures secret creation 07/19/23 00:25:13.877
    STEP: Deleting a secret 07/19/23 00:25:15.88
    STEP: Ensuring resource quota status released usage 07/19/23 00:25:15.883
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:17.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1270" for this suite. 07/19/23 00:25:17.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:17.909
Jul 19 00:25:17.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubelet-test 07/19/23 00:25:17.909
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:17.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:17.918
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 07/19/23 00:25:17.923
Jul 19 00:25:17.923: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases201b7c36-eaac-4d57-9e29-17d53078e860" in namespace "kubelet-test-4565" to be "completed"
Jul 19 00:25:17.925: INFO: Pod "agnhost-host-aliases201b7c36-eaac-4d57-9e29-17d53078e860": Phase="Pending", Reason="", readiness=false. Elapsed: 1.430323ms
Jul 19 00:25:19.928: INFO: Pod "agnhost-host-aliases201b7c36-eaac-4d57-9e29-17d53078e860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004377358s
Jul 19 00:25:21.928: INFO: Pod "agnhost-host-aliases201b7c36-eaac-4d57-9e29-17d53078e860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004375174s
Jul 19 00:25:21.928: INFO: Pod "agnhost-host-aliases201b7c36-eaac-4d57-9e29-17d53078e860" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:21.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4565" for this suite. 07/19/23 00:25:21.934
------------------------------
• [4.028 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:17.909
    Jul 19 00:25:17.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubelet-test 07/19/23 00:25:17.909
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:17.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:17.918
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 07/19/23 00:25:17.923
    Jul 19 00:25:17.923: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases201b7c36-eaac-4d57-9e29-17d53078e860" in namespace "kubelet-test-4565" to be "completed"
    Jul 19 00:25:17.925: INFO: Pod "agnhost-host-aliases201b7c36-eaac-4d57-9e29-17d53078e860": Phase="Pending", Reason="", readiness=false. Elapsed: 1.430323ms
    Jul 19 00:25:19.928: INFO: Pod "agnhost-host-aliases201b7c36-eaac-4d57-9e29-17d53078e860": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004377358s
    Jul 19 00:25:21.928: INFO: Pod "agnhost-host-aliases201b7c36-eaac-4d57-9e29-17d53078e860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004375174s
    Jul 19 00:25:21.928: INFO: Pod "agnhost-host-aliases201b7c36-eaac-4d57-9e29-17d53078e860" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:21.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4565" for this suite. 07/19/23 00:25:21.934
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:21.937
Jul 19 00:25:21.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:25:21.937
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:21.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:21.947
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:25:21.956
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:25:22.419
STEP: Deploying the webhook pod 07/19/23 00:25:22.423
STEP: Wait for the deployment to be ready 07/19/23 00:25:22.428
Jul 19 00:25:22.436: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:25:24.441
STEP: Verifying the service has paired with the endpoint 07/19/23 00:25:24.45
Jul 19 00:25:25.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 07/19/23 00:25:25.452
STEP: create a namespace for the webhook 07/19/23 00:25:25.462
STEP: create a configmap should be unconditionally rejected by the webhook 07/19/23 00:25:25.466
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:25.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9343" for this suite. 07/19/23 00:25:25.509
STEP: Destroying namespace "webhook-9343-markers" for this suite. 07/19/23 00:25:25.511
------------------------------
• [3.580 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:21.937
    Jul 19 00:25:21.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:25:21.937
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:21.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:21.947
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:25:21.956
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:25:22.419
    STEP: Deploying the webhook pod 07/19/23 00:25:22.423
    STEP: Wait for the deployment to be ready 07/19/23 00:25:22.428
    Jul 19 00:25:22.436: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:25:24.441
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:25:24.45
    Jul 19 00:25:25.450: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 07/19/23 00:25:25.452
    STEP: create a namespace for the webhook 07/19/23 00:25:25.462
    STEP: create a configmap should be unconditionally rejected by the webhook 07/19/23 00:25:25.466
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:25.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9343" for this suite. 07/19/23 00:25:25.509
    STEP: Destroying namespace "webhook-9343-markers" for this suite. 07/19/23 00:25:25.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:25.517
Jul 19 00:25:25.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:25:25.518
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:25.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:25.529
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 07/19/23 00:25:25.531
Jul 19 00:25:25.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 07/19/23 00:25:32.588
Jul 19 00:25:32.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:25:35.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:42.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3081" for this suite. 07/19/23 00:25:42.609
------------------------------
• [SLOW TEST] [17.094 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:25.517
    Jul 19 00:25:25.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:25:25.518
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:25.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:25.529
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 07/19/23 00:25:25.531
    Jul 19 00:25:25.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 07/19/23 00:25:32.588
    Jul 19 00:25:32.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:25:35.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:42.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3081" for this suite. 07/19/23 00:25:42.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:42.612
Jul 19 00:25:42.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename daemonsets 07/19/23 00:25:42.613
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:42.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:42.626
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 07/19/23 00:25:42.635
STEP: Check that daemon pods launch on every node of the cluster. 07/19/23 00:25:42.638
Jul 19 00:25:42.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:25:42.652: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:25:43.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:25:43.657: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:25:44.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 19 00:25:44.657: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 07/19/23 00:25:44.658
STEP: DeleteCollection of the DaemonSets 07/19/23 00:25:44.661
STEP: Verify that ReplicaSets have been deleted 07/19/23 00:25:44.664
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jul 19 00:25:44.671: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"61206"},"items":null}

Jul 19 00:25:44.673: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"61206"},"items":[{"metadata":{"name":"daemon-set-8pgc9","generateName":"daemon-set-","namespace":"daemonsets-3652","uid":"1fa95e3e-0bfc-4b6e-8167-594b2309251d","resourceVersion":"61200","creationTimestamp":"2023-07-19T00:25:42Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"04f7a8476af9427f4bde0d7fb5960eabcf8f53f48170fc50da2f3f1706dd4311","cni.projectcalico.org/podIP":"172.16.192.125/32","cni.projectcalico.org/podIPs":"172.16.192.125/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.192.125\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.192.125\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c9b21a49-56fc-4c29-a7ca-19f2b0079342","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9b21a49-56fc-4c29-a7ca-19f2b0079342\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gzvv5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gzvv5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"controller-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["controller-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:43Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:43Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:42Z"}],"hostIP":"192.168.206.2","podIP":"172.16.192.125","podIPs":[{"ip":"172.16.192.125"}],"startTime":"2023-07-19T00:25:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-19T00:25:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e8cbf3c288ba2f3f4074e6d3cbcb7c1405e21fc216337dea70abea4efa89aabe","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-snjvt","generateName":"daemon-set-","namespace":"daemonsets-3652","uid":"17cd9665-ab06-4305-9988-e0685f8f117c","resourceVersion":"61202","creationTimestamp":"2023-07-19T00:25:42Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7625b8ac448eded609c8a0587042c82bee2181ac6b6668a068fa97d6a0eb7c1f","cni.projectcalico.org/podIP":"172.16.166.149/32","cni.projectcalico.org/podIPs":"172.16.166.149/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.166.149\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.166.149\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c9b21a49-56fc-4c29-a7ca-19f2b0079342","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9b21a49-56fc-4c29-a7ca-19f2b0079342\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b6ql2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b6ql2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"controller-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["controller-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:42Z"}],"hostIP":"192.168.206.3","podIP":"172.16.166.149","podIPs":[{"ip":"172.16.166.149"}],"startTime":"2023-07-19T00:25:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-19T00:25:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://c08c501686aa5864f3d5e279a44d25e76caf87ba763acb6bcca45016bc3f2c56","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:44.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3652" for this suite. 07/19/23 00:25:44.683
------------------------------
• [2.073 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:42.612
    Jul 19 00:25:42.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename daemonsets 07/19/23 00:25:42.613
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:42.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:42.626
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 07/19/23 00:25:42.635
    STEP: Check that daemon pods launch on every node of the cluster. 07/19/23 00:25:42.638
    Jul 19 00:25:42.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:25:42.652: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:25:43.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:25:43.657: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:25:44.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 19 00:25:44.657: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 07/19/23 00:25:44.658
    STEP: DeleteCollection of the DaemonSets 07/19/23 00:25:44.661
    STEP: Verify that ReplicaSets have been deleted 07/19/23 00:25:44.664
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jul 19 00:25:44.671: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"61206"},"items":null}

    Jul 19 00:25:44.673: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"61206"},"items":[{"metadata":{"name":"daemon-set-8pgc9","generateName":"daemon-set-","namespace":"daemonsets-3652","uid":"1fa95e3e-0bfc-4b6e-8167-594b2309251d","resourceVersion":"61200","creationTimestamp":"2023-07-19T00:25:42Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"04f7a8476af9427f4bde0d7fb5960eabcf8f53f48170fc50da2f3f1706dd4311","cni.projectcalico.org/podIP":"172.16.192.125/32","cni.projectcalico.org/podIPs":"172.16.192.125/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.192.125\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.192.125\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c9b21a49-56fc-4c29-a7ca-19f2b0079342","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9b21a49-56fc-4c29-a7ca-19f2b0079342\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gzvv5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gzvv5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"controller-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["controller-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:43Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:43Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:42Z"}],"hostIP":"192.168.206.2","podIP":"172.16.192.125","podIPs":[{"ip":"172.16.192.125"}],"startTime":"2023-07-19T00:25:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-19T00:25:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e8cbf3c288ba2f3f4074e6d3cbcb7c1405e21fc216337dea70abea4efa89aabe","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-snjvt","generateName":"daemon-set-","namespace":"daemonsets-3652","uid":"17cd9665-ab06-4305-9988-e0685f8f117c","resourceVersion":"61202","creationTimestamp":"2023-07-19T00:25:42Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7625b8ac448eded609c8a0587042c82bee2181ac6b6668a068fa97d6a0eb7c1f","cni.projectcalico.org/podIP":"172.16.166.149/32","cni.projectcalico.org/podIPs":"172.16.166.149/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.166.149\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.166.149\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"c9b21a49-56fc-4c29-a7ca-19f2b0079342","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9b21a49-56fc-4c29-a7ca-19f2b0079342\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-19T00:25:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b6ql2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b6ql2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"controller-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["controller-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:44Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:44Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-19T00:25:42Z"}],"hostIP":"192.168.206.3","podIP":"172.16.166.149","podIPs":[{"ip":"172.16.166.149"}],"startTime":"2023-07-19T00:25:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-19T00:25:43Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://c08c501686aa5864f3d5e279a44d25e76caf87ba763acb6bcca45016bc3f2c56","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:44.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3652" for this suite. 07/19/23 00:25:44.683
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:44.686
Jul 19 00:25:44.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:25:44.686
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:44.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:44.695
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-dd8f8829-f428-40ae-a048-e049c6f6bf91 07/19/23 00:25:44.697
STEP: Creating a pod to test consume configMaps 07/19/23 00:25:44.699
Jul 19 00:25:44.703: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59" in namespace "configmap-4417" to be "Succeeded or Failed"
Jul 19 00:25:44.704: INFO: Pod "pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59": Phase="Pending", Reason="", readiness=false. Elapsed: 1.320828ms
Jul 19 00:25:46.707: INFO: Pod "pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004478175s
Jul 19 00:25:48.706: INFO: Pod "pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003210888s
STEP: Saw pod success 07/19/23 00:25:48.706
Jul 19 00:25:48.706: INFO: Pod "pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59" satisfied condition "Succeeded or Failed"
Jul 19 00:25:48.708: INFO: Trying to get logs from node controller-1 pod pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:25:48.713
Jul 19 00:25:48.721: INFO: Waiting for pod pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59 to disappear
Jul 19 00:25:48.723: INFO: Pod pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:48.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4417" for this suite. 07/19/23 00:25:48.725
------------------------------
• [4.042 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:44.686
    Jul 19 00:25:44.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:25:44.686
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:44.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:44.695
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-dd8f8829-f428-40ae-a048-e049c6f6bf91 07/19/23 00:25:44.697
    STEP: Creating a pod to test consume configMaps 07/19/23 00:25:44.699
    Jul 19 00:25:44.703: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59" in namespace "configmap-4417" to be "Succeeded or Failed"
    Jul 19 00:25:44.704: INFO: Pod "pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59": Phase="Pending", Reason="", readiness=false. Elapsed: 1.320828ms
    Jul 19 00:25:46.707: INFO: Pod "pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004478175s
    Jul 19 00:25:48.706: INFO: Pod "pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003210888s
    STEP: Saw pod success 07/19/23 00:25:48.706
    Jul 19 00:25:48.706: INFO: Pod "pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59" satisfied condition "Succeeded or Failed"
    Jul 19 00:25:48.708: INFO: Trying to get logs from node controller-1 pod pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:25:48.713
    Jul 19 00:25:48.721: INFO: Waiting for pod pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59 to disappear
    Jul 19 00:25:48.723: INFO: Pod pod-configmaps-8c341257-09e0-4830-b3b5-c1950b4beb59 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:48.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4417" for this suite. 07/19/23 00:25:48.725
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:48.728
Jul 19 00:25:48.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:25:48.729
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:48.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:48.739
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 07/19/23 00:25:48.741
Jul 19 00:25:48.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-8872 cluster-info'
Jul 19 00:25:48.803: INFO: stderr: ""
Jul 19 00:25:48.803: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:48.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8872" for this suite. 07/19/23 00:25:48.805
------------------------------
• [0.080 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:48.728
    Jul 19 00:25:48.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:25:48.729
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:48.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:48.739
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 07/19/23 00:25:48.741
    Jul 19 00:25:48.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-8872 cluster-info'
    Jul 19 00:25:48.803: INFO: stderr: ""
    Jul 19 00:25:48.803: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:48.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8872" for this suite. 07/19/23 00:25:48.805
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:48.808
Jul 19 00:25:48.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename runtimeclass 07/19/23 00:25:48.809
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:48.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:48.817
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:48.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2632" for this suite. 07/19/23 00:25:48.824
------------------------------
• [0.019 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:48.808
    Jul 19 00:25:48.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename runtimeclass 07/19/23 00:25:48.809
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:48.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:48.817
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:48.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2632" for this suite. 07/19/23 00:25:48.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:48.828
Jul 19 00:25:48.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename var-expansion 07/19/23 00:25:48.829
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:48.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:48.837
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 07/19/23 00:25:48.839
Jul 19 00:25:48.842: INFO: Waiting up to 5m0s for pod "var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7" in namespace "var-expansion-9163" to be "Succeeded or Failed"
Jul 19 00:25:48.844: INFO: Pod "var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.59328ms
Jul 19 00:25:50.846: INFO: Pod "var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00327863s
Jul 19 00:25:52.846: INFO: Pod "var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003610134s
STEP: Saw pod success 07/19/23 00:25:52.846
Jul 19 00:25:52.846: INFO: Pod "var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7" satisfied condition "Succeeded or Failed"
Jul 19 00:25:52.848: INFO: Trying to get logs from node controller-1 pod var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7 container dapi-container: <nil>
STEP: delete the pod 07/19/23 00:25:52.851
Jul 19 00:25:52.859: INFO: Waiting for pod var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7 to disappear
Jul 19 00:25:52.861: INFO: Pod var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:52.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9163" for this suite. 07/19/23 00:25:52.863
------------------------------
• [4.037 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:48.828
    Jul 19 00:25:48.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename var-expansion 07/19/23 00:25:48.829
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:48.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:48.837
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 07/19/23 00:25:48.839
    Jul 19 00:25:48.842: INFO: Waiting up to 5m0s for pod "var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7" in namespace "var-expansion-9163" to be "Succeeded or Failed"
    Jul 19 00:25:48.844: INFO: Pod "var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.59328ms
    Jul 19 00:25:50.846: INFO: Pod "var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00327863s
    Jul 19 00:25:52.846: INFO: Pod "var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003610134s
    STEP: Saw pod success 07/19/23 00:25:52.846
    Jul 19 00:25:52.846: INFO: Pod "var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7" satisfied condition "Succeeded or Failed"
    Jul 19 00:25:52.848: INFO: Trying to get logs from node controller-1 pod var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7 container dapi-container: <nil>
    STEP: delete the pod 07/19/23 00:25:52.851
    Jul 19 00:25:52.859: INFO: Waiting for pod var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7 to disappear
    Jul 19 00:25:52.861: INFO: Pod var-expansion-76d3a646-63bc-4cca-8a79-9e6cc35cf6d7 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:52.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9163" for this suite. 07/19/23 00:25:52.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:52.866
Jul 19 00:25:52.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename endpointslice 07/19/23 00:25:52.866
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:52.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:52.882
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jul 19 00:25:54.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1043" for this suite. 07/19/23 00:25:54.914
------------------------------
• [2.051 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:52.866
    Jul 19 00:25:52.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename endpointslice 07/19/23 00:25:52.866
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:52.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:52.882
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:25:54.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1043" for this suite. 07/19/23 00:25:54.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:25:54.918
Jul 19 00:25:54.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:25:54.918
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:54.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:54.926
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-3439 07/19/23 00:25:54.928
STEP: creating service affinity-clusterip-transition in namespace services-3439 07/19/23 00:25:54.928
STEP: creating replication controller affinity-clusterip-transition in namespace services-3439 07/19/23 00:25:54.936
I0719 00:25:54.942232      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3439, replica count: 3
I0719 00:25:57.993531      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 00:25:57.997: INFO: Creating new exec pod
Jul 19 00:25:58.014: INFO: Waiting up to 5m0s for pod "execpod-affinity7dvmz" in namespace "services-3439" to be "running"
Jul 19 00:25:58.017: INFO: Pod "execpod-affinity7dvmz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414394ms
Jul 19 00:26:00.019: INFO: Pod "execpod-affinity7dvmz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004529584s
Jul 19 00:26:00.019: INFO: Pod "execpod-affinity7dvmz" satisfied condition "running"
Jul 19 00:26:01.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-3439 exec execpod-affinity7dvmz -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jul 19 00:26:01.122: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul 19 00:26:01.122: INFO: stdout: ""
Jul 19 00:26:01.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-3439 exec execpod-affinity7dvmz -- /bin/sh -x -c nc -v -z -w 2 10.110.255.17 80'
Jul 19 00:26:01.225: INFO: stderr: "+ nc -v -z -w 2 10.110.255.17 80\nConnection to 10.110.255.17 80 port [tcp/http] succeeded!\n"
Jul 19 00:26:01.225: INFO: stdout: ""
Jul 19 00:26:01.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-3439 exec execpod-affinity7dvmz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.255.17:80/ ; done'
Jul 19 00:26:01.387: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n"
Jul 19 00:26:01.387: INFO: stdout: "\naffinity-clusterip-transition-gxbgz\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-rljts\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-gxbgz\naffinity-clusterip-transition-rljts\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-rljts\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-gxbgz\naffinity-clusterip-transition-gxbgz\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-rljts"
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-gxbgz
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-rljts
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-gxbgz
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-rljts
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-rljts
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-gxbgz
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-gxbgz
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-rljts
Jul 19 00:26:01.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-3439 exec execpod-affinity7dvmz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.255.17:80/ ; done'
Jul 19 00:26:01.550: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n"
Jul 19 00:26:01.550: INFO: stdout: "\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt"
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
Jul 19 00:26:01.550: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3439, will wait for the garbage collector to delete the pods 07/19/23 00:26:01.554
Jul 19 00:26:01.609: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.907116ms
Jul 19 00:26:01.710: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.719393ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:03.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3439" for this suite. 07/19/23 00:26:03.421
------------------------------
• [SLOW TEST] [8.506 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:25:54.918
    Jul 19 00:25:54.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:25:54.918
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:25:54.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:25:54.926
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-3439 07/19/23 00:25:54.928
    STEP: creating service affinity-clusterip-transition in namespace services-3439 07/19/23 00:25:54.928
    STEP: creating replication controller affinity-clusterip-transition in namespace services-3439 07/19/23 00:25:54.936
    I0719 00:25:54.942232      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3439, replica count: 3
    I0719 00:25:57.993531      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 19 00:25:57.997: INFO: Creating new exec pod
    Jul 19 00:25:58.014: INFO: Waiting up to 5m0s for pod "execpod-affinity7dvmz" in namespace "services-3439" to be "running"
    Jul 19 00:25:58.017: INFO: Pod "execpod-affinity7dvmz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414394ms
    Jul 19 00:26:00.019: INFO: Pod "execpod-affinity7dvmz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004529584s
    Jul 19 00:26:00.019: INFO: Pod "execpod-affinity7dvmz" satisfied condition "running"
    Jul 19 00:26:01.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-3439 exec execpod-affinity7dvmz -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jul 19 00:26:01.122: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jul 19 00:26:01.122: INFO: stdout: ""
    Jul 19 00:26:01.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-3439 exec execpod-affinity7dvmz -- /bin/sh -x -c nc -v -z -w 2 10.110.255.17 80'
    Jul 19 00:26:01.225: INFO: stderr: "+ nc -v -z -w 2 10.110.255.17 80\nConnection to 10.110.255.17 80 port [tcp/http] succeeded!\n"
    Jul 19 00:26:01.225: INFO: stdout: ""
    Jul 19 00:26:01.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-3439 exec execpod-affinity7dvmz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.255.17:80/ ; done'
    Jul 19 00:26:01.387: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n"
    Jul 19 00:26:01.387: INFO: stdout: "\naffinity-clusterip-transition-gxbgz\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-rljts\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-gxbgz\naffinity-clusterip-transition-rljts\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-rljts\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-gxbgz\naffinity-clusterip-transition-gxbgz\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-rljts"
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-gxbgz
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-rljts
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-gxbgz
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-rljts
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-rljts
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-gxbgz
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-gxbgz
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.387: INFO: Received response from host: affinity-clusterip-transition-rljts
    Jul 19 00:26:01.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-3439 exec execpod-affinity7dvmz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.255.17:80/ ; done'
    Jul 19 00:26:01.550: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.255.17:80/\n"
    Jul 19 00:26:01.550: INFO: stdout: "\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt\naffinity-clusterip-transition-hk7kt"
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Received response from host: affinity-clusterip-transition-hk7kt
    Jul 19 00:26:01.550: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3439, will wait for the garbage collector to delete the pods 07/19/23 00:26:01.554
    Jul 19 00:26:01.609: INFO: Deleting ReplicationController affinity-clusterip-transition took: 2.907116ms
    Jul 19 00:26:01.710: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.719393ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:03.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3439" for this suite. 07/19/23 00:26:03.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:03.425
Jul 19 00:26:03.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:26:03.426
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:03.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:03.434
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:26:03.442
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:26:04.032
STEP: Deploying the webhook pod 07/19/23 00:26:04.036
STEP: Wait for the deployment to be ready 07/19/23 00:26:04.041
Jul 19 00:26:04.044: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 07/19/23 00:26:06.049
STEP: Verifying the service has paired with the endpoint 07/19/23 00:26:06.059
Jul 19 00:26:07.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 07/19/23 00:26:07.062
Jul 19 00:26:07.081: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod 07/19/23 00:26:07.187
Jul 19 00:26:07.191: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8880" to be "running"
Jul 19 00:26:07.193: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.309392ms
Jul 19 00:26:09.195: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003859483s
Jul 19 00:26:09.195: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 07/19/23 00:26:09.195
Jul 19 00:26:09.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=webhook-8880 attach --namespace=webhook-8880 to-be-attached-pod -i -c=container1'
Jul 19 00:26:09.265: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:09.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8880" for this suite. 07/19/23 00:26:09.286
STEP: Destroying namespace "webhook-8880-markers" for this suite. 07/19/23 00:26:09.29
------------------------------
• [SLOW TEST] [5.871 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:03.425
    Jul 19 00:26:03.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:26:03.426
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:03.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:03.434
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:26:03.442
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:26:04.032
    STEP: Deploying the webhook pod 07/19/23 00:26:04.036
    STEP: Wait for the deployment to be ready 07/19/23 00:26:04.041
    Jul 19 00:26:04.044: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 07/19/23 00:26:06.049
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:26:06.059
    Jul 19 00:26:07.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 07/19/23 00:26:07.062
    Jul 19 00:26:07.081: INFO: Waiting for webhook configuration to be ready...
    STEP: create a pod 07/19/23 00:26:07.187
    Jul 19 00:26:07.191: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8880" to be "running"
    Jul 19 00:26:07.193: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.309392ms
    Jul 19 00:26:09.195: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003859483s
    Jul 19 00:26:09.195: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 07/19/23 00:26:09.195
    Jul 19 00:26:09.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=webhook-8880 attach --namespace=webhook-8880 to-be-attached-pod -i -c=container1'
    Jul 19 00:26:09.265: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:09.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8880" for this suite. 07/19/23 00:26:09.286
    STEP: Destroying namespace "webhook-8880-markers" for this suite. 07/19/23 00:26:09.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:09.298
Jul 19 00:26:09.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:26:09.299
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:09.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:09.307
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jul 19 00:26:09.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/19/23 00:26:11.28
Jul 19 00:26:11.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 create -f -'
Jul 19 00:26:12.090: INFO: stderr: ""
Jul 19 00:26:12.090: INFO: stdout: "e2e-test-crd-publish-openapi-9736-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 19 00:26:12.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 delete e2e-test-crd-publish-openapi-9736-crds test-cr'
Jul 19 00:26:12.156: INFO: stderr: ""
Jul 19 00:26:12.156: INFO: stdout: "e2e-test-crd-publish-openapi-9736-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 19 00:26:12.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 apply -f -'
Jul 19 00:26:12.825: INFO: stderr: ""
Jul 19 00:26:12.825: INFO: stdout: "e2e-test-crd-publish-openapi-9736-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 19 00:26:12.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 delete e2e-test-crd-publish-openapi-9736-crds test-cr'
Jul 19 00:26:12.887: INFO: stderr: ""
Jul 19 00:26:12.887: INFO: stdout: "e2e-test-crd-publish-openapi-9736-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 07/19/23 00:26:12.887
Jul 19 00:26:12.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4502 explain e2e-test-crd-publish-openapi-9736-crds'
Jul 19 00:26:13.091: INFO: stderr: ""
Jul 19 00:26:13.091: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9736-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:15.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4502" for this suite. 07/19/23 00:26:15.084
------------------------------
• [SLOW TEST] [5.790 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:09.298
    Jul 19 00:26:09.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:26:09.299
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:09.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:09.307
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jul 19 00:26:09.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/19/23 00:26:11.28
    Jul 19 00:26:11.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 create -f -'
    Jul 19 00:26:12.090: INFO: stderr: ""
    Jul 19 00:26:12.090: INFO: stdout: "e2e-test-crd-publish-openapi-9736-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jul 19 00:26:12.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 delete e2e-test-crd-publish-openapi-9736-crds test-cr'
    Jul 19 00:26:12.156: INFO: stderr: ""
    Jul 19 00:26:12.156: INFO: stdout: "e2e-test-crd-publish-openapi-9736-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jul 19 00:26:12.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 apply -f -'
    Jul 19 00:26:12.825: INFO: stderr: ""
    Jul 19 00:26:12.825: INFO: stdout: "e2e-test-crd-publish-openapi-9736-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jul 19 00:26:12.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 delete e2e-test-crd-publish-openapi-9736-crds test-cr'
    Jul 19 00:26:12.887: INFO: stderr: ""
    Jul 19 00:26:12.887: INFO: stdout: "e2e-test-crd-publish-openapi-9736-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 07/19/23 00:26:12.887
    Jul 19 00:26:12.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4502 explain e2e-test-crd-publish-openapi-9736-crds'
    Jul 19 00:26:13.091: INFO: stderr: ""
    Jul 19 00:26:13.091: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9736-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:15.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4502" for this suite. 07/19/23 00:26:15.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:15.088
Jul 19 00:26:15.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename events 07/19/23 00:26:15.089
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:15.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:15.101
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 07/19/23 00:26:15.102
STEP: listing all events in all namespaces 07/19/23 00:26:15.105
STEP: patching the test event 07/19/23 00:26:15.127
STEP: fetching the test event 07/19/23 00:26:15.138
STEP: updating the test event 07/19/23 00:26:15.14
STEP: getting the test event 07/19/23 00:26:15.145
STEP: deleting the test event 07/19/23 00:26:15.146
STEP: listing all events in all namespaces 07/19/23 00:26:15.149
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:15.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8293" for this suite. 07/19/23 00:26:15.165
------------------------------
• [0.080 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:15.088
    Jul 19 00:26:15.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename events 07/19/23 00:26:15.089
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:15.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:15.101
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 07/19/23 00:26:15.102
    STEP: listing all events in all namespaces 07/19/23 00:26:15.105
    STEP: patching the test event 07/19/23 00:26:15.127
    STEP: fetching the test event 07/19/23 00:26:15.138
    STEP: updating the test event 07/19/23 00:26:15.14
    STEP: getting the test event 07/19/23 00:26:15.145
    STEP: deleting the test event 07/19/23 00:26:15.146
    STEP: listing all events in all namespaces 07/19/23 00:26:15.149
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:15.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8293" for this suite. 07/19/23 00:26:15.165
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:15.168
Jul 19 00:26:15.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename events 07/19/23 00:26:15.169
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:15.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:15.177
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 07/19/23 00:26:15.179
STEP: listing events in all namespaces 07/19/23 00:26:15.182
STEP: listing events in test namespace 07/19/23 00:26:15.195
STEP: listing events with field selection filtering on source 07/19/23 00:26:15.197
STEP: listing events with field selection filtering on reportingController 07/19/23 00:26:15.198
STEP: getting the test event 07/19/23 00:26:15.2
STEP: patching the test event 07/19/23 00:26:15.201
STEP: getting the test event 07/19/23 00:26:15.206
STEP: updating the test event 07/19/23 00:26:15.207
STEP: getting the test event 07/19/23 00:26:15.211
STEP: deleting the test event 07/19/23 00:26:15.214
STEP: listing events in all namespaces 07/19/23 00:26:15.216
STEP: listing events in test namespace 07/19/23 00:26:15.229
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:15.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5515" for this suite. 07/19/23 00:26:15.233
------------------------------
• [0.067 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:15.168
    Jul 19 00:26:15.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename events 07/19/23 00:26:15.169
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:15.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:15.177
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 07/19/23 00:26:15.179
    STEP: listing events in all namespaces 07/19/23 00:26:15.182
    STEP: listing events in test namespace 07/19/23 00:26:15.195
    STEP: listing events with field selection filtering on source 07/19/23 00:26:15.197
    STEP: listing events with field selection filtering on reportingController 07/19/23 00:26:15.198
    STEP: getting the test event 07/19/23 00:26:15.2
    STEP: patching the test event 07/19/23 00:26:15.201
    STEP: getting the test event 07/19/23 00:26:15.206
    STEP: updating the test event 07/19/23 00:26:15.207
    STEP: getting the test event 07/19/23 00:26:15.211
    STEP: deleting the test event 07/19/23 00:26:15.214
    STEP: listing events in all namespaces 07/19/23 00:26:15.216
    STEP: listing events in test namespace 07/19/23 00:26:15.229
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:15.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5515" for this suite. 07/19/23 00:26:15.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:15.235
Jul 19 00:26:15.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:26:15.236
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:15.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:15.244
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 07/19/23 00:26:15.246
Jul 19 00:26:15.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:26:17.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:25.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3843" for this suite. 07/19/23 00:26:25.756
------------------------------
• [SLOW TEST] [10.523 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:15.235
    Jul 19 00:26:15.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:26:15.236
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:15.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:15.244
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 07/19/23 00:26:15.246
    Jul 19 00:26:15.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:26:17.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:25.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3843" for this suite. 07/19/23 00:26:25.756
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:25.758
Jul 19 00:26:25.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:26:25.759
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:25.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:25.766
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jul 19 00:26:25.773: INFO: Waiting up to 5m0s for pod "pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa" in namespace "svcaccounts-9412" to be "running"
Jul 19 00:26:25.775: INFO: Pod "pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.936075ms
Jul 19 00:26:27.777: INFO: Pod "pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa": Phase="Running", Reason="", readiness=true. Elapsed: 2.004453449s
Jul 19 00:26:27.777: INFO: Pod "pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa" satisfied condition "running"
STEP: reading a file in the container 07/19/23 00:26:27.777
Jul 19 00:26:27.777: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9412 pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 07/19/23 00:26:27.883
Jul 19 00:26:27.883: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9412 pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 07/19/23 00:26:27.985
Jul 19 00:26:27.985: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9412 pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jul 19 00:26:28.090: INFO: Got root ca configmap in namespace "svcaccounts-9412"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:28.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9412" for this suite. 07/19/23 00:26:28.094
------------------------------
• [2.338 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:25.758
    Jul 19 00:26:25.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:26:25.759
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:25.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:25.766
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jul 19 00:26:25.773: INFO: Waiting up to 5m0s for pod "pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa" in namespace "svcaccounts-9412" to be "running"
    Jul 19 00:26:25.775: INFO: Pod "pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.936075ms
    Jul 19 00:26:27.777: INFO: Pod "pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa": Phase="Running", Reason="", readiness=true. Elapsed: 2.004453449s
    Jul 19 00:26:27.777: INFO: Pod "pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa" satisfied condition "running"
    STEP: reading a file in the container 07/19/23 00:26:27.777
    Jul 19 00:26:27.777: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9412 pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 07/19/23 00:26:27.883
    Jul 19 00:26:27.883: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9412 pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 07/19/23 00:26:27.985
    Jul 19 00:26:27.985: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9412 pod-service-account-9ed96762-917b-4f8a-93c6-253a8835effa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jul 19 00:26:28.090: INFO: Got root ca configmap in namespace "svcaccounts-9412"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:28.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9412" for this suite. 07/19/23 00:26:28.094
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:28.096
Jul 19 00:26:28.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 00:26:28.097
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:28.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:28.104
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:26:28.106
Jul 19 00:26:28.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc" in namespace "downward-api-9201" to be "Succeeded or Failed"
Jul 19 00:26:28.111: INFO: Pod "downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.528247ms
Jul 19 00:26:30.114: INFO: Pod "downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00413848s
Jul 19 00:26:32.115: INFO: Pod "downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004947853s
STEP: Saw pod success 07/19/23 00:26:32.115
Jul 19 00:26:32.115: INFO: Pod "downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc" satisfied condition "Succeeded or Failed"
Jul 19 00:26:32.116: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc container client-container: <nil>
STEP: delete the pod 07/19/23 00:26:32.125
Jul 19 00:26:32.133: INFO: Waiting for pod downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc to disappear
Jul 19 00:26:32.134: INFO: Pod downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:32.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9201" for this suite. 07/19/23 00:26:32.136
------------------------------
• [4.043 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:28.096
    Jul 19 00:26:28.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 00:26:28.097
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:28.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:28.104
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:26:28.106
    Jul 19 00:26:28.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc" in namespace "downward-api-9201" to be "Succeeded or Failed"
    Jul 19 00:26:28.111: INFO: Pod "downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.528247ms
    Jul 19 00:26:30.114: INFO: Pod "downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00413848s
    Jul 19 00:26:32.115: INFO: Pod "downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004947853s
    STEP: Saw pod success 07/19/23 00:26:32.115
    Jul 19 00:26:32.115: INFO: Pod "downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc" satisfied condition "Succeeded or Failed"
    Jul 19 00:26:32.116: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc container client-container: <nil>
    STEP: delete the pod 07/19/23 00:26:32.125
    Jul 19 00:26:32.133: INFO: Waiting for pod downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc to disappear
    Jul 19 00:26:32.134: INFO: Pod downwardapi-volume-f1611e54-e889-429f-bc7d-3cd02b7095fc no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:32.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9201" for this suite. 07/19/23 00:26:32.136
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:32.139
Jul 19 00:26:32.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:26:32.14
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:32.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:32.147
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:26:32.154
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:26:33.029
STEP: Deploying the webhook pod 07/19/23 00:26:33.032
STEP: Wait for the deployment to be ready 07/19/23 00:26:33.039
Jul 19 00:26:33.045: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:26:35.052
STEP: Verifying the service has paired with the endpoint 07/19/23 00:26:35.06
Jul 19 00:26:36.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jul 19 00:26:36.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5256-crds.webhook.example.com via the AdmissionRegistration API 07/19/23 00:26:36.568
STEP: Creating a custom resource that should be mutated by the webhook 07/19/23 00:26:36.593
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:39.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8013" for this suite. 07/19/23 00:26:39.384
STEP: Destroying namespace "webhook-8013-markers" for this suite. 07/19/23 00:26:39.392
------------------------------
• [SLOW TEST] [7.255 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:32.139
    Jul 19 00:26:32.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:26:32.14
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:32.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:32.147
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:26:32.154
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:26:33.029
    STEP: Deploying the webhook pod 07/19/23 00:26:33.032
    STEP: Wait for the deployment to be ready 07/19/23 00:26:33.039
    Jul 19 00:26:33.045: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:26:35.052
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:26:35.06
    Jul 19 00:26:36.060: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jul 19 00:26:36.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5256-crds.webhook.example.com via the AdmissionRegistration API 07/19/23 00:26:36.568
    STEP: Creating a custom resource that should be mutated by the webhook 07/19/23 00:26:36.593
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:39.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8013" for this suite. 07/19/23 00:26:39.384
    STEP: Destroying namespace "webhook-8013-markers" for this suite. 07/19/23 00:26:39.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:39.395
Jul 19 00:26:39.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 00:26:39.396
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:39.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:39.406
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 07/19/23 00:26:39.408
Jul 19 00:26:39.412: INFO: Waiting up to 5m0s for pod "downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f" in namespace "downward-api-6714" to be "Succeeded or Failed"
Jul 19 00:26:39.416: INFO: Pod "downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.495262ms
Jul 19 00:26:41.418: INFO: Pod "downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005939951s
Jul 19 00:26:43.419: INFO: Pod "downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006568824s
STEP: Saw pod success 07/19/23 00:26:43.419
Jul 19 00:26:43.419: INFO: Pod "downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f" satisfied condition "Succeeded or Failed"
Jul 19 00:26:43.420: INFO: Trying to get logs from node controller-1 pod downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f container dapi-container: <nil>
STEP: delete the pod 07/19/23 00:26:43.424
Jul 19 00:26:43.431: INFO: Waiting for pod downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f to disappear
Jul 19 00:26:43.432: INFO: Pod downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:43.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6714" for this suite. 07/19/23 00:26:43.434
------------------------------
• [4.042 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:39.395
    Jul 19 00:26:39.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 00:26:39.396
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:39.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:39.406
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 07/19/23 00:26:39.408
    Jul 19 00:26:39.412: INFO: Waiting up to 5m0s for pod "downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f" in namespace "downward-api-6714" to be "Succeeded or Failed"
    Jul 19 00:26:39.416: INFO: Pod "downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.495262ms
    Jul 19 00:26:41.418: INFO: Pod "downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005939951s
    Jul 19 00:26:43.419: INFO: Pod "downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006568824s
    STEP: Saw pod success 07/19/23 00:26:43.419
    Jul 19 00:26:43.419: INFO: Pod "downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f" satisfied condition "Succeeded or Failed"
    Jul 19 00:26:43.420: INFO: Trying to get logs from node controller-1 pod downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f container dapi-container: <nil>
    STEP: delete the pod 07/19/23 00:26:43.424
    Jul 19 00:26:43.431: INFO: Waiting for pod downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f to disappear
    Jul 19 00:26:43.432: INFO: Pod downward-api-46aa0462-cc26-47ba-b43a-5dce1630e34f no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:43.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6714" for this suite. 07/19/23 00:26:43.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:43.438
Jul 19 00:26:43.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename var-expansion 07/19/23 00:26:43.438
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:43.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:43.445
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jul 19 00:26:43.450: INFO: Waiting up to 2m0s for pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932" in namespace "var-expansion-7486" to be "container 0 failed with reason CreateContainerConfigError"
Jul 19 00:26:43.455: INFO: Pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932": Phase="Pending", Reason="", readiness=false. Elapsed: 4.575001ms
Jul 19 00:26:45.457: INFO: Pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006718185s
Jul 19 00:26:45.457: INFO: Pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jul 19 00:26:45.457: INFO: Deleting pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932" in namespace "var-expansion-7486"
Jul 19 00:26:45.460: INFO: Wait up to 5m0s for pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:47.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7486" for this suite. 07/19/23 00:26:47.468
------------------------------
• [4.033 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:43.438
    Jul 19 00:26:43.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename var-expansion 07/19/23 00:26:43.438
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:43.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:43.445
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jul 19 00:26:43.450: INFO: Waiting up to 2m0s for pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932" in namespace "var-expansion-7486" to be "container 0 failed with reason CreateContainerConfigError"
    Jul 19 00:26:43.455: INFO: Pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932": Phase="Pending", Reason="", readiness=false. Elapsed: 4.575001ms
    Jul 19 00:26:45.457: INFO: Pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006718185s
    Jul 19 00:26:45.457: INFO: Pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jul 19 00:26:45.457: INFO: Deleting pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932" in namespace "var-expansion-7486"
    Jul 19 00:26:45.460: INFO: Wait up to 5m0s for pod "var-expansion-6469afdc-4036-455c-9cbc-f7dde9aa8932" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:47.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7486" for this suite. 07/19/23 00:26:47.468
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:47.47
Jul 19 00:26:47.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:26:47.472
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:47.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:47.478
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 07/19/23 00:26:47.48
Jul 19 00:26:47.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: mark a version not serverd 07/19/23 00:26:51.719
STEP: check the unserved version gets removed 07/19/23 00:26:51.731
STEP: check the other version is not changed 07/19/23 00:26:53.73
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:26:57.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8952" for this suite. 07/19/23 00:26:57.207
------------------------------
• [SLOW TEST] [9.740 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:47.47
    Jul 19 00:26:47.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:26:47.472
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:47.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:47.478
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 07/19/23 00:26:47.48
    Jul 19 00:26:47.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: mark a version not serverd 07/19/23 00:26:51.719
    STEP: check the unserved version gets removed 07/19/23 00:26:51.731
    STEP: check the other version is not changed 07/19/23 00:26:53.73
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:26:57.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8952" for this suite. 07/19/23 00:26:57.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:26:57.212
Jul 19 00:26:57.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/19/23 00:26:57.212
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:57.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:57.22
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 07/19/23 00:26:57.227
STEP: watching for Pod to be ready 07/19/23 00:26:57.232
Jul 19 00:26:57.233: INFO: observed Pod pod-test in namespace pods-4722 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jul 19 00:26:57.234: INFO: observed Pod pod-test in namespace pods-4722 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  }]
Jul 19 00:26:57.243: INFO: observed Pod pod-test in namespace pods-4722 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  }]
Jul 19 00:26:57.718: INFO: observed Pod pod-test in namespace pods-4722 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  }]
Jul 19 00:26:57.736: INFO: observed Pod pod-test in namespace pods-4722 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  }]
Jul 19 00:26:58.492: INFO: Found Pod pod-test in namespace pods-4722 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 07/19/23 00:26:58.495
STEP: getting the Pod and ensuring that it's patched 07/19/23 00:26:58.501
STEP: replacing the Pod's status Ready condition to False 07/19/23 00:26:58.503
STEP: check the Pod again to ensure its Ready conditions are False 07/19/23 00:26:58.509
STEP: deleting the Pod via a Collection with a LabelSelector 07/19/23 00:26:58.509
STEP: watching for the Pod to be deleted 07/19/23 00:26:58.514
Jul 19 00:26:58.515: INFO: observed event type MODIFIED
Jul 19 00:27:00.499: INFO: observed event type MODIFIED
Jul 19 00:27:00.624: INFO: observed event type MODIFIED
Jul 19 00:27:01.500: INFO: observed event type MODIFIED
Jul 19 00:27:01.529: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jul 19 00:27:01.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4722" for this suite. 07/19/23 00:27:01.537
------------------------------
• [4.328 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:26:57.212
    Jul 19 00:26:57.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/19/23 00:26:57.212
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:26:57.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:26:57.22
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 07/19/23 00:26:57.227
    STEP: watching for Pod to be ready 07/19/23 00:26:57.232
    Jul 19 00:26:57.233: INFO: observed Pod pod-test in namespace pods-4722 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jul 19 00:26:57.234: INFO: observed Pod pod-test in namespace pods-4722 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  }]
    Jul 19 00:26:57.243: INFO: observed Pod pod-test in namespace pods-4722 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  }]
    Jul 19 00:26:57.718: INFO: observed Pod pod-test in namespace pods-4722 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  }]
    Jul 19 00:26:57.736: INFO: observed Pod pod-test in namespace pods-4722 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  }]
    Jul 19 00:26:58.492: INFO: Found Pod pod-test in namespace pods-4722 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-19 00:26:57 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 07/19/23 00:26:58.495
    STEP: getting the Pod and ensuring that it's patched 07/19/23 00:26:58.501
    STEP: replacing the Pod's status Ready condition to False 07/19/23 00:26:58.503
    STEP: check the Pod again to ensure its Ready conditions are False 07/19/23 00:26:58.509
    STEP: deleting the Pod via a Collection with a LabelSelector 07/19/23 00:26:58.509
    STEP: watching for the Pod to be deleted 07/19/23 00:26:58.514
    Jul 19 00:26:58.515: INFO: observed event type MODIFIED
    Jul 19 00:27:00.499: INFO: observed event type MODIFIED
    Jul 19 00:27:00.624: INFO: observed event type MODIFIED
    Jul 19 00:27:01.500: INFO: observed event type MODIFIED
    Jul 19 00:27:01.529: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:27:01.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4722" for this suite. 07/19/23 00:27:01.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:27:01.541
Jul 19 00:27:01.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename lease-test 07/19/23 00:27:01.542
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:01.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:01.55
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jul 19 00:27:01.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-6362" for this suite. 07/19/23 00:27:01.577
------------------------------
• [0.038 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:27:01.541
    Jul 19 00:27:01.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename lease-test 07/19/23 00:27:01.542
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:01.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:01.55
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:27:01.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-6362" for this suite. 07/19/23 00:27:01.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:27:01.581
Jul 19 00:27:01.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename discovery 07/19/23 00:27:01.581
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:01.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:01.589
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 07/19/23 00:27:01.591
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jul 19 00:27:02.166: INFO: Checking APIGroup: apiregistration.k8s.io
Jul 19 00:27:02.167: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul 19 00:27:02.167: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jul 19 00:27:02.167: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul 19 00:27:02.167: INFO: Checking APIGroup: apps
Jul 19 00:27:02.168: INFO: PreferredVersion.GroupVersion: apps/v1
Jul 19 00:27:02.168: INFO: Versions found [{apps/v1 v1}]
Jul 19 00:27:02.168: INFO: apps/v1 matches apps/v1
Jul 19 00:27:02.168: INFO: Checking APIGroup: events.k8s.io
Jul 19 00:27:02.168: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul 19 00:27:02.168: INFO: Versions found [{events.k8s.io/v1 v1}]
Jul 19 00:27:02.168: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul 19 00:27:02.168: INFO: Checking APIGroup: authentication.k8s.io
Jul 19 00:27:02.169: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul 19 00:27:02.169: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jul 19 00:27:02.169: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul 19 00:27:02.169: INFO: Checking APIGroup: authorization.k8s.io
Jul 19 00:27:02.169: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul 19 00:27:02.169: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jul 19 00:27:02.169: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul 19 00:27:02.169: INFO: Checking APIGroup: autoscaling
Jul 19 00:27:02.170: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jul 19 00:27:02.170: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jul 19 00:27:02.170: INFO: autoscaling/v2 matches autoscaling/v2
Jul 19 00:27:02.170: INFO: Checking APIGroup: batch
Jul 19 00:27:02.170: INFO: PreferredVersion.GroupVersion: batch/v1
Jul 19 00:27:02.170: INFO: Versions found [{batch/v1 v1}]
Jul 19 00:27:02.170: INFO: batch/v1 matches batch/v1
Jul 19 00:27:02.170: INFO: Checking APIGroup: certificates.k8s.io
Jul 19 00:27:02.171: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul 19 00:27:02.171: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jul 19 00:27:02.171: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul 19 00:27:02.171: INFO: Checking APIGroup: networking.k8s.io
Jul 19 00:27:02.172: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul 19 00:27:02.172: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jul 19 00:27:02.172: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul 19 00:27:02.172: INFO: Checking APIGroup: policy
Jul 19 00:27:02.172: INFO: PreferredVersion.GroupVersion: policy/v1
Jul 19 00:27:02.172: INFO: Versions found [{policy/v1 v1}]
Jul 19 00:27:02.172: INFO: policy/v1 matches policy/v1
Jul 19 00:27:02.172: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul 19 00:27:02.173: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul 19 00:27:02.173: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jul 19 00:27:02.173: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul 19 00:27:02.173: INFO: Checking APIGroup: storage.k8s.io
Jul 19 00:27:02.173: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul 19 00:27:02.173: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul 19 00:27:02.173: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul 19 00:27:02.173: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul 19 00:27:02.174: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul 19 00:27:02.174: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jul 19 00:27:02.174: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul 19 00:27:02.174: INFO: Checking APIGroup: apiextensions.k8s.io
Jul 19 00:27:02.174: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul 19 00:27:02.175: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jul 19 00:27:02.175: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul 19 00:27:02.175: INFO: Checking APIGroup: scheduling.k8s.io
Jul 19 00:27:02.175: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul 19 00:27:02.175: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jul 19 00:27:02.175: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul 19 00:27:02.175: INFO: Checking APIGroup: coordination.k8s.io
Jul 19 00:27:02.176: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul 19 00:27:02.176: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jul 19 00:27:02.176: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul 19 00:27:02.176: INFO: Checking APIGroup: node.k8s.io
Jul 19 00:27:02.176: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jul 19 00:27:02.176: INFO: Versions found [{node.k8s.io/v1 v1}]
Jul 19 00:27:02.176: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jul 19 00:27:02.176: INFO: Checking APIGroup: discovery.k8s.io
Jul 19 00:27:02.177: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jul 19 00:27:02.177: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jul 19 00:27:02.177: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jul 19 00:27:02.177: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jul 19 00:27:02.177: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jul 19 00:27:02.177: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jul 19 00:27:02.177: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jul 19 00:27:02.177: INFO: Checking APIGroup: acme.cert-manager.io
Jul 19 00:27:02.178: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Jul 19 00:27:02.178: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Jul 19 00:27:02.178: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Jul 19 00:27:02.178: INFO: Checking APIGroup: cert-manager.io
Jul 19 00:27:02.179: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Jul 19 00:27:02.179: INFO: Versions found [{cert-manager.io/v1 v1}]
Jul 19 00:27:02.179: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Jul 19 00:27:02.179: INFO: Checking APIGroup: crd.projectcalico.org
Jul 19 00:27:02.179: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jul 19 00:27:02.179: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jul 19 00:27:02.179: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jul 19 00:27:02.179: INFO: Checking APIGroup: k8s.cni.cncf.io
Jul 19 00:27:02.180: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jul 19 00:27:02.180: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jul 19 00:27:02.180: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jul 19 00:27:02.180: INFO: Checking APIGroup: starlingx.windriver.com
Jul 19 00:27:02.180: INFO: PreferredVersion.GroupVersion: starlingx.windriver.com/v1
Jul 19 00:27:02.180: INFO: Versions found [{starlingx.windriver.com/v1 v1}]
Jul 19 00:27:02.180: INFO: starlingx.windriver.com/v1 matches starlingx.windriver.com/v1
Jul 19 00:27:02.180: INFO: Checking APIGroup: source.toolkit.fluxcd.io
Jul 19 00:27:02.181: INFO: PreferredVersion.GroupVersion: source.toolkit.fluxcd.io/v1beta2
Jul 19 00:27:02.181: INFO: Versions found [{source.toolkit.fluxcd.io/v1beta2 v1beta2} {source.toolkit.fluxcd.io/v1beta1 v1beta1}]
Jul 19 00:27:02.181: INFO: source.toolkit.fluxcd.io/v1beta2 matches source.toolkit.fluxcd.io/v1beta2
Jul 19 00:27:02.181: INFO: Checking APIGroup: helm.toolkit.fluxcd.io
Jul 19 00:27:02.182: INFO: PreferredVersion.GroupVersion: helm.toolkit.fluxcd.io/v2beta1
Jul 19 00:27:02.182: INFO: Versions found [{helm.toolkit.fluxcd.io/v2beta1 v2beta1}]
Jul 19 00:27:02.182: INFO: helm.toolkit.fluxcd.io/v2beta1 matches helm.toolkit.fluxcd.io/v2beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jul 19 00:27:02.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-4018" for this suite. 07/19/23 00:27:02.184
------------------------------
• [0.605 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:27:01.581
    Jul 19 00:27:01.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename discovery 07/19/23 00:27:01.581
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:01.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:01.589
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 07/19/23 00:27:01.591
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jul 19 00:27:02.166: INFO: Checking APIGroup: apiregistration.k8s.io
    Jul 19 00:27:02.167: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jul 19 00:27:02.167: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jul 19 00:27:02.167: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jul 19 00:27:02.167: INFO: Checking APIGroup: apps
    Jul 19 00:27:02.168: INFO: PreferredVersion.GroupVersion: apps/v1
    Jul 19 00:27:02.168: INFO: Versions found [{apps/v1 v1}]
    Jul 19 00:27:02.168: INFO: apps/v1 matches apps/v1
    Jul 19 00:27:02.168: INFO: Checking APIGroup: events.k8s.io
    Jul 19 00:27:02.168: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jul 19 00:27:02.168: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jul 19 00:27:02.168: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jul 19 00:27:02.168: INFO: Checking APIGroup: authentication.k8s.io
    Jul 19 00:27:02.169: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jul 19 00:27:02.169: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jul 19 00:27:02.169: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jul 19 00:27:02.169: INFO: Checking APIGroup: authorization.k8s.io
    Jul 19 00:27:02.169: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jul 19 00:27:02.169: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jul 19 00:27:02.169: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jul 19 00:27:02.169: INFO: Checking APIGroup: autoscaling
    Jul 19 00:27:02.170: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jul 19 00:27:02.170: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jul 19 00:27:02.170: INFO: autoscaling/v2 matches autoscaling/v2
    Jul 19 00:27:02.170: INFO: Checking APIGroup: batch
    Jul 19 00:27:02.170: INFO: PreferredVersion.GroupVersion: batch/v1
    Jul 19 00:27:02.170: INFO: Versions found [{batch/v1 v1}]
    Jul 19 00:27:02.170: INFO: batch/v1 matches batch/v1
    Jul 19 00:27:02.170: INFO: Checking APIGroup: certificates.k8s.io
    Jul 19 00:27:02.171: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jul 19 00:27:02.171: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jul 19 00:27:02.171: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jul 19 00:27:02.171: INFO: Checking APIGroup: networking.k8s.io
    Jul 19 00:27:02.172: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jul 19 00:27:02.172: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jul 19 00:27:02.172: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jul 19 00:27:02.172: INFO: Checking APIGroup: policy
    Jul 19 00:27:02.172: INFO: PreferredVersion.GroupVersion: policy/v1
    Jul 19 00:27:02.172: INFO: Versions found [{policy/v1 v1}]
    Jul 19 00:27:02.172: INFO: policy/v1 matches policy/v1
    Jul 19 00:27:02.172: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jul 19 00:27:02.173: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jul 19 00:27:02.173: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jul 19 00:27:02.173: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jul 19 00:27:02.173: INFO: Checking APIGroup: storage.k8s.io
    Jul 19 00:27:02.173: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jul 19 00:27:02.173: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jul 19 00:27:02.173: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jul 19 00:27:02.173: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jul 19 00:27:02.174: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jul 19 00:27:02.174: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jul 19 00:27:02.174: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jul 19 00:27:02.174: INFO: Checking APIGroup: apiextensions.k8s.io
    Jul 19 00:27:02.174: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jul 19 00:27:02.175: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jul 19 00:27:02.175: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jul 19 00:27:02.175: INFO: Checking APIGroup: scheduling.k8s.io
    Jul 19 00:27:02.175: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jul 19 00:27:02.175: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jul 19 00:27:02.175: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jul 19 00:27:02.175: INFO: Checking APIGroup: coordination.k8s.io
    Jul 19 00:27:02.176: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jul 19 00:27:02.176: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jul 19 00:27:02.176: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jul 19 00:27:02.176: INFO: Checking APIGroup: node.k8s.io
    Jul 19 00:27:02.176: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jul 19 00:27:02.176: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jul 19 00:27:02.176: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jul 19 00:27:02.176: INFO: Checking APIGroup: discovery.k8s.io
    Jul 19 00:27:02.177: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jul 19 00:27:02.177: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jul 19 00:27:02.177: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jul 19 00:27:02.177: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jul 19 00:27:02.177: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jul 19 00:27:02.177: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jul 19 00:27:02.177: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jul 19 00:27:02.177: INFO: Checking APIGroup: acme.cert-manager.io
    Jul 19 00:27:02.178: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Jul 19 00:27:02.178: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    Jul 19 00:27:02.178: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Jul 19 00:27:02.178: INFO: Checking APIGroup: cert-manager.io
    Jul 19 00:27:02.179: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Jul 19 00:27:02.179: INFO: Versions found [{cert-manager.io/v1 v1}]
    Jul 19 00:27:02.179: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Jul 19 00:27:02.179: INFO: Checking APIGroup: crd.projectcalico.org
    Jul 19 00:27:02.179: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jul 19 00:27:02.179: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jul 19 00:27:02.179: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jul 19 00:27:02.179: INFO: Checking APIGroup: k8s.cni.cncf.io
    Jul 19 00:27:02.180: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Jul 19 00:27:02.180: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Jul 19 00:27:02.180: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Jul 19 00:27:02.180: INFO: Checking APIGroup: starlingx.windriver.com
    Jul 19 00:27:02.180: INFO: PreferredVersion.GroupVersion: starlingx.windriver.com/v1
    Jul 19 00:27:02.180: INFO: Versions found [{starlingx.windriver.com/v1 v1}]
    Jul 19 00:27:02.180: INFO: starlingx.windriver.com/v1 matches starlingx.windriver.com/v1
    Jul 19 00:27:02.180: INFO: Checking APIGroup: source.toolkit.fluxcd.io
    Jul 19 00:27:02.181: INFO: PreferredVersion.GroupVersion: source.toolkit.fluxcd.io/v1beta2
    Jul 19 00:27:02.181: INFO: Versions found [{source.toolkit.fluxcd.io/v1beta2 v1beta2} {source.toolkit.fluxcd.io/v1beta1 v1beta1}]
    Jul 19 00:27:02.181: INFO: source.toolkit.fluxcd.io/v1beta2 matches source.toolkit.fluxcd.io/v1beta2
    Jul 19 00:27:02.181: INFO: Checking APIGroup: helm.toolkit.fluxcd.io
    Jul 19 00:27:02.182: INFO: PreferredVersion.GroupVersion: helm.toolkit.fluxcd.io/v2beta1
    Jul 19 00:27:02.182: INFO: Versions found [{helm.toolkit.fluxcd.io/v2beta1 v2beta1}]
    Jul 19 00:27:02.182: INFO: helm.toolkit.fluxcd.io/v2beta1 matches helm.toolkit.fluxcd.io/v2beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:27:02.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-4018" for this suite. 07/19/23 00:27:02.184
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:27:02.187
Jul 19 00:27:02.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:27:02.187
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:02.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:02.195
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-82e6c3ec-c729-4402-9e6f-67161bc9b79b 07/19/23 00:27:02.199
STEP: Creating the pod 07/19/23 00:27:02.201
Jul 19 00:27:02.206: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb078d38-8a79-4d03-bae0-c0afce032ae0" in namespace "configmap-5911" to be "running"
Jul 19 00:27:02.207: INFO: Pod "pod-configmaps-bb078d38-8a79-4d03-bae0-c0afce032ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.429066ms
Jul 19 00:27:04.210: INFO: Pod "pod-configmaps-bb078d38-8a79-4d03-bae0-c0afce032ae0": Phase="Running", Reason="", readiness=false. Elapsed: 2.003821384s
Jul 19 00:27:04.210: INFO: Pod "pod-configmaps-bb078d38-8a79-4d03-bae0-c0afce032ae0" satisfied condition "running"
STEP: Waiting for pod with text data 07/19/23 00:27:04.21
STEP: Waiting for pod with binary data 07/19/23 00:27:04.214
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:27:04.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5911" for this suite. 07/19/23 00:27:04.218
------------------------------
• [2.035 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:27:02.187
    Jul 19 00:27:02.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:27:02.187
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:02.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:02.195
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-82e6c3ec-c729-4402-9e6f-67161bc9b79b 07/19/23 00:27:02.199
    STEP: Creating the pod 07/19/23 00:27:02.201
    Jul 19 00:27:02.206: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb078d38-8a79-4d03-bae0-c0afce032ae0" in namespace "configmap-5911" to be "running"
    Jul 19 00:27:02.207: INFO: Pod "pod-configmaps-bb078d38-8a79-4d03-bae0-c0afce032ae0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.429066ms
    Jul 19 00:27:04.210: INFO: Pod "pod-configmaps-bb078d38-8a79-4d03-bae0-c0afce032ae0": Phase="Running", Reason="", readiness=false. Elapsed: 2.003821384s
    Jul 19 00:27:04.210: INFO: Pod "pod-configmaps-bb078d38-8a79-4d03-bae0-c0afce032ae0" satisfied condition "running"
    STEP: Waiting for pod with text data 07/19/23 00:27:04.21
    STEP: Waiting for pod with binary data 07/19/23 00:27:04.214
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:27:04.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5911" for this suite. 07/19/23 00:27:04.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:27:04.222
Jul 19 00:27:04.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 00:27:04.222
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:04.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:04.232
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 07/19/23 00:27:04.234
Jul 19 00:27:04.237: INFO: Waiting up to 5m0s for pod "downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2" in namespace "downward-api-2546" to be "Succeeded or Failed"
Jul 19 00:27:04.240: INFO: Pod "downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.769054ms
Jul 19 00:27:06.243: INFO: Pod "downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005811188s
Jul 19 00:27:08.243: INFO: Pod "downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005697579s
STEP: Saw pod success 07/19/23 00:27:08.243
Jul 19 00:27:08.243: INFO: Pod "downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2" satisfied condition "Succeeded or Failed"
Jul 19 00:27:08.245: INFO: Trying to get logs from node controller-0 pod downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2 container dapi-container: <nil>
STEP: delete the pod 07/19/23 00:27:08.256
Jul 19 00:27:08.262: INFO: Waiting for pod downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2 to disappear
Jul 19 00:27:08.263: INFO: Pod downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jul 19 00:27:08.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2546" for this suite. 07/19/23 00:27:08.265
------------------------------
• [4.046 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:27:04.222
    Jul 19 00:27:04.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 00:27:04.222
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:04.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:04.232
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 07/19/23 00:27:04.234
    Jul 19 00:27:04.237: INFO: Waiting up to 5m0s for pod "downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2" in namespace "downward-api-2546" to be "Succeeded or Failed"
    Jul 19 00:27:04.240: INFO: Pod "downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.769054ms
    Jul 19 00:27:06.243: INFO: Pod "downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005811188s
    Jul 19 00:27:08.243: INFO: Pod "downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005697579s
    STEP: Saw pod success 07/19/23 00:27:08.243
    Jul 19 00:27:08.243: INFO: Pod "downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2" satisfied condition "Succeeded or Failed"
    Jul 19 00:27:08.245: INFO: Trying to get logs from node controller-0 pod downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2 container dapi-container: <nil>
    STEP: delete the pod 07/19/23 00:27:08.256
    Jul 19 00:27:08.262: INFO: Waiting for pod downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2 to disappear
    Jul 19 00:27:08.263: INFO: Pod downward-api-6a74043d-7f88-43c0-bb8e-0eba310d34e2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:27:08.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2546" for this suite. 07/19/23 00:27:08.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:27:08.268
Jul 19 00:27:08.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename deployment 07/19/23 00:27:08.269
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:08.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:08.279
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jul 19 00:27:08.285: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 19 00:27:13.287: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/19/23 00:27:13.287
Jul 19 00:27:13.287: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 07/19/23 00:27:13.292
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 19 00:27:13.299: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2586  c905e528-2214-4907-bde9-9143f300e069 62566 1 2023-07-19 00:27:13 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-07-19 00:27:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004604548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jul 19 00:27:13.300: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jul 19 00:27:13.300: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul 19 00:27:13.300: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2586  a79ec9cb-41f8-4a9f-b425-f45a29bcb704 62569 1 2023-07-19 00:27:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c905e528-2214-4907-bde9-9143f300e069 0xc004d9a317 0xc004d9a318}] [] [{e2e.test Update apps/v1 2023-07-19 00:27:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:27:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-19 00:27:13 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c905e528-2214-4907-bde9-9143f300e069\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004d9a3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 19 00:27:13.303: INFO: Pod "test-cleanup-controller-5zjfg" is available:
&Pod{ObjectMeta:{test-cleanup-controller-5zjfg test-cleanup-controller- deployment-2586  654a375d-b1b9-4f2b-b5b8-4280b32cabb5 62517 0 2023-07-19 00:27:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:576d5971519fa1a0cc089c88926cdee455029cbdd92f84fc8e944a610e3aaf7f cni.projectcalico.org/podIP:172.16.192.114/32 cni.projectcalico.org/podIPs:172.16.192.114/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.114"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.114"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-cleanup-controller a79ec9cb-41f8-4a9f-b425-f45a29bcb704 0xc004d9a6d7 0xc004d9a6d8}] [] [{calico Update v1 2023-07-19 00:27:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 00:27:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a79ec9cb-41f8-4a9f-b425-f45a29bcb704\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 00:27:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:27:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwz9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwz9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:27:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:27:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:27:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:27:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.114,StartTime:2023-07-19 00:27:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:27:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9850267ca3382db98cf3715f06d0615165124f26235c6e6e0d7aa90672eae447,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jul 19 00:27:13.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2586" for this suite. 07/19/23 00:27:13.311
------------------------------
• [SLOW TEST] [5.054 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:27:08.268
    Jul 19 00:27:08.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename deployment 07/19/23 00:27:08.269
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:08.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:08.279
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jul 19 00:27:08.285: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jul 19 00:27:13.287: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/19/23 00:27:13.287
    Jul 19 00:27:13.287: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 07/19/23 00:27:13.292
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 19 00:27:13.299: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2586  c905e528-2214-4907-bde9-9143f300e069 62566 1 2023-07-19 00:27:13 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-07-19 00:27:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004604548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jul 19 00:27:13.300: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jul 19 00:27:13.300: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jul 19 00:27:13.300: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2586  a79ec9cb-41f8-4a9f-b425-f45a29bcb704 62569 1 2023-07-19 00:27:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c905e528-2214-4907-bde9-9143f300e069 0xc004d9a317 0xc004d9a318}] [] [{e2e.test Update apps/v1 2023-07-19 00:27:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:27:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-19 00:27:13 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c905e528-2214-4907-bde9-9143f300e069\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004d9a3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 00:27:13.303: INFO: Pod "test-cleanup-controller-5zjfg" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-5zjfg test-cleanup-controller- deployment-2586  654a375d-b1b9-4f2b-b5b8-4280b32cabb5 62517 0 2023-07-19 00:27:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:576d5971519fa1a0cc089c88926cdee455029cbdd92f84fc8e944a610e3aaf7f cni.projectcalico.org/podIP:172.16.192.114/32 cni.projectcalico.org/podIPs:172.16.192.114/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.114"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.114"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-cleanup-controller a79ec9cb-41f8-4a9f-b425-f45a29bcb704 0xc004d9a6d7 0xc004d9a6d8}] [] [{calico Update v1 2023-07-19 00:27:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 00:27:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a79ec9cb-41f8-4a9f-b425-f45a29bcb704\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 00:27:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:27:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mwz9j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mwz9j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:27:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:27:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:27:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:27:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.114,StartTime:2023-07-19 00:27:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:27:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9850267ca3382db98cf3715f06d0615165124f26235c6e6e0d7aa90672eae447,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.114,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:27:13.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2586" for this suite. 07/19/23 00:27:13.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:27:13.323
Jul 19 00:27:13.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replicaset 07/19/23 00:27:13.324
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:13.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:13.339
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 07/19/23 00:27:13.341
Jul 19 00:27:13.345: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 19 00:27:18.350: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/19/23 00:27:18.35
STEP: getting scale subresource 07/19/23 00:27:18.35
STEP: updating a scale subresource 07/19/23 00:27:18.356
STEP: verifying the replicaset Spec.Replicas was modified 07/19/23 00:27:18.359
STEP: Patch a scale subresource 07/19/23 00:27:18.36
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:27:18.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4743" for this suite. 07/19/23 00:27:18.373
------------------------------
• [SLOW TEST] [5.065 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:27:13.323
    Jul 19 00:27:13.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replicaset 07/19/23 00:27:13.324
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:13.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:13.339
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 07/19/23 00:27:13.341
    Jul 19 00:27:13.345: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jul 19 00:27:18.350: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/19/23 00:27:18.35
    STEP: getting scale subresource 07/19/23 00:27:18.35
    STEP: updating a scale subresource 07/19/23 00:27:18.356
    STEP: verifying the replicaset Spec.Replicas was modified 07/19/23 00:27:18.359
    STEP: Patch a scale subresource 07/19/23 00:27:18.36
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:27:18.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4743" for this suite. 07/19/23 00:27:18.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:27:18.388
Jul 19 00:27:18.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:27:18.389
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:18.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:18.403
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 07/19/23 00:27:18.405
Jul 19 00:27:18.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-8113 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jul 19 00:27:18.467: INFO: stderr: ""
Jul 19 00:27:18.467: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 07/19/23 00:27:18.467
STEP: verifying the pod e2e-test-httpd-pod was created 07/19/23 00:27:23.518
Jul 19 00:27:23.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-8113 get pod e2e-test-httpd-pod -o json'
Jul 19 00:27:23.579: INFO: stderr: ""
Jul 19 00:27:23.579: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"eac2ff9d992e83ba335fcf68b5cff5cdfad60128b6a7651a04f2896e8faad142\",\n            \"cni.projectcalico.org/podIP\": \"172.16.166.129/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.16.166.129/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"chain\\\",\\n    \\\"ips\\\": [\\n        \\\"172.16.166.129\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"chain\\\",\\n    \\\"ips\\\": [\\n        \\\"172.16.166.129\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2023-07-19T00:27:18Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8113\",\n        \"resourceVersion\": \"62696\",\n        \"uid\": \"8dabb1ed-a28c-4576-8d48-d26c0ea6c358\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-h5gcq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"controller-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 30\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 30\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-h5gcq\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-19T00:27:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-19T00:27:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-19T00:27:19Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-19T00:27:18Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://4bf1e7ec7affb5c0ca81601b7ca285112f73d62e1b0a2dc217178da1fe2ffb47\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-19T00:27:19Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.206.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.166.129\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.166.129\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-19T00:27:18Z\"\n    }\n}\n"
STEP: replace the image in the pod 07/19/23 00:27:23.58
Jul 19 00:27:23.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-8113 replace -f -'
Jul 19 00:27:24.314: INFO: stderr: ""
Jul 19 00:27:24.314: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 07/19/23 00:27:24.314
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jul 19 00:27:24.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-8113 delete pods e2e-test-httpd-pod'
Jul 19 00:27:25.577: INFO: stderr: ""
Jul 19 00:27:25.577: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:27:25.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8113" for this suite. 07/19/23 00:27:25.58
------------------------------
• [SLOW TEST] [7.194 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:27:18.388
    Jul 19 00:27:18.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:27:18.389
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:18.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:18.403
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 07/19/23 00:27:18.405
    Jul 19 00:27:18.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-8113 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jul 19 00:27:18.467: INFO: stderr: ""
    Jul 19 00:27:18.467: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 07/19/23 00:27:18.467
    STEP: verifying the pod e2e-test-httpd-pod was created 07/19/23 00:27:23.518
    Jul 19 00:27:23.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-8113 get pod e2e-test-httpd-pod -o json'
    Jul 19 00:27:23.579: INFO: stderr: ""
    Jul 19 00:27:23.579: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"eac2ff9d992e83ba335fcf68b5cff5cdfad60128b6a7651a04f2896e8faad142\",\n            \"cni.projectcalico.org/podIP\": \"172.16.166.129/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.16.166.129/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"chain\\\",\\n    \\\"ips\\\": [\\n        \\\"172.16.166.129\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"chain\\\",\\n    \\\"ips\\\": [\\n        \\\"172.16.166.129\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\"\n        },\n        \"creationTimestamp\": \"2023-07-19T00:27:18Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8113\",\n        \"resourceVersion\": \"62696\",\n        \"uid\": \"8dabb1ed-a28c-4576-8d48-d26c0ea6c358\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-h5gcq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"controller-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 30\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 30\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-h5gcq\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-19T00:27:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-19T00:27:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-19T00:27:19Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-19T00:27:18Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://4bf1e7ec7affb5c0ca81601b7ca285112f73d62e1b0a2dc217178da1fe2ffb47\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-19T00:27:19Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.206.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.166.129\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.166.129\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-19T00:27:18Z\"\n    }\n}\n"
    STEP: replace the image in the pod 07/19/23 00:27:23.58
    Jul 19 00:27:23.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-8113 replace -f -'
    Jul 19 00:27:24.314: INFO: stderr: ""
    Jul 19 00:27:24.314: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 07/19/23 00:27:24.314
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jul 19 00:27:24.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-8113 delete pods e2e-test-httpd-pod'
    Jul 19 00:27:25.577: INFO: stderr: ""
    Jul 19 00:27:25.577: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:27:25.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8113" for this suite. 07/19/23 00:27:25.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:27:25.584
Jul 19 00:27:25.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename cronjob 07/19/23 00:27:25.584
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:25.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:25.596
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 07/19/23 00:27:25.597
STEP: Ensuring a job is scheduled 07/19/23 00:27:25.6
STEP: Ensuring exactly one is scheduled 07/19/23 00:28:01.602
STEP: Ensuring exactly one running job exists by listing jobs explicitly 07/19/23 00:28:01.603
STEP: Ensuring the job is replaced with a new one 07/19/23 00:28:01.605
STEP: Removing cronjob 07/19/23 00:29:01.608
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jul 19 00:29:01.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9942" for this suite. 07/19/23 00:29:01.612
------------------------------
• [SLOW TEST] [96.032 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:27:25.584
    Jul 19 00:27:25.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename cronjob 07/19/23 00:27:25.584
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:27:25.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:27:25.596
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 07/19/23 00:27:25.597
    STEP: Ensuring a job is scheduled 07/19/23 00:27:25.6
    STEP: Ensuring exactly one is scheduled 07/19/23 00:28:01.602
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 07/19/23 00:28:01.603
    STEP: Ensuring the job is replaced with a new one 07/19/23 00:28:01.605
    STEP: Removing cronjob 07/19/23 00:29:01.608
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:29:01.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9942" for this suite. 07/19/23 00:29:01.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:29:01.615
Jul 19 00:29:01.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename job 07/19/23 00:29:01.616
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:29:01.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:29:01.636
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 07/19/23 00:29:01.637
STEP: Ensuring job reaches completions 07/19/23 00:29:01.64
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jul 19 00:29:13.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4837" for this suite. 07/19/23 00:29:13.646
------------------------------
• [SLOW TEST] [12.033 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:29:01.615
    Jul 19 00:29:01.616: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename job 07/19/23 00:29:01.616
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:29:01.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:29:01.636
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 07/19/23 00:29:01.637
    STEP: Ensuring job reaches completions 07/19/23 00:29:01.64
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:29:13.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4837" for this suite. 07/19/23 00:29:13.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:29:13.65
Jul 19 00:29:13.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:29:13.651
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:29:13.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:29:13.663
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 07/19/23 00:29:13.664
Jul 19 00:29:13.668: INFO: Waiting up to 5m0s for pod "pod-cd6cd768-106c-4658-912d-82b42c9936fa" in namespace "emptydir-1987" to be "Succeeded or Failed"
Jul 19 00:29:13.671: INFO: Pod "pod-cd6cd768-106c-4658-912d-82b42c9936fa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.01909ms
Jul 19 00:29:15.674: INFO: Pod "pod-cd6cd768-106c-4658-912d-82b42c9936fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005299546s
Jul 19 00:29:17.674: INFO: Pod "pod-cd6cd768-106c-4658-912d-82b42c9936fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005727899s
STEP: Saw pod success 07/19/23 00:29:17.674
Jul 19 00:29:17.674: INFO: Pod "pod-cd6cd768-106c-4658-912d-82b42c9936fa" satisfied condition "Succeeded or Failed"
Jul 19 00:29:17.676: INFO: Trying to get logs from node controller-1 pod pod-cd6cd768-106c-4658-912d-82b42c9936fa container test-container: <nil>
STEP: delete the pod 07/19/23 00:29:17.685
Jul 19 00:29:17.691: INFO: Waiting for pod pod-cd6cd768-106c-4658-912d-82b42c9936fa to disappear
Jul 19 00:29:17.692: INFO: Pod pod-cd6cd768-106c-4658-912d-82b42c9936fa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:29:17.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1987" for this suite. 07/19/23 00:29:17.694
------------------------------
• [4.047 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:29:13.65
    Jul 19 00:29:13.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:29:13.651
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:29:13.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:29:13.663
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 07/19/23 00:29:13.664
    Jul 19 00:29:13.668: INFO: Waiting up to 5m0s for pod "pod-cd6cd768-106c-4658-912d-82b42c9936fa" in namespace "emptydir-1987" to be "Succeeded or Failed"
    Jul 19 00:29:13.671: INFO: Pod "pod-cd6cd768-106c-4658-912d-82b42c9936fa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.01909ms
    Jul 19 00:29:15.674: INFO: Pod "pod-cd6cd768-106c-4658-912d-82b42c9936fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005299546s
    Jul 19 00:29:17.674: INFO: Pod "pod-cd6cd768-106c-4658-912d-82b42c9936fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005727899s
    STEP: Saw pod success 07/19/23 00:29:17.674
    Jul 19 00:29:17.674: INFO: Pod "pod-cd6cd768-106c-4658-912d-82b42c9936fa" satisfied condition "Succeeded or Failed"
    Jul 19 00:29:17.676: INFO: Trying to get logs from node controller-1 pod pod-cd6cd768-106c-4658-912d-82b42c9936fa container test-container: <nil>
    STEP: delete the pod 07/19/23 00:29:17.685
    Jul 19 00:29:17.691: INFO: Waiting for pod pod-cd6cd768-106c-4658-912d-82b42c9936fa to disappear
    Jul 19 00:29:17.692: INFO: Pod pod-cd6cd768-106c-4658-912d-82b42c9936fa no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:29:17.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1987" for this suite. 07/19/23 00:29:17.694
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:29:17.697
Jul 19 00:29:17.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-runtime 07/19/23 00:29:17.698
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:29:17.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:29:17.706
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 07/19/23 00:29:17.708
STEP: wait for the container to reach Succeeded 07/19/23 00:29:17.712
STEP: get the container status 07/19/23 00:29:21.723
STEP: the container should be terminated 07/19/23 00:29:21.724
STEP: the termination message should be set 07/19/23 00:29:21.724
Jul 19 00:29:21.724: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 07/19/23 00:29:21.724
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jul 19 00:29:21.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6202" for this suite. 07/19/23 00:29:21.737
------------------------------
• [4.043 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:29:17.697
    Jul 19 00:29:17.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-runtime 07/19/23 00:29:17.698
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:29:17.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:29:17.706
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 07/19/23 00:29:17.708
    STEP: wait for the container to reach Succeeded 07/19/23 00:29:17.712
    STEP: get the container status 07/19/23 00:29:21.723
    STEP: the container should be terminated 07/19/23 00:29:21.724
    STEP: the termination message should be set 07/19/23 00:29:21.724
    Jul 19 00:29:21.724: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 07/19/23 00:29:21.724
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:29:21.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6202" for this suite. 07/19/23 00:29:21.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:29:21.741
Jul 19 00:29:21.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pod-network-test 07/19/23 00:29:21.742
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:29:21.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:29:21.751
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7129 07/19/23 00:29:21.752
STEP: creating a selector 07/19/23 00:29:21.752
STEP: Creating the service pods in kubernetes 07/19/23 00:29:21.752
Jul 19 00:29:21.752: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 19 00:29:21.765: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7129" to be "running and ready"
Jul 19 00:29:21.770: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.724416ms
Jul 19 00:29:21.770: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:29:23.774: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00804443s
Jul 19 00:29:23.774: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 19 00:29:25.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007545514s
Jul 19 00:29:25.773: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 19 00:29:27.772: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007006303s
Jul 19 00:29:27.773: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 19 00:29:29.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00742359s
Jul 19 00:29:29.773: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 19 00:29:31.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007447817s
Jul 19 00:29:31.773: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jul 19 00:29:33.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.007545438s
Jul 19 00:29:33.773: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jul 19 00:29:33.773: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jul 19 00:29:33.774: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7129" to be "running and ready"
Jul 19 00:29:33.776: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.609206ms
Jul 19 00:29:33.776: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jul 19 00:29:33.776: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 07/19/23 00:29:33.778
Jul 19 00:29:33.780: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7129" to be "running"
Jul 19 00:29:33.784: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.274161ms
Jul 19 00:29:35.787: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006435586s
Jul 19 00:29:35.787: INFO: Pod "test-container-pod" satisfied condition "running"
Jul 19 00:29:35.788: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 19 00:29:35.788: INFO: Breadth first check of 172.16.192.70 on host 192.168.206.2...
Jul 19 00:29:35.790: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.143:9080/dial?request=hostname&protocol=http&host=172.16.192.70&port=8083&tries=1'] Namespace:pod-network-test-7129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:29:35.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:29:35.790: INFO: ExecWithOptions: Clientset creation
Jul 19 00:29:35.790: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.166.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.192.70%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul 19 00:29:35.838: INFO: Waiting for responses: map[]
Jul 19 00:29:35.838: INFO: reached 172.16.192.70 after 0/1 tries
Jul 19 00:29:35.838: INFO: Breadth first check of 172.16.166.151 on host 192.168.206.3...
Jul 19 00:29:35.840: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.143:9080/dial?request=hostname&protocol=http&host=172.16.166.151&port=8083&tries=1'] Namespace:pod-network-test-7129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:29:35.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:29:35.841: INFO: ExecWithOptions: Clientset creation
Jul 19 00:29:35.841: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.166.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.166.151%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jul 19 00:29:35.889: INFO: Waiting for responses: map[]
Jul 19 00:29:35.889: INFO: reached 172.16.166.151 after 0/1 tries
Jul 19 00:29:35.889: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jul 19 00:29:35.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7129" for this suite. 07/19/23 00:29:35.891
------------------------------
• [SLOW TEST] [14.152 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:29:21.741
    Jul 19 00:29:21.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pod-network-test 07/19/23 00:29:21.742
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:29:21.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:29:21.751
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7129 07/19/23 00:29:21.752
    STEP: creating a selector 07/19/23 00:29:21.752
    STEP: Creating the service pods in kubernetes 07/19/23 00:29:21.752
    Jul 19 00:29:21.752: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jul 19 00:29:21.765: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7129" to be "running and ready"
    Jul 19 00:29:21.770: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.724416ms
    Jul 19 00:29:21.770: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:29:23.774: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00804443s
    Jul 19 00:29:23.774: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 19 00:29:25.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.007545514s
    Jul 19 00:29:25.773: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 19 00:29:27.772: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007006303s
    Jul 19 00:29:27.773: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 19 00:29:29.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.00742359s
    Jul 19 00:29:29.773: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 19 00:29:31.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007447817s
    Jul 19 00:29:31.773: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jul 19 00:29:33.773: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.007545438s
    Jul 19 00:29:33.773: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jul 19 00:29:33.773: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jul 19 00:29:33.774: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7129" to be "running and ready"
    Jul 19 00:29:33.776: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.609206ms
    Jul 19 00:29:33.776: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jul 19 00:29:33.776: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 07/19/23 00:29:33.778
    Jul 19 00:29:33.780: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7129" to be "running"
    Jul 19 00:29:33.784: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.274161ms
    Jul 19 00:29:35.787: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006435586s
    Jul 19 00:29:35.787: INFO: Pod "test-container-pod" satisfied condition "running"
    Jul 19 00:29:35.788: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jul 19 00:29:35.788: INFO: Breadth first check of 172.16.192.70 on host 192.168.206.2...
    Jul 19 00:29:35.790: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.143:9080/dial?request=hostname&protocol=http&host=172.16.192.70&port=8083&tries=1'] Namespace:pod-network-test-7129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:29:35.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:29:35.790: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:29:35.790: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.166.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.192.70%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jul 19 00:29:35.838: INFO: Waiting for responses: map[]
    Jul 19 00:29:35.838: INFO: reached 172.16.192.70 after 0/1 tries
    Jul 19 00:29:35.838: INFO: Breadth first check of 172.16.166.151 on host 192.168.206.3...
    Jul 19 00:29:35.840: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.166.143:9080/dial?request=hostname&protocol=http&host=172.16.166.151&port=8083&tries=1'] Namespace:pod-network-test-7129 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:29:35.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:29:35.841: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:29:35.841: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7129/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.166.143%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.166.151%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jul 19 00:29:35.889: INFO: Waiting for responses: map[]
    Jul 19 00:29:35.889: INFO: reached 172.16.166.151 after 0/1 tries
    Jul 19 00:29:35.889: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:29:35.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7129" for this suite. 07/19/23 00:29:35.891
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:29:35.894
Jul 19 00:29:35.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:29:35.895
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:29:35.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:29:35.903
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jul 19 00:29:35.911: INFO: created pod
Jul 19 00:29:35.911: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2295" to be "Succeeded or Failed"
Jul 19 00:29:35.912: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.286738ms
Jul 19 00:29:37.914: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00339955s
Jul 19 00:29:39.914: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003365396s
STEP: Saw pod success 07/19/23 00:29:39.914
Jul 19 00:29:39.914: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jul 19 00:30:09.915: INFO: polling logs
Jul 19 00:30:09.919: INFO: Pod logs: 
I0719 00:29:36.566472       1 log.go:198] OK: Got token
I0719 00:29:36.566500       1 log.go:198] validating with in-cluster discovery
I0719 00:29:36.566767       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0719 00:29:36.566794       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2295:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1689727176, NotBefore:1689726576, IssuedAt:1689726576, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2295", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ede6c787-80b6-48e8-9fad-9708f0b7795d"}}}
I0719 00:29:36.574810       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0719 00:29:36.579792       1 log.go:198] OK: Validated signature on JWT
I0719 00:29:36.579865       1 log.go:198] OK: Got valid claims from token!
I0719 00:29:36.579887       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2295:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1689727176, NotBefore:1689726576, IssuedAt:1689726576, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2295", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ede6c787-80b6-48e8-9fad-9708f0b7795d"}}}

Jul 19 00:30:09.919: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jul 19 00:30:09.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2295" for this suite. 07/19/23 00:30:09.923
------------------------------
• [SLOW TEST] [34.032 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:29:35.894
    Jul 19 00:29:35.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:29:35.895
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:29:35.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:29:35.903
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jul 19 00:29:35.911: INFO: created pod
    Jul 19 00:29:35.911: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2295" to be "Succeeded or Failed"
    Jul 19 00:29:35.912: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.286738ms
    Jul 19 00:29:37.914: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00339955s
    Jul 19 00:29:39.914: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003365396s
    STEP: Saw pod success 07/19/23 00:29:39.914
    Jul 19 00:29:39.914: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jul 19 00:30:09.915: INFO: polling logs
    Jul 19 00:30:09.919: INFO: Pod logs: 
    I0719 00:29:36.566472       1 log.go:198] OK: Got token
    I0719 00:29:36.566500       1 log.go:198] validating with in-cluster discovery
    I0719 00:29:36.566767       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0719 00:29:36.566794       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2295:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1689727176, NotBefore:1689726576, IssuedAt:1689726576, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2295", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ede6c787-80b6-48e8-9fad-9708f0b7795d"}}}
    I0719 00:29:36.574810       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0719 00:29:36.579792       1 log.go:198] OK: Validated signature on JWT
    I0719 00:29:36.579865       1 log.go:198] OK: Got valid claims from token!
    I0719 00:29:36.579887       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2295:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1689727176, NotBefore:1689726576, IssuedAt:1689726576, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2295", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ede6c787-80b6-48e8-9fad-9708f0b7795d"}}}

    Jul 19 00:30:09.919: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:30:09.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2295" for this suite. 07/19/23 00:30:09.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:30:09.926
Jul 19 00:30:09.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:30:09.927
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:09.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:09.935
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-52417c7c-75f5-42bc-af23-0e6151a1b62d 07/19/23 00:30:09.937
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:30:09.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-218" for this suite. 07/19/23 00:30:09.94
------------------------------
• [0.016 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:30:09.926
    Jul 19 00:30:09.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:30:09.927
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:09.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:09.935
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-52417c7c-75f5-42bc-af23-0e6151a1b62d 07/19/23 00:30:09.937
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:30:09.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-218" for this suite. 07/19/23 00:30:09.94
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:30:09.942
Jul 19 00:30:09.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename daemonsets 07/19/23 00:30:09.943
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:09.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:09.951
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 07/19/23 00:30:09.961
STEP: Check that daemon pods launch on every node of the cluster. 07/19/23 00:30:09.964
Jul 19 00:30:09.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:30:09.969: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:30:10.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:30:10.974: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:30:11.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 19 00:30:11.974: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 07/19/23 00:30:11.975
Jul 19 00:30:11.978: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 07/19/23 00:30:11.978
Jul 19 00:30:11.982: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 07/19/23 00:30:11.982
Jul 19 00:30:11.984: INFO: Observed &DaemonSet event: ADDED
Jul 19 00:30:11.984: INFO: Observed &DaemonSet event: MODIFIED
Jul 19 00:30:11.984: INFO: Observed &DaemonSet event: MODIFIED
Jul 19 00:30:11.984: INFO: Observed &DaemonSet event: MODIFIED
Jul 19 00:30:11.984: INFO: Found daemon set daemon-set in namespace daemonsets-4332 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 19 00:30:11.984: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 07/19/23 00:30:11.984
STEP: watching for the daemon set status to be patched 07/19/23 00:30:11.987
Jul 19 00:30:11.988: INFO: Observed &DaemonSet event: ADDED
Jul 19 00:30:11.989: INFO: Observed &DaemonSet event: MODIFIED
Jul 19 00:30:11.989: INFO: Observed &DaemonSet event: MODIFIED
Jul 19 00:30:11.989: INFO: Observed &DaemonSet event: MODIFIED
Jul 19 00:30:11.989: INFO: Observed daemon set daemon-set in namespace daemonsets-4332 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 19 00:30:11.989: INFO: Observed &DaemonSet event: MODIFIED
Jul 19 00:30:11.989: INFO: Found daemon set daemon-set in namespace daemonsets-4332 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jul 19 00:30:11.989: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:30:11.99
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4332, will wait for the garbage collector to delete the pods 07/19/23 00:30:11.99
Jul 19 00:30:12.047: INFO: Deleting DaemonSet.extensions daemon-set took: 2.483092ms
Jul 19 00:30:12.148: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.84943ms
Jul 19 00:30:14.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:30:14.750: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 19 00:30:14.752: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"64272"},"items":null}

Jul 19 00:30:14.753: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"64272"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:30:14.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4332" for this suite. 07/19/23 00:30:14.761
------------------------------
• [4.821 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:30:09.942
    Jul 19 00:30:09.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename daemonsets 07/19/23 00:30:09.943
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:09.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:09.951
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 07/19/23 00:30:09.961
    STEP: Check that daemon pods launch on every node of the cluster. 07/19/23 00:30:09.964
    Jul 19 00:30:09.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:30:09.969: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:30:10.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:30:10.974: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:30:11.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 19 00:30:11.974: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 07/19/23 00:30:11.975
    Jul 19 00:30:11.978: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 07/19/23 00:30:11.978
    Jul 19 00:30:11.982: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 07/19/23 00:30:11.982
    Jul 19 00:30:11.984: INFO: Observed &DaemonSet event: ADDED
    Jul 19 00:30:11.984: INFO: Observed &DaemonSet event: MODIFIED
    Jul 19 00:30:11.984: INFO: Observed &DaemonSet event: MODIFIED
    Jul 19 00:30:11.984: INFO: Observed &DaemonSet event: MODIFIED
    Jul 19 00:30:11.984: INFO: Found daemon set daemon-set in namespace daemonsets-4332 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jul 19 00:30:11.984: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 07/19/23 00:30:11.984
    STEP: watching for the daemon set status to be patched 07/19/23 00:30:11.987
    Jul 19 00:30:11.988: INFO: Observed &DaemonSet event: ADDED
    Jul 19 00:30:11.989: INFO: Observed &DaemonSet event: MODIFIED
    Jul 19 00:30:11.989: INFO: Observed &DaemonSet event: MODIFIED
    Jul 19 00:30:11.989: INFO: Observed &DaemonSet event: MODIFIED
    Jul 19 00:30:11.989: INFO: Observed daemon set daemon-set in namespace daemonsets-4332 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jul 19 00:30:11.989: INFO: Observed &DaemonSet event: MODIFIED
    Jul 19 00:30:11.989: INFO: Found daemon set daemon-set in namespace daemonsets-4332 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jul 19 00:30:11.989: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:30:11.99
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4332, will wait for the garbage collector to delete the pods 07/19/23 00:30:11.99
    Jul 19 00:30:12.047: INFO: Deleting DaemonSet.extensions daemon-set took: 2.483092ms
    Jul 19 00:30:12.148: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.84943ms
    Jul 19 00:30:14.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:30:14.750: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 19 00:30:14.752: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"64272"},"items":null}

    Jul 19 00:30:14.753: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"64272"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:30:14.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4332" for this suite. 07/19/23 00:30:14.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:30:14.765
Jul 19 00:30:14.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replication-controller 07/19/23 00:30:14.765
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:14.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:14.774
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 07/19/23 00:30:14.778
STEP: waiting for RC to be added 07/19/23 00:30:14.78
STEP: waiting for available Replicas 07/19/23 00:30:14.78
STEP: patching ReplicationController 07/19/23 00:30:16.008
STEP: waiting for RC to be modified 07/19/23 00:30:16.013
STEP: patching ReplicationController status 07/19/23 00:30:16.013
STEP: waiting for RC to be modified 07/19/23 00:30:16.016
STEP: waiting for available Replicas 07/19/23 00:30:16.016
STEP: fetching ReplicationController status 07/19/23 00:30:16.019
STEP: patching ReplicationController scale 07/19/23 00:30:16.021
STEP: waiting for RC to be modified 07/19/23 00:30:16.024
STEP: waiting for ReplicationController's scale to be the max amount 07/19/23 00:30:16.024
STEP: fetching ReplicationController; ensuring that it's patched 07/19/23 00:30:17.669
STEP: updating ReplicationController status 07/19/23 00:30:17.671
STEP: waiting for RC to be modified 07/19/23 00:30:17.674
STEP: listing all ReplicationControllers 07/19/23 00:30:17.674
STEP: checking that ReplicationController has expected values 07/19/23 00:30:17.675
STEP: deleting ReplicationControllers by collection 07/19/23 00:30:17.675
STEP: waiting for ReplicationController to have a DELETED watchEvent 07/19/23 00:30:17.682
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jul 19 00:30:17.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2501" for this suite. 07/19/23 00:30:17.716
------------------------------
• [2.954 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:30:14.765
    Jul 19 00:30:14.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replication-controller 07/19/23 00:30:14.765
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:14.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:14.774
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 07/19/23 00:30:14.778
    STEP: waiting for RC to be added 07/19/23 00:30:14.78
    STEP: waiting for available Replicas 07/19/23 00:30:14.78
    STEP: patching ReplicationController 07/19/23 00:30:16.008
    STEP: waiting for RC to be modified 07/19/23 00:30:16.013
    STEP: patching ReplicationController status 07/19/23 00:30:16.013
    STEP: waiting for RC to be modified 07/19/23 00:30:16.016
    STEP: waiting for available Replicas 07/19/23 00:30:16.016
    STEP: fetching ReplicationController status 07/19/23 00:30:16.019
    STEP: patching ReplicationController scale 07/19/23 00:30:16.021
    STEP: waiting for RC to be modified 07/19/23 00:30:16.024
    STEP: waiting for ReplicationController's scale to be the max amount 07/19/23 00:30:16.024
    STEP: fetching ReplicationController; ensuring that it's patched 07/19/23 00:30:17.669
    STEP: updating ReplicationController status 07/19/23 00:30:17.671
    STEP: waiting for RC to be modified 07/19/23 00:30:17.674
    STEP: listing all ReplicationControllers 07/19/23 00:30:17.674
    STEP: checking that ReplicationController has expected values 07/19/23 00:30:17.675
    STEP: deleting ReplicationControllers by collection 07/19/23 00:30:17.675
    STEP: waiting for ReplicationController to have a DELETED watchEvent 07/19/23 00:30:17.682
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:30:17.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2501" for this suite. 07/19/23 00:30:17.716
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:30:17.719
Jul 19 00:30:17.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:30:17.719
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:17.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:17.727
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-7230 07/19/23 00:30:17.729
STEP: creating service affinity-clusterip in namespace services-7230 07/19/23 00:30:17.729
STEP: creating replication controller affinity-clusterip in namespace services-7230 07/19/23 00:30:17.737
I0719 00:30:17.741238      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7230, replica count: 3
I0719 00:30:20.792490      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 00:30:20.795: INFO: Creating new exec pod
Jul 19 00:30:20.801: INFO: Waiting up to 5m0s for pod "execpod-affinity5z96d" in namespace "services-7230" to be "running"
Jul 19 00:30:20.802: INFO: Pod "execpod-affinity5z96d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.377523ms
Jul 19 00:30:22.806: INFO: Pod "execpod-affinity5z96d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005086941s
Jul 19 00:30:22.806: INFO: Pod "execpod-affinity5z96d" satisfied condition "running"
Jul 19 00:30:23.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7230 exec execpod-affinity5z96d -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jul 19 00:30:23.931: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul 19 00:30:23.931: INFO: stdout: ""
Jul 19 00:30:23.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7230 exec execpod-affinity5z96d -- /bin/sh -x -c nc -v -z -w 2 10.99.148.89 80'
Jul 19 00:30:24.038: INFO: stderr: "+ nc -v -z -w 2 10.99.148.89 80\nConnection to 10.99.148.89 80 port [tcp/http] succeeded!\n"
Jul 19 00:30:24.038: INFO: stdout: ""
Jul 19 00:30:24.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7230 exec execpod-affinity5z96d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.148.89:80/ ; done'
Jul 19 00:30:24.192: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n"
Jul 19 00:30:24.192: INFO: stdout: "\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt"
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
Jul 19 00:30:24.192: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7230, will wait for the garbage collector to delete the pods 07/19/23 00:30:24.199
Jul 19 00:30:24.255: INFO: Deleting ReplicationController affinity-clusterip took: 2.681872ms
Jul 19 00:30:24.355: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.729785ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:30:26.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7230" for this suite. 07/19/23 00:30:26.169
------------------------------
• [SLOW TEST] [8.453 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:30:17.719
    Jul 19 00:30:17.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:30:17.719
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:17.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:17.727
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-7230 07/19/23 00:30:17.729
    STEP: creating service affinity-clusterip in namespace services-7230 07/19/23 00:30:17.729
    STEP: creating replication controller affinity-clusterip in namespace services-7230 07/19/23 00:30:17.737
    I0719 00:30:17.741238      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7230, replica count: 3
    I0719 00:30:20.792490      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 19 00:30:20.795: INFO: Creating new exec pod
    Jul 19 00:30:20.801: INFO: Waiting up to 5m0s for pod "execpod-affinity5z96d" in namespace "services-7230" to be "running"
    Jul 19 00:30:20.802: INFO: Pod "execpod-affinity5z96d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.377523ms
    Jul 19 00:30:22.806: INFO: Pod "execpod-affinity5z96d": Phase="Running", Reason="", readiness=true. Elapsed: 2.005086941s
    Jul 19 00:30:22.806: INFO: Pod "execpod-affinity5z96d" satisfied condition "running"
    Jul 19 00:30:23.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7230 exec execpod-affinity5z96d -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jul 19 00:30:23.931: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jul 19 00:30:23.931: INFO: stdout: ""
    Jul 19 00:30:23.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7230 exec execpod-affinity5z96d -- /bin/sh -x -c nc -v -z -w 2 10.99.148.89 80'
    Jul 19 00:30:24.038: INFO: stderr: "+ nc -v -z -w 2 10.99.148.89 80\nConnection to 10.99.148.89 80 port [tcp/http] succeeded!\n"
    Jul 19 00:30:24.038: INFO: stdout: ""
    Jul 19 00:30:24.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-7230 exec execpod-affinity5z96d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.148.89:80/ ; done'
    Jul 19 00:30:24.192: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.148.89:80/\n"
    Jul 19 00:30:24.192: INFO: stdout: "\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt\naffinity-clusterip-6q9nt"
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Received response from host: affinity-clusterip-6q9nt
    Jul 19 00:30:24.192: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-7230, will wait for the garbage collector to delete the pods 07/19/23 00:30:24.199
    Jul 19 00:30:24.255: INFO: Deleting ReplicationController affinity-clusterip took: 2.681872ms
    Jul 19 00:30:24.355: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.729785ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:30:26.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7230" for this suite. 07/19/23 00:30:26.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:30:26.172
Jul 19 00:30:26.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:30:26.173
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:26.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:26.181
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 07/19/23 00:30:26.182
Jul 19 00:30:26.182: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul 19 00:30:26.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
Jul 19 00:30:26.392: INFO: stderr: ""
Jul 19 00:30:26.392: INFO: stdout: "service/agnhost-replica created\n"
Jul 19 00:30:26.392: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul 19 00:30:26.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
Jul 19 00:30:27.071: INFO: stderr: ""
Jul 19 00:30:27.071: INFO: stdout: "service/agnhost-primary created\n"
Jul 19 00:30:27.071: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 19 00:30:27.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
Jul 19 00:30:27.284: INFO: stderr: ""
Jul 19 00:30:27.284: INFO: stdout: "service/frontend created\n"
Jul 19 00:30:27.284: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 19 00:30:27.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
Jul 19 00:30:27.495: INFO: stderr: ""
Jul 19 00:30:27.495: INFO: stdout: "deployment.apps/frontend created\n"
Jul 19 00:30:27.495: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 19 00:30:27.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
Jul 19 00:30:27.703: INFO: stderr: ""
Jul 19 00:30:27.703: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul 19 00:30:27.703: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 19 00:30:27.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
Jul 19 00:30:27.911: INFO: stderr: ""
Jul 19 00:30:27.911: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 07/19/23 00:30:27.911
Jul 19 00:30:27.912: INFO: Waiting for all frontend pods to be Running.
Jul 19 00:30:32.962: INFO: Waiting for frontend to serve content.
Jul 19 00:30:32.967: INFO: Trying to add a new entry to the guestbook.
Jul 19 00:30:32.973: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 07/19/23 00:30:32.977
Jul 19 00:30:32.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
Jul 19 00:30:33.045: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 00:30:33.045: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 07/19/23 00:30:33.045
Jul 19 00:30:33.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
Jul 19 00:30:33.114: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 00:30:33.114: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 07/19/23 00:30:33.114
Jul 19 00:30:33.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
Jul 19 00:30:33.182: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 00:30:33.182: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 07/19/23 00:30:33.182
Jul 19 00:30:33.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
Jul 19 00:30:33.243: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 00:30:33.243: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 07/19/23 00:30:33.243
Jul 19 00:30:33.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
Jul 19 00:30:33.309: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 00:30:33.309: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 07/19/23 00:30:33.309
Jul 19 00:30:33.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
Jul 19 00:30:33.372: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 00:30:33.372: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:30:33.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4128" for this suite. 07/19/23 00:30:33.376
------------------------------
• [SLOW TEST] [7.214 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:30:26.172
    Jul 19 00:30:26.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:30:26.173
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:26.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:26.181
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 07/19/23 00:30:26.182
    Jul 19 00:30:26.182: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jul 19 00:30:26.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
    Jul 19 00:30:26.392: INFO: stderr: ""
    Jul 19 00:30:26.392: INFO: stdout: "service/agnhost-replica created\n"
    Jul 19 00:30:26.392: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jul 19 00:30:26.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
    Jul 19 00:30:27.071: INFO: stderr: ""
    Jul 19 00:30:27.071: INFO: stdout: "service/agnhost-primary created\n"
    Jul 19 00:30:27.071: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jul 19 00:30:27.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
    Jul 19 00:30:27.284: INFO: stderr: ""
    Jul 19 00:30:27.284: INFO: stdout: "service/frontend created\n"
    Jul 19 00:30:27.284: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jul 19 00:30:27.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
    Jul 19 00:30:27.495: INFO: stderr: ""
    Jul 19 00:30:27.495: INFO: stdout: "deployment.apps/frontend created\n"
    Jul 19 00:30:27.495: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jul 19 00:30:27.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
    Jul 19 00:30:27.703: INFO: stderr: ""
    Jul 19 00:30:27.703: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jul 19 00:30:27.703: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jul 19 00:30:27.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 create -f -'
    Jul 19 00:30:27.911: INFO: stderr: ""
    Jul 19 00:30:27.911: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 07/19/23 00:30:27.911
    Jul 19 00:30:27.912: INFO: Waiting for all frontend pods to be Running.
    Jul 19 00:30:32.962: INFO: Waiting for frontend to serve content.
    Jul 19 00:30:32.967: INFO: Trying to add a new entry to the guestbook.
    Jul 19 00:30:32.973: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 07/19/23 00:30:32.977
    Jul 19 00:30:32.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
    Jul 19 00:30:33.045: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 19 00:30:33.045: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 07/19/23 00:30:33.045
    Jul 19 00:30:33.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
    Jul 19 00:30:33.114: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 19 00:30:33.114: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 07/19/23 00:30:33.114
    Jul 19 00:30:33.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
    Jul 19 00:30:33.182: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 19 00:30:33.182: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 07/19/23 00:30:33.182
    Jul 19 00:30:33.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
    Jul 19 00:30:33.243: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 19 00:30:33.243: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 07/19/23 00:30:33.243
    Jul 19 00:30:33.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
    Jul 19 00:30:33.309: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 19 00:30:33.309: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 07/19/23 00:30:33.309
    Jul 19 00:30:33.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-4128 delete --grace-period=0 --force -f -'
    Jul 19 00:30:33.372: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 19 00:30:33.372: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:30:33.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4128" for this suite. 07/19/23 00:30:33.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:30:33.388
Jul 19 00:30:33.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:30:33.389
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:33.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:33.402
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 07/19/23 00:30:33.404
STEP: watching for the ServiceAccount to be added 07/19/23 00:30:33.408
STEP: patching the ServiceAccount 07/19/23 00:30:33.409
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 07/19/23 00:30:33.412
STEP: deleting the ServiceAccount 07/19/23 00:30:33.416
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jul 19 00:30:33.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4041" for this suite. 07/19/23 00:30:33.425
------------------------------
• [0.040 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:30:33.388
    Jul 19 00:30:33.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:30:33.389
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:33.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:33.402
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 07/19/23 00:30:33.404
    STEP: watching for the ServiceAccount to be added 07/19/23 00:30:33.408
    STEP: patching the ServiceAccount 07/19/23 00:30:33.409
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 07/19/23 00:30:33.412
    STEP: deleting the ServiceAccount 07/19/23 00:30:33.416
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:30:33.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4041" for this suite. 07/19/23 00:30:33.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:30:33.429
Jul 19 00:30:33.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 00:30:33.43
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:33.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:33.439
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:30:33.441
Jul 19 00:30:33.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f" in namespace "downward-api-1753" to be "Succeeded or Failed"
Jul 19 00:30:33.447: INFO: Pod "downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.565419ms
Jul 19 00:30:35.449: INFO: Pod "downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00362382s
Jul 19 00:30:37.450: INFO: Pod "downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004723617s
STEP: Saw pod success 07/19/23 00:30:37.45
Jul 19 00:30:37.450: INFO: Pod "downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f" satisfied condition "Succeeded or Failed"
Jul 19 00:30:37.452: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f container client-container: <nil>
STEP: delete the pod 07/19/23 00:30:37.455
Jul 19 00:30:37.462: INFO: Waiting for pod downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f to disappear
Jul 19 00:30:37.464: INFO: Pod downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 19 00:30:37.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1753" for this suite. 07/19/23 00:30:37.466
------------------------------
• [4.039 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:30:33.429
    Jul 19 00:30:33.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 00:30:33.43
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:33.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:33.439
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:30:33.441
    Jul 19 00:30:33.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f" in namespace "downward-api-1753" to be "Succeeded or Failed"
    Jul 19 00:30:33.447: INFO: Pod "downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.565419ms
    Jul 19 00:30:35.449: INFO: Pod "downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00362382s
    Jul 19 00:30:37.450: INFO: Pod "downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004723617s
    STEP: Saw pod success 07/19/23 00:30:37.45
    Jul 19 00:30:37.450: INFO: Pod "downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f" satisfied condition "Succeeded or Failed"
    Jul 19 00:30:37.452: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f container client-container: <nil>
    STEP: delete the pod 07/19/23 00:30:37.455
    Jul 19 00:30:37.462: INFO: Waiting for pod downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f to disappear
    Jul 19 00:30:37.464: INFO: Pod downwardapi-volume-5a93aac0-984e-4b78-b0d9-094d1324259f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:30:37.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1753" for this suite. 07/19/23 00:30:37.466
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:30:37.469
Jul 19 00:30:37.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:30:37.469
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:37.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:37.478
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:30:37.479
Jul 19 00:30:37.483: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27" in namespace "projected-4580" to be "Succeeded or Failed"
Jul 19 00:30:37.485: INFO: Pod "downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472327ms
Jul 19 00:30:39.487: INFO: Pod "downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003570812s
Jul 19 00:30:41.488: INFO: Pod "downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004212186s
STEP: Saw pod success 07/19/23 00:30:41.488
Jul 19 00:30:41.488: INFO: Pod "downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27" satisfied condition "Succeeded or Failed"
Jul 19 00:30:41.491: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27 container client-container: <nil>
STEP: delete the pod 07/19/23 00:30:41.494
Jul 19 00:30:41.502: INFO: Waiting for pod downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27 to disappear
Jul 19 00:30:41.504: INFO: Pod downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 19 00:30:41.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4580" for this suite. 07/19/23 00:30:41.506
------------------------------
• [4.040 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:30:37.469
    Jul 19 00:30:37.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:30:37.469
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:37.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:37.478
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:30:37.479
    Jul 19 00:30:37.483: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27" in namespace "projected-4580" to be "Succeeded or Failed"
    Jul 19 00:30:37.485: INFO: Pod "downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472327ms
    Jul 19 00:30:39.487: INFO: Pod "downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003570812s
    Jul 19 00:30:41.488: INFO: Pod "downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004212186s
    STEP: Saw pod success 07/19/23 00:30:41.488
    Jul 19 00:30:41.488: INFO: Pod "downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27" satisfied condition "Succeeded or Failed"
    Jul 19 00:30:41.491: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27 container client-container: <nil>
    STEP: delete the pod 07/19/23 00:30:41.494
    Jul 19 00:30:41.502: INFO: Waiting for pod downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27 to disappear
    Jul 19 00:30:41.504: INFO: Pod downwardapi-volume-80e6038a-d751-43e0-9d5e-e8108d58bd27 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:30:41.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4580" for this suite. 07/19/23 00:30:41.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:30:41.509
Jul 19 00:30:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:30:41.51
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:41.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:41.519
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:30:41.527
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:30:42.055
STEP: Deploying the webhook pod 07/19/23 00:30:42.058
STEP: Wait for the deployment to be ready 07/19/23 00:30:42.064
Jul 19 00:30:42.068: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:30:44.074
STEP: Verifying the service has paired with the endpoint 07/19/23 00:30:44.082
Jul 19 00:30:45.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 07/19/23 00:30:45.084
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 07/19/23 00:30:45.085
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 07/19/23 00:30:45.085
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 07/19/23 00:30:45.085
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 07/19/23 00:30:45.086
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 07/19/23 00:30:45.086
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 07/19/23 00:30:45.087
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:30:45.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4739" for this suite. 07/19/23 00:30:45.112
STEP: Destroying namespace "webhook-4739-markers" for this suite. 07/19/23 00:30:45.115
------------------------------
• [3.612 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:30:41.509
    Jul 19 00:30:41.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:30:41.51
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:41.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:41.519
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:30:41.527
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:30:42.055
    STEP: Deploying the webhook pod 07/19/23 00:30:42.058
    STEP: Wait for the deployment to be ready 07/19/23 00:30:42.064
    Jul 19 00:30:42.068: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:30:44.074
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:30:44.082
    Jul 19 00:30:45.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 07/19/23 00:30:45.084
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 07/19/23 00:30:45.085
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 07/19/23 00:30:45.085
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 07/19/23 00:30:45.085
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 07/19/23 00:30:45.086
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 07/19/23 00:30:45.086
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 07/19/23 00:30:45.087
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:30:45.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4739" for this suite. 07/19/23 00:30:45.112
    STEP: Destroying namespace "webhook-4739-markers" for this suite. 07/19/23 00:30:45.115
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:30:45.121
Jul 19 00:30:45.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-probe 07/19/23 00:30:45.122
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:45.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:45.133
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jul 19 00:31:45.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8263" for this suite. 07/19/23 00:31:45.144
------------------------------
• [SLOW TEST] [60.025 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:30:45.121
    Jul 19 00:30:45.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-probe 07/19/23 00:30:45.122
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:30:45.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:30:45.133
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:31:45.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8263" for this suite. 07/19/23 00:31:45.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:31:45.147
Jul 19 00:31:45.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename hostport 07/19/23 00:31:45.148
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:31:45.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:31:45.157
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 07/19/23 00:31:45.16
Jul 19 00:31:45.164: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2759" to be "running and ready"
Jul 19 00:31:45.166: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.308885ms
Jul 19 00:31:45.166: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:31:47.168: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003730803s
Jul 19 00:31:47.168: INFO: The phase of Pod pod1 is Running (Ready = true)
Jul 19 00:31:47.168: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.206.2 on the node which pod1 resides and expect scheduled 07/19/23 00:31:47.168
Jul 19 00:31:47.171: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2759" to be "running and ready"
Jul 19 00:31:47.172: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.488009ms
Jul 19 00:31:47.172: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:31:49.175: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004677929s
Jul 19 00:31:49.175: INFO: The phase of Pod pod2 is Running (Ready = true)
Jul 19 00:31:49.175: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.206.2 but use UDP protocol on the node which pod2 resides 07/19/23 00:31:49.175
Jul 19 00:31:49.178: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2759" to be "running and ready"
Jul 19 00:31:49.182: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.963998ms
Jul 19 00:31:49.182: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:31:51.184: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006042076s
Jul 19 00:31:51.184: INFO: The phase of Pod pod3 is Running (Ready = true)
Jul 19 00:31:51.184: INFO: Pod "pod3" satisfied condition "running and ready"
Jul 19 00:31:51.187: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2759" to be "running and ready"
Jul 19 00:31:51.189: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.571491ms
Jul 19 00:31:51.189: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:31:53.192: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005592753s
Jul 19 00:31:53.192: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jul 19 00:31:53.192: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 07/19/23 00:31:53.194
Jul 19 00:31:53.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.206.2 http://127.0.0.1:54323/hostname] Namespace:hostport-2759 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:31:53.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:31:53.194: INFO: ExecWithOptions: Clientset creation
Jul 19 00:31:53.194: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2759/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.206.2+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.206.2, port: 54323 07/19/23 00:31:53.253
Jul 19 00:31:53.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.206.2:54323/hostname] Namespace:hostport-2759 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:31:53.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:31:53.253: INFO: ExecWithOptions: Clientset creation
Jul 19 00:31:53.253: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2759/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.206.2%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.206.2, port: 54323 UDP 07/19/23 00:31:53.292
Jul 19 00:31:53.292: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.206.2 54323] Namespace:hostport-2759 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:31:53.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:31:53.293: INFO: ExecWithOptions: Clientset creation
Jul 19 00:31:53.293: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2759/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.206.2+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jul 19 00:31:58.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-2759" for this suite. 07/19/23 00:31:58.344
------------------------------
• [SLOW TEST] [13.199 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:31:45.147
    Jul 19 00:31:45.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename hostport 07/19/23 00:31:45.148
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:31:45.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:31:45.157
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 07/19/23 00:31:45.16
    Jul 19 00:31:45.164: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2759" to be "running and ready"
    Jul 19 00:31:45.166: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.308885ms
    Jul 19 00:31:45.166: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:31:47.168: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003730803s
    Jul 19 00:31:47.168: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jul 19 00:31:47.168: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.206.2 on the node which pod1 resides and expect scheduled 07/19/23 00:31:47.168
    Jul 19 00:31:47.171: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2759" to be "running and ready"
    Jul 19 00:31:47.172: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.488009ms
    Jul 19 00:31:47.172: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:31:49.175: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004677929s
    Jul 19 00:31:49.175: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jul 19 00:31:49.175: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.206.2 but use UDP protocol on the node which pod2 resides 07/19/23 00:31:49.175
    Jul 19 00:31:49.178: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2759" to be "running and ready"
    Jul 19 00:31:49.182: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.963998ms
    Jul 19 00:31:49.182: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:31:51.184: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006042076s
    Jul 19 00:31:51.184: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jul 19 00:31:51.184: INFO: Pod "pod3" satisfied condition "running and ready"
    Jul 19 00:31:51.187: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2759" to be "running and ready"
    Jul 19 00:31:51.189: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.571491ms
    Jul 19 00:31:51.189: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:31:53.192: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005592753s
    Jul 19 00:31:53.192: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jul 19 00:31:53.192: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 07/19/23 00:31:53.194
    Jul 19 00:31:53.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.206.2 http://127.0.0.1:54323/hostname] Namespace:hostport-2759 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:31:53.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:31:53.194: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:31:53.194: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2759/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.206.2+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.206.2, port: 54323 07/19/23 00:31:53.253
    Jul 19 00:31:53.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.206.2:54323/hostname] Namespace:hostport-2759 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:31:53.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:31:53.253: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:31:53.253: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2759/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.206.2%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.206.2, port: 54323 UDP 07/19/23 00:31:53.292
    Jul 19 00:31:53.292: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.206.2 54323] Namespace:hostport-2759 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:31:53.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:31:53.293: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:31:53.293: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2759/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.206.2+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:31:58.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-2759" for this suite. 07/19/23 00:31:58.344
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:31:58.347
Jul 19 00:31:58.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 00:31:58.348
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:31:58.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:31:58.357
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-b66ba600-fce9-420b-ae3a-de02b0f55bff 07/19/23 00:31:58.359
STEP: Creating a pod to test consume secrets 07/19/23 00:31:58.361
Jul 19 00:31:58.365: INFO: Waiting up to 5m0s for pod "pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc" in namespace "secrets-4546" to be "Succeeded or Failed"
Jul 19 00:31:58.367: INFO: Pod "pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815845ms
Jul 19 00:32:00.370: INFO: Pod "pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005750434s
Jul 19 00:32:02.371: INFO: Pod "pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006738256s
STEP: Saw pod success 07/19/23 00:32:02.371
Jul 19 00:32:02.371: INFO: Pod "pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc" satisfied condition "Succeeded or Failed"
Jul 19 00:32:02.373: INFO: Trying to get logs from node controller-1 pod pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc container secret-env-test: <nil>
STEP: delete the pod 07/19/23 00:32:02.378
Jul 19 00:32:02.385: INFO: Waiting for pod pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc to disappear
Jul 19 00:32:02.386: INFO: Pod pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 00:32:02.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4546" for this suite. 07/19/23 00:32:02.388
------------------------------
• [4.044 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:31:58.347
    Jul 19 00:31:58.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 00:31:58.348
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:31:58.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:31:58.357
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-b66ba600-fce9-420b-ae3a-de02b0f55bff 07/19/23 00:31:58.359
    STEP: Creating a pod to test consume secrets 07/19/23 00:31:58.361
    Jul 19 00:31:58.365: INFO: Waiting up to 5m0s for pod "pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc" in namespace "secrets-4546" to be "Succeeded or Failed"
    Jul 19 00:31:58.367: INFO: Pod "pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.815845ms
    Jul 19 00:32:00.370: INFO: Pod "pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005750434s
    Jul 19 00:32:02.371: INFO: Pod "pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006738256s
    STEP: Saw pod success 07/19/23 00:32:02.371
    Jul 19 00:32:02.371: INFO: Pod "pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc" satisfied condition "Succeeded or Failed"
    Jul 19 00:32:02.373: INFO: Trying to get logs from node controller-1 pod pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc container secret-env-test: <nil>
    STEP: delete the pod 07/19/23 00:32:02.378
    Jul 19 00:32:02.385: INFO: Waiting for pod pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc to disappear
    Jul 19 00:32:02.386: INFO: Pod pod-secrets-337bc83b-cc08-4b5c-a868-6067f6f322cc no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:32:02.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4546" for this suite. 07/19/23 00:32:02.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:32:02.391
Jul 19 00:32:02.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replication-controller 07/19/23 00:32:02.392
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:32:02.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:32:02.4
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58 07/19/23 00:32:02.402
Jul 19 00:32:02.406: INFO: Pod name my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58: Found 0 pods out of 1
Jul 19 00:32:07.408: INFO: Pod name my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58: Found 1 pods out of 1
Jul 19 00:32:07.408: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58" are running
Jul 19 00:32:07.408: INFO: Waiting up to 5m0s for pod "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk" in namespace "replication-controller-5795" to be "running"
Jul 19 00:32:07.409: INFO: Pod "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk": Phase="Running", Reason="", readiness=true. Elapsed: 1.42899ms
Jul 19 00:32:07.409: INFO: Pod "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk" satisfied condition "running"
Jul 19 00:32:07.409: INFO: Pod "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-19 00:32:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-19 00:32:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-19 00:32:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-19 00:32:02 +0000 UTC Reason: Message:}])
Jul 19 00:32:07.410: INFO: Trying to dial the pod
Jul 19 00:32:12.417: INFO: Controller my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58: Got expected result from replica 1 [my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk]: "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jul 19 00:32:12.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5795" for this suite. 07/19/23 00:32:12.419
------------------------------
• [SLOW TEST] [10.030 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:32:02.391
    Jul 19 00:32:02.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replication-controller 07/19/23 00:32:02.392
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:32:02.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:32:02.4
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58 07/19/23 00:32:02.402
    Jul 19 00:32:02.406: INFO: Pod name my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58: Found 0 pods out of 1
    Jul 19 00:32:07.408: INFO: Pod name my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58: Found 1 pods out of 1
    Jul 19 00:32:07.408: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58" are running
    Jul 19 00:32:07.408: INFO: Waiting up to 5m0s for pod "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk" in namespace "replication-controller-5795" to be "running"
    Jul 19 00:32:07.409: INFO: Pod "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk": Phase="Running", Reason="", readiness=true. Elapsed: 1.42899ms
    Jul 19 00:32:07.409: INFO: Pod "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk" satisfied condition "running"
    Jul 19 00:32:07.409: INFO: Pod "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-19 00:32:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-19 00:32:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-19 00:32:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-19 00:32:02 +0000 UTC Reason: Message:}])
    Jul 19 00:32:07.410: INFO: Trying to dial the pod
    Jul 19 00:32:12.417: INFO: Controller my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58: Got expected result from replica 1 [my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk]: "my-hostname-basic-47f3cefe-603d-4e08-9e1e-7d0b6d0aba58-zl8vk", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:32:12.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5795" for this suite. 07/19/23 00:32:12.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:32:12.422
Jul 19 00:32:12.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename prestop 07/19/23 00:32:12.423
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:32:12.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:32:12.435
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9514 07/19/23 00:32:12.437
STEP: Waiting for pods to come up. 07/19/23 00:32:12.441
Jul 19 00:32:12.441: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9514" to be "running"
Jul 19 00:32:12.443: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.41019ms
Jul 19 00:32:14.446: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.004516225s
Jul 19 00:32:14.446: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9514 07/19/23 00:32:14.447
Jul 19 00:32:14.450: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9514" to be "running"
Jul 19 00:32:14.451: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.669331ms
Jul 19 00:32:16.454: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.004409535s
Jul 19 00:32:16.454: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 07/19/23 00:32:16.454
Jul 19 00:32:21.461: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 07/19/23 00:32:21.461
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jul 19 00:32:21.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-9514" for this suite. 07/19/23 00:32:21.47
------------------------------
• [SLOW TEST] [9.051 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:32:12.422
    Jul 19 00:32:12.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename prestop 07/19/23 00:32:12.423
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:32:12.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:32:12.435
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9514 07/19/23 00:32:12.437
    STEP: Waiting for pods to come up. 07/19/23 00:32:12.441
    Jul 19 00:32:12.441: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9514" to be "running"
    Jul 19 00:32:12.443: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.41019ms
    Jul 19 00:32:14.446: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.004516225s
    Jul 19 00:32:14.446: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9514 07/19/23 00:32:14.447
    Jul 19 00:32:14.450: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9514" to be "running"
    Jul 19 00:32:14.451: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.669331ms
    Jul 19 00:32:16.454: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.004409535s
    Jul 19 00:32:16.454: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 07/19/23 00:32:16.454
    Jul 19 00:32:21.461: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 07/19/23 00:32:21.461
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:32:21.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-9514" for this suite. 07/19/23 00:32:21.47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:32:21.473
Jul 19 00:32:21.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir-wrapper 07/19/23 00:32:21.474
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:32:21.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:32:21.49
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 07/19/23 00:32:21.492
STEP: Creating RC which spawns configmap-volume pods 07/19/23 00:32:21.727
Jul 19 00:32:21.832: INFO: Pod name wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b: Found 5 pods out of 5
STEP: Ensuring each pod is running 07/19/23 00:32:21.832
Jul 19 00:32:21.832: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:32:21.877: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 45.440367ms
Jul 19 00:32:23.881: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048614386s
Jul 19 00:32:25.880: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047745655s
Jul 19 00:32:27.882: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049579564s
Jul 19 00:32:29.881: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048535344s
Jul 19 00:32:31.880: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047784549s
Jul 19 00:32:33.881: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 12.049350427s
Jul 19 00:32:35.880: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Running", Reason="", readiness=true. Elapsed: 14.047949878s
Jul 19 00:32:35.880: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55" satisfied condition "running"
Jul 19 00:32:35.880: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-845vw" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:32:35.882: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-845vw": Phase="Running", Reason="", readiness=true. Elapsed: 1.683935ms
Jul 19 00:32:35.882: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-845vw" satisfied condition "running"
Jul 19 00:32:35.882: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-cxnp2" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:32:35.883: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-cxnp2": Phase="Running", Reason="", readiness=true. Elapsed: 1.707225ms
Jul 19 00:32:35.883: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-cxnp2" satisfied condition "running"
Jul 19 00:32:35.883: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-gh6kt" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:32:35.885: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-gh6kt": Phase="Running", Reason="", readiness=true. Elapsed: 1.84817ms
Jul 19 00:32:35.885: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-gh6kt" satisfied condition "running"
Jul 19 00:32:35.885: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-p8qjb" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:32:35.887: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-p8qjb": Phase="Running", Reason="", readiness=true. Elapsed: 1.687217ms
Jul 19 00:32:35.887: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-p8qjb" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b in namespace emptydir-wrapper-8305, will wait for the garbage collector to delete the pods 07/19/23 00:32:35.887
Jul 19 00:32:35.942: INFO: Deleting ReplicationController wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b took: 2.710885ms
Jul 19 00:32:36.043: INFO: Terminating ReplicationController wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b pods took: 100.933552ms
STEP: Creating RC which spawns configmap-volume pods 07/19/23 00:32:39.846
Jul 19 00:32:39.854: INFO: Pod name wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1: Found 0 pods out of 5
Jul 19 00:32:44.859: INFO: Pod name wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1: Found 5 pods out of 5
STEP: Ensuring each pod is running 07/19/23 00:32:44.859
Jul 19 00:32:44.859: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:32:44.861: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.706192ms
Jul 19 00:32:46.864: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00455201s
Jul 19 00:32:48.867: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007030396s
Jul 19 00:32:50.864: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004677109s
Jul 19 00:32:52.864: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004781148s
Jul 19 00:32:54.864: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Running", Reason="", readiness=true. Elapsed: 10.00462967s
Jul 19 00:32:54.864: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8" satisfied condition "running"
Jul 19 00:32:54.864: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-7b4qz" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:32:54.867: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-7b4qz": Phase="Running", Reason="", readiness=true. Elapsed: 2.350786ms
Jul 19 00:32:54.867: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-7b4qz" satisfied condition "running"
Jul 19 00:32:54.867: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-f879q" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:32:54.869: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-f879q": Phase="Running", Reason="", readiness=true. Elapsed: 1.97549ms
Jul 19 00:32:54.869: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-f879q" satisfied condition "running"
Jul 19 00:32:54.869: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-p7lhb" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:32:54.870: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-p7lhb": Phase="Running", Reason="", readiness=true. Elapsed: 1.640666ms
Jul 19 00:32:54.870: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-p7lhb" satisfied condition "running"
Jul 19 00:32:54.870: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-plznt" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:32:54.872: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-plznt": Phase="Running", Reason="", readiness=true. Elapsed: 1.814022ms
Jul 19 00:32:54.872: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-plznt" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1 in namespace emptydir-wrapper-8305, will wait for the garbage collector to delete the pods 07/19/23 00:32:54.872
Jul 19 00:32:54.928: INFO: Deleting ReplicationController wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1 took: 3.412036ms
Jul 19 00:32:55.029: INFO: Terminating ReplicationController wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1 pods took: 100.232885ms
STEP: Creating RC which spawns configmap-volume pods 07/19/23 00:32:58.932
Jul 19 00:32:58.942: INFO: Pod name wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d: Found 0 pods out of 5
Jul 19 00:33:03.948: INFO: Pod name wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d: Found 5 pods out of 5
STEP: Ensuring each pod is running 07/19/23 00:33:03.948
Jul 19 00:33:03.948: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:33:03.950: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Pending", Reason="", readiness=false. Elapsed: 1.967245ms
Jul 19 00:33:05.954: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005510484s
Jul 19 00:33:07.954: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005675235s
Jul 19 00:33:09.954: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005277336s
Jul 19 00:33:11.953: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004863173s
Jul 19 00:33:13.954: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Running", Reason="", readiness=true. Elapsed: 10.005412173s
Jul 19 00:33:13.954: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s" satisfied condition "running"
Jul 19 00:33:13.954: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-k24wx" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:33:13.956: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-k24wx": Phase="Running", Reason="", readiness=true. Elapsed: 2.042023ms
Jul 19 00:33:13.956: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-k24wx" satisfied condition "running"
Jul 19 00:33:13.956: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-mpq29" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:33:13.958: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-mpq29": Phase="Running", Reason="", readiness=true. Elapsed: 1.80729ms
Jul 19 00:33:13.958: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-mpq29" satisfied condition "running"
Jul 19 00:33:13.958: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-tq4pb" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:33:13.959: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-tq4pb": Phase="Running", Reason="", readiness=true. Elapsed: 1.674444ms
Jul 19 00:33:13.959: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-tq4pb" satisfied condition "running"
Jul 19 00:33:13.959: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-zd2lf" in namespace "emptydir-wrapper-8305" to be "running"
Jul 19 00:33:13.961: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-zd2lf": Phase="Running", Reason="", readiness=true. Elapsed: 1.609319ms
Jul 19 00:33:13.961: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-zd2lf" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d in namespace emptydir-wrapper-8305, will wait for the garbage collector to delete the pods 07/19/23 00:33:13.961
Jul 19 00:33:14.017: INFO: Deleting ReplicationController wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d took: 2.986224ms
Jul 19 00:33:14.118: INFO: Terminating ReplicationController wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d pods took: 100.886364ms
STEP: Cleaning up the configMaps 07/19/23 00:33:17.018
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:33:17.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8305" for this suite. 07/19/23 00:33:17.139
------------------------------
• [SLOW TEST] [55.668 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:32:21.473
    Jul 19 00:32:21.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir-wrapper 07/19/23 00:32:21.474
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:32:21.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:32:21.49
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 07/19/23 00:32:21.492
    STEP: Creating RC which spawns configmap-volume pods 07/19/23 00:32:21.727
    Jul 19 00:32:21.832: INFO: Pod name wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b: Found 5 pods out of 5
    STEP: Ensuring each pod is running 07/19/23 00:32:21.832
    Jul 19 00:32:21.832: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:32:21.877: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 45.440367ms
    Jul 19 00:32:23.881: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048614386s
    Jul 19 00:32:25.880: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047745655s
    Jul 19 00:32:27.882: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049579564s
    Jul 19 00:32:29.881: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048535344s
    Jul 19 00:32:31.880: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047784549s
    Jul 19 00:32:33.881: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Pending", Reason="", readiness=false. Elapsed: 12.049350427s
    Jul 19 00:32:35.880: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55": Phase="Running", Reason="", readiness=true. Elapsed: 14.047949878s
    Jul 19 00:32:35.880: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-56z55" satisfied condition "running"
    Jul 19 00:32:35.880: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-845vw" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:32:35.882: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-845vw": Phase="Running", Reason="", readiness=true. Elapsed: 1.683935ms
    Jul 19 00:32:35.882: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-845vw" satisfied condition "running"
    Jul 19 00:32:35.882: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-cxnp2" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:32:35.883: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-cxnp2": Phase="Running", Reason="", readiness=true. Elapsed: 1.707225ms
    Jul 19 00:32:35.883: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-cxnp2" satisfied condition "running"
    Jul 19 00:32:35.883: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-gh6kt" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:32:35.885: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-gh6kt": Phase="Running", Reason="", readiness=true. Elapsed: 1.84817ms
    Jul 19 00:32:35.885: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-gh6kt" satisfied condition "running"
    Jul 19 00:32:35.885: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-p8qjb" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:32:35.887: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-p8qjb": Phase="Running", Reason="", readiness=true. Elapsed: 1.687217ms
    Jul 19 00:32:35.887: INFO: Pod "wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b-p8qjb" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b in namespace emptydir-wrapper-8305, will wait for the garbage collector to delete the pods 07/19/23 00:32:35.887
    Jul 19 00:32:35.942: INFO: Deleting ReplicationController wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b took: 2.710885ms
    Jul 19 00:32:36.043: INFO: Terminating ReplicationController wrapped-volume-race-b06c9bf9-f888-441b-8d4b-32a4d739434b pods took: 100.933552ms
    STEP: Creating RC which spawns configmap-volume pods 07/19/23 00:32:39.846
    Jul 19 00:32:39.854: INFO: Pod name wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1: Found 0 pods out of 5
    Jul 19 00:32:44.859: INFO: Pod name wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1: Found 5 pods out of 5
    STEP: Ensuring each pod is running 07/19/23 00:32:44.859
    Jul 19 00:32:44.859: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:32:44.861: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.706192ms
    Jul 19 00:32:46.864: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00455201s
    Jul 19 00:32:48.867: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007030396s
    Jul 19 00:32:50.864: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004677109s
    Jul 19 00:32:52.864: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004781148s
    Jul 19 00:32:54.864: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8": Phase="Running", Reason="", readiness=true. Elapsed: 10.00462967s
    Jul 19 00:32:54.864: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-6h8f8" satisfied condition "running"
    Jul 19 00:32:54.864: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-7b4qz" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:32:54.867: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-7b4qz": Phase="Running", Reason="", readiness=true. Elapsed: 2.350786ms
    Jul 19 00:32:54.867: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-7b4qz" satisfied condition "running"
    Jul 19 00:32:54.867: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-f879q" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:32:54.869: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-f879q": Phase="Running", Reason="", readiness=true. Elapsed: 1.97549ms
    Jul 19 00:32:54.869: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-f879q" satisfied condition "running"
    Jul 19 00:32:54.869: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-p7lhb" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:32:54.870: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-p7lhb": Phase="Running", Reason="", readiness=true. Elapsed: 1.640666ms
    Jul 19 00:32:54.870: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-p7lhb" satisfied condition "running"
    Jul 19 00:32:54.870: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-plznt" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:32:54.872: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-plznt": Phase="Running", Reason="", readiness=true. Elapsed: 1.814022ms
    Jul 19 00:32:54.872: INFO: Pod "wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1-plznt" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1 in namespace emptydir-wrapper-8305, will wait for the garbage collector to delete the pods 07/19/23 00:32:54.872
    Jul 19 00:32:54.928: INFO: Deleting ReplicationController wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1 took: 3.412036ms
    Jul 19 00:32:55.029: INFO: Terminating ReplicationController wrapped-volume-race-706faf04-6ce5-43c6-b192-2816bdfa37d1 pods took: 100.232885ms
    STEP: Creating RC which spawns configmap-volume pods 07/19/23 00:32:58.932
    Jul 19 00:32:58.942: INFO: Pod name wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d: Found 0 pods out of 5
    Jul 19 00:33:03.948: INFO: Pod name wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d: Found 5 pods out of 5
    STEP: Ensuring each pod is running 07/19/23 00:33:03.948
    Jul 19 00:33:03.948: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:33:03.950: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Pending", Reason="", readiness=false. Elapsed: 1.967245ms
    Jul 19 00:33:05.954: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005510484s
    Jul 19 00:33:07.954: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005675235s
    Jul 19 00:33:09.954: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005277336s
    Jul 19 00:33:11.953: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004863173s
    Jul 19 00:33:13.954: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s": Phase="Running", Reason="", readiness=true. Elapsed: 10.005412173s
    Jul 19 00:33:13.954: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-fcv9s" satisfied condition "running"
    Jul 19 00:33:13.954: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-k24wx" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:33:13.956: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-k24wx": Phase="Running", Reason="", readiness=true. Elapsed: 2.042023ms
    Jul 19 00:33:13.956: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-k24wx" satisfied condition "running"
    Jul 19 00:33:13.956: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-mpq29" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:33:13.958: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-mpq29": Phase="Running", Reason="", readiness=true. Elapsed: 1.80729ms
    Jul 19 00:33:13.958: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-mpq29" satisfied condition "running"
    Jul 19 00:33:13.958: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-tq4pb" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:33:13.959: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-tq4pb": Phase="Running", Reason="", readiness=true. Elapsed: 1.674444ms
    Jul 19 00:33:13.959: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-tq4pb" satisfied condition "running"
    Jul 19 00:33:13.959: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-zd2lf" in namespace "emptydir-wrapper-8305" to be "running"
    Jul 19 00:33:13.961: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-zd2lf": Phase="Running", Reason="", readiness=true. Elapsed: 1.609319ms
    Jul 19 00:33:13.961: INFO: Pod "wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d-zd2lf" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d in namespace emptydir-wrapper-8305, will wait for the garbage collector to delete the pods 07/19/23 00:33:13.961
    Jul 19 00:33:14.017: INFO: Deleting ReplicationController wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d took: 2.986224ms
    Jul 19 00:33:14.118: INFO: Terminating ReplicationController wrapped-volume-race-ed67874f-bc84-4685-a333-91ec50418b4d pods took: 100.886364ms
    STEP: Cleaning up the configMaps 07/19/23 00:33:17.018
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:33:17.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8305" for this suite. 07/19/23 00:33:17.139
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:33:17.142
Jul 19 00:33:17.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename watch 07/19/23 00:33:17.142
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:17.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:17.154
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 07/19/23 00:33:17.155
STEP: creating a watch on configmaps with label B 07/19/23 00:33:17.156
STEP: creating a watch on configmaps with label A or B 07/19/23 00:33:17.157
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 07/19/23 00:33:17.158
Jul 19 00:33:17.160: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67042 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:33:17.160: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67042 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 07/19/23 00:33:17.16
Jul 19 00:33:17.164: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67043 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:33:17.164: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67043 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 07/19/23 00:33:17.164
Jul 19 00:33:17.167: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67044 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:33:17.167: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67044 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 07/19/23 00:33:17.167
Jul 19 00:33:17.171: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67045 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:33:17.171: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67045 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 07/19/23 00:33:17.171
Jul 19 00:33:17.173: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7818  b623bbc5-2f69-42fb-9cad-a4ea72a8f967 67046 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:33:17.173: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7818  b623bbc5-2f69-42fb-9cad-a4ea72a8f967 67046 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 07/19/23 00:33:27.174
Jul 19 00:33:27.177: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7818  b623bbc5-2f69-42fb-9cad-a4ea72a8f967 67329 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 19 00:33:27.177: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7818  b623bbc5-2f69-42fb-9cad-a4ea72a8f967 67329 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jul 19 00:33:37.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7818" for this suite. 07/19/23 00:33:37.181
------------------------------
• [SLOW TEST] [20.047 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:33:17.142
    Jul 19 00:33:17.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename watch 07/19/23 00:33:17.142
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:17.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:17.154
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 07/19/23 00:33:17.155
    STEP: creating a watch on configmaps with label B 07/19/23 00:33:17.156
    STEP: creating a watch on configmaps with label A or B 07/19/23 00:33:17.157
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 07/19/23 00:33:17.158
    Jul 19 00:33:17.160: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67042 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:33:17.160: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67042 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 07/19/23 00:33:17.16
    Jul 19 00:33:17.164: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67043 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:33:17.164: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67043 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 07/19/23 00:33:17.164
    Jul 19 00:33:17.167: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67044 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:33:17.167: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67044 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 07/19/23 00:33:17.167
    Jul 19 00:33:17.171: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67045 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:33:17.171: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7818  b948fef8-49ce-4a56-b5e1-7421b108a704 67045 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 07/19/23 00:33:17.171
    Jul 19 00:33:17.173: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7818  b623bbc5-2f69-42fb-9cad-a4ea72a8f967 67046 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:33:17.173: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7818  b623bbc5-2f69-42fb-9cad-a4ea72a8f967 67046 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 07/19/23 00:33:27.174
    Jul 19 00:33:27.177: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7818  b623bbc5-2f69-42fb-9cad-a4ea72a8f967 67329 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jul 19 00:33:27.177: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7818  b623bbc5-2f69-42fb-9cad-a4ea72a8f967 67329 0 2023-07-19 00:33:17 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-19 00:33:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:33:37.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7818" for this suite. 07/19/23 00:33:37.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:33:37.189
Jul 19 00:33:37.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename custom-resource-definition 07/19/23 00:33:37.19
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:37.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:37.2
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 07/19/23 00:33:37.202
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 07/19/23 00:33:37.203
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 07/19/23 00:33:37.203
STEP: fetching the /apis/apiextensions.k8s.io discovery document 07/19/23 00:33:37.203
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 07/19/23 00:33:37.203
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 07/19/23 00:33:37.203
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 07/19/23 00:33:37.204
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:33:37.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3832" for this suite. 07/19/23 00:33:37.206
------------------------------
• [0.020 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:33:37.189
    Jul 19 00:33:37.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename custom-resource-definition 07/19/23 00:33:37.19
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:37.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:37.2
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 07/19/23 00:33:37.202
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 07/19/23 00:33:37.203
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 07/19/23 00:33:37.203
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 07/19/23 00:33:37.203
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 07/19/23 00:33:37.203
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 07/19/23 00:33:37.203
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 07/19/23 00:33:37.204
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:33:37.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3832" for this suite. 07/19/23 00:33:37.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:33:37.209
Jul 19 00:33:37.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:33:37.21
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:37.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:37.217
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  07/19/23 00:33:37.219
Jul 19 00:33:37.222: INFO: Waiting up to 5m0s for pod "test-pod-7c3c303b-39e9-4938-8173-5261b279191a" in namespace "svcaccounts-2732" to be "Succeeded or Failed"
Jul 19 00:33:37.226: INFO: Pod "test-pod-7c3c303b-39e9-4938-8173-5261b279191a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.757885ms
Jul 19 00:33:39.229: INFO: Pod "test-pod-7c3c303b-39e9-4938-8173-5261b279191a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006207606s
Jul 19 00:33:41.229: INFO: Pod "test-pod-7c3c303b-39e9-4938-8173-5261b279191a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006347267s
STEP: Saw pod success 07/19/23 00:33:41.229
Jul 19 00:33:41.229: INFO: Pod "test-pod-7c3c303b-39e9-4938-8173-5261b279191a" satisfied condition "Succeeded or Failed"
Jul 19 00:33:41.230: INFO: Trying to get logs from node controller-1 pod test-pod-7c3c303b-39e9-4938-8173-5261b279191a container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:33:41.24
Jul 19 00:33:41.247: INFO: Waiting for pod test-pod-7c3c303b-39e9-4938-8173-5261b279191a to disappear
Jul 19 00:33:41.248: INFO: Pod test-pod-7c3c303b-39e9-4938-8173-5261b279191a no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jul 19 00:33:41.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2732" for this suite. 07/19/23 00:33:41.25
------------------------------
• [4.044 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:33:37.209
    Jul 19 00:33:37.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:33:37.21
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:37.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:37.217
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  07/19/23 00:33:37.219
    Jul 19 00:33:37.222: INFO: Waiting up to 5m0s for pod "test-pod-7c3c303b-39e9-4938-8173-5261b279191a" in namespace "svcaccounts-2732" to be "Succeeded or Failed"
    Jul 19 00:33:37.226: INFO: Pod "test-pod-7c3c303b-39e9-4938-8173-5261b279191a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.757885ms
    Jul 19 00:33:39.229: INFO: Pod "test-pod-7c3c303b-39e9-4938-8173-5261b279191a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006207606s
    Jul 19 00:33:41.229: INFO: Pod "test-pod-7c3c303b-39e9-4938-8173-5261b279191a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006347267s
    STEP: Saw pod success 07/19/23 00:33:41.229
    Jul 19 00:33:41.229: INFO: Pod "test-pod-7c3c303b-39e9-4938-8173-5261b279191a" satisfied condition "Succeeded or Failed"
    Jul 19 00:33:41.230: INFO: Trying to get logs from node controller-1 pod test-pod-7c3c303b-39e9-4938-8173-5261b279191a container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:33:41.24
    Jul 19 00:33:41.247: INFO: Waiting for pod test-pod-7c3c303b-39e9-4938-8173-5261b279191a to disappear
    Jul 19 00:33:41.248: INFO: Pod test-pod-7c3c303b-39e9-4938-8173-5261b279191a no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:33:41.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2732" for this suite. 07/19/23 00:33:41.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:33:41.257
Jul 19 00:33:41.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename security-context-test 07/19/23 00:33:41.258
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:41.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:41.269
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jul 19 00:33:41.274: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3" in namespace "security-context-test-7995" to be "Succeeded or Failed"
Jul 19 00:33:41.277: INFO: Pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.762344ms
Jul 19 00:33:43.280: INFO: Pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004994037s
Jul 19 00:33:45.280: INFO: Pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005000723s
Jul 19 00:33:45.280: INFO: Pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3" satisfied condition "Succeeded or Failed"
Jul 19 00:33:45.283: INFO: Got logs for pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jul 19 00:33:45.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7995" for this suite. 07/19/23 00:33:45.285
------------------------------
• [4.031 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:33:41.257
    Jul 19 00:33:41.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename security-context-test 07/19/23 00:33:41.258
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:41.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:41.269
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jul 19 00:33:41.274: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3" in namespace "security-context-test-7995" to be "Succeeded or Failed"
    Jul 19 00:33:41.277: INFO: Pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.762344ms
    Jul 19 00:33:43.280: INFO: Pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004994037s
    Jul 19 00:33:45.280: INFO: Pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005000723s
    Jul 19 00:33:45.280: INFO: Pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3" satisfied condition "Succeeded or Failed"
    Jul 19 00:33:45.283: INFO: Got logs for pod "busybox-privileged-false-44eed1e6-08e7-477e-9d1f-da58a72668f3": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:33:45.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7995" for this suite. 07/19/23 00:33:45.285
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:33:45.288
Jul 19 00:33:45.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename namespaces 07/19/23 00:33:45.289
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:45.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:45.299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 07/19/23 00:33:45.301
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:45.307
STEP: Creating a pod in the namespace 07/19/23 00:33:45.309
STEP: Waiting for the pod to have running status 07/19/23 00:33:45.312
Jul 19 00:33:45.312: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8448" to be "running"
Jul 19 00:33:45.313: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.225692ms
Jul 19 00:33:47.317: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004587141s
Jul 19 00:33:47.317: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 07/19/23 00:33:47.317
STEP: Waiting for the namespace to be removed. 07/19/23 00:33:47.319
STEP: Recreating the namespace 07/19/23 00:33:58.322
STEP: Verifying there are no pods in the namespace 07/19/23 00:33:58.329
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:33:58.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9271" for this suite. 07/19/23 00:33:58.334
STEP: Destroying namespace "nsdeletetest-8448" for this suite. 07/19/23 00:33:58.337
Jul 19 00:33:58.338: INFO: Namespace nsdeletetest-8448 was already deleted
STEP: Destroying namespace "nsdeletetest-993" for this suite. 07/19/23 00:33:58.338
------------------------------
• [SLOW TEST] [13.052 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:33:45.288
    Jul 19 00:33:45.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename namespaces 07/19/23 00:33:45.289
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:45.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:45.299
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 07/19/23 00:33:45.301
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:45.307
    STEP: Creating a pod in the namespace 07/19/23 00:33:45.309
    STEP: Waiting for the pod to have running status 07/19/23 00:33:45.312
    Jul 19 00:33:45.312: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8448" to be "running"
    Jul 19 00:33:45.313: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.225692ms
    Jul 19 00:33:47.317: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004587141s
    Jul 19 00:33:47.317: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 07/19/23 00:33:47.317
    STEP: Waiting for the namespace to be removed. 07/19/23 00:33:47.319
    STEP: Recreating the namespace 07/19/23 00:33:58.322
    STEP: Verifying there are no pods in the namespace 07/19/23 00:33:58.329
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:33:58.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9271" for this suite. 07/19/23 00:33:58.334
    STEP: Destroying namespace "nsdeletetest-8448" for this suite. 07/19/23 00:33:58.337
    Jul 19 00:33:58.338: INFO: Namespace nsdeletetest-8448 was already deleted
    STEP: Destroying namespace "nsdeletetest-993" for this suite. 07/19/23 00:33:58.338
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:33:58.34
Jul 19 00:33:58.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename statefulset 07/19/23 00:33:58.341
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:58.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:58.35
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7294 07/19/23 00:33:58.352
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 07/19/23 00:33:58.354
Jul 19 00:33:58.359: INFO: Found 0 stateful pods, waiting for 3
Jul 19 00:34:08.363: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 00:34:08.363: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 00:34:08.363: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 00:34:08.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-7294 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 00:34:08.472: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 00:34:08.472: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 00:34:08.472: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 07/19/23 00:34:18.48
Jul 19 00:34:18.495: INFO: Updating stateful set ss2
STEP: Creating a new revision 07/19/23 00:34:18.495
STEP: Updating Pods in reverse ordinal order 07/19/23 00:34:28.504
Jul 19 00:34:28.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-7294 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 00:34:28.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 00:34:28.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 00:34:28.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 07/19/23 00:34:38.622
Jul 19 00:34:38.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-7294 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 00:34:38.736: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 00:34:38.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 00:34:38.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 00:34:48.763: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 07/19/23 00:34:58.771
Jul 19 00:34:58.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-7294 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 00:34:58.885: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 00:34:58.885: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 00:34:58.885: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jul 19 00:35:08.899: INFO: Deleting all statefulset in ns statefulset-7294
Jul 19 00:35:08.900: INFO: Scaling statefulset ss2 to 0
Jul 19 00:35:18.910: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 00:35:18.911: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:35:18.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7294" for this suite. 07/19/23 00:35:18.919
------------------------------
• [SLOW TEST] [80.581 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:33:58.34
    Jul 19 00:33:58.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename statefulset 07/19/23 00:33:58.341
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:33:58.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:33:58.35
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7294 07/19/23 00:33:58.352
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 07/19/23 00:33:58.354
    Jul 19 00:33:58.359: INFO: Found 0 stateful pods, waiting for 3
    Jul 19 00:34:08.363: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 00:34:08.363: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 00:34:08.363: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 00:34:08.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-7294 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 19 00:34:08.472: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 19 00:34:08.472: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 19 00:34:08.472: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 07/19/23 00:34:18.48
    Jul 19 00:34:18.495: INFO: Updating stateful set ss2
    STEP: Creating a new revision 07/19/23 00:34:18.495
    STEP: Updating Pods in reverse ordinal order 07/19/23 00:34:28.504
    Jul 19 00:34:28.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-7294 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 19 00:34:28.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 19 00:34:28.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 19 00:34:28.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 07/19/23 00:34:38.622
    Jul 19 00:34:38.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-7294 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 19 00:34:38.736: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 19 00:34:38.736: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 19 00:34:38.736: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 19 00:34:48.763: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 07/19/23 00:34:58.771
    Jul 19 00:34:58.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-7294 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 19 00:34:58.885: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 19 00:34:58.885: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 19 00:34:58.885: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jul 19 00:35:08.899: INFO: Deleting all statefulset in ns statefulset-7294
    Jul 19 00:35:08.900: INFO: Scaling statefulset ss2 to 0
    Jul 19 00:35:18.910: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 19 00:35:18.911: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:35:18.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7294" for this suite. 07/19/23 00:35:18.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:35:18.923
Jul 19 00:35:18.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename custom-resource-definition 07/19/23 00:35:18.923
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:35:18.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:35:18.931
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jul 19 00:35:18.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:35:19.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8425" for this suite. 07/19/23 00:35:19.945
------------------------------
• [1.025 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:35:18.923
    Jul 19 00:35:18.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename custom-resource-definition 07/19/23 00:35:18.923
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:35:18.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:35:18.931
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jul 19 00:35:18.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:35:19.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8425" for this suite. 07/19/23 00:35:19.945
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:35:19.948
Jul 19 00:35:19.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename cronjob 07/19/23 00:35:19.949
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:35:19.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:35:19.957
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 07/19/23 00:35:19.959
STEP: Ensuring more than one job is running at a time 07/19/23 00:35:19.961
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 07/19/23 00:37:01.965
STEP: Removing cronjob 07/19/23 00:37:01.967
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:01.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7007" for this suite. 07/19/23 00:37:01.972
------------------------------
• [SLOW TEST] [102.026 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:35:19.948
    Jul 19 00:35:19.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename cronjob 07/19/23 00:35:19.949
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:35:19.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:35:19.957
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 07/19/23 00:35:19.959
    STEP: Ensuring more than one job is running at a time 07/19/23 00:35:19.961
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 07/19/23 00:37:01.965
    STEP: Removing cronjob 07/19/23 00:37:01.967
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:01.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7007" for this suite. 07/19/23 00:37:01.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:01.976
Jul 19 00:37:01.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename namespaces 07/19/23 00:37:01.977
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:01.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:01.988
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 07/19/23 00:37:01.989
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:02.001
STEP: Creating a service in the namespace 07/19/23 00:37:02.003
STEP: Deleting the namespace 07/19/23 00:37:02.022
STEP: Waiting for the namespace to be removed. 07/19/23 00:37:02.026
STEP: Recreating the namespace 07/19/23 00:37:08.028
STEP: Verifying there is no service in the namespace 07/19/23 00:37:08.036
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:08.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4823" for this suite. 07/19/23 00:37:08.041
STEP: Destroying namespace "nsdeletetest-7324" for this suite. 07/19/23 00:37:08.044
Jul 19 00:37:08.045: INFO: Namespace nsdeletetest-7324 was already deleted
STEP: Destroying namespace "nsdeletetest-8892" for this suite. 07/19/23 00:37:08.045
------------------------------
• [SLOW TEST] [6.072 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:01.976
    Jul 19 00:37:01.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename namespaces 07/19/23 00:37:01.977
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:01.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:01.988
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 07/19/23 00:37:01.989
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:02.001
    STEP: Creating a service in the namespace 07/19/23 00:37:02.003
    STEP: Deleting the namespace 07/19/23 00:37:02.022
    STEP: Waiting for the namespace to be removed. 07/19/23 00:37:02.026
    STEP: Recreating the namespace 07/19/23 00:37:08.028
    STEP: Verifying there is no service in the namespace 07/19/23 00:37:08.036
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:08.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4823" for this suite. 07/19/23 00:37:08.041
    STEP: Destroying namespace "nsdeletetest-7324" for this suite. 07/19/23 00:37:08.044
    Jul 19 00:37:08.045: INFO: Namespace nsdeletetest-7324 was already deleted
    STEP: Destroying namespace "nsdeletetest-8892" for this suite. 07/19/23 00:37:08.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:08.048
Jul 19 00:37:08.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 00:37:08.049
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:08.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:08.058
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-37fe08dd-888d-4dc3-8376-64102e42700e 07/19/23 00:37:08.069
STEP: Creating a pod to test consume secrets 07/19/23 00:37:08.074
Jul 19 00:37:08.077: INFO: Waiting up to 5m0s for pod "pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72" in namespace "secrets-7686" to be "Succeeded or Failed"
Jul 19 00:37:08.078: INFO: Pod "pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72": Phase="Pending", Reason="", readiness=false. Elapsed: 1.226456ms
Jul 19 00:37:10.081: INFO: Pod "pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003807347s
Jul 19 00:37:12.081: INFO: Pod "pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003848512s
STEP: Saw pod success 07/19/23 00:37:12.081
Jul 19 00:37:12.081: INFO: Pod "pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72" satisfied condition "Succeeded or Failed"
Jul 19 00:37:12.083: INFO: Trying to get logs from node controller-1 pod pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72 container secret-volume-test: <nil>
STEP: delete the pod 07/19/23 00:37:12.093
Jul 19 00:37:12.101: INFO: Waiting for pod pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72 to disappear
Jul 19 00:37:12.106: INFO: Pod pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:12.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7686" for this suite. 07/19/23 00:37:12.108
STEP: Destroying namespace "secret-namespace-1288" for this suite. 07/19/23 00:37:12.11
------------------------------
• [4.064 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:08.048
    Jul 19 00:37:08.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 00:37:08.049
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:08.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:08.058
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-37fe08dd-888d-4dc3-8376-64102e42700e 07/19/23 00:37:08.069
    STEP: Creating a pod to test consume secrets 07/19/23 00:37:08.074
    Jul 19 00:37:08.077: INFO: Waiting up to 5m0s for pod "pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72" in namespace "secrets-7686" to be "Succeeded or Failed"
    Jul 19 00:37:08.078: INFO: Pod "pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72": Phase="Pending", Reason="", readiness=false. Elapsed: 1.226456ms
    Jul 19 00:37:10.081: INFO: Pod "pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003807347s
    Jul 19 00:37:12.081: INFO: Pod "pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003848512s
    STEP: Saw pod success 07/19/23 00:37:12.081
    Jul 19 00:37:12.081: INFO: Pod "pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72" satisfied condition "Succeeded or Failed"
    Jul 19 00:37:12.083: INFO: Trying to get logs from node controller-1 pod pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72 container secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:37:12.093
    Jul 19 00:37:12.101: INFO: Waiting for pod pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72 to disappear
    Jul 19 00:37:12.106: INFO: Pod pod-secrets-9a69f0d0-542b-44cf-a0ed-939a92d26f72 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:12.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7686" for this suite. 07/19/23 00:37:12.108
    STEP: Destroying namespace "secret-namespace-1288" for this suite. 07/19/23 00:37:12.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:12.115
Jul 19 00:37:12.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:37:12.115
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:12.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:12.126
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 07/19/23 00:37:12.128
Jul 19 00:37:12.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-7737 create -f -'
Jul 19 00:37:13.166: INFO: stderr: ""
Jul 19 00:37:13.166: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 07/19/23 00:37:13.166
Jul 19 00:37:14.169: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 00:37:14.169: INFO: Found 1 / 1
Jul 19 00:37:14.169: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 07/19/23 00:37:14.169
Jul 19 00:37:14.171: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 00:37:14.171: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 19 00:37:14.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-7737 patch pod agnhost-primary-28rh5 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 19 00:37:14.236: INFO: stderr: ""
Jul 19 00:37:14.236: INFO: stdout: "pod/agnhost-primary-28rh5 patched\n"
STEP: checking annotations 07/19/23 00:37:14.236
Jul 19 00:37:14.238: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 19 00:37:14.238: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:14.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7737" for this suite. 07/19/23 00:37:14.241
------------------------------
• [2.129 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:12.115
    Jul 19 00:37:12.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:37:12.115
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:12.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:12.126
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 07/19/23 00:37:12.128
    Jul 19 00:37:12.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-7737 create -f -'
    Jul 19 00:37:13.166: INFO: stderr: ""
    Jul 19 00:37:13.166: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 07/19/23 00:37:13.166
    Jul 19 00:37:14.169: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 19 00:37:14.169: INFO: Found 1 / 1
    Jul 19 00:37:14.169: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 07/19/23 00:37:14.169
    Jul 19 00:37:14.171: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 19 00:37:14.171: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jul 19 00:37:14.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-7737 patch pod agnhost-primary-28rh5 -p {"metadata":{"annotations":{"x":"y"}}}'
    Jul 19 00:37:14.236: INFO: stderr: ""
    Jul 19 00:37:14.236: INFO: stdout: "pod/agnhost-primary-28rh5 patched\n"
    STEP: checking annotations 07/19/23 00:37:14.236
    Jul 19 00:37:14.238: INFO: Selector matched 1 pods for map[app:agnhost]
    Jul 19 00:37:14.238: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:14.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7737" for this suite. 07/19/23 00:37:14.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:14.244
Jul 19 00:37:14.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename gc 07/19/23 00:37:14.244
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:14.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:14.253
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 07/19/23 00:37:14.254
STEP: delete the rc 07/19/23 00:37:19.259
STEP: wait for all pods to be garbage collected 07/19/23 00:37:19.261
STEP: Gathering metrics 07/19/23 00:37:24.266
Jul 19 00:37:24.279: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
Jul 19 00:37:24.280: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.734362ms
Jul 19 00:37:24.280: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
Jul 19 00:37:24.280: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
Jul 19 00:37:24.321: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:24.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3377" for this suite. 07/19/23 00:37:24.324
------------------------------
• [SLOW TEST] [10.082 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:14.244
    Jul 19 00:37:14.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename gc 07/19/23 00:37:14.244
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:14.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:14.253
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 07/19/23 00:37:14.254
    STEP: delete the rc 07/19/23 00:37:19.259
    STEP: wait for all pods to be garbage collected 07/19/23 00:37:19.261
    STEP: Gathering metrics 07/19/23 00:37:24.266
    Jul 19 00:37:24.279: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
    Jul 19 00:37:24.280: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.734362ms
    Jul 19 00:37:24.280: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
    Jul 19 00:37:24.280: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
    Jul 19 00:37:24.321: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:24.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3377" for this suite. 07/19/23 00:37:24.324
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:24.327
Jul 19 00:37:24.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename certificates 07/19/23 00:37:24.327
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:24.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:24.336
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 07/19/23 00:37:24.841
STEP: getting /apis/certificates.k8s.io 07/19/23 00:37:24.843
STEP: getting /apis/certificates.k8s.io/v1 07/19/23 00:37:24.844
STEP: creating 07/19/23 00:37:24.844
STEP: getting 07/19/23 00:37:24.854
STEP: listing 07/19/23 00:37:24.858
STEP: watching 07/19/23 00:37:24.859
Jul 19 00:37:24.860: INFO: starting watch
STEP: patching 07/19/23 00:37:24.86
STEP: updating 07/19/23 00:37:24.863
Jul 19 00:37:24.868: INFO: waiting for watch events with expected annotations
Jul 19 00:37:24.868: INFO: saw patched and updated annotations
STEP: getting /approval 07/19/23 00:37:24.868
STEP: patching /approval 07/19/23 00:37:24.869
STEP: updating /approval 07/19/23 00:37:24.872
STEP: getting /status 07/19/23 00:37:24.875
STEP: patching /status 07/19/23 00:37:24.878
STEP: updating /status 07/19/23 00:37:24.881
STEP: deleting 07/19/23 00:37:24.885
STEP: deleting a collection 07/19/23 00:37:24.892
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:24.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-127" for this suite. 07/19/23 00:37:24.903
------------------------------
• [0.578 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:24.327
    Jul 19 00:37:24.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename certificates 07/19/23 00:37:24.327
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:24.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:24.336
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 07/19/23 00:37:24.841
    STEP: getting /apis/certificates.k8s.io 07/19/23 00:37:24.843
    STEP: getting /apis/certificates.k8s.io/v1 07/19/23 00:37:24.844
    STEP: creating 07/19/23 00:37:24.844
    STEP: getting 07/19/23 00:37:24.854
    STEP: listing 07/19/23 00:37:24.858
    STEP: watching 07/19/23 00:37:24.859
    Jul 19 00:37:24.860: INFO: starting watch
    STEP: patching 07/19/23 00:37:24.86
    STEP: updating 07/19/23 00:37:24.863
    Jul 19 00:37:24.868: INFO: waiting for watch events with expected annotations
    Jul 19 00:37:24.868: INFO: saw patched and updated annotations
    STEP: getting /approval 07/19/23 00:37:24.868
    STEP: patching /approval 07/19/23 00:37:24.869
    STEP: updating /approval 07/19/23 00:37:24.872
    STEP: getting /status 07/19/23 00:37:24.875
    STEP: patching /status 07/19/23 00:37:24.878
    STEP: updating /status 07/19/23 00:37:24.881
    STEP: deleting 07/19/23 00:37:24.885
    STEP: deleting a collection 07/19/23 00:37:24.892
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:24.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-127" for this suite. 07/19/23 00:37:24.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:24.906
Jul 19 00:37:24.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 00:37:24.906
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:24.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:24.914
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-184f6442-1772-4b7d-ba3f-2e3bd3b5b913 07/19/23 00:37:24.916
STEP: Creating a pod to test consume secrets 07/19/23 00:37:24.918
Jul 19 00:37:24.921: INFO: Waiting up to 5m0s for pod "pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c" in namespace "secrets-5513" to be "Succeeded or Failed"
Jul 19 00:37:24.922: INFO: Pod "pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.239296ms
Jul 19 00:37:26.925: INFO: Pod "pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004034151s
Jul 19 00:37:28.925: INFO: Pod "pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003928285s
STEP: Saw pod success 07/19/23 00:37:28.925
Jul 19 00:37:28.925: INFO: Pod "pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c" satisfied condition "Succeeded or Failed"
Jul 19 00:37:28.927: INFO: Trying to get logs from node controller-0 pod pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c container secret-volume-test: <nil>
STEP: delete the pod 07/19/23 00:37:28.938
Jul 19 00:37:28.944: INFO: Waiting for pod pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c to disappear
Jul 19 00:37:28.945: INFO: Pod pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:28.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5513" for this suite. 07/19/23 00:37:28.947
------------------------------
• [4.045 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:24.906
    Jul 19 00:37:24.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 00:37:24.906
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:24.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:24.914
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-184f6442-1772-4b7d-ba3f-2e3bd3b5b913 07/19/23 00:37:24.916
    STEP: Creating a pod to test consume secrets 07/19/23 00:37:24.918
    Jul 19 00:37:24.921: INFO: Waiting up to 5m0s for pod "pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c" in namespace "secrets-5513" to be "Succeeded or Failed"
    Jul 19 00:37:24.922: INFO: Pod "pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.239296ms
    Jul 19 00:37:26.925: INFO: Pod "pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004034151s
    Jul 19 00:37:28.925: INFO: Pod "pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003928285s
    STEP: Saw pod success 07/19/23 00:37:28.925
    Jul 19 00:37:28.925: INFO: Pod "pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c" satisfied condition "Succeeded or Failed"
    Jul 19 00:37:28.927: INFO: Trying to get logs from node controller-0 pod pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c container secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:37:28.938
    Jul 19 00:37:28.944: INFO: Waiting for pod pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c to disappear
    Jul 19 00:37:28.945: INFO: Pod pod-secrets-53fbd50d-1662-4029-b948-1bb865cf459c no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:28.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5513" for this suite. 07/19/23 00:37:28.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:28.951
Jul 19 00:37:28.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:37:28.952
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:28.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:28.96
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-333b8f95-94c5-44c1-8f56-0410ceb9df6d 07/19/23 00:37:28.961
STEP: Creating a pod to test consume configMaps 07/19/23 00:37:28.963
Jul 19 00:37:28.967: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642" in namespace "projected-1325" to be "Succeeded or Failed"
Jul 19 00:37:28.970: INFO: Pod "pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774433ms
Jul 19 00:37:30.973: INFO: Pod "pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006027s
Jul 19 00:37:32.973: INFO: Pod "pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006184753s
STEP: Saw pod success 07/19/23 00:37:32.973
Jul 19 00:37:32.973: INFO: Pod "pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642" satisfied condition "Succeeded or Failed"
Jul 19 00:37:32.975: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:37:32.978
Jul 19 00:37:32.987: INFO: Waiting for pod pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642 to disappear
Jul 19 00:37:32.988: INFO: Pod pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:32.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1325" for this suite. 07/19/23 00:37:32.99
------------------------------
• [4.041 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:28.951
    Jul 19 00:37:28.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:37:28.952
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:28.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:28.96
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-333b8f95-94c5-44c1-8f56-0410ceb9df6d 07/19/23 00:37:28.961
    STEP: Creating a pod to test consume configMaps 07/19/23 00:37:28.963
    Jul 19 00:37:28.967: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642" in namespace "projected-1325" to be "Succeeded or Failed"
    Jul 19 00:37:28.970: INFO: Pod "pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642": Phase="Pending", Reason="", readiness=false. Elapsed: 2.774433ms
    Jul 19 00:37:30.973: INFO: Pod "pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006027s
    Jul 19 00:37:32.973: INFO: Pod "pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006184753s
    STEP: Saw pod success 07/19/23 00:37:32.973
    Jul 19 00:37:32.973: INFO: Pod "pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642" satisfied condition "Succeeded or Failed"
    Jul 19 00:37:32.975: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:37:32.978
    Jul 19 00:37:32.987: INFO: Waiting for pod pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642 to disappear
    Jul 19 00:37:32.988: INFO: Pod pod-projected-configmaps-3825ccfc-3b34-423b-872c-31192a203642 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:32.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1325" for this suite. 07/19/23 00:37:32.99
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:32.992
Jul 19 00:37:32.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename init-container 07/19/23 00:37:32.993
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:33.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:33.018
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 07/19/23 00:37:33.019
Jul 19 00:37:33.019: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:37.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5497" for this suite. 07/19/23 00:37:37.218
------------------------------
• [4.228 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:32.992
    Jul 19 00:37:32.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename init-container 07/19/23 00:37:32.993
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:33.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:33.018
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 07/19/23 00:37:33.019
    Jul 19 00:37:33.019: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:37.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5497" for this suite. 07/19/23 00:37:37.218
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:37.221
Jul 19 00:37:37.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:37:37.222
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:37.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:37.231
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 07/19/23 00:37:37.232
Jul 19 00:37:37.237: INFO: Waiting up to 5m0s for pod "pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5" in namespace "emptydir-6199" to be "Succeeded or Failed"
Jul 19 00:37:37.242: INFO: Pod "pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.636407ms
Jul 19 00:37:39.245: INFO: Pod "pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007892674s
Jul 19 00:37:41.244: INFO: Pod "pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006966889s
STEP: Saw pod success 07/19/23 00:37:41.244
Jul 19 00:37:41.244: INFO: Pod "pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5" satisfied condition "Succeeded or Failed"
Jul 19 00:37:41.246: INFO: Trying to get logs from node controller-1 pod pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5 container test-container: <nil>
STEP: delete the pod 07/19/23 00:37:41.249
Jul 19 00:37:41.258: INFO: Waiting for pod pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5 to disappear
Jul 19 00:37:41.259: INFO: Pod pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:41.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6199" for this suite. 07/19/23 00:37:41.261
------------------------------
• [4.043 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:37.221
    Jul 19 00:37:37.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:37:37.222
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:37.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:37.231
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 07/19/23 00:37:37.232
    Jul 19 00:37:37.237: INFO: Waiting up to 5m0s for pod "pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5" in namespace "emptydir-6199" to be "Succeeded or Failed"
    Jul 19 00:37:37.242: INFO: Pod "pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.636407ms
    Jul 19 00:37:39.245: INFO: Pod "pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007892674s
    Jul 19 00:37:41.244: INFO: Pod "pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006966889s
    STEP: Saw pod success 07/19/23 00:37:41.244
    Jul 19 00:37:41.244: INFO: Pod "pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5" satisfied condition "Succeeded or Failed"
    Jul 19 00:37:41.246: INFO: Trying to get logs from node controller-1 pod pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5 container test-container: <nil>
    STEP: delete the pod 07/19/23 00:37:41.249
    Jul 19 00:37:41.258: INFO: Waiting for pod pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5 to disappear
    Jul 19 00:37:41.259: INFO: Pod pod-0457360f-1260-4dd1-bd7e-ee60ec74eae5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:41.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6199" for this suite. 07/19/23 00:37:41.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:41.265
Jul 19 00:37:41.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:37:41.266
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:41.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:41.277
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-ff893d14-29cc-4578-ba1b-ec0c879bb7aa 07/19/23 00:37:41.281
STEP: Creating the pod 07/19/23 00:37:41.283
Jul 19 00:37:41.287: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07" in namespace "configmap-215" to be "running and ready"
Jul 19 00:37:41.288: INFO: Pod "pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597914ms
Jul 19 00:37:41.288: INFO: The phase of Pod pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:37:43.291: INFO: Pod "pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07": Phase="Running", Reason="", readiness=true. Elapsed: 2.004093979s
Jul 19 00:37:43.291: INFO: The phase of Pod pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07 is Running (Ready = true)
Jul 19 00:37:43.291: INFO: Pod "pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-ff893d14-29cc-4578-ba1b-ec0c879bb7aa 07/19/23 00:37:43.296
STEP: waiting to observe update in volume 07/19/23 00:37:43.298
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:45.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-215" for this suite. 07/19/23 00:37:45.307
------------------------------
• [4.045 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:41.265
    Jul 19 00:37:41.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:37:41.266
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:41.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:41.277
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-ff893d14-29cc-4578-ba1b-ec0c879bb7aa 07/19/23 00:37:41.281
    STEP: Creating the pod 07/19/23 00:37:41.283
    Jul 19 00:37:41.287: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07" in namespace "configmap-215" to be "running and ready"
    Jul 19 00:37:41.288: INFO: Pod "pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597914ms
    Jul 19 00:37:41.288: INFO: The phase of Pod pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:37:43.291: INFO: Pod "pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07": Phase="Running", Reason="", readiness=true. Elapsed: 2.004093979s
    Jul 19 00:37:43.291: INFO: The phase of Pod pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07 is Running (Ready = true)
    Jul 19 00:37:43.291: INFO: Pod "pod-configmaps-9ed86ba6-442c-4a5c-af00-ae8ea8a06e07" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-ff893d14-29cc-4578-ba1b-ec0c879bb7aa 07/19/23 00:37:43.296
    STEP: waiting to observe update in volume 07/19/23 00:37:43.298
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:45.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-215" for this suite. 07/19/23 00:37:45.307
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:45.31
Jul 19 00:37:45.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-runtime 07/19/23 00:37:45.311
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:45.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:45.319
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 07/19/23 00:37:45.321
STEP: wait for the container to reach Failed 07/19/23 00:37:45.325
STEP: get the container status 07/19/23 00:37:48.335
STEP: the container should be terminated 07/19/23 00:37:48.337
STEP: the termination message should be set 07/19/23 00:37:48.337
Jul 19 00:37:48.337: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 07/19/23 00:37:48.337
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:48.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4461" for this suite. 07/19/23 00:37:48.349
------------------------------
• [3.041 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:45.31
    Jul 19 00:37:45.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-runtime 07/19/23 00:37:45.311
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:45.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:45.319
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 07/19/23 00:37:45.321
    STEP: wait for the container to reach Failed 07/19/23 00:37:45.325
    STEP: get the container status 07/19/23 00:37:48.335
    STEP: the container should be terminated 07/19/23 00:37:48.337
    STEP: the termination message should be set 07/19/23 00:37:48.337
    Jul 19 00:37:48.337: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 07/19/23 00:37:48.337
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:48.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4461" for this suite. 07/19/23 00:37:48.349
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:48.351
Jul 19 00:37:48.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 00:37:48.352
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:48.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:48.361
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-425ede93-901d-4e88-b250-d3e56d41d6fb 07/19/23 00:37:48.363
STEP: Creating a pod to test consume secrets 07/19/23 00:37:48.365
Jul 19 00:37:48.370: INFO: Waiting up to 5m0s for pod "pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114" in namespace "secrets-2671" to be "Succeeded or Failed"
Jul 19 00:37:48.372: INFO: Pod "pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114": Phase="Pending", Reason="", readiness=false. Elapsed: 1.344664ms
Jul 19 00:37:50.374: INFO: Pod "pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003621644s
Jul 19 00:37:52.374: INFO: Pod "pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004022049s
STEP: Saw pod success 07/19/23 00:37:52.374
Jul 19 00:37:52.374: INFO: Pod "pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114" satisfied condition "Succeeded or Failed"
Jul 19 00:37:52.376: INFO: Trying to get logs from node controller-1 pod pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114 container secret-volume-test: <nil>
STEP: delete the pod 07/19/23 00:37:52.379
Jul 19 00:37:52.385: INFO: Waiting for pod pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114 to disappear
Jul 19 00:37:52.386: INFO: Pod pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:52.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2671" for this suite. 07/19/23 00:37:52.389
------------------------------
• [4.040 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:48.351
    Jul 19 00:37:48.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 00:37:48.352
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:48.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:48.361
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-425ede93-901d-4e88-b250-d3e56d41d6fb 07/19/23 00:37:48.363
    STEP: Creating a pod to test consume secrets 07/19/23 00:37:48.365
    Jul 19 00:37:48.370: INFO: Waiting up to 5m0s for pod "pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114" in namespace "secrets-2671" to be "Succeeded or Failed"
    Jul 19 00:37:48.372: INFO: Pod "pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114": Phase="Pending", Reason="", readiness=false. Elapsed: 1.344664ms
    Jul 19 00:37:50.374: INFO: Pod "pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003621644s
    Jul 19 00:37:52.374: INFO: Pod "pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004022049s
    STEP: Saw pod success 07/19/23 00:37:52.374
    Jul 19 00:37:52.374: INFO: Pod "pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114" satisfied condition "Succeeded or Failed"
    Jul 19 00:37:52.376: INFO: Trying to get logs from node controller-1 pod pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114 container secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:37:52.379
    Jul 19 00:37:52.385: INFO: Waiting for pod pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114 to disappear
    Jul 19 00:37:52.386: INFO: Pod pod-secrets-f94cfc4d-54c1-476c-9b56-69f494895114 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:52.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2671" for this suite. 07/19/23 00:37:52.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:52.394
Jul 19 00:37:52.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:37:52.395
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:52.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:52.403
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-3cbf409f-b6ad-4103-9404-07fe175cc2f3 07/19/23 00:37:52.407
STEP: Creating secret with name s-test-opt-upd-c1134f3f-1620-4e37-98d5-3df30b30a5aa 07/19/23 00:37:52.409
STEP: Creating the pod 07/19/23 00:37:52.411
Jul 19 00:37:52.416: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb" in namespace "projected-278" to be "running and ready"
Jul 19 00:37:52.425: INFO: Pod "pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.658923ms
Jul 19 00:37:52.425: INFO: The phase of Pod pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:37:54.427: INFO: Pod "pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010878883s
Jul 19 00:37:54.427: INFO: The phase of Pod pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb is Running (Ready = true)
Jul 19 00:37:54.427: INFO: Pod "pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-3cbf409f-b6ad-4103-9404-07fe175cc2f3 07/19/23 00:37:54.438
STEP: Updating secret s-test-opt-upd-c1134f3f-1620-4e37-98d5-3df30b30a5aa 07/19/23 00:37:54.44
STEP: Creating secret with name s-test-opt-create-3ff37f48-ae49-49c8-9357-1c705b1204e6 07/19/23 00:37:54.442
STEP: waiting to observe update in volume 07/19/23 00:37:54.446
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jul 19 00:37:58.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-278" for this suite. 07/19/23 00:37:58.467
------------------------------
• [SLOW TEST] [6.075 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:52.394
    Jul 19 00:37:52.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:37:52.395
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:52.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:52.403
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-3cbf409f-b6ad-4103-9404-07fe175cc2f3 07/19/23 00:37:52.407
    STEP: Creating secret with name s-test-opt-upd-c1134f3f-1620-4e37-98d5-3df30b30a5aa 07/19/23 00:37:52.409
    STEP: Creating the pod 07/19/23 00:37:52.411
    Jul 19 00:37:52.416: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb" in namespace "projected-278" to be "running and ready"
    Jul 19 00:37:52.425: INFO: Pod "pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.658923ms
    Jul 19 00:37:52.425: INFO: The phase of Pod pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:37:54.427: INFO: Pod "pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010878883s
    Jul 19 00:37:54.427: INFO: The phase of Pod pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb is Running (Ready = true)
    Jul 19 00:37:54.427: INFO: Pod "pod-projected-secrets-0f1fe872-a0cf-41fc-9293-53602699ecfb" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-3cbf409f-b6ad-4103-9404-07fe175cc2f3 07/19/23 00:37:54.438
    STEP: Updating secret s-test-opt-upd-c1134f3f-1620-4e37-98d5-3df30b30a5aa 07/19/23 00:37:54.44
    STEP: Creating secret with name s-test-opt-create-3ff37f48-ae49-49c8-9357-1c705b1204e6 07/19/23 00:37:54.442
    STEP: waiting to observe update in volume 07/19/23 00:37:54.446
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:37:58.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-278" for this suite. 07/19/23 00:37:58.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:37:58.47
Jul 19 00:37:58.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename dns 07/19/23 00:37:58.471
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:58.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:58.483
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 07/19/23 00:37:58.485
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 4.150.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.150.4_udp@PTR;check="$$(dig +tcp +noall +answer +search 4.150.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.150.4_tcp@PTR;sleep 1; done
 07/19/23 00:37:58.497
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 4.150.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.150.4_udp@PTR;check="$$(dig +tcp +noall +answer +search 4.150.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.150.4_tcp@PTR;sleep 1; done
 07/19/23 00:37:58.497
STEP: creating a pod to probe DNS 07/19/23 00:37:58.497
STEP: submitting the pod to kubernetes 07/19/23 00:37:58.498
Jul 19 00:37:58.505: INFO: Waiting up to 15m0s for pod "dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa" in namespace "dns-8872" to be "running"
Jul 19 00:37:58.515: INFO: Pod "dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024245ms
Jul 19 00:38:00.518: INFO: Pod "dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa": Phase="Running", Reason="", readiness=true. Elapsed: 2.012671755s
Jul 19 00:38:00.518: INFO: Pod "dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa" satisfied condition "running"
STEP: retrieving the pod 07/19/23 00:38:00.518
STEP: looking for the results for each expected name from probers 07/19/23 00:38:00.52
Jul 19 00:38:00.522: INFO: Unable to read wheezy_udp@dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
Jul 19 00:38:00.523: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
Jul 19 00:38:00.525: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
Jul 19 00:38:00.527: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
Jul 19 00:38:00.535: INFO: Unable to read jessie_udp@dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
Jul 19 00:38:00.536: INFO: Unable to read jessie_tcp@dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
Jul 19 00:38:00.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
Jul 19 00:38:00.539: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
Jul 19 00:38:00.546: INFO: Lookups using dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa failed for: [wheezy_udp@dns-test-service.dns-8872.svc.cluster.local wheezy_tcp@dns-test-service.dns-8872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local jessie_udp@dns-test-service.dns-8872.svc.cluster.local jessie_tcp@dns-test-service.dns-8872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local]

Jul 19 00:38:05.572: INFO: DNS probes using dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa succeeded

STEP: deleting the pod 07/19/23 00:38:05.572
STEP: deleting the test service 07/19/23 00:38:05.579
STEP: deleting the test headless service 07/19/23 00:38:05.593
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jul 19 00:38:05.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8872" for this suite. 07/19/23 00:38:05.606
------------------------------
• [SLOW TEST] [7.138 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:37:58.47
    Jul 19 00:37:58.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename dns 07/19/23 00:37:58.471
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:37:58.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:37:58.483
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 07/19/23 00:37:58.485
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 4.150.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.150.4_udp@PTR;check="$$(dig +tcp +noall +answer +search 4.150.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.150.4_tcp@PTR;sleep 1; done
     07/19/23 00:37:58.497
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 4.150.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.150.4_udp@PTR;check="$$(dig +tcp +noall +answer +search 4.150.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.150.4_tcp@PTR;sleep 1; done
     07/19/23 00:37:58.497
    STEP: creating a pod to probe DNS 07/19/23 00:37:58.497
    STEP: submitting the pod to kubernetes 07/19/23 00:37:58.498
    Jul 19 00:37:58.505: INFO: Waiting up to 15m0s for pod "dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa" in namespace "dns-8872" to be "running"
    Jul 19 00:37:58.515: INFO: Pod "dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024245ms
    Jul 19 00:38:00.518: INFO: Pod "dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa": Phase="Running", Reason="", readiness=true. Elapsed: 2.012671755s
    Jul 19 00:38:00.518: INFO: Pod "dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa" satisfied condition "running"
    STEP: retrieving the pod 07/19/23 00:38:00.518
    STEP: looking for the results for each expected name from probers 07/19/23 00:38:00.52
    Jul 19 00:38:00.522: INFO: Unable to read wheezy_udp@dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
    Jul 19 00:38:00.523: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
    Jul 19 00:38:00.525: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
    Jul 19 00:38:00.527: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
    Jul 19 00:38:00.535: INFO: Unable to read jessie_udp@dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
    Jul 19 00:38:00.536: INFO: Unable to read jessie_tcp@dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
    Jul 19 00:38:00.538: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
    Jul 19 00:38:00.539: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local from pod dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa: the server could not find the requested resource (get pods dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa)
    Jul 19 00:38:00.546: INFO: Lookups using dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa failed for: [wheezy_udp@dns-test-service.dns-8872.svc.cluster.local wheezy_tcp@dns-test-service.dns-8872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local jessie_udp@dns-test-service.dns-8872.svc.cluster.local jessie_tcp@dns-test-service.dns-8872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8872.svc.cluster.local]

    Jul 19 00:38:05.572: INFO: DNS probes using dns-8872/dns-test-e425d7c6-20ab-42db-9e8f-e82bf49e8caa succeeded

    STEP: deleting the pod 07/19/23 00:38:05.572
    STEP: deleting the test service 07/19/23 00:38:05.579
    STEP: deleting the test headless service 07/19/23 00:38:05.593
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:38:05.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8872" for this suite. 07/19/23 00:38:05.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:38:05.609
Jul 19 00:38:05.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:38:05.61
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:38:05.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:38:05.622
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 07/19/23 00:38:05.624
Jul 19 00:38:05.628: INFO: Waiting up to 5m0s for pod "pod-f95e0052-4d63-4511-8293-44764b9a477b" in namespace "emptydir-1044" to be "Succeeded or Failed"
Jul 19 00:38:05.633: INFO: Pod "pod-f95e0052-4d63-4511-8293-44764b9a477b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828073ms
Jul 19 00:38:07.635: INFO: Pod "pod-f95e0052-4d63-4511-8293-44764b9a477b": Phase="Running", Reason="", readiness=false. Elapsed: 2.007493081s
Jul 19 00:38:09.635: INFO: Pod "pod-f95e0052-4d63-4511-8293-44764b9a477b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007253774s
STEP: Saw pod success 07/19/23 00:38:09.635
Jul 19 00:38:09.635: INFO: Pod "pod-f95e0052-4d63-4511-8293-44764b9a477b" satisfied condition "Succeeded or Failed"
Jul 19 00:38:09.637: INFO: Trying to get logs from node controller-1 pod pod-f95e0052-4d63-4511-8293-44764b9a477b container test-container: <nil>
STEP: delete the pod 07/19/23 00:38:09.64
Jul 19 00:38:09.647: INFO: Waiting for pod pod-f95e0052-4d63-4511-8293-44764b9a477b to disappear
Jul 19 00:38:09.649: INFO: Pod pod-f95e0052-4d63-4511-8293-44764b9a477b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:38:09.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1044" for this suite. 07/19/23 00:38:09.651
------------------------------
• [4.044 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:38:05.609
    Jul 19 00:38:05.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:38:05.61
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:38:05.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:38:05.622
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 07/19/23 00:38:05.624
    Jul 19 00:38:05.628: INFO: Waiting up to 5m0s for pod "pod-f95e0052-4d63-4511-8293-44764b9a477b" in namespace "emptydir-1044" to be "Succeeded or Failed"
    Jul 19 00:38:05.633: INFO: Pod "pod-f95e0052-4d63-4511-8293-44764b9a477b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828073ms
    Jul 19 00:38:07.635: INFO: Pod "pod-f95e0052-4d63-4511-8293-44764b9a477b": Phase="Running", Reason="", readiness=false. Elapsed: 2.007493081s
    Jul 19 00:38:09.635: INFO: Pod "pod-f95e0052-4d63-4511-8293-44764b9a477b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007253774s
    STEP: Saw pod success 07/19/23 00:38:09.635
    Jul 19 00:38:09.635: INFO: Pod "pod-f95e0052-4d63-4511-8293-44764b9a477b" satisfied condition "Succeeded or Failed"
    Jul 19 00:38:09.637: INFO: Trying to get logs from node controller-1 pod pod-f95e0052-4d63-4511-8293-44764b9a477b container test-container: <nil>
    STEP: delete the pod 07/19/23 00:38:09.64
    Jul 19 00:38:09.647: INFO: Waiting for pod pod-f95e0052-4d63-4511-8293-44764b9a477b to disappear
    Jul 19 00:38:09.649: INFO: Pod pod-f95e0052-4d63-4511-8293-44764b9a477b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:38:09.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1044" for this suite. 07/19/23 00:38:09.651
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:38:09.653
Jul 19 00:38:09.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:38:09.654
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:38:09.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:38:09.665
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:38:09.671
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:38:10.144
STEP: Deploying the webhook pod 07/19/23 00:38:10.148
STEP: Wait for the deployment to be ready 07/19/23 00:38:10.156
Jul 19 00:38:10.162: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:38:12.167
STEP: Verifying the service has paired with the endpoint 07/19/23 00:38:12.177
Jul 19 00:38:13.178: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jul 19 00:38:13.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7582-crds.webhook.example.com via the AdmissionRegistration API 07/19/23 00:38:13.687
STEP: Creating a custom resource that should be mutated by the webhook 07/19/23 00:38:13.695
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:38:16.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8651" for this suite. 07/19/23 00:38:16.321
STEP: Destroying namespace "webhook-8651-markers" for this suite. 07/19/23 00:38:16.323
------------------------------
• [SLOW TEST] [6.676 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:38:09.653
    Jul 19 00:38:09.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:38:09.654
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:38:09.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:38:09.665
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:38:09.671
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:38:10.144
    STEP: Deploying the webhook pod 07/19/23 00:38:10.148
    STEP: Wait for the deployment to be ready 07/19/23 00:38:10.156
    Jul 19 00:38:10.162: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:38:12.167
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:38:12.177
    Jul 19 00:38:13.178: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jul 19 00:38:13.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7582-crds.webhook.example.com via the AdmissionRegistration API 07/19/23 00:38:13.687
    STEP: Creating a custom resource that should be mutated by the webhook 07/19/23 00:38:13.695
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:38:16.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8651" for this suite. 07/19/23 00:38:16.321
    STEP: Destroying namespace "webhook-8651-markers" for this suite. 07/19/23 00:38:16.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:38:16.331
Jul 19 00:38:16.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename gc 07/19/23 00:38:16.331
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:38:16.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:38:16.341
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 07/19/23 00:38:16.345
STEP: delete the rc 07/19/23 00:38:21.413
STEP: wait for the rc to be deleted 07/19/23 00:38:21.42
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 07/19/23 00:38:26.422
STEP: Gathering metrics 07/19/23 00:38:56.432
Jul 19 00:38:56.444: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
Jul 19 00:38:56.446: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.845205ms
Jul 19 00:38:56.446: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
Jul 19 00:38:56.446: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
Jul 19 00:38:56.483: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 19 00:38:56.483: INFO: Deleting pod "simpletest.rc-26nl2" in namespace "gc-600"
Jul 19 00:38:56.490: INFO: Deleting pod "simpletest.rc-2jzsw" in namespace "gc-600"
Jul 19 00:38:56.500: INFO: Deleting pod "simpletest.rc-2slbb" in namespace "gc-600"
Jul 19 00:38:56.507: INFO: Deleting pod "simpletest.rc-2whhm" in namespace "gc-600"
Jul 19 00:38:56.514: INFO: Deleting pod "simpletest.rc-44j5d" in namespace "gc-600"
Jul 19 00:38:56.521: INFO: Deleting pod "simpletest.rc-49cph" in namespace "gc-600"
Jul 19 00:38:56.533: INFO: Deleting pod "simpletest.rc-4gfzf" in namespace "gc-600"
Jul 19 00:38:56.544: INFO: Deleting pod "simpletest.rc-4lsdl" in namespace "gc-600"
Jul 19 00:38:56.553: INFO: Deleting pod "simpletest.rc-4tng4" in namespace "gc-600"
Jul 19 00:38:56.561: INFO: Deleting pod "simpletest.rc-4x87q" in namespace "gc-600"
Jul 19 00:38:56.573: INFO: Deleting pod "simpletest.rc-5lvc6" in namespace "gc-600"
Jul 19 00:38:56.582: INFO: Deleting pod "simpletest.rc-5rd6k" in namespace "gc-600"
Jul 19 00:38:56.596: INFO: Deleting pod "simpletest.rc-5rplw" in namespace "gc-600"
Jul 19 00:38:56.609: INFO: Deleting pod "simpletest.rc-5vfcc" in namespace "gc-600"
Jul 19 00:38:56.639: INFO: Deleting pod "simpletest.rc-6kfk5" in namespace "gc-600"
Jul 19 00:38:56.650: INFO: Deleting pod "simpletest.rc-6s2p8" in namespace "gc-600"
Jul 19 00:38:56.660: INFO: Deleting pod "simpletest.rc-6z77l" in namespace "gc-600"
Jul 19 00:38:56.671: INFO: Deleting pod "simpletest.rc-72r2f" in namespace "gc-600"
Jul 19 00:38:56.679: INFO: Deleting pod "simpletest.rc-7g8zl" in namespace "gc-600"
Jul 19 00:38:56.687: INFO: Deleting pod "simpletest.rc-7nlwb" in namespace "gc-600"
Jul 19 00:38:56.701: INFO: Deleting pod "simpletest.rc-848fp" in namespace "gc-600"
Jul 19 00:38:56.709: INFO: Deleting pod "simpletest.rc-84w6p" in namespace "gc-600"
Jul 19 00:38:56.719: INFO: Deleting pod "simpletest.rc-8ghnk" in namespace "gc-600"
Jul 19 00:38:56.726: INFO: Deleting pod "simpletest.rc-8mvvs" in namespace "gc-600"
Jul 19 00:38:56.736: INFO: Deleting pod "simpletest.rc-8nbgl" in namespace "gc-600"
Jul 19 00:38:56.741: INFO: Deleting pod "simpletest.rc-8ph28" in namespace "gc-600"
Jul 19 00:38:56.750: INFO: Deleting pod "simpletest.rc-8vrlv" in namespace "gc-600"
Jul 19 00:38:56.759: INFO: Deleting pod "simpletest.rc-9482g" in namespace "gc-600"
Jul 19 00:38:56.769: INFO: Deleting pod "simpletest.rc-9tvxc" in namespace "gc-600"
Jul 19 00:38:56.780: INFO: Deleting pod "simpletest.rc-b4fsh" in namespace "gc-600"
Jul 19 00:38:56.789: INFO: Deleting pod "simpletest.rc-b9kk6" in namespace "gc-600"
Jul 19 00:38:56.794: INFO: Deleting pod "simpletest.rc-bcfr2" in namespace "gc-600"
Jul 19 00:38:56.809: INFO: Deleting pod "simpletest.rc-bk7zv" in namespace "gc-600"
Jul 19 00:38:56.818: INFO: Deleting pod "simpletest.rc-brd7q" in namespace "gc-600"
Jul 19 00:38:56.840: INFO: Deleting pod "simpletest.rc-ccv74" in namespace "gc-600"
Jul 19 00:38:56.867: INFO: Deleting pod "simpletest.rc-ch7zj" in namespace "gc-600"
Jul 19 00:38:56.878: INFO: Deleting pod "simpletest.rc-cr26z" in namespace "gc-600"
Jul 19 00:38:56.905: INFO: Deleting pod "simpletest.rc-dhkh9" in namespace "gc-600"
Jul 19 00:38:56.926: INFO: Deleting pod "simpletest.rc-dp8x4" in namespace "gc-600"
Jul 19 00:38:56.933: INFO: Deleting pod "simpletest.rc-dtn22" in namespace "gc-600"
Jul 19 00:38:56.940: INFO: Deleting pod "simpletest.rc-f992l" in namespace "gc-600"
Jul 19 00:38:56.951: INFO: Deleting pod "simpletest.rc-g2rm4" in namespace "gc-600"
Jul 19 00:38:56.957: INFO: Deleting pod "simpletest.rc-glt74" in namespace "gc-600"
Jul 19 00:38:56.965: INFO: Deleting pod "simpletest.rc-gmr8t" in namespace "gc-600"
Jul 19 00:38:56.990: INFO: Deleting pod "simpletest.rc-gp59v" in namespace "gc-600"
Jul 19 00:38:57.039: INFO: Deleting pod "simpletest.rc-gpfmb" in namespace "gc-600"
Jul 19 00:38:57.046: INFO: Deleting pod "simpletest.rc-hjw7r" in namespace "gc-600"
Jul 19 00:38:57.053: INFO: Deleting pod "simpletest.rc-hksrs" in namespace "gc-600"
Jul 19 00:38:57.062: INFO: Deleting pod "simpletest.rc-hstrh" in namespace "gc-600"
Jul 19 00:38:57.070: INFO: Deleting pod "simpletest.rc-hsxf6" in namespace "gc-600"
Jul 19 00:38:57.078: INFO: Deleting pod "simpletest.rc-ht2zv" in namespace "gc-600"
Jul 19 00:38:57.085: INFO: Deleting pod "simpletest.rc-j4frt" in namespace "gc-600"
Jul 19 00:38:57.090: INFO: Deleting pod "simpletest.rc-jmbct" in namespace "gc-600"
Jul 19 00:38:57.123: INFO: Deleting pod "simpletest.rc-jnr28" in namespace "gc-600"
Jul 19 00:38:57.130: INFO: Deleting pod "simpletest.rc-jqd4q" in namespace "gc-600"
Jul 19 00:38:57.145: INFO: Deleting pod "simpletest.rc-kkf7b" in namespace "gc-600"
Jul 19 00:38:57.152: INFO: Deleting pod "simpletest.rc-kncqb" in namespace "gc-600"
Jul 19 00:38:57.160: INFO: Deleting pod "simpletest.rc-kzpph" in namespace "gc-600"
Jul 19 00:38:57.168: INFO: Deleting pod "simpletest.rc-lkngr" in namespace "gc-600"
Jul 19 00:38:57.177: INFO: Deleting pod "simpletest.rc-lvr4c" in namespace "gc-600"
Jul 19 00:38:57.185: INFO: Deleting pod "simpletest.rc-m9sqw" in namespace "gc-600"
Jul 19 00:38:57.193: INFO: Deleting pod "simpletest.rc-mhmm8" in namespace "gc-600"
Jul 19 00:38:57.235: INFO: Deleting pod "simpletest.rc-mm5gg" in namespace "gc-600"
Jul 19 00:38:57.344: INFO: Deleting pod "simpletest.rc-mpmp5" in namespace "gc-600"
Jul 19 00:38:57.360: INFO: Deleting pod "simpletest.rc-n9sgc" in namespace "gc-600"
Jul 19 00:38:57.381: INFO: Deleting pod "simpletest.rc-nh54x" in namespace "gc-600"
Jul 19 00:38:57.434: INFO: Deleting pod "simpletest.rc-njrj5" in namespace "gc-600"
Jul 19 00:38:57.483: INFO: Deleting pod "simpletest.rc-ptfc6" in namespace "gc-600"
Jul 19 00:38:57.543: INFO: Deleting pod "simpletest.rc-q4gb9" in namespace "gc-600"
Jul 19 00:38:57.674: INFO: Deleting pod "simpletest.rc-q8trk" in namespace "gc-600"
Jul 19 00:38:57.680: INFO: Deleting pod "simpletest.rc-qz8g5" in namespace "gc-600"
Jul 19 00:38:57.688: INFO: Deleting pod "simpletest.rc-r5xnh" in namespace "gc-600"
Jul 19 00:38:57.731: INFO: Deleting pod "simpletest.rc-rbwxh" in namespace "gc-600"
Jul 19 00:38:57.783: INFO: Deleting pod "simpletest.rc-rd444" in namespace "gc-600"
Jul 19 00:38:57.835: INFO: Deleting pod "simpletest.rc-rjbqw" in namespace "gc-600"
Jul 19 00:38:57.891: INFO: Deleting pod "simpletest.rc-srxlk" in namespace "gc-600"
Jul 19 00:38:57.970: INFO: Deleting pod "simpletest.rc-sv89k" in namespace "gc-600"
Jul 19 00:38:57.979: INFO: Deleting pod "simpletest.rc-sxww8" in namespace "gc-600"
Jul 19 00:38:58.099: INFO: Deleting pod "simpletest.rc-t57vf" in namespace "gc-600"
Jul 19 00:38:58.140: INFO: Deleting pod "simpletest.rc-v9dfx" in namespace "gc-600"
Jul 19 00:38:58.166: INFO: Deleting pod "simpletest.rc-vhld2" in namespace "gc-600"
Jul 19 00:38:58.193: INFO: Deleting pod "simpletest.rc-vrvdw" in namespace "gc-600"
Jul 19 00:38:58.246: INFO: Deleting pod "simpletest.rc-w5sqb" in namespace "gc-600"
Jul 19 00:38:58.287: INFO: Deleting pod "simpletest.rc-w9szl" in namespace "gc-600"
Jul 19 00:38:58.333: INFO: Deleting pod "simpletest.rc-wh6j4" in namespace "gc-600"
Jul 19 00:38:58.384: INFO: Deleting pod "simpletest.rc-whs7h" in namespace "gc-600"
Jul 19 00:38:58.431: INFO: Deleting pod "simpletest.rc-wp9wj" in namespace "gc-600"
Jul 19 00:38:58.479: INFO: Deleting pod "simpletest.rc-wtgc6" in namespace "gc-600"
Jul 19 00:38:58.545: INFO: Deleting pod "simpletest.rc-x92kr" in namespace "gc-600"
Jul 19 00:38:58.584: INFO: Deleting pod "simpletest.rc-xdwsn" in namespace "gc-600"
Jul 19 00:38:58.632: INFO: Deleting pod "simpletest.rc-xf8nr" in namespace "gc-600"
Jul 19 00:38:58.682: INFO: Deleting pod "simpletest.rc-xnwn6" in namespace "gc-600"
Jul 19 00:38:58.731: INFO: Deleting pod "simpletest.rc-xps52" in namespace "gc-600"
Jul 19 00:38:58.782: INFO: Deleting pod "simpletest.rc-zbd76" in namespace "gc-600"
Jul 19 00:38:58.833: INFO: Deleting pod "simpletest.rc-zcsd7" in namespace "gc-600"
Jul 19 00:38:58.884: INFO: Deleting pod "simpletest.rc-zgtf4" in namespace "gc-600"
Jul 19 00:38:58.973: INFO: Deleting pod "simpletest.rc-zkt27" in namespace "gc-600"
Jul 19 00:38:59.007: INFO: Deleting pod "simpletest.rc-zvl9c" in namespace "gc-600"
Jul 19 00:38:59.035: INFO: Deleting pod "simpletest.rc-zxr2q" in namespace "gc-600"
Jul 19 00:38:59.122: INFO: Deleting pod "simpletest.rc-zzg7w" in namespace "gc-600"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jul 19 00:38:59.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-600" for this suite. 07/19/23 00:38:59.178
------------------------------
• [SLOW TEST] [42.902 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:38:16.331
    Jul 19 00:38:16.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename gc 07/19/23 00:38:16.331
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:38:16.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:38:16.341
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 07/19/23 00:38:16.345
    STEP: delete the rc 07/19/23 00:38:21.413
    STEP: wait for the rc to be deleted 07/19/23 00:38:21.42
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 07/19/23 00:38:26.422
    STEP: Gathering metrics 07/19/23 00:38:56.432
    Jul 19 00:38:56.444: INFO: Waiting up to 5m0s for pod "kube-controller-manager-controller-1" in namespace "kube-system" to be "running and ready"
    Jul 19 00:38:56.446: INFO: Pod "kube-controller-manager-controller-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.845205ms
    Jul 19 00:38:56.446: INFO: The phase of Pod kube-controller-manager-controller-1 is Running (Ready = true)
    Jul 19 00:38:56.446: INFO: Pod "kube-controller-manager-controller-1" satisfied condition "running and ready"
    Jul 19 00:38:56.483: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jul 19 00:38:56.483: INFO: Deleting pod "simpletest.rc-26nl2" in namespace "gc-600"
    Jul 19 00:38:56.490: INFO: Deleting pod "simpletest.rc-2jzsw" in namespace "gc-600"
    Jul 19 00:38:56.500: INFO: Deleting pod "simpletest.rc-2slbb" in namespace "gc-600"
    Jul 19 00:38:56.507: INFO: Deleting pod "simpletest.rc-2whhm" in namespace "gc-600"
    Jul 19 00:38:56.514: INFO: Deleting pod "simpletest.rc-44j5d" in namespace "gc-600"
    Jul 19 00:38:56.521: INFO: Deleting pod "simpletest.rc-49cph" in namespace "gc-600"
    Jul 19 00:38:56.533: INFO: Deleting pod "simpletest.rc-4gfzf" in namespace "gc-600"
    Jul 19 00:38:56.544: INFO: Deleting pod "simpletest.rc-4lsdl" in namespace "gc-600"
    Jul 19 00:38:56.553: INFO: Deleting pod "simpletest.rc-4tng4" in namespace "gc-600"
    Jul 19 00:38:56.561: INFO: Deleting pod "simpletest.rc-4x87q" in namespace "gc-600"
    Jul 19 00:38:56.573: INFO: Deleting pod "simpletest.rc-5lvc6" in namespace "gc-600"
    Jul 19 00:38:56.582: INFO: Deleting pod "simpletest.rc-5rd6k" in namespace "gc-600"
    Jul 19 00:38:56.596: INFO: Deleting pod "simpletest.rc-5rplw" in namespace "gc-600"
    Jul 19 00:38:56.609: INFO: Deleting pod "simpletest.rc-5vfcc" in namespace "gc-600"
    Jul 19 00:38:56.639: INFO: Deleting pod "simpletest.rc-6kfk5" in namespace "gc-600"
    Jul 19 00:38:56.650: INFO: Deleting pod "simpletest.rc-6s2p8" in namespace "gc-600"
    Jul 19 00:38:56.660: INFO: Deleting pod "simpletest.rc-6z77l" in namespace "gc-600"
    Jul 19 00:38:56.671: INFO: Deleting pod "simpletest.rc-72r2f" in namespace "gc-600"
    Jul 19 00:38:56.679: INFO: Deleting pod "simpletest.rc-7g8zl" in namespace "gc-600"
    Jul 19 00:38:56.687: INFO: Deleting pod "simpletest.rc-7nlwb" in namespace "gc-600"
    Jul 19 00:38:56.701: INFO: Deleting pod "simpletest.rc-848fp" in namespace "gc-600"
    Jul 19 00:38:56.709: INFO: Deleting pod "simpletest.rc-84w6p" in namespace "gc-600"
    Jul 19 00:38:56.719: INFO: Deleting pod "simpletest.rc-8ghnk" in namespace "gc-600"
    Jul 19 00:38:56.726: INFO: Deleting pod "simpletest.rc-8mvvs" in namespace "gc-600"
    Jul 19 00:38:56.736: INFO: Deleting pod "simpletest.rc-8nbgl" in namespace "gc-600"
    Jul 19 00:38:56.741: INFO: Deleting pod "simpletest.rc-8ph28" in namespace "gc-600"
    Jul 19 00:38:56.750: INFO: Deleting pod "simpletest.rc-8vrlv" in namespace "gc-600"
    Jul 19 00:38:56.759: INFO: Deleting pod "simpletest.rc-9482g" in namespace "gc-600"
    Jul 19 00:38:56.769: INFO: Deleting pod "simpletest.rc-9tvxc" in namespace "gc-600"
    Jul 19 00:38:56.780: INFO: Deleting pod "simpletest.rc-b4fsh" in namespace "gc-600"
    Jul 19 00:38:56.789: INFO: Deleting pod "simpletest.rc-b9kk6" in namespace "gc-600"
    Jul 19 00:38:56.794: INFO: Deleting pod "simpletest.rc-bcfr2" in namespace "gc-600"
    Jul 19 00:38:56.809: INFO: Deleting pod "simpletest.rc-bk7zv" in namespace "gc-600"
    Jul 19 00:38:56.818: INFO: Deleting pod "simpletest.rc-brd7q" in namespace "gc-600"
    Jul 19 00:38:56.840: INFO: Deleting pod "simpletest.rc-ccv74" in namespace "gc-600"
    Jul 19 00:38:56.867: INFO: Deleting pod "simpletest.rc-ch7zj" in namespace "gc-600"
    Jul 19 00:38:56.878: INFO: Deleting pod "simpletest.rc-cr26z" in namespace "gc-600"
    Jul 19 00:38:56.905: INFO: Deleting pod "simpletest.rc-dhkh9" in namespace "gc-600"
    Jul 19 00:38:56.926: INFO: Deleting pod "simpletest.rc-dp8x4" in namespace "gc-600"
    Jul 19 00:38:56.933: INFO: Deleting pod "simpletest.rc-dtn22" in namespace "gc-600"
    Jul 19 00:38:56.940: INFO: Deleting pod "simpletest.rc-f992l" in namespace "gc-600"
    Jul 19 00:38:56.951: INFO: Deleting pod "simpletest.rc-g2rm4" in namespace "gc-600"
    Jul 19 00:38:56.957: INFO: Deleting pod "simpletest.rc-glt74" in namespace "gc-600"
    Jul 19 00:38:56.965: INFO: Deleting pod "simpletest.rc-gmr8t" in namespace "gc-600"
    Jul 19 00:38:56.990: INFO: Deleting pod "simpletest.rc-gp59v" in namespace "gc-600"
    Jul 19 00:38:57.039: INFO: Deleting pod "simpletest.rc-gpfmb" in namespace "gc-600"
    Jul 19 00:38:57.046: INFO: Deleting pod "simpletest.rc-hjw7r" in namespace "gc-600"
    Jul 19 00:38:57.053: INFO: Deleting pod "simpletest.rc-hksrs" in namespace "gc-600"
    Jul 19 00:38:57.062: INFO: Deleting pod "simpletest.rc-hstrh" in namespace "gc-600"
    Jul 19 00:38:57.070: INFO: Deleting pod "simpletest.rc-hsxf6" in namespace "gc-600"
    Jul 19 00:38:57.078: INFO: Deleting pod "simpletest.rc-ht2zv" in namespace "gc-600"
    Jul 19 00:38:57.085: INFO: Deleting pod "simpletest.rc-j4frt" in namespace "gc-600"
    Jul 19 00:38:57.090: INFO: Deleting pod "simpletest.rc-jmbct" in namespace "gc-600"
    Jul 19 00:38:57.123: INFO: Deleting pod "simpletest.rc-jnr28" in namespace "gc-600"
    Jul 19 00:38:57.130: INFO: Deleting pod "simpletest.rc-jqd4q" in namespace "gc-600"
    Jul 19 00:38:57.145: INFO: Deleting pod "simpletest.rc-kkf7b" in namespace "gc-600"
    Jul 19 00:38:57.152: INFO: Deleting pod "simpletest.rc-kncqb" in namespace "gc-600"
    Jul 19 00:38:57.160: INFO: Deleting pod "simpletest.rc-kzpph" in namespace "gc-600"
    Jul 19 00:38:57.168: INFO: Deleting pod "simpletest.rc-lkngr" in namespace "gc-600"
    Jul 19 00:38:57.177: INFO: Deleting pod "simpletest.rc-lvr4c" in namespace "gc-600"
    Jul 19 00:38:57.185: INFO: Deleting pod "simpletest.rc-m9sqw" in namespace "gc-600"
    Jul 19 00:38:57.193: INFO: Deleting pod "simpletest.rc-mhmm8" in namespace "gc-600"
    Jul 19 00:38:57.235: INFO: Deleting pod "simpletest.rc-mm5gg" in namespace "gc-600"
    Jul 19 00:38:57.344: INFO: Deleting pod "simpletest.rc-mpmp5" in namespace "gc-600"
    Jul 19 00:38:57.360: INFO: Deleting pod "simpletest.rc-n9sgc" in namespace "gc-600"
    Jul 19 00:38:57.381: INFO: Deleting pod "simpletest.rc-nh54x" in namespace "gc-600"
    Jul 19 00:38:57.434: INFO: Deleting pod "simpletest.rc-njrj5" in namespace "gc-600"
    Jul 19 00:38:57.483: INFO: Deleting pod "simpletest.rc-ptfc6" in namespace "gc-600"
    Jul 19 00:38:57.543: INFO: Deleting pod "simpletest.rc-q4gb9" in namespace "gc-600"
    Jul 19 00:38:57.674: INFO: Deleting pod "simpletest.rc-q8trk" in namespace "gc-600"
    Jul 19 00:38:57.680: INFO: Deleting pod "simpletest.rc-qz8g5" in namespace "gc-600"
    Jul 19 00:38:57.688: INFO: Deleting pod "simpletest.rc-r5xnh" in namespace "gc-600"
    Jul 19 00:38:57.731: INFO: Deleting pod "simpletest.rc-rbwxh" in namespace "gc-600"
    Jul 19 00:38:57.783: INFO: Deleting pod "simpletest.rc-rd444" in namespace "gc-600"
    Jul 19 00:38:57.835: INFO: Deleting pod "simpletest.rc-rjbqw" in namespace "gc-600"
    Jul 19 00:38:57.891: INFO: Deleting pod "simpletest.rc-srxlk" in namespace "gc-600"
    Jul 19 00:38:57.970: INFO: Deleting pod "simpletest.rc-sv89k" in namespace "gc-600"
    Jul 19 00:38:57.979: INFO: Deleting pod "simpletest.rc-sxww8" in namespace "gc-600"
    Jul 19 00:38:58.099: INFO: Deleting pod "simpletest.rc-t57vf" in namespace "gc-600"
    Jul 19 00:38:58.140: INFO: Deleting pod "simpletest.rc-v9dfx" in namespace "gc-600"
    Jul 19 00:38:58.166: INFO: Deleting pod "simpletest.rc-vhld2" in namespace "gc-600"
    Jul 19 00:38:58.193: INFO: Deleting pod "simpletest.rc-vrvdw" in namespace "gc-600"
    Jul 19 00:38:58.246: INFO: Deleting pod "simpletest.rc-w5sqb" in namespace "gc-600"
    Jul 19 00:38:58.287: INFO: Deleting pod "simpletest.rc-w9szl" in namespace "gc-600"
    Jul 19 00:38:58.333: INFO: Deleting pod "simpletest.rc-wh6j4" in namespace "gc-600"
    Jul 19 00:38:58.384: INFO: Deleting pod "simpletest.rc-whs7h" in namespace "gc-600"
    Jul 19 00:38:58.431: INFO: Deleting pod "simpletest.rc-wp9wj" in namespace "gc-600"
    Jul 19 00:38:58.479: INFO: Deleting pod "simpletest.rc-wtgc6" in namespace "gc-600"
    Jul 19 00:38:58.545: INFO: Deleting pod "simpletest.rc-x92kr" in namespace "gc-600"
    Jul 19 00:38:58.584: INFO: Deleting pod "simpletest.rc-xdwsn" in namespace "gc-600"
    Jul 19 00:38:58.632: INFO: Deleting pod "simpletest.rc-xf8nr" in namespace "gc-600"
    Jul 19 00:38:58.682: INFO: Deleting pod "simpletest.rc-xnwn6" in namespace "gc-600"
    Jul 19 00:38:58.731: INFO: Deleting pod "simpletest.rc-xps52" in namespace "gc-600"
    Jul 19 00:38:58.782: INFO: Deleting pod "simpletest.rc-zbd76" in namespace "gc-600"
    Jul 19 00:38:58.833: INFO: Deleting pod "simpletest.rc-zcsd7" in namespace "gc-600"
    Jul 19 00:38:58.884: INFO: Deleting pod "simpletest.rc-zgtf4" in namespace "gc-600"
    Jul 19 00:38:58.973: INFO: Deleting pod "simpletest.rc-zkt27" in namespace "gc-600"
    Jul 19 00:38:59.007: INFO: Deleting pod "simpletest.rc-zvl9c" in namespace "gc-600"
    Jul 19 00:38:59.035: INFO: Deleting pod "simpletest.rc-zxr2q" in namespace "gc-600"
    Jul 19 00:38:59.122: INFO: Deleting pod "simpletest.rc-zzg7w" in namespace "gc-600"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:38:59.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-600" for this suite. 07/19/23 00:38:59.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:38:59.233
Jul 19 00:38:59.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 00:38:59.233
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:38:59.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:38:59.25
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 07/19/23 00:38:59.252
Jul 19 00:38:59.275: INFO: Waiting up to 5m0s for pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33" in namespace "downward-api-8862" to be "running and ready"
Jul 19 00:38:59.276: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 1.349915ms
Jul 19 00:38:59.276: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:39:01.280: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004732737s
Jul 19 00:39:01.280: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:39:03.279: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003939683s
Jul 19 00:39:03.279: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:39:05.279: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003940841s
Jul 19 00:39:05.279: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:39:07.279: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004372705s
Jul 19 00:39:07.279: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:39:09.279: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Running", Reason="", readiness=true. Elapsed: 10.00418025s
Jul 19 00:39:09.279: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Running (Ready = true)
Jul 19 00:39:09.279: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33" satisfied condition "running and ready"
Jul 19 00:39:09.791: INFO: Successfully updated pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:11.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8862" for this suite. 07/19/23 00:39:11.805
------------------------------
• [SLOW TEST] [12.575 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:38:59.233
    Jul 19 00:38:59.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 00:38:59.233
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:38:59.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:38:59.25
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 07/19/23 00:38:59.252
    Jul 19 00:38:59.275: INFO: Waiting up to 5m0s for pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33" in namespace "downward-api-8862" to be "running and ready"
    Jul 19 00:38:59.276: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 1.349915ms
    Jul 19 00:38:59.276: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:39:01.280: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004732737s
    Jul 19 00:39:01.280: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:39:03.279: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003939683s
    Jul 19 00:39:03.279: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:39:05.279: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 6.003940841s
    Jul 19 00:39:05.279: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:39:07.279: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004372705s
    Jul 19 00:39:07.279: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:39:09.279: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33": Phase="Running", Reason="", readiness=true. Elapsed: 10.00418025s
    Jul 19 00:39:09.279: INFO: The phase of Pod annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33 is Running (Ready = true)
    Jul 19 00:39:09.279: INFO: Pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33" satisfied condition "running and ready"
    Jul 19 00:39:09.791: INFO: Successfully updated pod "annotationupdated52391eb-77fb-466f-89e8-ff81aa96bd33"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:11.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8862" for this suite. 07/19/23 00:39:11.805
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:11.808
Jul 19 00:39:11.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename security-context 07/19/23 00:39:11.808
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:11.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:11.817
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 07/19/23 00:39:11.819
Jul 19 00:39:11.823: INFO: Waiting up to 5m0s for pod "security-context-6754f183-0966-450d-b192-cd179ff77e55" in namespace "security-context-4839" to be "Succeeded or Failed"
Jul 19 00:39:11.825: INFO: Pod "security-context-6754f183-0966-450d-b192-cd179ff77e55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.659788ms
Jul 19 00:39:13.828: INFO: Pod "security-context-6754f183-0966-450d-b192-cd179ff77e55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005473167s
Jul 19 00:39:15.828: INFO: Pod "security-context-6754f183-0966-450d-b192-cd179ff77e55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005414743s
STEP: Saw pod success 07/19/23 00:39:15.828
Jul 19 00:39:15.828: INFO: Pod "security-context-6754f183-0966-450d-b192-cd179ff77e55" satisfied condition "Succeeded or Failed"
Jul 19 00:39:15.830: INFO: Trying to get logs from node controller-1 pod security-context-6754f183-0966-450d-b192-cd179ff77e55 container test-container: <nil>
STEP: delete the pod 07/19/23 00:39:15.833
Jul 19 00:39:15.840: INFO: Waiting for pod security-context-6754f183-0966-450d-b192-cd179ff77e55 to disappear
Jul 19 00:39:15.842: INFO: Pod security-context-6754f183-0966-450d-b192-cd179ff77e55 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:15.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4839" for this suite. 07/19/23 00:39:15.844
------------------------------
• [4.042 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:11.808
    Jul 19 00:39:11.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename security-context 07/19/23 00:39:11.808
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:11.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:11.817
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 07/19/23 00:39:11.819
    Jul 19 00:39:11.823: INFO: Waiting up to 5m0s for pod "security-context-6754f183-0966-450d-b192-cd179ff77e55" in namespace "security-context-4839" to be "Succeeded or Failed"
    Jul 19 00:39:11.825: INFO: Pod "security-context-6754f183-0966-450d-b192-cd179ff77e55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.659788ms
    Jul 19 00:39:13.828: INFO: Pod "security-context-6754f183-0966-450d-b192-cd179ff77e55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005473167s
    Jul 19 00:39:15.828: INFO: Pod "security-context-6754f183-0966-450d-b192-cd179ff77e55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005414743s
    STEP: Saw pod success 07/19/23 00:39:15.828
    Jul 19 00:39:15.828: INFO: Pod "security-context-6754f183-0966-450d-b192-cd179ff77e55" satisfied condition "Succeeded or Failed"
    Jul 19 00:39:15.830: INFO: Trying to get logs from node controller-1 pod security-context-6754f183-0966-450d-b192-cd179ff77e55 container test-container: <nil>
    STEP: delete the pod 07/19/23 00:39:15.833
    Jul 19 00:39:15.840: INFO: Waiting for pod security-context-6754f183-0966-450d-b192-cd179ff77e55 to disappear
    Jul 19 00:39:15.842: INFO: Pod security-context-6754f183-0966-450d-b192-cd179ff77e55 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:15.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4839" for this suite. 07/19/23 00:39:15.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:15.851
Jul 19 00:39:15.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:39:15.851
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:15.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:15.859
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:39:15.867
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:39:16.214
STEP: Deploying the webhook pod 07/19/23 00:39:16.218
STEP: Wait for the deployment to be ready 07/19/23 00:39:16.224
Jul 19 00:39:16.230: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:39:18.248
STEP: Verifying the service has paired with the endpoint 07/19/23 00:39:18.256
Jul 19 00:39:19.257: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 07/19/23 00:39:19.259
STEP: create a pod that should be updated by the webhook 07/19/23 00:39:19.267
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:19.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3592" for this suite. 07/19/23 00:39:19.309
STEP: Destroying namespace "webhook-3592-markers" for this suite. 07/19/23 00:39:19.312
------------------------------
• [3.464 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:15.851
    Jul 19 00:39:15.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:39:15.851
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:15.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:15.859
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:39:15.867
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:39:16.214
    STEP: Deploying the webhook pod 07/19/23 00:39:16.218
    STEP: Wait for the deployment to be ready 07/19/23 00:39:16.224
    Jul 19 00:39:16.230: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:39:18.248
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:39:18.256
    Jul 19 00:39:19.257: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 07/19/23 00:39:19.259
    STEP: create a pod that should be updated by the webhook 07/19/23 00:39:19.267
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:19.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3592" for this suite. 07/19/23 00:39:19.309
    STEP: Destroying namespace "webhook-3592-markers" for this suite. 07/19/23 00:39:19.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:19.315
Jul 19 00:39:19.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename deployment 07/19/23 00:39:19.316
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:19.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:19.33
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jul 19 00:39:19.332: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 19 00:39:19.336: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 19 00:39:24.339: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/19/23 00:39:24.339
Jul 19 00:39:24.339: INFO: Creating deployment "test-rolling-update-deployment"
Jul 19 00:39:24.341: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 19 00:39:24.344: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 19 00:39:26.348: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 19 00:39:26.350: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 19 00:39:26.355: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3433  98dda19b-db50-4315-9b73-a1c80ab4eced 73522 1 2023-07-19 00:39:24 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-19 00:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cd9148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-19 00:39:24 +0000 UTC,LastTransitionTime:2023-07-19 00:39:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-07-19 00:39:25 +0000 UTC,LastTransitionTime:2023-07-19 00:39:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 19 00:39:26.356: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-3433  7a6d8099-999e-4c9a-b6c9-d79dc34dddd6 73512 1 2023-07-19 00:39:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 98dda19b-db50-4315-9b73-a1c80ab4eced 0xc0050f9d97 0xc0050f9d98}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98dda19b-db50-4315-9b73-a1c80ab4eced\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:39:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050f9e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 19 00:39:26.356: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 19 00:39:26.356: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3433  3329db7a-13be-4f67-943f-52e2f2a450f9 73521 2 2023-07-19 00:39:19 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 98dda19b-db50-4315-9b73-a1c80ab4eced 0xc0050f9c67 0xc0050f9c68}] [] [{e2e.test Update apps/v1 2023-07-19 00:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98dda19b-db50-4315-9b73-a1c80ab4eced\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:39:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0050f9d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 00:39:26.359: INFO: Pod "test-rolling-update-deployment-7549d9f46d-g4xdp" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-g4xdp test-rolling-update-deployment-7549d9f46d- deployment-3433  34cf644f-25e7-4f41-977d-0bbeacff3c52 73511 0 2023-07-19 00:39:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:f6a31bf3d7748ffd3e6052cfcd380af9b28873c2dedddb8e772c296d3561f7a2 cni.projectcalico.org/podIP:172.16.166.164/32 cni.projectcalico.org/podIPs:172.16.166.164/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.164"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.164"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 7a6d8099-999e-4c9a-b6c9-d79dc34dddd6 0xc000bb22a7 0xc000bb22a8}] [] [{calico Update v1 2023-07-19 00:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 00:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a6d8099-999e-4c9a-b6c9-d79dc34dddd6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 00:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:39:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-md8vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-md8vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:39:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:39:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:39:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.164,StartTime:2023-07-19 00:39:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:39:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://57a08bee61b8655772af78f7db1665665911fc130d5685031192e6d81c1d2f41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:26.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3433" for this suite. 07/19/23 00:39:26.361
------------------------------
• [SLOW TEST] [7.048 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:19.315
    Jul 19 00:39:19.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename deployment 07/19/23 00:39:19.316
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:19.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:19.33
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jul 19 00:39:19.332: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jul 19 00:39:19.336: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jul 19 00:39:24.339: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/19/23 00:39:24.339
    Jul 19 00:39:24.339: INFO: Creating deployment "test-rolling-update-deployment"
    Jul 19 00:39:24.341: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jul 19 00:39:24.344: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jul 19 00:39:26.348: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jul 19 00:39:26.350: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 19 00:39:26.355: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3433  98dda19b-db50-4315-9b73-a1c80ab4eced 73522 1 2023-07-19 00:39:24 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-19 00:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cd9148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-19 00:39:24 +0000 UTC,LastTransitionTime:2023-07-19 00:39:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-07-19 00:39:25 +0000 UTC,LastTransitionTime:2023-07-19 00:39:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jul 19 00:39:26.356: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-3433  7a6d8099-999e-4c9a-b6c9-d79dc34dddd6 73512 1 2023-07-19 00:39:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 98dda19b-db50-4315-9b73-a1c80ab4eced 0xc0050f9d97 0xc0050f9d98}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98dda19b-db50-4315-9b73-a1c80ab4eced\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:39:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050f9e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 00:39:26.356: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jul 19 00:39:26.356: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3433  3329db7a-13be-4f67-943f-52e2f2a450f9 73521 2 2023-07-19 00:39:19 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 98dda19b-db50-4315-9b73-a1c80ab4eced 0xc0050f9c67 0xc0050f9c68}] [] [{e2e.test Update apps/v1 2023-07-19 00:39:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98dda19b-db50-4315-9b73-a1c80ab4eced\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:39:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0050f9d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 00:39:26.359: INFO: Pod "test-rolling-update-deployment-7549d9f46d-g4xdp" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-g4xdp test-rolling-update-deployment-7549d9f46d- deployment-3433  34cf644f-25e7-4f41-977d-0bbeacff3c52 73511 0 2023-07-19 00:39:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:f6a31bf3d7748ffd3e6052cfcd380af9b28873c2dedddb8e772c296d3561f7a2 cni.projectcalico.org/podIP:172.16.166.164/32 cni.projectcalico.org/podIPs:172.16.166.164/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.164"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.164"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 7a6d8099-999e-4c9a-b6c9-d79dc34dddd6 0xc000bb22a7 0xc000bb22a8}] [] [{calico Update v1 2023-07-19 00:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 00:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a6d8099-999e-4c9a-b6c9-d79dc34dddd6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 00:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:39:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-md8vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-md8vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:39:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:39:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:39:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:39:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.164,StartTime:2023-07-19 00:39:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:39:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://57a08bee61b8655772af78f7db1665665911fc130d5685031192e6d81c1d2f41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:26.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3433" for this suite. 07/19/23 00:39:26.361
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:26.364
Jul 19 00:39:26.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename daemonsets 07/19/23 00:39:26.365
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:26.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:26.377
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jul 19 00:39:26.389: INFO: Create a RollingUpdate DaemonSet
Jul 19 00:39:26.391: INFO: Check that daemon pods launch on every node of the cluster
Jul 19 00:39:26.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:39:26.395: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:39:27.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:39:27.400: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:39:28.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 19 00:39:28.400: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jul 19 00:39:28.400: INFO: Update the DaemonSet to trigger a rollout
Jul 19 00:39:28.405: INFO: Updating DaemonSet daemon-set
Jul 19 00:39:31.415: INFO: Roll back the DaemonSet before rollout is complete
Jul 19 00:39:31.420: INFO: Updating DaemonSet daemon-set
Jul 19 00:39:31.420: INFO: Make sure DaemonSet rollback is complete
Jul 19 00:39:31.422: INFO: Wrong image for pod: daemon-set-lrxhz. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jul 19 00:39:31.422: INFO: Pod daemon-set-lrxhz is not available
Jul 19 00:39:33.429: INFO: Pod daemon-set-nfrnf is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:39:33.435
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7055, will wait for the garbage collector to delete the pods 07/19/23 00:39:33.435
Jul 19 00:39:33.491: INFO: Deleting DaemonSet.extensions daemon-set took: 3.605319ms
Jul 19 00:39:33.592: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.447251ms
Jul 19 00:39:35.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:39:35.194: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 19 00:39:35.195: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"73723"},"items":null}

Jul 19 00:39:35.196: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"73723"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:35.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7055" for this suite. 07/19/23 00:39:35.203
------------------------------
• [SLOW TEST] [8.842 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:26.364
    Jul 19 00:39:26.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename daemonsets 07/19/23 00:39:26.365
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:26.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:26.377
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jul 19 00:39:26.389: INFO: Create a RollingUpdate DaemonSet
    Jul 19 00:39:26.391: INFO: Check that daemon pods launch on every node of the cluster
    Jul 19 00:39:26.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:39:26.395: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:39:27.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:39:27.400: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:39:28.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 19 00:39:28.400: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jul 19 00:39:28.400: INFO: Update the DaemonSet to trigger a rollout
    Jul 19 00:39:28.405: INFO: Updating DaemonSet daemon-set
    Jul 19 00:39:31.415: INFO: Roll back the DaemonSet before rollout is complete
    Jul 19 00:39:31.420: INFO: Updating DaemonSet daemon-set
    Jul 19 00:39:31.420: INFO: Make sure DaemonSet rollback is complete
    Jul 19 00:39:31.422: INFO: Wrong image for pod: daemon-set-lrxhz. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jul 19 00:39:31.422: INFO: Pod daemon-set-lrxhz is not available
    Jul 19 00:39:33.429: INFO: Pod daemon-set-nfrnf is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:39:33.435
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7055, will wait for the garbage collector to delete the pods 07/19/23 00:39:33.435
    Jul 19 00:39:33.491: INFO: Deleting DaemonSet.extensions daemon-set took: 3.605319ms
    Jul 19 00:39:33.592: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.447251ms
    Jul 19 00:39:35.194: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:39:35.194: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 19 00:39:35.195: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"73723"},"items":null}

    Jul 19 00:39:35.196: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"73723"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:35.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7055" for this suite. 07/19/23 00:39:35.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:35.206
Jul 19 00:39:35.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:39:35.207
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:35.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:35.227
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:35.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5257" for this suite. 07/19/23 00:39:35.232
------------------------------
• [0.029 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:35.206
    Jul 19 00:39:35.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:39:35.207
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:35.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:35.227
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:35.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5257" for this suite. 07/19/23 00:39:35.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:35.235
Jul 19 00:39:35.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-lifecycle-hook 07/19/23 00:39:35.236
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:35.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:35.25
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 07/19/23 00:39:35.254
Jul 19 00:39:35.259: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6882" to be "running and ready"
Jul 19 00:39:35.260: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.474807ms
Jul 19 00:39:35.260: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:39:37.263: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003914644s
Jul 19 00:39:37.263: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jul 19 00:39:37.263: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 07/19/23 00:39:37.264
Jul 19 00:39:37.267: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6882" to be "running and ready"
Jul 19 00:39:37.268: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.299965ms
Jul 19 00:39:37.268: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:39:39.272: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004748843s
Jul 19 00:39:39.272: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jul 19 00:39:39.272: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 07/19/23 00:39:39.273
Jul 19 00:39:39.276: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 00:39:39.280: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 19 00:39:41.280: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 19 00:39:41.282: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 07/19/23 00:39:41.282
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:41.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6882" for this suite. 07/19/23 00:39:41.287
------------------------------
• [SLOW TEST] [6.054 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:35.235
    Jul 19 00:39:35.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-lifecycle-hook 07/19/23 00:39:35.236
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:35.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:35.25
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 07/19/23 00:39:35.254
    Jul 19 00:39:35.259: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6882" to be "running and ready"
    Jul 19 00:39:35.260: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.474807ms
    Jul 19 00:39:35.260: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:39:37.263: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.003914644s
    Jul 19 00:39:37.263: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jul 19 00:39:37.263: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 07/19/23 00:39:37.264
    Jul 19 00:39:37.267: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6882" to be "running and ready"
    Jul 19 00:39:37.268: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.299965ms
    Jul 19 00:39:37.268: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:39:39.272: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004748843s
    Jul 19 00:39:39.272: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jul 19 00:39:39.272: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 07/19/23 00:39:39.273
    Jul 19 00:39:39.276: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jul 19 00:39:39.280: INFO: Pod pod-with-prestop-exec-hook still exists
    Jul 19 00:39:41.280: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jul 19 00:39:41.282: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 07/19/23 00:39:41.282
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:41.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6882" for this suite. 07/19/23 00:39:41.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:41.29
Jul 19 00:39:41.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 00:39:41.291
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:41.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:41.303
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-c761f2e8-e296-462f-877f-1c700fa6247e 07/19/23 00:39:41.304
STEP: Creating a pod to test consume secrets 07/19/23 00:39:41.306
Jul 19 00:39:41.310: INFO: Waiting up to 5m0s for pod "pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554" in namespace "secrets-7688" to be "Succeeded or Failed"
Jul 19 00:39:41.312: INFO: Pod "pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554": Phase="Pending", Reason="", readiness=false. Elapsed: 1.462574ms
Jul 19 00:39:43.314: INFO: Pod "pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003859394s
Jul 19 00:39:45.315: INFO: Pod "pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004952515s
STEP: Saw pod success 07/19/23 00:39:45.315
Jul 19 00:39:45.315: INFO: Pod "pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554" satisfied condition "Succeeded or Failed"
Jul 19 00:39:45.317: INFO: Trying to get logs from node controller-0 pod pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554 container secret-volume-test: <nil>
STEP: delete the pod 07/19/23 00:39:45.327
Jul 19 00:39:45.334: INFO: Waiting for pod pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554 to disappear
Jul 19 00:39:45.335: INFO: Pod pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:45.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7688" for this suite. 07/19/23 00:39:45.338
------------------------------
• [4.050 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:41.29
    Jul 19 00:39:41.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 00:39:41.291
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:41.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:41.303
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-c761f2e8-e296-462f-877f-1c700fa6247e 07/19/23 00:39:41.304
    STEP: Creating a pod to test consume secrets 07/19/23 00:39:41.306
    Jul 19 00:39:41.310: INFO: Waiting up to 5m0s for pod "pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554" in namespace "secrets-7688" to be "Succeeded or Failed"
    Jul 19 00:39:41.312: INFO: Pod "pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554": Phase="Pending", Reason="", readiness=false. Elapsed: 1.462574ms
    Jul 19 00:39:43.314: INFO: Pod "pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003859394s
    Jul 19 00:39:45.315: INFO: Pod "pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004952515s
    STEP: Saw pod success 07/19/23 00:39:45.315
    Jul 19 00:39:45.315: INFO: Pod "pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554" satisfied condition "Succeeded or Failed"
    Jul 19 00:39:45.317: INFO: Trying to get logs from node controller-0 pod pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554 container secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:39:45.327
    Jul 19 00:39:45.334: INFO: Waiting for pod pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554 to disappear
    Jul 19 00:39:45.335: INFO: Pod pod-secrets-8b2ef680-3dc5-47f1-a8dd-5d74eb927554 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:45.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7688" for this suite. 07/19/23 00:39:45.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:45.341
Jul 19 00:39:45.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename containers 07/19/23 00:39:45.341
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:45.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:45.354
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 07/19/23 00:39:45.356
Jul 19 00:39:45.363: INFO: Waiting up to 5m0s for pod "client-containers-359230d9-e357-4fb6-a267-5761cc8194a0" in namespace "containers-2867" to be "Succeeded or Failed"
Jul 19 00:39:45.365: INFO: Pod "client-containers-359230d9-e357-4fb6-a267-5761cc8194a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.373301ms
Jul 19 00:39:47.367: INFO: Pod "client-containers-359230d9-e357-4fb6-a267-5761cc8194a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00352108s
Jul 19 00:39:49.367: INFO: Pod "client-containers-359230d9-e357-4fb6-a267-5761cc8194a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00416577s
STEP: Saw pod success 07/19/23 00:39:49.367
Jul 19 00:39:49.368: INFO: Pod "client-containers-359230d9-e357-4fb6-a267-5761cc8194a0" satisfied condition "Succeeded or Failed"
Jul 19 00:39:49.369: INFO: Trying to get logs from node controller-0 pod client-containers-359230d9-e357-4fb6-a267-5761cc8194a0 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:39:49.372
Jul 19 00:39:49.378: INFO: Waiting for pod client-containers-359230d9-e357-4fb6-a267-5761cc8194a0 to disappear
Jul 19 00:39:49.381: INFO: Pod client-containers-359230d9-e357-4fb6-a267-5761cc8194a0 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:49.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2867" for this suite. 07/19/23 00:39:49.382
------------------------------
• [4.044 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:45.341
    Jul 19 00:39:45.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename containers 07/19/23 00:39:45.341
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:45.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:45.354
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 07/19/23 00:39:45.356
    Jul 19 00:39:45.363: INFO: Waiting up to 5m0s for pod "client-containers-359230d9-e357-4fb6-a267-5761cc8194a0" in namespace "containers-2867" to be "Succeeded or Failed"
    Jul 19 00:39:45.365: INFO: Pod "client-containers-359230d9-e357-4fb6-a267-5761cc8194a0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.373301ms
    Jul 19 00:39:47.367: INFO: Pod "client-containers-359230d9-e357-4fb6-a267-5761cc8194a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00352108s
    Jul 19 00:39:49.367: INFO: Pod "client-containers-359230d9-e357-4fb6-a267-5761cc8194a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00416577s
    STEP: Saw pod success 07/19/23 00:39:49.367
    Jul 19 00:39:49.368: INFO: Pod "client-containers-359230d9-e357-4fb6-a267-5761cc8194a0" satisfied condition "Succeeded or Failed"
    Jul 19 00:39:49.369: INFO: Trying to get logs from node controller-0 pod client-containers-359230d9-e357-4fb6-a267-5761cc8194a0 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:39:49.372
    Jul 19 00:39:49.378: INFO: Waiting for pod client-containers-359230d9-e357-4fb6-a267-5761cc8194a0 to disappear
    Jul 19 00:39:49.381: INFO: Pod client-containers-359230d9-e357-4fb6-a267-5761cc8194a0 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:49.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2867" for this suite. 07/19/23 00:39:49.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:49.385
Jul 19 00:39:49.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubelet-test 07/19/23 00:39:49.386
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:49.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:49.394
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jul 19 00:39:49.399: INFO: Waiting up to 5m0s for pod "busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193" in namespace "kubelet-test-6863" to be "running and ready"
Jul 19 00:39:49.400: INFO: Pod "busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193": Phase="Pending", Reason="", readiness=false. Elapsed: 1.238345ms
Jul 19 00:39:49.400: INFO: The phase of Pod busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:39:51.403: INFO: Pod "busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193": Phase="Running", Reason="", readiness=true. Elapsed: 2.004142423s
Jul 19 00:39:51.403: INFO: The phase of Pod busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193 is Running (Ready = true)
Jul 19 00:39:51.403: INFO: Pod "busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:51.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6863" for this suite. 07/19/23 00:39:51.41
------------------------------
• [2.028 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:49.385
    Jul 19 00:39:49.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubelet-test 07/19/23 00:39:49.386
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:49.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:49.394
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jul 19 00:39:49.399: INFO: Waiting up to 5m0s for pod "busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193" in namespace "kubelet-test-6863" to be "running and ready"
    Jul 19 00:39:49.400: INFO: Pod "busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193": Phase="Pending", Reason="", readiness=false. Elapsed: 1.238345ms
    Jul 19 00:39:49.400: INFO: The phase of Pod busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:39:51.403: INFO: Pod "busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193": Phase="Running", Reason="", readiness=true. Elapsed: 2.004142423s
    Jul 19 00:39:51.403: INFO: The phase of Pod busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193 is Running (Ready = true)
    Jul 19 00:39:51.403: INFO: Pod "busybox-scheduling-4b1e8be6-3028-48c6-b935-4d9343045193" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:51.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6863" for this suite. 07/19/23 00:39:51.41
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:51.413
Jul 19 00:39:51.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:39:51.413
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:51.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:51.425
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-19420a7c-afb0-47e9-80bf-97c65e38716a 07/19/23 00:39:51.427
STEP: Creating a pod to test consume configMaps 07/19/23 00:39:51.429
Jul 19 00:39:51.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88" in namespace "configmap-5384" to be "Succeeded or Failed"
Jul 19 00:39:51.434: INFO: Pod "pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.356185ms
Jul 19 00:39:53.437: INFO: Pod "pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003711856s
Jul 19 00:39:55.437: INFO: Pod "pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003523034s
STEP: Saw pod success 07/19/23 00:39:55.437
Jul 19 00:39:55.437: INFO: Pod "pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88" satisfied condition "Succeeded or Failed"
Jul 19 00:39:55.438: INFO: Trying to get logs from node controller-0 pod pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88 container configmap-volume-test: <nil>
STEP: delete the pod 07/19/23 00:39:55.441
Jul 19 00:39:55.448: INFO: Waiting for pod pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88 to disappear
Jul 19 00:39:55.450: INFO: Pod pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:39:55.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5384" for this suite. 07/19/23 00:39:55.452
------------------------------
• [4.042 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:51.413
    Jul 19 00:39:51.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:39:51.413
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:51.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:51.425
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-19420a7c-afb0-47e9-80bf-97c65e38716a 07/19/23 00:39:51.427
    STEP: Creating a pod to test consume configMaps 07/19/23 00:39:51.429
    Jul 19 00:39:51.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88" in namespace "configmap-5384" to be "Succeeded or Failed"
    Jul 19 00:39:51.434: INFO: Pod "pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.356185ms
    Jul 19 00:39:53.437: INFO: Pod "pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003711856s
    Jul 19 00:39:55.437: INFO: Pod "pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003523034s
    STEP: Saw pod success 07/19/23 00:39:55.437
    Jul 19 00:39:55.437: INFO: Pod "pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88" satisfied condition "Succeeded or Failed"
    Jul 19 00:39:55.438: INFO: Trying to get logs from node controller-0 pod pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88 container configmap-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:39:55.441
    Jul 19 00:39:55.448: INFO: Waiting for pod pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88 to disappear
    Jul 19 00:39:55.450: INFO: Pod pod-configmaps-2ddd1596-e847-4627-bb0a-a38472110e88 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:39:55.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5384" for this suite. 07/19/23 00:39:55.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:39:55.455
Jul 19 00:39:55.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:39:55.456
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:55.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:55.467
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:39:55.475
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:39:55.875
STEP: Deploying the webhook pod 07/19/23 00:39:55.878
STEP: Wait for the deployment to be ready 07/19/23 00:39:55.884
Jul 19 00:39:55.889: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 07/19/23 00:39:57.894
STEP: Verifying the service has paired with the endpoint 07/19/23 00:39:57.904
Jul 19 00:39:58.904: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 07/19/23 00:39:58.906
STEP: create a pod that should be denied by the webhook 07/19/23 00:39:58.916
STEP: create a pod that causes the webhook to hang 07/19/23 00:39:58.926
STEP: create a configmap that should be denied by the webhook 07/19/23 00:40:08.93
STEP: create a configmap that should be admitted by the webhook 07/19/23 00:40:08.936
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 07/19/23 00:40:08.941
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 07/19/23 00:40:08.946
STEP: create a namespace that bypass the webhook 07/19/23 00:40:08.949
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 07/19/23 00:40:08.952
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:40:08.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5123" for this suite. 07/19/23 00:40:08.986
STEP: Destroying namespace "webhook-5123-markers" for this suite. 07/19/23 00:40:08.988
------------------------------
• [SLOW TEST] [13.540 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:39:55.455
    Jul 19 00:39:55.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:39:55.456
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:39:55.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:39:55.467
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:39:55.475
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:39:55.875
    STEP: Deploying the webhook pod 07/19/23 00:39:55.878
    STEP: Wait for the deployment to be ready 07/19/23 00:39:55.884
    Jul 19 00:39:55.889: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 07/19/23 00:39:57.894
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:39:57.904
    Jul 19 00:39:58.904: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 07/19/23 00:39:58.906
    STEP: create a pod that should be denied by the webhook 07/19/23 00:39:58.916
    STEP: create a pod that causes the webhook to hang 07/19/23 00:39:58.926
    STEP: create a configmap that should be denied by the webhook 07/19/23 00:40:08.93
    STEP: create a configmap that should be admitted by the webhook 07/19/23 00:40:08.936
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 07/19/23 00:40:08.941
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 07/19/23 00:40:08.946
    STEP: create a namespace that bypass the webhook 07/19/23 00:40:08.949
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 07/19/23 00:40:08.952
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:40:08.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5123" for this suite. 07/19/23 00:40:08.986
    STEP: Destroying namespace "webhook-5123-markers" for this suite. 07/19/23 00:40:08.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:40:08.997
Jul 19 00:40:08.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/19/23 00:40:08.998
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:40:09.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:40:09.009
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jul 19 00:40:09.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: creating the pod 07/19/23 00:40:09.011
STEP: submitting the pod to kubernetes 07/19/23 00:40:09.011
Jul 19 00:40:09.017: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355" in namespace "pods-507" to be "running and ready"
Jul 19 00:40:09.020: INFO: Pod "pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355": Phase="Pending", Reason="", readiness=false. Elapsed: 3.227262ms
Jul 19 00:40:09.020: INFO: The phase of Pod pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:40:11.022: INFO: Pod "pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355": Phase="Running", Reason="", readiness=true. Elapsed: 2.005438274s
Jul 19 00:40:11.022: INFO: The phase of Pod pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355 is Running (Ready = true)
Jul 19 00:40:11.022: INFO: Pod "pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jul 19 00:40:11.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-507" for this suite. 07/19/23 00:40:11.039
------------------------------
• [2.045 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:40:08.997
    Jul 19 00:40:08.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/19/23 00:40:08.998
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:40:09.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:40:09.009
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jul 19 00:40:09.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: creating the pod 07/19/23 00:40:09.011
    STEP: submitting the pod to kubernetes 07/19/23 00:40:09.011
    Jul 19 00:40:09.017: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355" in namespace "pods-507" to be "running and ready"
    Jul 19 00:40:09.020: INFO: Pod "pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355": Phase="Pending", Reason="", readiness=false. Elapsed: 3.227262ms
    Jul 19 00:40:09.020: INFO: The phase of Pod pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:40:11.022: INFO: Pod "pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355": Phase="Running", Reason="", readiness=true. Elapsed: 2.005438274s
    Jul 19 00:40:11.022: INFO: The phase of Pod pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355 is Running (Ready = true)
    Jul 19 00:40:11.022: INFO: Pod "pod-logs-websocket-2b52d13c-c592-4b88-b1e1-2151f115e355" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:40:11.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-507" for this suite. 07/19/23 00:40:11.039
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:40:11.042
Jul 19 00:40:11.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 00:40:11.043
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:40:11.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:40:11.051
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 00:40:11.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1570" for this suite. 07/19/23 00:40:11.08
------------------------------
• [0.040 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:40:11.042
    Jul 19 00:40:11.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 00:40:11.043
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:40:11.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:40:11.051
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:40:11.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1570" for this suite. 07/19/23 00:40:11.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:40:11.083
Jul 19 00:40:11.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:40:11.084
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:40:11.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:40:11.092
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:40:11.099
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:40:11.937
STEP: Deploying the webhook pod 07/19/23 00:40:11.94
STEP: Wait for the deployment to be ready 07/19/23 00:40:11.946
Jul 19 00:40:11.955: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:40:13.961
STEP: Verifying the service has paired with the endpoint 07/19/23 00:40:13.974
Jul 19 00:40:14.975: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 07/19/23 00:40:14.977
STEP: create a configmap that should be updated by the webhook 07/19/23 00:40:14.986
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:40:14.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2284" for this suite. 07/19/23 00:40:15.018
STEP: Destroying namespace "webhook-2284-markers" for this suite. 07/19/23 00:40:15.02
------------------------------
• [3.941 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:40:11.083
    Jul 19 00:40:11.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:40:11.084
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:40:11.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:40:11.092
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:40:11.099
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:40:11.937
    STEP: Deploying the webhook pod 07/19/23 00:40:11.94
    STEP: Wait for the deployment to be ready 07/19/23 00:40:11.946
    Jul 19 00:40:11.955: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:40:13.961
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:40:13.974
    Jul 19 00:40:14.975: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 07/19/23 00:40:14.977
    STEP: create a configmap that should be updated by the webhook 07/19/23 00:40:14.986
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:40:14.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2284" for this suite. 07/19/23 00:40:15.018
    STEP: Destroying namespace "webhook-2284-markers" for this suite. 07/19/23 00:40:15.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:40:15.025
Jul 19 00:40:15.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename taint-multiple-pods 07/19/23 00:40:15.026
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:40:15.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:40:15.034
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jul 19 00:40:15.036: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 00:41:15.067: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jul 19 00:41:15.069: INFO: Starting informer...
STEP: Starting pods... 07/19/23 00:41:15.069
Jul 19 00:41:15.279: INFO: Pod1 is running on controller-1. Tainting Node
Jul 19 00:41:15.484: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9424" to be "running"
Jul 19 00:41:15.486: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.511356ms
Jul 19 00:41:17.489: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004465753s
Jul 19 00:41:17.489: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jul 19 00:41:17.489: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9424" to be "running"
Jul 19 00:41:17.490: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.446926ms
Jul 19 00:41:17.490: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jul 19 00:41:17.490: INFO: Pod2 is running on controller-1. Tainting Node
STEP: Trying to apply a taint on the Node 07/19/23 00:41:17.49
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/19/23 00:41:17.504
STEP: Waiting for Pod1 and Pod2 to be deleted 07/19/23 00:41:17.508
Jul 19 00:41:24.923: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 19 00:41:42.962: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/19/23 00:41:42.97
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:41:42.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-9424" for this suite. 07/19/23 00:41:42.98
------------------------------
• [SLOW TEST] [87.965 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:40:15.025
    Jul 19 00:40:15.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename taint-multiple-pods 07/19/23 00:40:15.026
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:40:15.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:40:15.034
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jul 19 00:40:15.036: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 19 00:41:15.067: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jul 19 00:41:15.069: INFO: Starting informer...
    STEP: Starting pods... 07/19/23 00:41:15.069
    Jul 19 00:41:15.279: INFO: Pod1 is running on controller-1. Tainting Node
    Jul 19 00:41:15.484: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9424" to be "running"
    Jul 19 00:41:15.486: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.511356ms
    Jul 19 00:41:17.489: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004465753s
    Jul 19 00:41:17.489: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jul 19 00:41:17.489: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9424" to be "running"
    Jul 19 00:41:17.490: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.446926ms
    Jul 19 00:41:17.490: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jul 19 00:41:17.490: INFO: Pod2 is running on controller-1. Tainting Node
    STEP: Trying to apply a taint on the Node 07/19/23 00:41:17.49
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/19/23 00:41:17.504
    STEP: Waiting for Pod1 and Pod2 to be deleted 07/19/23 00:41:17.508
    Jul 19 00:41:24.923: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jul 19 00:41:42.962: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 07/19/23 00:41:42.97
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:41:42.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-9424" for this suite. 07/19/23 00:41:42.98
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:41:42.99
Jul 19 00:41:42.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replicaset 07/19/23 00:41:42.991
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:41:43.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:41:43.004
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 07/19/23 00:41:43.006
Jul 19 00:41:43.012: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3554" to be "running and ready"
Jul 19 00:41:43.015: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750779ms
Jul 19 00:41:43.015: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:41:45.017: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004962301s
Jul 19 00:41:45.017: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:41:47.018: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00536659s
Jul 19 00:41:47.018: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:41:49.018: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005257748s
Jul 19 00:41:49.018: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:41:51.018: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005591625s
Jul 19 00:41:51.018: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:41:53.018: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 10.005451712s
Jul 19 00:41:53.018: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jul 19 00:41:53.018: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 07/19/23 00:41:53.02
STEP: Then the orphan pod is adopted 07/19/23 00:41:53.022
STEP: When the matched label of one of its pods change 07/19/23 00:41:54.026
Jul 19 00:41:54.028: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 07/19/23 00:41:54.034
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:41:55.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3554" for this suite. 07/19/23 00:41:55.042
------------------------------
• [SLOW TEST] [12.055 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:41:42.99
    Jul 19 00:41:42.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replicaset 07/19/23 00:41:42.991
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:41:43.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:41:43.004
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 07/19/23 00:41:43.006
    Jul 19 00:41:43.012: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3554" to be "running and ready"
    Jul 19 00:41:43.015: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.750779ms
    Jul 19 00:41:43.015: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:41:45.017: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004962301s
    Jul 19 00:41:45.017: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:41:47.018: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00536659s
    Jul 19 00:41:47.018: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:41:49.018: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005257748s
    Jul 19 00:41:49.018: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:41:51.018: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005591625s
    Jul 19 00:41:51.018: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:41:53.018: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 10.005451712s
    Jul 19 00:41:53.018: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jul 19 00:41:53.018: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 07/19/23 00:41:53.02
    STEP: Then the orphan pod is adopted 07/19/23 00:41:53.022
    STEP: When the matched label of one of its pods change 07/19/23 00:41:54.026
    Jul 19 00:41:54.028: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 07/19/23 00:41:54.034
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:41:55.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3554" for this suite. 07/19/23 00:41:55.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:41:55.045
Jul 19 00:41:55.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:41:55.046
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:41:55.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:41:55.055
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:41:55.057
Jul 19 00:41:55.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769" in namespace "projected-1208" to be "Succeeded or Failed"
Jul 19 00:41:55.066: INFO: Pod "downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769": Phase="Pending", Reason="", readiness=false. Elapsed: 4.524163ms
Jul 19 00:41:57.069: INFO: Pod "downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007322101s
Jul 19 00:41:59.068: INFO: Pod "downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006810325s
STEP: Saw pod success 07/19/23 00:41:59.068
Jul 19 00:41:59.068: INFO: Pod "downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769" satisfied condition "Succeeded or Failed"
Jul 19 00:41:59.070: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769 container client-container: <nil>
STEP: delete the pod 07/19/23 00:41:59.08
Jul 19 00:41:59.091: INFO: Waiting for pod downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769 to disappear
Jul 19 00:41:59.092: INFO: Pod downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 19 00:41:59.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1208" for this suite. 07/19/23 00:41:59.094
------------------------------
• [4.051 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:41:55.045
    Jul 19 00:41:55.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:41:55.046
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:41:55.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:41:55.055
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:41:55.057
    Jul 19 00:41:55.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769" in namespace "projected-1208" to be "Succeeded or Failed"
    Jul 19 00:41:55.066: INFO: Pod "downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769": Phase="Pending", Reason="", readiness=false. Elapsed: 4.524163ms
    Jul 19 00:41:57.069: INFO: Pod "downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007322101s
    Jul 19 00:41:59.068: INFO: Pod "downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006810325s
    STEP: Saw pod success 07/19/23 00:41:59.068
    Jul 19 00:41:59.068: INFO: Pod "downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769" satisfied condition "Succeeded or Failed"
    Jul 19 00:41:59.070: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769 container client-container: <nil>
    STEP: delete the pod 07/19/23 00:41:59.08
    Jul 19 00:41:59.091: INFO: Waiting for pod downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769 to disappear
    Jul 19 00:41:59.092: INFO: Pod downwardapi-volume-d5d174d3-d16d-4e77-8bc9-dbdfcef19769 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:41:59.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1208" for this suite. 07/19/23 00:41:59.094
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:41:59.097
Jul 19 00:41:59.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replicaset 07/19/23 00:41:59.098
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:41:59.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:41:59.107
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 07/19/23 00:41:59.111
STEP: Verify that the required pods have come up. 07/19/23 00:41:59.114
Jul 19 00:41:59.116: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 19 00:42:04.121: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/19/23 00:42:04.121
STEP: Getting /status 07/19/23 00:42:04.121
Jul 19 00:42:04.123: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 07/19/23 00:42:04.123
Jul 19 00:42:04.128: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 07/19/23 00:42:04.128
Jul 19 00:42:04.129: INFO: Observed &ReplicaSet event: ADDED
Jul 19 00:42:04.129: INFO: Observed &ReplicaSet event: MODIFIED
Jul 19 00:42:04.129: INFO: Observed &ReplicaSet event: MODIFIED
Jul 19 00:42:04.129: INFO: Observed &ReplicaSet event: MODIFIED
Jul 19 00:42:04.129: INFO: Found replicaset test-rs in namespace replicaset-9944 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 19 00:42:04.129: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 07/19/23 00:42:04.129
Jul 19 00:42:04.129: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul 19 00:42:04.132: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 07/19/23 00:42:04.132
Jul 19 00:42:04.133: INFO: Observed &ReplicaSet event: ADDED
Jul 19 00:42:04.134: INFO: Observed &ReplicaSet event: MODIFIED
Jul 19 00:42:04.134: INFO: Observed &ReplicaSet event: MODIFIED
Jul 19 00:42:04.134: INFO: Observed &ReplicaSet event: MODIFIED
Jul 19 00:42:04.134: INFO: Observed replicaset test-rs in namespace replicaset-9944 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 19 00:42:04.134: INFO: Observed &ReplicaSet event: MODIFIED
Jul 19 00:42:04.134: INFO: Found replicaset test-rs in namespace replicaset-9944 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jul 19 00:42:04.134: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:42:04.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9944" for this suite. 07/19/23 00:42:04.137
------------------------------
• [SLOW TEST] [5.042 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:41:59.097
    Jul 19 00:41:59.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replicaset 07/19/23 00:41:59.098
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:41:59.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:41:59.107
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 07/19/23 00:41:59.111
    STEP: Verify that the required pods have come up. 07/19/23 00:41:59.114
    Jul 19 00:41:59.116: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jul 19 00:42:04.121: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/19/23 00:42:04.121
    STEP: Getting /status 07/19/23 00:42:04.121
    Jul 19 00:42:04.123: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 07/19/23 00:42:04.123
    Jul 19 00:42:04.128: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 07/19/23 00:42:04.128
    Jul 19 00:42:04.129: INFO: Observed &ReplicaSet event: ADDED
    Jul 19 00:42:04.129: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 19 00:42:04.129: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 19 00:42:04.129: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 19 00:42:04.129: INFO: Found replicaset test-rs in namespace replicaset-9944 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jul 19 00:42:04.129: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 07/19/23 00:42:04.129
    Jul 19 00:42:04.129: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jul 19 00:42:04.132: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 07/19/23 00:42:04.132
    Jul 19 00:42:04.133: INFO: Observed &ReplicaSet event: ADDED
    Jul 19 00:42:04.134: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 19 00:42:04.134: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 19 00:42:04.134: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 19 00:42:04.134: INFO: Observed replicaset test-rs in namespace replicaset-9944 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jul 19 00:42:04.134: INFO: Observed &ReplicaSet event: MODIFIED
    Jul 19 00:42:04.134: INFO: Found replicaset test-rs in namespace replicaset-9944 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jul 19 00:42:04.134: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:42:04.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9944" for this suite. 07/19/23 00:42:04.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:42:04.14
Jul 19 00:42:04.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:42:04.141
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:04.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:04.148
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-d8b95387-7de6-47d8-8583-ebb027520df0 07/19/23 00:42:04.15
STEP: Creating a pod to test consume secrets 07/19/23 00:42:04.152
Jul 19 00:42:04.156: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d" in namespace "projected-2162" to be "Succeeded or Failed"
Jul 19 00:42:04.160: INFO: Pod "pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.773077ms
Jul 19 00:42:06.162: INFO: Pod "pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006398824s
Jul 19 00:42:08.162: INFO: Pod "pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006313521s
STEP: Saw pod success 07/19/23 00:42:08.162
Jul 19 00:42:08.162: INFO: Pod "pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d" satisfied condition "Succeeded or Failed"
Jul 19 00:42:08.164: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d container projected-secret-volume-test: <nil>
STEP: delete the pod 07/19/23 00:42:08.173
Jul 19 00:42:08.182: INFO: Waiting for pod pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d to disappear
Jul 19 00:42:08.183: INFO: Pod pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jul 19 00:42:08.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2162" for this suite. 07/19/23 00:42:08.185
------------------------------
• [4.048 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:42:04.14
    Jul 19 00:42:04.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:42:04.141
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:04.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:04.148
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-d8b95387-7de6-47d8-8583-ebb027520df0 07/19/23 00:42:04.15
    STEP: Creating a pod to test consume secrets 07/19/23 00:42:04.152
    Jul 19 00:42:04.156: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d" in namespace "projected-2162" to be "Succeeded or Failed"
    Jul 19 00:42:04.160: INFO: Pod "pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.773077ms
    Jul 19 00:42:06.162: INFO: Pod "pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006398824s
    Jul 19 00:42:08.162: INFO: Pod "pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006313521s
    STEP: Saw pod success 07/19/23 00:42:08.162
    Jul 19 00:42:08.162: INFO: Pod "pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d" satisfied condition "Succeeded or Failed"
    Jul 19 00:42:08.164: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d container projected-secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:42:08.173
    Jul 19 00:42:08.182: INFO: Waiting for pod pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d to disappear
    Jul 19 00:42:08.183: INFO: Pod pod-projected-secrets-5a9da5fd-feef-479d-84f4-e8c5ea31855d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:42:08.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2162" for this suite. 07/19/23 00:42:08.185
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:42:08.189
Jul 19 00:42:08.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename statefulset 07/19/23 00:42:08.19
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:08.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:08.198
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3283 07/19/23 00:42:08.2
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-3283 07/19/23 00:42:08.202
Jul 19 00:42:08.206: INFO: Found 0 stateful pods, waiting for 1
Jul 19 00:42:18.210: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 07/19/23 00:42:18.213
STEP: updating a scale subresource 07/19/23 00:42:18.215
STEP: verifying the statefulset Spec.Replicas was modified 07/19/23 00:42:18.218
STEP: Patch a scale subresource 07/19/23 00:42:18.222
STEP: verifying the statefulset Spec.Replicas was modified 07/19/23 00:42:18.226
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jul 19 00:42:18.235: INFO: Deleting all statefulset in ns statefulset-3283
Jul 19 00:42:18.239: INFO: Scaling statefulset ss to 0
Jul 19 00:42:28.249: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 00:42:28.251: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:42:28.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3283" for this suite. 07/19/23 00:42:28.288
------------------------------
• [SLOW TEST] [20.108 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:42:08.189
    Jul 19 00:42:08.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename statefulset 07/19/23 00:42:08.19
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:08.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:08.198
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3283 07/19/23 00:42:08.2
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-3283 07/19/23 00:42:08.202
    Jul 19 00:42:08.206: INFO: Found 0 stateful pods, waiting for 1
    Jul 19 00:42:18.210: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 07/19/23 00:42:18.213
    STEP: updating a scale subresource 07/19/23 00:42:18.215
    STEP: verifying the statefulset Spec.Replicas was modified 07/19/23 00:42:18.218
    STEP: Patch a scale subresource 07/19/23 00:42:18.222
    STEP: verifying the statefulset Spec.Replicas was modified 07/19/23 00:42:18.226
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jul 19 00:42:18.235: INFO: Deleting all statefulset in ns statefulset-3283
    Jul 19 00:42:18.239: INFO: Scaling statefulset ss to 0
    Jul 19 00:42:28.249: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 19 00:42:28.251: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:42:28.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3283" for this suite. 07/19/23 00:42:28.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:42:28.297
Jul 19 00:42:28.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-webhook 07/19/23 00:42:28.298
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:28.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:28.315
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 07/19/23 00:42:28.317
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 07/19/23 00:42:28.469
STEP: Deploying the custom resource conversion webhook pod 07/19/23 00:42:28.473
STEP: Wait for the deployment to be ready 07/19/23 00:42:28.479
Jul 19 00:42:28.484: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:42:30.49
STEP: Verifying the service has paired with the endpoint 07/19/23 00:42:30.505
Jul 19 00:42:31.505: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jul 19 00:42:31.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Creating a v1 custom resource 07/19/23 00:42:34.108
STEP: v2 custom resource should be converted 07/19/23 00:42:34.111
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:42:34.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-1351" for this suite. 07/19/23 00:42:34.654
------------------------------
• [SLOW TEST] [6.360 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:42:28.297
    Jul 19 00:42:28.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-webhook 07/19/23 00:42:28.298
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:28.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:28.315
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 07/19/23 00:42:28.317
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 07/19/23 00:42:28.469
    STEP: Deploying the custom resource conversion webhook pod 07/19/23 00:42:28.473
    STEP: Wait for the deployment to be ready 07/19/23 00:42:28.479
    Jul 19 00:42:28.484: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:42:30.49
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:42:30.505
    Jul 19 00:42:31.505: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jul 19 00:42:31.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Creating a v1 custom resource 07/19/23 00:42:34.108
    STEP: v2 custom resource should be converted 07/19/23 00:42:34.111
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:42:34.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-1351" for this suite. 07/19/23 00:42:34.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:42:34.657
Jul 19 00:42:34.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename endpointslicemirroring 07/19/23 00:42:34.657
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:34.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:34.676
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 07/19/23 00:42:34.69
Jul 19 00:42:34.696: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 07/19/23 00:42:36.698
Jul 19 00:42:36.702: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 07/19/23 00:42:38.704
Jul 19 00:42:38.708: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jul 19 00:42:40.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-6045" for this suite. 07/19/23 00:42:40.712
------------------------------
• [SLOW TEST] [6.058 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:42:34.657
    Jul 19 00:42:34.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename endpointslicemirroring 07/19/23 00:42:34.657
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:34.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:34.676
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 07/19/23 00:42:34.69
    Jul 19 00:42:34.696: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 07/19/23 00:42:36.698
    Jul 19 00:42:36.702: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 07/19/23 00:42:38.704
    Jul 19 00:42:38.708: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:42:40.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-6045" for this suite. 07/19/23 00:42:40.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:42:40.716
Jul 19 00:42:40.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename csiinlinevolumes 07/19/23 00:42:40.716
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:40.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:40.727
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 07/19/23 00:42:40.729
STEP: getting 07/19/23 00:42:40.739
STEP: listing in namespace 07/19/23 00:42:40.741
STEP: patching 07/19/23 00:42:40.744
STEP: deleting 07/19/23 00:42:40.756
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:42:40.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-1265" for this suite. 07/19/23 00:42:40.764
------------------------------
• [0.051 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:42:40.716
    Jul 19 00:42:40.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename csiinlinevolumes 07/19/23 00:42:40.716
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:40.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:40.727
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 07/19/23 00:42:40.729
    STEP: getting 07/19/23 00:42:40.739
    STEP: listing in namespace 07/19/23 00:42:40.741
    STEP: patching 07/19/23 00:42:40.744
    STEP: deleting 07/19/23 00:42:40.756
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:42:40.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-1265" for this suite. 07/19/23 00:42:40.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:42:40.767
Jul 19 00:42:40.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sched-preemption 07/19/23 00:42:40.768
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:40.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:40.777
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jul 19 00:42:40.785: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 00:43:40.818: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 07/19/23 00:43:40.82
Jul 19 00:43:40.833: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jul 19 00:43:40.835: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jul 19 00:43:40.848: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jul 19 00:43:40.851: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 07/19/23 00:43:40.851
Jul 19 00:43:40.852: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7254" to be "running"
Jul 19 00:43:40.854: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274791ms
Jul 19 00:43:42.856: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.004484566s
Jul 19 00:43:42.856: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jul 19 00:43:42.856: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7254" to be "running"
Jul 19 00:43:42.857: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.434192ms
Jul 19 00:43:42.857: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jul 19 00:43:42.857: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7254" to be "running"
Jul 19 00:43:42.859: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.470968ms
Jul 19 00:43:42.859: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jul 19 00:43:42.859: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7254" to be "running"
Jul 19 00:43:42.860: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.357078ms
Jul 19 00:43:42.860: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 07/19/23 00:43:42.86
Jul 19 00:43:42.866: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jul 19 00:43:42.869: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250459ms
Jul 19 00:43:44.871: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005182196s
Jul 19 00:43:46.872: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006183702s
Jul 19 00:43:48.872: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006221551s
Jul 19 00:43:48.872: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:43:48.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7254" for this suite. 07/19/23 00:43:48.909
------------------------------
• [SLOW TEST] [68.144 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:42:40.767
    Jul 19 00:42:40.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sched-preemption 07/19/23 00:42:40.768
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:42:40.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:42:40.777
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jul 19 00:42:40.785: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 19 00:43:40.818: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 07/19/23 00:43:40.82
    Jul 19 00:43:40.833: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jul 19 00:43:40.835: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jul 19 00:43:40.848: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jul 19 00:43:40.851: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 07/19/23 00:43:40.851
    Jul 19 00:43:40.852: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7254" to be "running"
    Jul 19 00:43:40.854: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.274791ms
    Jul 19 00:43:42.856: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.004484566s
    Jul 19 00:43:42.856: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jul 19 00:43:42.856: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7254" to be "running"
    Jul 19 00:43:42.857: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.434192ms
    Jul 19 00:43:42.857: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jul 19 00:43:42.857: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7254" to be "running"
    Jul 19 00:43:42.859: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.470968ms
    Jul 19 00:43:42.859: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jul 19 00:43:42.859: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7254" to be "running"
    Jul 19 00:43:42.860: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.357078ms
    Jul 19 00:43:42.860: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 07/19/23 00:43:42.86
    Jul 19 00:43:42.866: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jul 19 00:43:42.869: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.250459ms
    Jul 19 00:43:44.871: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005182196s
    Jul 19 00:43:46.872: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006183702s
    Jul 19 00:43:48.872: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006221551s
    Jul 19 00:43:48.872: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:43:48.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7254" for this suite. 07/19/23 00:43:48.909
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:43:48.912
Jul 19 00:43:48.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubelet-test 07/19/23 00:43:48.913
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:43:48.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:43:48.925
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jul 19 00:43:48.931: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198" in namespace "kubelet-test-301" to be "running and ready"
Jul 19 00:43:48.932: INFO: Pod "busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198": Phase="Pending", Reason="", readiness=false. Elapsed: 1.268117ms
Jul 19 00:43:48.932: INFO: The phase of Pod busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:43:50.935: INFO: Pod "busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198": Phase="Running", Reason="", readiness=true. Elapsed: 2.004340299s
Jul 19 00:43:50.935: INFO: The phase of Pod busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198 is Running (Ready = true)
Jul 19 00:43:50.935: INFO: Pod "busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:43:50.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-301" for this suite. 07/19/23 00:43:50.95
------------------------------
• [2.041 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:43:48.912
    Jul 19 00:43:48.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubelet-test 07/19/23 00:43:48.913
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:43:48.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:43:48.925
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jul 19 00:43:48.931: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198" in namespace "kubelet-test-301" to be "running and ready"
    Jul 19 00:43:48.932: INFO: Pod "busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198": Phase="Pending", Reason="", readiness=false. Elapsed: 1.268117ms
    Jul 19 00:43:48.932: INFO: The phase of Pod busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:43:50.935: INFO: Pod "busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198": Phase="Running", Reason="", readiness=true. Elapsed: 2.004340299s
    Jul 19 00:43:50.935: INFO: The phase of Pod busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198 is Running (Ready = true)
    Jul 19 00:43:50.935: INFO: Pod "busybox-readonly-fsa4326e34-e51f-467c-bf0a-8bea0bc7d198" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:43:50.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-301" for this suite. 07/19/23 00:43:50.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:43:50.953
Jul 19 00:43:50.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename limitrange 07/19/23 00:43:50.954
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:43:50.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:43:50.971
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-hrpw8" in namespace "limitrange-86" 07/19/23 00:43:50.973
STEP: Creating another limitRange in another namespace 07/19/23 00:43:50.975
Jul 19 00:43:50.983: INFO: Namespace "e2e-limitrange-hrpw8-5875" created
Jul 19 00:43:50.983: INFO: Creating LimitRange "e2e-limitrange-hrpw8" in namespace "e2e-limitrange-hrpw8-5875"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-hrpw8" 07/19/23 00:43:50.988
Jul 19 00:43:50.990: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-hrpw8" in "limitrange-86" namespace 07/19/23 00:43:50.99
Jul 19 00:43:50.994: INFO: LimitRange "e2e-limitrange-hrpw8" has been patched
STEP: Delete LimitRange "e2e-limitrange-hrpw8" by Collection with labelSelector: "e2e-limitrange-hrpw8=patched" 07/19/23 00:43:50.994
STEP: Confirm that the limitRange "e2e-limitrange-hrpw8" has been deleted 07/19/23 00:43:50.996
Jul 19 00:43:50.996: INFO: Requesting list of LimitRange to confirm quantity
Jul 19 00:43:50.998: INFO: Found 0 LimitRange with label "e2e-limitrange-hrpw8=patched"
Jul 19 00:43:50.998: INFO: LimitRange "e2e-limitrange-hrpw8" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-hrpw8" 07/19/23 00:43:50.998
Jul 19 00:43:50.999: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jul 19 00:43:50.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-86" for this suite. 07/19/23 00:43:51.001
STEP: Destroying namespace "e2e-limitrange-hrpw8-5875" for this suite. 07/19/23 00:43:51.004
------------------------------
• [0.055 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:43:50.953
    Jul 19 00:43:50.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename limitrange 07/19/23 00:43:50.954
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:43:50.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:43:50.971
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-hrpw8" in namespace "limitrange-86" 07/19/23 00:43:50.973
    STEP: Creating another limitRange in another namespace 07/19/23 00:43:50.975
    Jul 19 00:43:50.983: INFO: Namespace "e2e-limitrange-hrpw8-5875" created
    Jul 19 00:43:50.983: INFO: Creating LimitRange "e2e-limitrange-hrpw8" in namespace "e2e-limitrange-hrpw8-5875"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-hrpw8" 07/19/23 00:43:50.988
    Jul 19 00:43:50.990: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-hrpw8" in "limitrange-86" namespace 07/19/23 00:43:50.99
    Jul 19 00:43:50.994: INFO: LimitRange "e2e-limitrange-hrpw8" has been patched
    STEP: Delete LimitRange "e2e-limitrange-hrpw8" by Collection with labelSelector: "e2e-limitrange-hrpw8=patched" 07/19/23 00:43:50.994
    STEP: Confirm that the limitRange "e2e-limitrange-hrpw8" has been deleted 07/19/23 00:43:50.996
    Jul 19 00:43:50.996: INFO: Requesting list of LimitRange to confirm quantity
    Jul 19 00:43:50.998: INFO: Found 0 LimitRange with label "e2e-limitrange-hrpw8=patched"
    Jul 19 00:43:50.998: INFO: LimitRange "e2e-limitrange-hrpw8" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-hrpw8" 07/19/23 00:43:50.998
    Jul 19 00:43:50.999: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:43:50.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-86" for this suite. 07/19/23 00:43:51.001
    STEP: Destroying namespace "e2e-limitrange-hrpw8-5875" for this suite. 07/19/23 00:43:51.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:43:51.008
Jul 19 00:43:51.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:43:51.009
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:43:51.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:43:51.016
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-a2f6fada-8b14-4871-8a47-a0cb16f141bb 07/19/23 00:43:51.018
STEP: Creating a pod to test consume secrets 07/19/23 00:43:51.02
Jul 19 00:43:51.024: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8" in namespace "projected-358" to be "Succeeded or Failed"
Jul 19 00:43:51.026: INFO: Pod "pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.71858ms
Jul 19 00:43:53.029: INFO: Pod "pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004878421s
Jul 19 00:43:55.031: INFO: Pod "pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007115696s
STEP: Saw pod success 07/19/23 00:43:55.031
Jul 19 00:43:55.031: INFO: Pod "pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8" satisfied condition "Succeeded or Failed"
Jul 19 00:43:55.032: INFO: Trying to get logs from node controller-0 pod pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8 container secret-volume-test: <nil>
STEP: delete the pod 07/19/23 00:43:55.043
Jul 19 00:43:55.050: INFO: Waiting for pod pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8 to disappear
Jul 19 00:43:55.051: INFO: Pod pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jul 19 00:43:55.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-358" for this suite. 07/19/23 00:43:55.053
------------------------------
• [4.047 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:43:51.008
    Jul 19 00:43:51.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:43:51.009
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:43:51.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:43:51.016
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-a2f6fada-8b14-4871-8a47-a0cb16f141bb 07/19/23 00:43:51.018
    STEP: Creating a pod to test consume secrets 07/19/23 00:43:51.02
    Jul 19 00:43:51.024: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8" in namespace "projected-358" to be "Succeeded or Failed"
    Jul 19 00:43:51.026: INFO: Pod "pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.71858ms
    Jul 19 00:43:53.029: INFO: Pod "pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004878421s
    Jul 19 00:43:55.031: INFO: Pod "pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007115696s
    STEP: Saw pod success 07/19/23 00:43:55.031
    Jul 19 00:43:55.031: INFO: Pod "pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8" satisfied condition "Succeeded or Failed"
    Jul 19 00:43:55.032: INFO: Trying to get logs from node controller-0 pod pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8 container secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:43:55.043
    Jul 19 00:43:55.050: INFO: Waiting for pod pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8 to disappear
    Jul 19 00:43:55.051: INFO: Pod pod-projected-secrets-f17d2a51-ae6c-4009-a30a-a474c900aea8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:43:55.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-358" for this suite. 07/19/23 00:43:55.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:43:55.057
Jul 19 00:43:55.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:43:55.057
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:43:55.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:43:55.068
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 07/19/23 00:43:55.07
Jul 19 00:43:55.074: INFO: Waiting up to 5m0s for pod "pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67" in namespace "emptydir-8621" to be "Succeeded or Failed"
Jul 19 00:43:55.075: INFO: Pod "pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67": Phase="Pending", Reason="", readiness=false. Elapsed: 1.275936ms
Jul 19 00:43:57.077: INFO: Pod "pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003509108s
Jul 19 00:43:59.077: INFO: Pod "pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003480952s
STEP: Saw pod success 07/19/23 00:43:59.077
Jul 19 00:43:59.077: INFO: Pod "pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67" satisfied condition "Succeeded or Failed"
Jul 19 00:43:59.079: INFO: Trying to get logs from node controller-0 pod pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67 container test-container: <nil>
STEP: delete the pod 07/19/23 00:43:59.082
Jul 19 00:43:59.090: INFO: Waiting for pod pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67 to disappear
Jul 19 00:43:59.091: INFO: Pod pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:43:59.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8621" for this suite. 07/19/23 00:43:59.093
------------------------------
• [4.039 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:43:55.057
    Jul 19 00:43:55.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:43:55.057
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:43:55.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:43:55.068
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 07/19/23 00:43:55.07
    Jul 19 00:43:55.074: INFO: Waiting up to 5m0s for pod "pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67" in namespace "emptydir-8621" to be "Succeeded or Failed"
    Jul 19 00:43:55.075: INFO: Pod "pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67": Phase="Pending", Reason="", readiness=false. Elapsed: 1.275936ms
    Jul 19 00:43:57.077: INFO: Pod "pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003509108s
    Jul 19 00:43:59.077: INFO: Pod "pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003480952s
    STEP: Saw pod success 07/19/23 00:43:59.077
    Jul 19 00:43:59.077: INFO: Pod "pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67" satisfied condition "Succeeded or Failed"
    Jul 19 00:43:59.079: INFO: Trying to get logs from node controller-0 pod pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67 container test-container: <nil>
    STEP: delete the pod 07/19/23 00:43:59.082
    Jul 19 00:43:59.090: INFO: Waiting for pod pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67 to disappear
    Jul 19 00:43:59.091: INFO: Pod pod-d1e2e4d6-f880-493a-9cfe-c7966904fe67 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:43:59.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8621" for this suite. 07/19/23 00:43:59.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:43:59.096
Jul 19 00:43:59.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:43:59.097
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:43:59.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:43:59.134
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 07/19/23 00:43:59.136
Jul 19 00:43:59.139: INFO: Waiting up to 5m0s for pod "pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa" in namespace "emptydir-2148" to be "Succeeded or Failed"
Jul 19 00:43:59.142: INFO: Pod "pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871321ms
Jul 19 00:44:01.145: INFO: Pod "pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005546649s
Jul 19 00:44:03.145: INFO: Pod "pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006461882s
STEP: Saw pod success 07/19/23 00:44:03.146
Jul 19 00:44:03.146: INFO: Pod "pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa" satisfied condition "Succeeded or Failed"
Jul 19 00:44:03.147: INFO: Trying to get logs from node controller-0 pod pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa container test-container: <nil>
STEP: delete the pod 07/19/23 00:44:03.151
Jul 19 00:44:03.157: INFO: Waiting for pod pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa to disappear
Jul 19 00:44:03.159: INFO: Pod pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:44:03.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2148" for this suite. 07/19/23 00:44:03.161
------------------------------
• [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:43:59.096
    Jul 19 00:43:59.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:43:59.097
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:43:59.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:43:59.134
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 07/19/23 00:43:59.136
    Jul 19 00:43:59.139: INFO: Waiting up to 5m0s for pod "pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa" in namespace "emptydir-2148" to be "Succeeded or Failed"
    Jul 19 00:43:59.142: INFO: Pod "pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.871321ms
    Jul 19 00:44:01.145: INFO: Pod "pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005546649s
    Jul 19 00:44:03.145: INFO: Pod "pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006461882s
    STEP: Saw pod success 07/19/23 00:44:03.146
    Jul 19 00:44:03.146: INFO: Pod "pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa" satisfied condition "Succeeded or Failed"
    Jul 19 00:44:03.147: INFO: Trying to get logs from node controller-0 pod pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa container test-container: <nil>
    STEP: delete the pod 07/19/23 00:44:03.151
    Jul 19 00:44:03.157: INFO: Waiting for pod pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa to disappear
    Jul 19 00:44:03.159: INFO: Pod pod-2be2a0b6-5458-480c-b0dc-57e07f2021fa no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:44:03.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2148" for this suite. 07/19/23 00:44:03.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:44:03.165
Jul 19 00:44:03.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename deployment 07/19/23 00:44:03.166
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:44:03.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:44:03.176
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 07/19/23 00:44:03.179
Jul 19 00:44:03.179: INFO: Creating simple deployment test-deployment-s5h22
Jul 19 00:44:03.186: INFO: deployment "test-deployment-s5h22" doesn't have the required revision set
STEP: Getting /status 07/19/23 00:44:05.193
Jul 19 00:44:05.195: INFO: Deployment test-deployment-s5h22 has Conditions: [{Available True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s5h22-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 07/19/23 00:44:05.196
Jul 19 00:44:05.200: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 44, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 44, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 44, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 44, 3, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-s5h22-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 07/19/23 00:44:05.2
Jul 19 00:44:05.201: INFO: Observed &Deployment event: ADDED
Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s5h22-54bc444df"}
Jul 19 00:44:05.201: INFO: Observed &Deployment event: MODIFIED
Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s5h22-54bc444df"}
Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 19 00:44:05.201: INFO: Observed &Deployment event: MODIFIED
Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-s5h22-54bc444df" is progressing.}
Jul 19 00:44:05.201: INFO: Observed &Deployment event: MODIFIED
Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s5h22-54bc444df" has successfully progressed.}
Jul 19 00:44:05.201: INFO: Observed &Deployment event: MODIFIED
Jul 19 00:44:05.202: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 19 00:44:05.202: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s5h22-54bc444df" has successfully progressed.}
Jul 19 00:44:05.202: INFO: Found Deployment test-deployment-s5h22 in namespace deployment-9955 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 19 00:44:05.202: INFO: Deployment test-deployment-s5h22 has an updated status
STEP: patching the Statefulset Status 07/19/23 00:44:05.202
Jul 19 00:44:05.202: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul 19 00:44:05.205: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 07/19/23 00:44:05.205
Jul 19 00:44:05.206: INFO: Observed &Deployment event: ADDED
Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s5h22-54bc444df"}
Jul 19 00:44:05.206: INFO: Observed &Deployment event: MODIFIED
Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s5h22-54bc444df"}
Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 19 00:44:05.206: INFO: Observed &Deployment event: MODIFIED
Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-s5h22-54bc444df" is progressing.}
Jul 19 00:44:05.206: INFO: Observed &Deployment event: MODIFIED
Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s5h22-54bc444df" has successfully progressed.}
Jul 19 00:44:05.207: INFO: Observed &Deployment event: MODIFIED
Jul 19 00:44:05.207: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 19 00:44:05.207: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s5h22-54bc444df" has successfully progressed.}
Jul 19 00:44:05.207: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 19 00:44:05.207: INFO: Observed &Deployment event: MODIFIED
Jul 19 00:44:05.207: INFO: Found deployment test-deployment-s5h22 in namespace deployment-9955 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jul 19 00:44:05.207: INFO: Deployment test-deployment-s5h22 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 19 00:44:05.209: INFO: Deployment "test-deployment-s5h22":
&Deployment{ObjectMeta:{test-deployment-s5h22  deployment-9955  21bf2a68-22bf-4ef9-a523-6d06df508e36 76908 1 2023-07-19 00:44:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-19 00:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-07-19 00:44:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00325e9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 19 00:44:05.213: INFO: New ReplicaSet "test-deployment-s5h22-54bc444df" of Deployment "test-deployment-s5h22":
&ReplicaSet{ObjectMeta:{test-deployment-s5h22-54bc444df  deployment-9955  941e0c58-05c4-4fd1-9343-a01913c55460 76894 1 2023-07-19 00:44:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-s5h22 21bf2a68-22bf-4ef9-a523-6d06df508e36 0xc00325ed50 0xc00325ed51}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21bf2a68-22bf-4ef9-a523-6d06df508e36\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:44:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00325edf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 19 00:44:05.215: INFO: Pod "test-deployment-s5h22-54bc444df-sxlz4" is available:
&Pod{ObjectMeta:{test-deployment-s5h22-54bc444df-sxlz4 test-deployment-s5h22-54bc444df- deployment-9955  a588cbda-a158-4d10-951b-12ca083071c0 76893 0 2023-07-19 00:44:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:fb1497c0c4404c4301a45e03bf64e2524db62a2b17b0e496bf1131b9cf51a948 cni.projectcalico.org/podIP:172.16.192.112/32 cni.projectcalico.org/podIPs:172.16.192.112/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.112"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.112"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-s5h22-54bc444df 941e0c58-05c4-4fd1-9343-a01913c55460 0xc003498f70 0xc003498f71}] [] [{calico Update v1 2023-07-19 00:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 00:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"941e0c58-05c4-4fd1-9343-a01913c55460\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 00:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:44:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5vsgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5vsgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:44:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:44:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:44:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:44:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.112,StartTime:2023-07-19 00:44:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:44:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8c156d36914eb8bd2a2de0d009472cdc88632cf2bf0c14ffba5c1a4429ffd488,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jul 19 00:44:05.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9955" for this suite. 07/19/23 00:44:05.218
------------------------------
• [2.055 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:44:03.165
    Jul 19 00:44:03.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename deployment 07/19/23 00:44:03.166
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:44:03.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:44:03.176
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 07/19/23 00:44:03.179
    Jul 19 00:44:03.179: INFO: Creating simple deployment test-deployment-s5h22
    Jul 19 00:44:03.186: INFO: deployment "test-deployment-s5h22" doesn't have the required revision set
    STEP: Getting /status 07/19/23 00:44:05.193
    Jul 19 00:44:05.195: INFO: Deployment test-deployment-s5h22 has Conditions: [{Available True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s5h22-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 07/19/23 00:44:05.196
    Jul 19 00:44:05.200: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 44, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 44, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 0, 44, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 0, 44, 3, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-s5h22-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 07/19/23 00:44:05.2
    Jul 19 00:44:05.201: INFO: Observed &Deployment event: ADDED
    Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s5h22-54bc444df"}
    Jul 19 00:44:05.201: INFO: Observed &Deployment event: MODIFIED
    Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s5h22-54bc444df"}
    Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jul 19 00:44:05.201: INFO: Observed &Deployment event: MODIFIED
    Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-s5h22-54bc444df" is progressing.}
    Jul 19 00:44:05.201: INFO: Observed &Deployment event: MODIFIED
    Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jul 19 00:44:05.201: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s5h22-54bc444df" has successfully progressed.}
    Jul 19 00:44:05.201: INFO: Observed &Deployment event: MODIFIED
    Jul 19 00:44:05.202: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jul 19 00:44:05.202: INFO: Observed Deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s5h22-54bc444df" has successfully progressed.}
    Jul 19 00:44:05.202: INFO: Found Deployment test-deployment-s5h22 in namespace deployment-9955 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jul 19 00:44:05.202: INFO: Deployment test-deployment-s5h22 has an updated status
    STEP: patching the Statefulset Status 07/19/23 00:44:05.202
    Jul 19 00:44:05.202: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jul 19 00:44:05.205: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 07/19/23 00:44:05.205
    Jul 19 00:44:05.206: INFO: Observed &Deployment event: ADDED
    Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s5h22-54bc444df"}
    Jul 19 00:44:05.206: INFO: Observed &Deployment event: MODIFIED
    Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-s5h22-54bc444df"}
    Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jul 19 00:44:05.206: INFO: Observed &Deployment event: MODIFIED
    Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:03 +0000 UTC 2023-07-19 00:44:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-s5h22-54bc444df" is progressing.}
    Jul 19 00:44:05.206: INFO: Observed &Deployment event: MODIFIED
    Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jul 19 00:44:05.206: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s5h22-54bc444df" has successfully progressed.}
    Jul 19 00:44:05.207: INFO: Observed &Deployment event: MODIFIED
    Jul 19 00:44:05.207: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jul 19 00:44:05.207: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-19 00:44:04 +0000 UTC 2023-07-19 00:44:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-s5h22-54bc444df" has successfully progressed.}
    Jul 19 00:44:05.207: INFO: Observed deployment test-deployment-s5h22 in namespace deployment-9955 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jul 19 00:44:05.207: INFO: Observed &Deployment event: MODIFIED
    Jul 19 00:44:05.207: INFO: Found deployment test-deployment-s5h22 in namespace deployment-9955 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jul 19 00:44:05.207: INFO: Deployment test-deployment-s5h22 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 19 00:44:05.209: INFO: Deployment "test-deployment-s5h22":
    &Deployment{ObjectMeta:{test-deployment-s5h22  deployment-9955  21bf2a68-22bf-4ef9-a523-6d06df508e36 76908 1 2023-07-19 00:44:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-19 00:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:44:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-07-19 00:44:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00325e9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jul 19 00:44:05.213: INFO: New ReplicaSet "test-deployment-s5h22-54bc444df" of Deployment "test-deployment-s5h22":
    &ReplicaSet{ObjectMeta:{test-deployment-s5h22-54bc444df  deployment-9955  941e0c58-05c4-4fd1-9343-a01913c55460 76894 1 2023-07-19 00:44:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-s5h22 21bf2a68-22bf-4ef9-a523-6d06df508e36 0xc00325ed50 0xc00325ed51}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21bf2a68-22bf-4ef9-a523-6d06df508e36\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:44:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00325edf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 00:44:05.215: INFO: Pod "test-deployment-s5h22-54bc444df-sxlz4" is available:
    &Pod{ObjectMeta:{test-deployment-s5h22-54bc444df-sxlz4 test-deployment-s5h22-54bc444df- deployment-9955  a588cbda-a158-4d10-951b-12ca083071c0 76893 0 2023-07-19 00:44:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:fb1497c0c4404c4301a45e03bf64e2524db62a2b17b0e496bf1131b9cf51a948 cni.projectcalico.org/podIP:172.16.192.112/32 cni.projectcalico.org/podIPs:172.16.192.112/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.112"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.112"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-s5h22-54bc444df 941e0c58-05c4-4fd1-9343-a01913c55460 0xc003498f70 0xc003498f71}] [] [{calico Update v1 2023-07-19 00:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 00:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"941e0c58-05c4-4fd1-9343-a01913c55460\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 00:44:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 00:44:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5vsgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5vsgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:44:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:44:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:44:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:44:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.112,StartTime:2023-07-19 00:44:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 00:44:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8c156d36914eb8bd2a2de0d009472cdc88632cf2bf0c14ffba5c1a4429ffd488,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:44:05.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9955" for this suite. 07/19/23 00:44:05.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:44:05.221
Jul 19 00:44:05.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sched-preemption 07/19/23 00:44:05.222
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:44:05.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:44:05.231
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jul 19 00:44:05.239: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 00:45:05.268: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:45:05.269
Jul 19 00:45:05.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sched-preemption-path 07/19/23 00:45:05.27
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:05.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:05.281
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Jul 19 00:45:05.290: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jul 19 00:45:05.292: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jul 19 00:45:05.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:45:05.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8918" for this suite. 07/19/23 00:45:05.328
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7734" for this suite. 07/19/23 00:45:05.332
------------------------------
• [SLOW TEST] [60.113 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:44:05.221
    Jul 19 00:44:05.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sched-preemption 07/19/23 00:44:05.222
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:44:05.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:44:05.231
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jul 19 00:44:05.239: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 19 00:45:05.268: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:45:05.269
    Jul 19 00:45:05.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sched-preemption-path 07/19/23 00:45:05.27
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:05.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:05.281
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Jul 19 00:45:05.290: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jul 19 00:45:05.292: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:45:05.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:45:05.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8918" for this suite. 07/19/23 00:45:05.328
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7734" for this suite. 07/19/23 00:45:05.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:45:05.335
Jul 19 00:45:05.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename custom-resource-definition 07/19/23 00:45:05.336
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:05.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:05.347
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jul 19 00:45:05.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:45:08.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2277" for this suite. 07/19/23 00:45:08.509
------------------------------
• [3.177 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:45:05.335
    Jul 19 00:45:05.335: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename custom-resource-definition 07/19/23 00:45:05.336
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:05.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:05.347
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jul 19 00:45:05.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:45:08.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2277" for this suite. 07/19/23 00:45:08.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:45:08.513
Jul 19 00:45:08.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:45:08.514
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:08.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:08.526
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 07/19/23 00:45:08.529
STEP: watching for the Service to be added 07/19/23 00:45:08.537
Jul 19 00:45:08.538: INFO: Found Service test-service-tzl4v in namespace services-2594 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jul 19 00:45:08.538: INFO: Service test-service-tzl4v created
STEP: Getting /status 07/19/23 00:45:08.538
Jul 19 00:45:08.540: INFO: Service test-service-tzl4v has LoadBalancer: {[]}
STEP: patching the ServiceStatus 07/19/23 00:45:08.54
STEP: watching for the Service to be patched 07/19/23 00:45:08.543
Jul 19 00:45:08.544: INFO: observed Service test-service-tzl4v in namespace services-2594 with annotations: map[] & LoadBalancer: {[]}
Jul 19 00:45:08.544: INFO: Found Service test-service-tzl4v in namespace services-2594 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jul 19 00:45:08.544: INFO: Service test-service-tzl4v has service status patched
STEP: updating the ServiceStatus 07/19/23 00:45:08.544
Jul 19 00:45:08.548: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 07/19/23 00:45:08.549
Jul 19 00:45:08.549: INFO: Observed Service test-service-tzl4v in namespace services-2594 with annotations: map[] & Conditions: {[]}
Jul 19 00:45:08.549: INFO: Observed event: &Service{ObjectMeta:{test-service-tzl4v  services-2594  a008cc68-27ba-4571-adb9-b251a67d2072 77395 0 2023-07-19 00:45:08 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-19 00:45:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-19 00:45:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.101.236.215,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.101.236.215],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jul 19 00:45:08.549: INFO: Found Service test-service-tzl4v in namespace services-2594 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 19 00:45:08.550: INFO: Service test-service-tzl4v has service status updated
STEP: patching the service 07/19/23 00:45:08.55
STEP: watching for the Service to be patched 07/19/23 00:45:08.559
Jul 19 00:45:08.559: INFO: observed Service test-service-tzl4v in namespace services-2594 with labels: map[test-service-static:true]
Jul 19 00:45:08.560: INFO: observed Service test-service-tzl4v in namespace services-2594 with labels: map[test-service-static:true]
Jul 19 00:45:08.560: INFO: observed Service test-service-tzl4v in namespace services-2594 with labels: map[test-service-static:true]
Jul 19 00:45:08.560: INFO: Found Service test-service-tzl4v in namespace services-2594 with labels: map[test-service:patched test-service-static:true]
Jul 19 00:45:08.560: INFO: Service test-service-tzl4v patched
STEP: deleting the service 07/19/23 00:45:08.56
STEP: watching for the Service to be deleted 07/19/23 00:45:08.568
Jul 19 00:45:08.569: INFO: Observed event: ADDED
Jul 19 00:45:08.569: INFO: Observed event: MODIFIED
Jul 19 00:45:08.569: INFO: Observed event: MODIFIED
Jul 19 00:45:08.569: INFO: Observed event: MODIFIED
Jul 19 00:45:08.569: INFO: Found Service test-service-tzl4v in namespace services-2594 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jul 19 00:45:08.569: INFO: Service test-service-tzl4v deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:45:08.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2594" for this suite. 07/19/23 00:45:08.571
------------------------------
• [0.061 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:45:08.513
    Jul 19 00:45:08.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:45:08.514
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:08.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:08.526
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 07/19/23 00:45:08.529
    STEP: watching for the Service to be added 07/19/23 00:45:08.537
    Jul 19 00:45:08.538: INFO: Found Service test-service-tzl4v in namespace services-2594 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jul 19 00:45:08.538: INFO: Service test-service-tzl4v created
    STEP: Getting /status 07/19/23 00:45:08.538
    Jul 19 00:45:08.540: INFO: Service test-service-tzl4v has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 07/19/23 00:45:08.54
    STEP: watching for the Service to be patched 07/19/23 00:45:08.543
    Jul 19 00:45:08.544: INFO: observed Service test-service-tzl4v in namespace services-2594 with annotations: map[] & LoadBalancer: {[]}
    Jul 19 00:45:08.544: INFO: Found Service test-service-tzl4v in namespace services-2594 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jul 19 00:45:08.544: INFO: Service test-service-tzl4v has service status patched
    STEP: updating the ServiceStatus 07/19/23 00:45:08.544
    Jul 19 00:45:08.548: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 07/19/23 00:45:08.549
    Jul 19 00:45:08.549: INFO: Observed Service test-service-tzl4v in namespace services-2594 with annotations: map[] & Conditions: {[]}
    Jul 19 00:45:08.549: INFO: Observed event: &Service{ObjectMeta:{test-service-tzl4v  services-2594  a008cc68-27ba-4571-adb9-b251a67d2072 77395 0 2023-07-19 00:45:08 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-19 00:45:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-19 00:45:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.101.236.215,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.101.236.215],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jul 19 00:45:08.549: INFO: Found Service test-service-tzl4v in namespace services-2594 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jul 19 00:45:08.550: INFO: Service test-service-tzl4v has service status updated
    STEP: patching the service 07/19/23 00:45:08.55
    STEP: watching for the Service to be patched 07/19/23 00:45:08.559
    Jul 19 00:45:08.559: INFO: observed Service test-service-tzl4v in namespace services-2594 with labels: map[test-service-static:true]
    Jul 19 00:45:08.560: INFO: observed Service test-service-tzl4v in namespace services-2594 with labels: map[test-service-static:true]
    Jul 19 00:45:08.560: INFO: observed Service test-service-tzl4v in namespace services-2594 with labels: map[test-service-static:true]
    Jul 19 00:45:08.560: INFO: Found Service test-service-tzl4v in namespace services-2594 with labels: map[test-service:patched test-service-static:true]
    Jul 19 00:45:08.560: INFO: Service test-service-tzl4v patched
    STEP: deleting the service 07/19/23 00:45:08.56
    STEP: watching for the Service to be deleted 07/19/23 00:45:08.568
    Jul 19 00:45:08.569: INFO: Observed event: ADDED
    Jul 19 00:45:08.569: INFO: Observed event: MODIFIED
    Jul 19 00:45:08.569: INFO: Observed event: MODIFIED
    Jul 19 00:45:08.569: INFO: Observed event: MODIFIED
    Jul 19 00:45:08.569: INFO: Found Service test-service-tzl4v in namespace services-2594 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jul 19 00:45:08.569: INFO: Service test-service-tzl4v deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:45:08.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2594" for this suite. 07/19/23 00:45:08.571
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:45:08.574
Jul 19 00:45:08.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/19/23 00:45:08.575
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:08.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:08.584
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 07/19/23 00:45:08.585
STEP: setting up watch 07/19/23 00:45:08.585
STEP: submitting the pod to kubernetes 07/19/23 00:45:08.688
STEP: verifying the pod is in kubernetes 07/19/23 00:45:08.692
STEP: verifying pod creation was observed 07/19/23 00:45:08.695
Jul 19 00:45:08.695: INFO: Waiting up to 5m0s for pod "pod-submit-remove-c92fbc60-0e2d-4e89-b97a-ac79bc4413ad" in namespace "pods-5505" to be "running"
Jul 19 00:45:08.696: INFO: Pod "pod-submit-remove-c92fbc60-0e2d-4e89-b97a-ac79bc4413ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.361525ms
Jul 19 00:45:10.699: INFO: Pod "pod-submit-remove-c92fbc60-0e2d-4e89-b97a-ac79bc4413ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.00395147s
Jul 19 00:45:10.699: INFO: Pod "pod-submit-remove-c92fbc60-0e2d-4e89-b97a-ac79bc4413ad" satisfied condition "running"
STEP: deleting the pod gracefully 07/19/23 00:45:10.701
STEP: verifying pod deletion was observed 07/19/23 00:45:10.704
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jul 19 00:45:12.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5505" for this suite. 07/19/23 00:45:12.505
------------------------------
• [3.933 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:45:08.574
    Jul 19 00:45:08.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/19/23 00:45:08.575
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:08.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:08.584
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 07/19/23 00:45:08.585
    STEP: setting up watch 07/19/23 00:45:08.585
    STEP: submitting the pod to kubernetes 07/19/23 00:45:08.688
    STEP: verifying the pod is in kubernetes 07/19/23 00:45:08.692
    STEP: verifying pod creation was observed 07/19/23 00:45:08.695
    Jul 19 00:45:08.695: INFO: Waiting up to 5m0s for pod "pod-submit-remove-c92fbc60-0e2d-4e89-b97a-ac79bc4413ad" in namespace "pods-5505" to be "running"
    Jul 19 00:45:08.696: INFO: Pod "pod-submit-remove-c92fbc60-0e2d-4e89-b97a-ac79bc4413ad": Phase="Pending", Reason="", readiness=false. Elapsed: 1.361525ms
    Jul 19 00:45:10.699: INFO: Pod "pod-submit-remove-c92fbc60-0e2d-4e89-b97a-ac79bc4413ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.00395147s
    Jul 19 00:45:10.699: INFO: Pod "pod-submit-remove-c92fbc60-0e2d-4e89-b97a-ac79bc4413ad" satisfied condition "running"
    STEP: deleting the pod gracefully 07/19/23 00:45:10.701
    STEP: verifying pod deletion was observed 07/19/23 00:45:10.704
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:45:12.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5505" for this suite. 07/19/23 00:45:12.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:45:12.508
Jul 19 00:45:12.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename disruption 07/19/23 00:45:12.509
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:12.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:12.519
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 07/19/23 00:45:12.523
STEP: Updating PodDisruptionBudget status 07/19/23 00:45:14.529
STEP: Waiting for all pods to be running 07/19/23 00:45:14.533
Jul 19 00:45:14.534: INFO: running pods: 0 < 1
STEP: locating a running pod 07/19/23 00:45:16.537
STEP: Waiting for the pdb to be processed 07/19/23 00:45:16.542
STEP: Patching PodDisruptionBudget status 07/19/23 00:45:16.551
STEP: Waiting for the pdb to be processed 07/19/23 00:45:16.556
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jul 19 00:45:16.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5747" for this suite. 07/19/23 00:45:16.56
------------------------------
• [4.054 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:45:12.508
    Jul 19 00:45:12.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename disruption 07/19/23 00:45:12.509
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:12.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:12.519
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 07/19/23 00:45:12.523
    STEP: Updating PodDisruptionBudget status 07/19/23 00:45:14.529
    STEP: Waiting for all pods to be running 07/19/23 00:45:14.533
    Jul 19 00:45:14.534: INFO: running pods: 0 < 1
    STEP: locating a running pod 07/19/23 00:45:16.537
    STEP: Waiting for the pdb to be processed 07/19/23 00:45:16.542
    STEP: Patching PodDisruptionBudget status 07/19/23 00:45:16.551
    STEP: Waiting for the pdb to be processed 07/19/23 00:45:16.556
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:45:16.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5747" for this suite. 07/19/23 00:45:16.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:45:16.564
Jul 19 00:45:16.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-runtime 07/19/23 00:45:16.564
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:16.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:16.573
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 07/19/23 00:45:16.575
STEP: wait for the container to reach Succeeded 07/19/23 00:45:16.578
STEP: get the container status 07/19/23 00:45:19.588
STEP: the container should be terminated 07/19/23 00:45:19.59
STEP: the termination message should be set 07/19/23 00:45:19.59
Jul 19 00:45:19.590: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 07/19/23 00:45:19.59
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jul 19 00:45:19.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2671" for this suite. 07/19/23 00:45:19.6
------------------------------
• [3.039 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:45:16.564
    Jul 19 00:45:16.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-runtime 07/19/23 00:45:16.564
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:16.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:16.573
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 07/19/23 00:45:16.575
    STEP: wait for the container to reach Succeeded 07/19/23 00:45:16.578
    STEP: get the container status 07/19/23 00:45:19.588
    STEP: the container should be terminated 07/19/23 00:45:19.59
    STEP: the termination message should be set 07/19/23 00:45:19.59
    Jul 19 00:45:19.590: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 07/19/23 00:45:19.59
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:45:19.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2671" for this suite. 07/19/23 00:45:19.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:45:19.604
Jul 19 00:45:19.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:45:19.604
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:19.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:19.612
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 07/19/23 00:45:19.614
Jul 19 00:45:19.614: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-3747 proxy --unix-socket=/tmp/kubectl-proxy-unix2423624988/test'
STEP: retrieving proxy /api/ output 07/19/23 00:45:19.66
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:45:19.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3747" for this suite. 07/19/23 00:45:19.663
------------------------------
• [0.062 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:45:19.604
    Jul 19 00:45:19.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:45:19.604
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:19.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:19.612
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 07/19/23 00:45:19.614
    Jul 19 00:45:19.614: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-3747 proxy --unix-socket=/tmp/kubectl-proxy-unix2423624988/test'
    STEP: retrieving proxy /api/ output 07/19/23 00:45:19.66
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:45:19.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3747" for this suite. 07/19/23 00:45:19.663
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:45:19.666
Jul 19 00:45:19.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-probe 07/19/23 00:45:19.666
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:19.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:19.674
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3 in namespace container-probe-6818 07/19/23 00:45:19.676
Jul 19 00:45:19.680: INFO: Waiting up to 5m0s for pod "test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3" in namespace "container-probe-6818" to be "not pending"
Jul 19 00:45:19.681: INFO: Pod "test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31286ms
Jul 19 00:45:21.685: INFO: Pod "test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004646715s
Jul 19 00:45:21.685: INFO: Pod "test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3" satisfied condition "not pending"
Jul 19 00:45:21.685: INFO: Started pod test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3 in namespace container-probe-6818
STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 00:45:21.685
Jul 19 00:45:21.686: INFO: Initial restart count of pod test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3 is 0
STEP: deleting the pod 07/19/23 00:49:22.021
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jul 19 00:49:22.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6818" for this suite. 07/19/23 00:49:22.033
------------------------------
• [SLOW TEST] [242.370 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:45:19.666
    Jul 19 00:45:19.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-probe 07/19/23 00:45:19.666
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:45:19.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:45:19.674
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3 in namespace container-probe-6818 07/19/23 00:45:19.676
    Jul 19 00:45:19.680: INFO: Waiting up to 5m0s for pod "test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3" in namespace "container-probe-6818" to be "not pending"
    Jul 19 00:45:19.681: INFO: Pod "test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.31286ms
    Jul 19 00:45:21.685: INFO: Pod "test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004646715s
    Jul 19 00:45:21.685: INFO: Pod "test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3" satisfied condition "not pending"
    Jul 19 00:45:21.685: INFO: Started pod test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3 in namespace container-probe-6818
    STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 00:45:21.685
    Jul 19 00:45:21.686: INFO: Initial restart count of pod test-webserver-a641d349-9074-41a2-b8c5-c7b67160b4a3 is 0
    STEP: deleting the pod 07/19/23 00:49:22.021
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:49:22.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6818" for this suite. 07/19/23 00:49:22.033
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:49:22.036
Jul 19 00:49:22.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename svc-latency 07/19/23 00:49:22.037
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:49:22.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:49:22.045
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jul 19 00:49:22.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4830 07/19/23 00:49:22.048
I0719 00:49:22.051767      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4830, replica count: 1
I0719 00:49:23.102936      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 00:49:23.211: INFO: Created: latency-svc-vx454
Jul 19 00:49:23.215: INFO: Got endpoints: latency-svc-vx454 [12.352376ms]
Jul 19 00:49:23.226: INFO: Created: latency-svc-kkp59
Jul 19 00:49:23.228: INFO: Got endpoints: latency-svc-kkp59 [12.419536ms]
Jul 19 00:49:23.236: INFO: Created: latency-svc-rp6sv
Jul 19 00:49:23.262: INFO: Got endpoints: latency-svc-rp6sv [46.942601ms]
Jul 19 00:49:23.264: INFO: Created: latency-svc-nxc9k
Jul 19 00:49:23.268: INFO: Got endpoints: latency-svc-nxc9k [53.077868ms]
Jul 19 00:49:23.273: INFO: Created: latency-svc-5dnfq
Jul 19 00:49:23.280: INFO: Got endpoints: latency-svc-5dnfq [64.10107ms]
Jul 19 00:49:23.284: INFO: Created: latency-svc-hdzmv
Jul 19 00:49:23.306: INFO: Got endpoints: latency-svc-hdzmv [91.083315ms]
Jul 19 00:49:23.328: INFO: Created: latency-svc-jvxfq
Jul 19 00:49:23.348: INFO: Got endpoints: latency-svc-jvxfq [132.262845ms]
Jul 19 00:49:23.371: INFO: Created: latency-svc-nsbgr
Jul 19 00:49:23.390: INFO: Got endpoints: latency-svc-nsbgr [174.395894ms]
Jul 19 00:49:23.394: INFO: Created: latency-svc-wxtwb
Jul 19 00:49:23.413: INFO: Got endpoints: latency-svc-wxtwb [197.233547ms]
Jul 19 00:49:23.435: INFO: Created: latency-svc-jjqlg
Jul 19 00:49:23.453: INFO: Got endpoints: latency-svc-jjqlg [237.652796ms]
Jul 19 00:49:23.457: INFO: Created: latency-svc-whf6x
Jul 19 00:49:23.519: INFO: Got endpoints: latency-svc-whf6x [303.300735ms]
Jul 19 00:49:23.521: INFO: Created: latency-svc-4ggkr
Jul 19 00:49:23.523: INFO: Got endpoints: latency-svc-4ggkr [307.320503ms]
Jul 19 00:49:23.548: INFO: Created: latency-svc-92dq8
Jul 19 00:49:23.568: INFO: Got endpoints: latency-svc-92dq8 [352.396187ms]
Jul 19 00:49:23.588: INFO: Created: latency-svc-kd2pr
Jul 19 00:49:23.606: INFO: Got endpoints: latency-svc-kd2pr [390.359432ms]
Jul 19 00:49:23.612: INFO: Created: latency-svc-249s7
Jul 19 00:49:23.632: INFO: Got endpoints: latency-svc-249s7 [417.026142ms]
Jul 19 00:49:23.636: INFO: Created: latency-svc-v4bdm
Jul 19 00:49:23.655: INFO: Got endpoints: latency-svc-v4bdm [439.293213ms]
Jul 19 00:49:23.658: INFO: Created: latency-svc-hgr62
Jul 19 00:49:23.677: INFO: Got endpoints: latency-svc-hgr62 [448.907706ms]
Jul 19 00:49:23.682: INFO: Created: latency-svc-kgwfl
Jul 19 00:49:23.699: INFO: Got endpoints: latency-svc-kgwfl [437.109768ms]
Jul 19 00:49:23.705: INFO: Created: latency-svc-jd2sf
Jul 19 00:49:23.733: INFO: Got endpoints: latency-svc-jd2sf [464.287033ms]
Jul 19 00:49:23.748: INFO: Created: latency-svc-s4gk8
Jul 19 00:49:23.754: INFO: Got endpoints: latency-svc-s4gk8 [474.182982ms]
Jul 19 00:49:23.774: INFO: Created: latency-svc-lwwwv
Jul 19 00:49:23.782: INFO: Got endpoints: latency-svc-lwwwv [475.410285ms]
Jul 19 00:49:23.805: INFO: Created: latency-svc-m5vj2
Jul 19 00:49:23.807: INFO: Got endpoints: latency-svc-m5vj2 [459.554169ms]
Jul 19 00:49:23.814: INFO: Created: latency-svc-xlsqx
Jul 19 00:49:23.816: INFO: Got endpoints: latency-svc-xlsqx [426.303079ms]
Jul 19 00:49:23.821: INFO: Created: latency-svc-tht8c
Jul 19 00:49:23.839: INFO: Got endpoints: latency-svc-tht8c [426.722325ms]
Jul 19 00:49:23.843: INFO: Created: latency-svc-j8hk5
Jul 19 00:49:23.866: INFO: Got endpoints: latency-svc-j8hk5 [413.34805ms]
Jul 19 00:49:23.887: INFO: Created: latency-svc-hprh8
Jul 19 00:49:23.906: INFO: Got endpoints: latency-svc-hprh8 [386.751825ms]
Jul 19 00:49:23.911: INFO: Created: latency-svc-lhqjq
Jul 19 00:49:23.928: INFO: Got endpoints: latency-svc-lhqjq [404.808517ms]
Jul 19 00:49:23.932: INFO: Created: latency-svc-kqlxm
Jul 19 00:49:23.949: INFO: Got endpoints: latency-svc-kqlxm [381.4083ms]
Jul 19 00:49:23.953: INFO: Created: latency-svc-wt6lz
Jul 19 00:49:24.016: INFO: Got endpoints: latency-svc-wt6lz [410.497486ms]
Jul 19 00:49:24.018: INFO: Created: latency-svc-2ggjj
Jul 19 00:49:24.022: INFO: Got endpoints: latency-svc-2ggjj [389.014268ms]
Jul 19 00:49:24.026: INFO: Created: latency-svc-5q7qc
Jul 19 00:49:24.044: INFO: Got endpoints: latency-svc-5q7qc [389.462134ms]
Jul 19 00:49:24.069: INFO: Created: latency-svc-dn8k6
Jul 19 00:49:24.087: INFO: Got endpoints: latency-svc-dn8k6 [409.846075ms]
Jul 19 00:49:24.090: INFO: Created: latency-svc-8sjpq
Jul 19 00:49:24.109: INFO: Got endpoints: latency-svc-8sjpq [409.502373ms]
Jul 19 00:49:24.113: INFO: Created: latency-svc-2wk5x
Jul 19 00:49:24.135: INFO: Got endpoints: latency-svc-2wk5x [401.989107ms]
Jul 19 00:49:24.140: INFO: Created: latency-svc-vrqfj
Jul 19 00:49:24.158: INFO: Got endpoints: latency-svc-vrqfj [404.196798ms]
Jul 19 00:49:24.167: INFO: Created: latency-svc-6xx9f
Jul 19 00:49:24.169: INFO: Got endpoints: latency-svc-6xx9f [386.778491ms]
Jul 19 00:49:24.194: INFO: Created: latency-svc-c87lr
Jul 19 00:49:24.199: INFO: Got endpoints: latency-svc-c87lr [391.250709ms]
Jul 19 00:49:24.202: INFO: Created: latency-svc-nxzcc
Jul 19 00:49:24.204: INFO: Got endpoints: latency-svc-nxzcc [387.98973ms]
Jul 19 00:49:24.211: INFO: Created: latency-svc-fvvsp
Jul 19 00:49:24.228: INFO: Got endpoints: latency-svc-fvvsp [388.952018ms]
Jul 19 00:49:24.233: INFO: Created: latency-svc-4n8hg
Jul 19 00:49:24.257: INFO: Got endpoints: latency-svc-4n8hg [390.801741ms]
Jul 19 00:49:24.261: INFO: Created: latency-svc-9kbvw
Jul 19 00:49:24.263: INFO: Got endpoints: latency-svc-9kbvw [357.678405ms]
Jul 19 00:49:24.302: INFO: Created: latency-svc-kmdqq
Jul 19 00:49:24.305: INFO: Got endpoints: latency-svc-kmdqq [376.909361ms]
Jul 19 00:49:24.311: INFO: Created: latency-svc-svxwg
Jul 19 00:49:24.313: INFO: Got endpoints: latency-svc-svxwg [363.790949ms]
Jul 19 00:49:24.324: INFO: Created: latency-svc-xvpmg
Jul 19 00:49:24.328: INFO: Got endpoints: latency-svc-xvpmg [312.134593ms]
Jul 19 00:49:24.340: INFO: Created: latency-svc-2qn2j
Jul 19 00:49:24.341: INFO: Got endpoints: latency-svc-2qn2j [319.910292ms]
Jul 19 00:49:24.347: INFO: Created: latency-svc-nmfht
Jul 19 00:49:24.350: INFO: Got endpoints: latency-svc-nmfht [305.407401ms]
Jul 19 00:49:24.364: INFO: Created: latency-svc-rb64d
Jul 19 00:49:24.367: INFO: Got endpoints: latency-svc-rb64d [279.897264ms]
Jul 19 00:49:24.372: INFO: Created: latency-svc-xq6m7
Jul 19 00:49:24.378: INFO: Got endpoints: latency-svc-xq6m7 [269.389795ms]
Jul 19 00:49:24.381: INFO: Created: latency-svc-bj25x
Jul 19 00:49:24.383: INFO: Got endpoints: latency-svc-bj25x [248.549521ms]
Jul 19 00:49:24.388: INFO: Created: latency-svc-zn4mq
Jul 19 00:49:24.390: INFO: Got endpoints: latency-svc-zn4mq [232.20813ms]
Jul 19 00:49:24.396: INFO: Created: latency-svc-lclxx
Jul 19 00:49:24.398: INFO: Got endpoints: latency-svc-lclxx [229.800097ms]
Jul 19 00:49:24.407: INFO: Created: latency-svc-xl8d4
Jul 19 00:49:24.410: INFO: Got endpoints: latency-svc-xl8d4 [211.502163ms]
Jul 19 00:49:24.414: INFO: Created: latency-svc-qcpbl
Jul 19 00:49:24.415: INFO: Got endpoints: latency-svc-qcpbl [211.242029ms]
Jul 19 00:49:24.421: INFO: Created: latency-svc-fmwdc
Jul 19 00:49:24.424: INFO: Got endpoints: latency-svc-fmwdc [195.364168ms]
Jul 19 00:49:24.429: INFO: Created: latency-svc-2c559
Jul 19 00:49:24.436: INFO: Created: latency-svc-sdvzj
Jul 19 00:49:24.445: INFO: Created: latency-svc-h5st7
Jul 19 00:49:24.463: INFO: Created: latency-svc-7ffhb
Jul 19 00:49:24.499: INFO: Got endpoints: latency-svc-2c559 [241.901271ms]
Jul 19 00:49:24.501: INFO: Created: latency-svc-lfgkn
Jul 19 00:49:24.508: INFO: Created: latency-svc-scwwn
Jul 19 00:49:24.514: INFO: Created: latency-svc-s2t9d
Jul 19 00:49:24.515: INFO: Got endpoints: latency-svc-sdvzj [251.329843ms]
Jul 19 00:49:24.520: INFO: Created: latency-svc-p9wln
Jul 19 00:49:24.568: INFO: Got endpoints: latency-svc-h5st7 [263.602813ms]
Jul 19 00:49:24.583: INFO: Created: latency-svc-5vx7r
Jul 19 00:49:24.583: INFO: Created: latency-svc-k2vb9
Jul 19 00:49:24.583: INFO: Created: latency-svc-6gg28
Jul 19 00:49:24.583: INFO: Created: latency-svc-q8l5j
Jul 19 00:49:24.583: INFO: Created: latency-svc-stk5w
Jul 19 00:49:24.583: INFO: Created: latency-svc-tbhnz
Jul 19 00:49:24.583: INFO: Created: latency-svc-5pfp5
Jul 19 00:49:24.583: INFO: Created: latency-svc-mz9v2
Jul 19 00:49:24.583: INFO: Created: latency-svc-78qpm
Jul 19 00:49:24.583: INFO: Created: latency-svc-7cz6l
Jul 19 00:49:24.641: INFO: Got endpoints: latency-svc-7ffhb [327.954623ms]
Jul 19 00:49:24.650: INFO: Created: latency-svc-g8g8l
Jul 19 00:49:24.664: INFO: Got endpoints: latency-svc-lfgkn [335.57739ms]
Jul 19 00:49:24.678: INFO: Created: latency-svc-g862b
Jul 19 00:49:24.715: INFO: Got endpoints: latency-svc-scwwn [373.81544ms]
Jul 19 00:49:24.724: INFO: Created: latency-svc-8fdxt
Jul 19 00:49:24.768: INFO: Got endpoints: latency-svc-s2t9d [418.406212ms]
Jul 19 00:49:24.777: INFO: Created: latency-svc-sgmjx
Jul 19 00:49:24.814: INFO: Got endpoints: latency-svc-p9wln [447.799328ms]
Jul 19 00:49:24.824: INFO: Created: latency-svc-w72cr
Jul 19 00:49:24.865: INFO: Got endpoints: latency-svc-q8l5j [296.573968ms]
Jul 19 00:49:24.875: INFO: Created: latency-svc-m6w4m
Jul 19 00:49:24.916: INFO: Got endpoints: latency-svc-6gg28 [400.946084ms]
Jul 19 00:49:24.924: INFO: Created: latency-svc-kr7kd
Jul 19 00:49:24.965: INFO: Got endpoints: latency-svc-stk5w [465.574952ms]
Jul 19 00:49:24.976: INFO: Created: latency-svc-zqgsz
Jul 19 00:49:25.014: INFO: Got endpoints: latency-svc-5vx7r [599.022327ms]
Jul 19 00:49:25.022: INFO: Created: latency-svc-skl7s
Jul 19 00:49:25.064: INFO: Got endpoints: latency-svc-mz9v2 [680.888268ms]
Jul 19 00:49:25.074: INFO: Created: latency-svc-s65kk
Jul 19 00:49:25.114: INFO: Got endpoints: latency-svc-7cz6l [704.004447ms]
Jul 19 00:49:25.125: INFO: Created: latency-svc-vtsr5
Jul 19 00:49:25.167: INFO: Got endpoints: latency-svc-tbhnz [768.955976ms]
Jul 19 00:49:25.176: INFO: Created: latency-svc-689cg
Jul 19 00:49:25.217: INFO: Got endpoints: latency-svc-5pfp5 [839.129384ms]
Jul 19 00:49:25.226: INFO: Created: latency-svc-8tmsx
Jul 19 00:49:25.264: INFO: Got endpoints: latency-svc-78qpm [840.286824ms]
Jul 19 00:49:25.275: INFO: Created: latency-svc-96bdt
Jul 19 00:49:25.314: INFO: Got endpoints: latency-svc-k2vb9 [924.090705ms]
Jul 19 00:49:25.323: INFO: Created: latency-svc-xhlhz
Jul 19 00:49:25.365: INFO: Got endpoints: latency-svc-g8g8l [723.746182ms]
Jul 19 00:49:25.373: INFO: Created: latency-svc-65wjn
Jul 19 00:49:25.419: INFO: Got endpoints: latency-svc-g862b [754.795864ms]
Jul 19 00:49:25.428: INFO: Created: latency-svc-vn4wq
Jul 19 00:49:25.464: INFO: Got endpoints: latency-svc-8fdxt [749.055081ms]
Jul 19 00:49:25.475: INFO: Created: latency-svc-6fl2l
Jul 19 00:49:25.514: INFO: Got endpoints: latency-svc-sgmjx [746.019443ms]
Jul 19 00:49:25.522: INFO: Created: latency-svc-rxz5t
Jul 19 00:49:25.564: INFO: Got endpoints: latency-svc-w72cr [749.170062ms]
Jul 19 00:49:25.575: INFO: Created: latency-svc-x5c54
Jul 19 00:49:25.614: INFO: Got endpoints: latency-svc-m6w4m [749.692227ms]
Jul 19 00:49:25.625: INFO: Created: latency-svc-5bm25
Jul 19 00:49:25.664: INFO: Got endpoints: latency-svc-kr7kd [748.858762ms]
Jul 19 00:49:25.674: INFO: Created: latency-svc-twfh2
Jul 19 00:49:25.714: INFO: Got endpoints: latency-svc-zqgsz [749.473331ms]
Jul 19 00:49:25.727: INFO: Created: latency-svc-tbqxs
Jul 19 00:49:25.765: INFO: Got endpoints: latency-svc-skl7s [750.282741ms]
Jul 19 00:49:25.774: INFO: Created: latency-svc-lhvk7
Jul 19 00:49:25.815: INFO: Got endpoints: latency-svc-s65kk [750.351333ms]
Jul 19 00:49:25.825: INFO: Created: latency-svc-zv26w
Jul 19 00:49:25.865: INFO: Got endpoints: latency-svc-vtsr5 [750.591476ms]
Jul 19 00:49:25.877: INFO: Created: latency-svc-2b8zj
Jul 19 00:49:25.917: INFO: Got endpoints: latency-svc-689cg [749.905313ms]
Jul 19 00:49:25.927: INFO: Created: latency-svc-bfgql
Jul 19 00:49:25.964: INFO: Got endpoints: latency-svc-8tmsx [746.189119ms]
Jul 19 00:49:25.991: INFO: Created: latency-svc-t6jz8
Jul 19 00:49:26.019: INFO: Got endpoints: latency-svc-96bdt [754.615769ms]
Jul 19 00:49:26.027: INFO: Created: latency-svc-wn6pd
Jul 19 00:49:26.064: INFO: Got endpoints: latency-svc-xhlhz [749.303231ms]
Jul 19 00:49:26.076: INFO: Created: latency-svc-sckxx
Jul 19 00:49:26.114: INFO: Got endpoints: latency-svc-65wjn [749.247846ms]
Jul 19 00:49:26.127: INFO: Created: latency-svc-rx4dk
Jul 19 00:49:26.165: INFO: Got endpoints: latency-svc-vn4wq [745.803693ms]
Jul 19 00:49:26.176: INFO: Created: latency-svc-zt8c5
Jul 19 00:49:26.214: INFO: Got endpoints: latency-svc-6fl2l [749.914495ms]
Jul 19 00:49:26.227: INFO: Created: latency-svc-ww7df
Jul 19 00:49:26.265: INFO: Got endpoints: latency-svc-rxz5t [750.824278ms]
Jul 19 00:49:26.273: INFO: Created: latency-svc-fkbzq
Jul 19 00:49:26.315: INFO: Got endpoints: latency-svc-x5c54 [751.506623ms]
Jul 19 00:49:26.325: INFO: Created: latency-svc-g9qz6
Jul 19 00:49:26.364: INFO: Got endpoints: latency-svc-5bm25 [749.54799ms]
Jul 19 00:49:26.376: INFO: Created: latency-svc-c25d9
Jul 19 00:49:26.415: INFO: Got endpoints: latency-svc-twfh2 [750.150404ms]
Jul 19 00:49:26.426: INFO: Created: latency-svc-8kk8c
Jul 19 00:49:26.465: INFO: Got endpoints: latency-svc-tbqxs [750.712592ms]
Jul 19 00:49:26.474: INFO: Created: latency-svc-qwpz5
Jul 19 00:49:26.516: INFO: Got endpoints: latency-svc-lhvk7 [751.478829ms]
Jul 19 00:49:26.525: INFO: Created: latency-svc-86s7c
Jul 19 00:49:26.564: INFO: Got endpoints: latency-svc-zv26w [749.449273ms]
Jul 19 00:49:26.577: INFO: Created: latency-svc-j8phf
Jul 19 00:49:26.616: INFO: Got endpoints: latency-svc-2b8zj [751.28116ms]
Jul 19 00:49:26.625: INFO: Created: latency-svc-fv8sr
Jul 19 00:49:26.665: INFO: Got endpoints: latency-svc-bfgql [747.386498ms]
Jul 19 00:49:26.674: INFO: Created: latency-svc-gf2df
Jul 19 00:49:26.718: INFO: Got endpoints: latency-svc-t6jz8 [754.492785ms]
Jul 19 00:49:26.728: INFO: Created: latency-svc-4wgmb
Jul 19 00:49:26.772: INFO: Got endpoints: latency-svc-wn6pd [753.416309ms]
Jul 19 00:49:26.781: INFO: Created: latency-svc-xql5k
Jul 19 00:49:26.814: INFO: Got endpoints: latency-svc-sckxx [750.087299ms]
Jul 19 00:49:26.824: INFO: Created: latency-svc-ddr88
Jul 19 00:49:26.864: INFO: Got endpoints: latency-svc-rx4dk [750.235113ms]
Jul 19 00:49:26.917: INFO: Got endpoints: latency-svc-zt8c5 [752.337299ms]
Jul 19 00:49:26.938: INFO: Created: latency-svc-86zjl
Jul 19 00:49:26.949: INFO: Created: latency-svc-fmhpv
Jul 19 00:49:26.965: INFO: Got endpoints: latency-svc-ww7df [750.403239ms]
Jul 19 00:49:26.999: INFO: Created: latency-svc-9n4g8
Jul 19 00:49:27.059: INFO: Got endpoints: latency-svc-fkbzq [794.351494ms]
Jul 19 00:49:27.069: INFO: Got endpoints: latency-svc-g9qz6 [753.990957ms]
Jul 19 00:49:27.073: INFO: Created: latency-svc-qkmpd
Jul 19 00:49:27.095: INFO: Created: latency-svc-frcvd
Jul 19 00:49:27.117: INFO: Got endpoints: latency-svc-c25d9 [753.323998ms]
Jul 19 00:49:27.143: INFO: Created: latency-svc-strrt
Jul 19 00:49:27.185: INFO: Got endpoints: latency-svc-8kk8c [769.905304ms]
Jul 19 00:49:27.195: INFO: Created: latency-svc-p6dvk
Jul 19 00:49:27.217: INFO: Got endpoints: latency-svc-qwpz5 [751.596173ms]
Jul 19 00:49:27.248: INFO: Created: latency-svc-2tltt
Jul 19 00:49:27.265: INFO: Got endpoints: latency-svc-86s7c [748.687434ms]
Jul 19 00:49:27.316: INFO: Got endpoints: latency-svc-j8phf [751.722501ms]
Jul 19 00:49:27.316: INFO: Created: latency-svc-hwwwx
Jul 19 00:49:27.341: INFO: Created: latency-svc-tk8kx
Jul 19 00:49:27.364: INFO: Got endpoints: latency-svc-fv8sr [748.417631ms]
Jul 19 00:49:27.400: INFO: Created: latency-svc-gtkls
Jul 19 00:49:27.447: INFO: Got endpoints: latency-svc-gf2df [781.798989ms]
Jul 19 00:49:27.458: INFO: Created: latency-svc-n5nrj
Jul 19 00:49:27.480: INFO: Got endpoints: latency-svc-4wgmb [761.681786ms]
Jul 19 00:49:27.489: INFO: Created: latency-svc-vrrd5
Jul 19 00:49:27.514: INFO: Got endpoints: latency-svc-xql5k [741.908672ms]
Jul 19 00:49:27.540: INFO: Created: latency-svc-w2w5q
Jul 19 00:49:27.581: INFO: Got endpoints: latency-svc-ddr88 [766.807398ms]
Jul 19 00:49:27.592: INFO: Created: latency-svc-sqlc2
Jul 19 00:49:27.616: INFO: Got endpoints: latency-svc-86zjl [751.229942ms]
Jul 19 00:49:27.640: INFO: Created: latency-svc-xfbvh
Jul 19 00:49:27.663: INFO: Got endpoints: latency-svc-fmhpv [746.400429ms]
Jul 19 00:49:27.712: INFO: Created: latency-svc-jd67h
Jul 19 00:49:27.714: INFO: Got endpoints: latency-svc-9n4g8 [749.697752ms]
Jul 19 00:49:27.738: INFO: Created: latency-svc-kbnk7
Jul 19 00:49:27.764: INFO: Got endpoints: latency-svc-qkmpd [704.319469ms]
Jul 19 00:49:27.788: INFO: Created: latency-svc-m6nw5
Jul 19 00:49:27.838: INFO: Got endpoints: latency-svc-frcvd [768.456527ms]
Jul 19 00:49:27.848: INFO: Created: latency-svc-qk4wh
Jul 19 00:49:27.867: INFO: Got endpoints: latency-svc-strrt [749.979551ms]
Jul 19 00:49:27.898: INFO: Created: latency-svc-svgdk
Jul 19 00:49:27.915: INFO: Got endpoints: latency-svc-p6dvk [730.200776ms]
Jul 19 00:49:27.925: INFO: Created: latency-svc-fqptn
Jul 19 00:49:27.964: INFO: Got endpoints: latency-svc-2tltt [747.345375ms]
Jul 19 00:49:27.973: INFO: Created: latency-svc-mw6s5
Jul 19 00:49:28.015: INFO: Got endpoints: latency-svc-hwwwx [749.663761ms]
Jul 19 00:49:28.024: INFO: Created: latency-svc-hv7t4
Jul 19 00:49:28.065: INFO: Got endpoints: latency-svc-tk8kx [748.810446ms]
Jul 19 00:49:28.075: INFO: Created: latency-svc-g6frw
Jul 19 00:49:28.114: INFO: Got endpoints: latency-svc-gtkls [749.900319ms]
Jul 19 00:49:28.123: INFO: Created: latency-svc-f4gt8
Jul 19 00:49:28.165: INFO: Got endpoints: latency-svc-n5nrj [718.133049ms]
Jul 19 00:49:28.176: INFO: Created: latency-svc-pg92l
Jul 19 00:49:28.216: INFO: Got endpoints: latency-svc-vrrd5 [735.940841ms]
Jul 19 00:49:28.226: INFO: Created: latency-svc-vvqhl
Jul 19 00:49:28.269: INFO: Got endpoints: latency-svc-w2w5q [754.579339ms]
Jul 19 00:49:28.279: INFO: Created: latency-svc-zhx2q
Jul 19 00:49:28.316: INFO: Got endpoints: latency-svc-sqlc2 [735.010424ms]
Jul 19 00:49:28.327: INFO: Created: latency-svc-4pkxs
Jul 19 00:49:28.365: INFO: Got endpoints: latency-svc-xfbvh [749.455617ms]
Jul 19 00:49:28.384: INFO: Created: latency-svc-wbc6n
Jul 19 00:49:28.415: INFO: Got endpoints: latency-svc-jd67h [751.57088ms]
Jul 19 00:49:28.425: INFO: Created: latency-svc-kn8wb
Jul 19 00:49:28.465: INFO: Got endpoints: latency-svc-kbnk7 [750.644542ms]
Jul 19 00:49:28.484: INFO: Created: latency-svc-l27zb
Jul 19 00:49:28.514: INFO: Got endpoints: latency-svc-m6nw5 [750.509983ms]
Jul 19 00:49:28.532: INFO: Created: latency-svc-qgbbd
Jul 19 00:49:28.566: INFO: Got endpoints: latency-svc-qk4wh [728.309064ms]
Jul 19 00:49:28.574: INFO: Created: latency-svc-jd47q
Jul 19 00:49:28.615: INFO: Got endpoints: latency-svc-svgdk [747.164317ms]
Jul 19 00:49:28.625: INFO: Created: latency-svc-p9csc
Jul 19 00:49:28.665: INFO: Got endpoints: latency-svc-fqptn [750.520445ms]
Jul 19 00:49:28.674: INFO: Created: latency-svc-zzbtx
Jul 19 00:49:28.716: INFO: Got endpoints: latency-svc-mw6s5 [752.498807ms]
Jul 19 00:49:28.726: INFO: Created: latency-svc-s6gsj
Jul 19 00:49:28.768: INFO: Got endpoints: latency-svc-hv7t4 [753.328485ms]
Jul 19 00:49:28.777: INFO: Created: latency-svc-qnqgj
Jul 19 00:49:28.814: INFO: Got endpoints: latency-svc-g6frw [749.532867ms]
Jul 19 00:49:28.825: INFO: Created: latency-svc-wzgpq
Jul 19 00:49:28.864: INFO: Got endpoints: latency-svc-f4gt8 [750.100595ms]
Jul 19 00:49:28.882: INFO: Created: latency-svc-9jhjz
Jul 19 00:49:28.915: INFO: Got endpoints: latency-svc-pg92l [750.270402ms]
Jul 19 00:49:28.924: INFO: Created: latency-svc-mc9db
Jul 19 00:49:28.964: INFO: Got endpoints: latency-svc-vvqhl [748.283102ms]
Jul 19 00:49:28.974: INFO: Created: latency-svc-nz2xd
Jul 19 00:49:29.027: INFO: Got endpoints: latency-svc-zhx2q [758.059483ms]
Jul 19 00:49:29.035: INFO: Created: latency-svc-l954h
Jul 19 00:49:29.065: INFO: Got endpoints: latency-svc-4pkxs [748.9042ms]
Jul 19 00:49:29.074: INFO: Created: latency-svc-bbz44
Jul 19 00:49:29.116: INFO: Got endpoints: latency-svc-wbc6n [750.566873ms]
Jul 19 00:49:29.126: INFO: Created: latency-svc-dhqsp
Jul 19 00:49:29.166: INFO: Got endpoints: latency-svc-kn8wb [751.000429ms]
Jul 19 00:49:29.175: INFO: Created: latency-svc-mx5bz
Jul 19 00:49:29.214: INFO: Got endpoints: latency-svc-l27zb [749.141645ms]
Jul 19 00:49:29.222: INFO: Created: latency-svc-x6btz
Jul 19 00:49:29.264: INFO: Got endpoints: latency-svc-qgbbd [749.821374ms]
Jul 19 00:49:29.274: INFO: Created: latency-svc-pk4pc
Jul 19 00:49:29.315: INFO: Got endpoints: latency-svc-jd47q [749.309616ms]
Jul 19 00:49:29.324: INFO: Created: latency-svc-zpfg6
Jul 19 00:49:29.364: INFO: Got endpoints: latency-svc-p9csc [749.628887ms]
Jul 19 00:49:29.374: INFO: Created: latency-svc-r88km
Jul 19 00:49:29.414: INFO: Got endpoints: latency-svc-zzbtx [749.046349ms]
Jul 19 00:49:29.424: INFO: Created: latency-svc-4r5s5
Jul 19 00:49:29.466: INFO: Got endpoints: latency-svc-s6gsj [749.071314ms]
Jul 19 00:49:29.475: INFO: Created: latency-svc-rp6hx
Jul 19 00:49:29.514: INFO: Got endpoints: latency-svc-qnqgj [745.785354ms]
Jul 19 00:49:29.524: INFO: Created: latency-svc-fdwp5
Jul 19 00:49:29.565: INFO: Got endpoints: latency-svc-wzgpq [750.640372ms]
Jul 19 00:49:29.575: INFO: Created: latency-svc-mq5h7
Jul 19 00:49:29.614: INFO: Got endpoints: latency-svc-9jhjz [749.218798ms]
Jul 19 00:49:29.622: INFO: Created: latency-svc-fbvhp
Jul 19 00:49:29.664: INFO: Got endpoints: latency-svc-mc9db [748.997624ms]
Jul 19 00:49:29.679: INFO: Created: latency-svc-77fqp
Jul 19 00:49:29.715: INFO: Got endpoints: latency-svc-nz2xd [750.939528ms]
Jul 19 00:49:29.723: INFO: Created: latency-svc-bbr2j
Jul 19 00:49:29.764: INFO: Got endpoints: latency-svc-l954h [737.652084ms]
Jul 19 00:49:29.779: INFO: Created: latency-svc-xpwdl
Jul 19 00:49:29.814: INFO: Got endpoints: latency-svc-bbz44 [749.853724ms]
Jul 19 00:49:29.825: INFO: Created: latency-svc-ptdx5
Jul 19 00:49:29.865: INFO: Got endpoints: latency-svc-dhqsp [748.86408ms]
Jul 19 00:49:29.895: INFO: Created: latency-svc-tn45f
Jul 19 00:49:29.915: INFO: Got endpoints: latency-svc-mx5bz [749.236649ms]
Jul 19 00:49:29.924: INFO: Created: latency-svc-qm5r8
Jul 19 00:49:29.968: INFO: Got endpoints: latency-svc-x6btz [753.394349ms]
Jul 19 00:49:29.977: INFO: Created: latency-svc-9qq24
Jul 19 00:49:30.015: INFO: Got endpoints: latency-svc-pk4pc [750.621421ms]
Jul 19 00:49:30.025: INFO: Created: latency-svc-smmn8
Jul 19 00:49:30.065: INFO: Got endpoints: latency-svc-zpfg6 [750.099344ms]
Jul 19 00:49:30.075: INFO: Created: latency-svc-z2hxs
Jul 19 00:49:30.114: INFO: Got endpoints: latency-svc-r88km [750.126871ms]
Jul 19 00:49:30.183: INFO: Got endpoints: latency-svc-4r5s5 [768.715252ms]
Jul 19 00:49:30.183: INFO: Created: latency-svc-4ncv9
Jul 19 00:49:30.193: INFO: Created: latency-svc-xqhnq
Jul 19 00:49:30.214: INFO: Got endpoints: latency-svc-rp6hx [748.392822ms]
Jul 19 00:49:30.223: INFO: Created: latency-svc-mnvmg
Jul 19 00:49:30.264: INFO: Got endpoints: latency-svc-fdwp5 [750.577207ms]
Jul 19 00:49:30.274: INFO: Created: latency-svc-h7697
Jul 19 00:49:30.319: INFO: Got endpoints: latency-svc-mq5h7 [754.057698ms]
Jul 19 00:49:30.327: INFO: Created: latency-svc-sz8cr
Jul 19 00:49:30.364: INFO: Got endpoints: latency-svc-fbvhp [749.899268ms]
Jul 19 00:49:30.374: INFO: Created: latency-svc-xk8c9
Jul 19 00:49:30.413: INFO: Got endpoints: latency-svc-77fqp [749.370217ms]
Jul 19 00:49:30.430: INFO: Created: latency-svc-qkpl9
Jul 19 00:49:30.464: INFO: Got endpoints: latency-svc-bbr2j [748.909136ms]
Jul 19 00:49:30.473: INFO: Created: latency-svc-fdsjg
Jul 19 00:49:30.516: INFO: Got endpoints: latency-svc-xpwdl [751.356711ms]
Jul 19 00:49:30.528: INFO: Created: latency-svc-zchsw
Jul 19 00:49:30.565: INFO: Got endpoints: latency-svc-ptdx5 [750.233395ms]
Jul 19 00:49:30.574: INFO: Created: latency-svc-rd56h
Jul 19 00:49:30.616: INFO: Got endpoints: latency-svc-tn45f [751.306392ms]
Jul 19 00:49:30.624: INFO: Created: latency-svc-jqnj2
Jul 19 00:49:30.665: INFO: Got endpoints: latency-svc-qm5r8 [749.423903ms]
Jul 19 00:49:30.673: INFO: Created: latency-svc-p4p6p
Jul 19 00:49:30.714: INFO: Got endpoints: latency-svc-9qq24 [745.926206ms]
Jul 19 00:49:30.723: INFO: Created: latency-svc-rvgnx
Jul 19 00:49:30.769: INFO: Got endpoints: latency-svc-smmn8 [754.140412ms]
Jul 19 00:49:30.778: INFO: Created: latency-svc-5gwvt
Jul 19 00:49:30.814: INFO: Got endpoints: latency-svc-z2hxs [748.979337ms]
Jul 19 00:49:30.824: INFO: Created: latency-svc-xzs6p
Jul 19 00:49:30.865: INFO: Got endpoints: latency-svc-4ncv9 [750.233585ms]
Jul 19 00:49:30.874: INFO: Created: latency-svc-7l6b2
Jul 19 00:49:30.914: INFO: Got endpoints: latency-svc-xqhnq [730.899693ms]
Jul 19 00:49:30.923: INFO: Created: latency-svc-vk2tn
Jul 19 00:49:30.969: INFO: Got endpoints: latency-svc-mnvmg [755.454495ms]
Jul 19 00:49:30.977: INFO: Created: latency-svc-mzlsx
Jul 19 00:49:31.014: INFO: Got endpoints: latency-svc-h7697 [749.793627ms]
Jul 19 00:49:31.027: INFO: Created: latency-svc-dgw2c
Jul 19 00:49:31.065: INFO: Got endpoints: latency-svc-sz8cr [745.740293ms]
Jul 19 00:49:31.114: INFO: Got endpoints: latency-svc-xk8c9 [750.506009ms]
Jul 19 00:49:31.164: INFO: Got endpoints: latency-svc-qkpl9 [750.988231ms]
Jul 19 00:49:31.214: INFO: Got endpoints: latency-svc-fdsjg [749.749311ms]
Jul 19 00:49:31.264: INFO: Got endpoints: latency-svc-zchsw [748.389656ms]
Jul 19 00:49:31.315: INFO: Got endpoints: latency-svc-rd56h [750.644527ms]
Jul 19 00:49:31.365: INFO: Got endpoints: latency-svc-jqnj2 [748.927892ms]
Jul 19 00:49:31.414: INFO: Got endpoints: latency-svc-p4p6p [749.537519ms]
Jul 19 00:49:31.464: INFO: Got endpoints: latency-svc-rvgnx [750.514756ms]
Jul 19 00:49:31.516: INFO: Got endpoints: latency-svc-5gwvt [746.81507ms]
Jul 19 00:49:31.564: INFO: Got endpoints: latency-svc-xzs6p [749.839993ms]
Jul 19 00:49:31.613: INFO: Got endpoints: latency-svc-7l6b2 [748.773578ms]
Jul 19 00:49:31.665: INFO: Got endpoints: latency-svc-vk2tn [750.984285ms]
Jul 19 00:49:31.715: INFO: Got endpoints: latency-svc-mzlsx [745.783461ms]
Jul 19 00:49:31.764: INFO: Got endpoints: latency-svc-dgw2c [749.565633ms]
Jul 19 00:49:31.764: INFO: Latencies: [12.419536ms 46.942601ms 53.077868ms 64.10107ms 91.083315ms 132.262845ms 174.395894ms 195.364168ms 197.233547ms 211.242029ms 211.502163ms 229.800097ms 232.20813ms 237.652796ms 241.901271ms 248.549521ms 251.329843ms 263.602813ms 269.389795ms 279.897264ms 296.573968ms 303.300735ms 305.407401ms 307.320503ms 312.134593ms 319.910292ms 327.954623ms 335.57739ms 352.396187ms 357.678405ms 363.790949ms 373.81544ms 376.909361ms 381.4083ms 386.751825ms 386.778491ms 387.98973ms 388.952018ms 389.014268ms 389.462134ms 390.359432ms 390.801741ms 391.250709ms 400.946084ms 401.989107ms 404.196798ms 404.808517ms 409.502373ms 409.846075ms 410.497486ms 413.34805ms 417.026142ms 418.406212ms 426.303079ms 426.722325ms 437.109768ms 439.293213ms 447.799328ms 448.907706ms 459.554169ms 464.287033ms 465.574952ms 474.182982ms 475.410285ms 599.022327ms 680.888268ms 704.004447ms 704.319469ms 718.133049ms 723.746182ms 728.309064ms 730.200776ms 730.899693ms 735.010424ms 735.940841ms 737.652084ms 741.908672ms 745.740293ms 745.783461ms 745.785354ms 745.803693ms 745.926206ms 746.019443ms 746.189119ms 746.400429ms 746.81507ms 747.164317ms 747.345375ms 747.386498ms 748.283102ms 748.389656ms 748.392822ms 748.417631ms 748.687434ms 748.773578ms 748.810446ms 748.858762ms 748.86408ms 748.9042ms 748.909136ms 748.927892ms 748.979337ms 748.997624ms 749.046349ms 749.055081ms 749.071314ms 749.141645ms 749.170062ms 749.218798ms 749.236649ms 749.247846ms 749.303231ms 749.309616ms 749.370217ms 749.423903ms 749.449273ms 749.455617ms 749.473331ms 749.532867ms 749.537519ms 749.54799ms 749.565633ms 749.628887ms 749.663761ms 749.692227ms 749.697752ms 749.749311ms 749.793627ms 749.821374ms 749.839993ms 749.853724ms 749.899268ms 749.900319ms 749.905313ms 749.914495ms 749.979551ms 750.087299ms 750.099344ms 750.100595ms 750.126871ms 750.150404ms 750.233395ms 750.233585ms 750.235113ms 750.270402ms 750.282741ms 750.351333ms 750.403239ms 750.506009ms 750.509983ms 750.514756ms 750.520445ms 750.566873ms 750.577207ms 750.591476ms 750.621421ms 750.640372ms 750.644527ms 750.644542ms 750.712592ms 750.824278ms 750.939528ms 750.984285ms 750.988231ms 751.000429ms 751.229942ms 751.28116ms 751.306392ms 751.356711ms 751.478829ms 751.506623ms 751.57088ms 751.596173ms 751.722501ms 752.337299ms 752.498807ms 753.323998ms 753.328485ms 753.394349ms 753.416309ms 753.990957ms 754.057698ms 754.140412ms 754.492785ms 754.579339ms 754.615769ms 754.795864ms 755.454495ms 758.059483ms 761.681786ms 766.807398ms 768.456527ms 768.715252ms 768.955976ms 769.905304ms 781.798989ms 794.351494ms 839.129384ms 840.286824ms 924.090705ms]
Jul 19 00:49:31.764: INFO: 50 %ile: 748.927892ms
Jul 19 00:49:31.764: INFO: 90 %ile: 753.990957ms
Jul 19 00:49:31.764: INFO: 99 %ile: 840.286824ms
Jul 19 00:49:31.764: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jul 19 00:49:31.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-4830" for this suite. 07/19/23 00:49:31.767
------------------------------
• [SLOW TEST] [9.734 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:49:22.036
    Jul 19 00:49:22.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename svc-latency 07/19/23 00:49:22.037
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:49:22.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:49:22.045
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jul 19 00:49:22.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-4830 07/19/23 00:49:22.048
    I0719 00:49:22.051767      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4830, replica count: 1
    I0719 00:49:23.102936      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 19 00:49:23.211: INFO: Created: latency-svc-vx454
    Jul 19 00:49:23.215: INFO: Got endpoints: latency-svc-vx454 [12.352376ms]
    Jul 19 00:49:23.226: INFO: Created: latency-svc-kkp59
    Jul 19 00:49:23.228: INFO: Got endpoints: latency-svc-kkp59 [12.419536ms]
    Jul 19 00:49:23.236: INFO: Created: latency-svc-rp6sv
    Jul 19 00:49:23.262: INFO: Got endpoints: latency-svc-rp6sv [46.942601ms]
    Jul 19 00:49:23.264: INFO: Created: latency-svc-nxc9k
    Jul 19 00:49:23.268: INFO: Got endpoints: latency-svc-nxc9k [53.077868ms]
    Jul 19 00:49:23.273: INFO: Created: latency-svc-5dnfq
    Jul 19 00:49:23.280: INFO: Got endpoints: latency-svc-5dnfq [64.10107ms]
    Jul 19 00:49:23.284: INFO: Created: latency-svc-hdzmv
    Jul 19 00:49:23.306: INFO: Got endpoints: latency-svc-hdzmv [91.083315ms]
    Jul 19 00:49:23.328: INFO: Created: latency-svc-jvxfq
    Jul 19 00:49:23.348: INFO: Got endpoints: latency-svc-jvxfq [132.262845ms]
    Jul 19 00:49:23.371: INFO: Created: latency-svc-nsbgr
    Jul 19 00:49:23.390: INFO: Got endpoints: latency-svc-nsbgr [174.395894ms]
    Jul 19 00:49:23.394: INFO: Created: latency-svc-wxtwb
    Jul 19 00:49:23.413: INFO: Got endpoints: latency-svc-wxtwb [197.233547ms]
    Jul 19 00:49:23.435: INFO: Created: latency-svc-jjqlg
    Jul 19 00:49:23.453: INFO: Got endpoints: latency-svc-jjqlg [237.652796ms]
    Jul 19 00:49:23.457: INFO: Created: latency-svc-whf6x
    Jul 19 00:49:23.519: INFO: Got endpoints: latency-svc-whf6x [303.300735ms]
    Jul 19 00:49:23.521: INFO: Created: latency-svc-4ggkr
    Jul 19 00:49:23.523: INFO: Got endpoints: latency-svc-4ggkr [307.320503ms]
    Jul 19 00:49:23.548: INFO: Created: latency-svc-92dq8
    Jul 19 00:49:23.568: INFO: Got endpoints: latency-svc-92dq8 [352.396187ms]
    Jul 19 00:49:23.588: INFO: Created: latency-svc-kd2pr
    Jul 19 00:49:23.606: INFO: Got endpoints: latency-svc-kd2pr [390.359432ms]
    Jul 19 00:49:23.612: INFO: Created: latency-svc-249s7
    Jul 19 00:49:23.632: INFO: Got endpoints: latency-svc-249s7 [417.026142ms]
    Jul 19 00:49:23.636: INFO: Created: latency-svc-v4bdm
    Jul 19 00:49:23.655: INFO: Got endpoints: latency-svc-v4bdm [439.293213ms]
    Jul 19 00:49:23.658: INFO: Created: latency-svc-hgr62
    Jul 19 00:49:23.677: INFO: Got endpoints: latency-svc-hgr62 [448.907706ms]
    Jul 19 00:49:23.682: INFO: Created: latency-svc-kgwfl
    Jul 19 00:49:23.699: INFO: Got endpoints: latency-svc-kgwfl [437.109768ms]
    Jul 19 00:49:23.705: INFO: Created: latency-svc-jd2sf
    Jul 19 00:49:23.733: INFO: Got endpoints: latency-svc-jd2sf [464.287033ms]
    Jul 19 00:49:23.748: INFO: Created: latency-svc-s4gk8
    Jul 19 00:49:23.754: INFO: Got endpoints: latency-svc-s4gk8 [474.182982ms]
    Jul 19 00:49:23.774: INFO: Created: latency-svc-lwwwv
    Jul 19 00:49:23.782: INFO: Got endpoints: latency-svc-lwwwv [475.410285ms]
    Jul 19 00:49:23.805: INFO: Created: latency-svc-m5vj2
    Jul 19 00:49:23.807: INFO: Got endpoints: latency-svc-m5vj2 [459.554169ms]
    Jul 19 00:49:23.814: INFO: Created: latency-svc-xlsqx
    Jul 19 00:49:23.816: INFO: Got endpoints: latency-svc-xlsqx [426.303079ms]
    Jul 19 00:49:23.821: INFO: Created: latency-svc-tht8c
    Jul 19 00:49:23.839: INFO: Got endpoints: latency-svc-tht8c [426.722325ms]
    Jul 19 00:49:23.843: INFO: Created: latency-svc-j8hk5
    Jul 19 00:49:23.866: INFO: Got endpoints: latency-svc-j8hk5 [413.34805ms]
    Jul 19 00:49:23.887: INFO: Created: latency-svc-hprh8
    Jul 19 00:49:23.906: INFO: Got endpoints: latency-svc-hprh8 [386.751825ms]
    Jul 19 00:49:23.911: INFO: Created: latency-svc-lhqjq
    Jul 19 00:49:23.928: INFO: Got endpoints: latency-svc-lhqjq [404.808517ms]
    Jul 19 00:49:23.932: INFO: Created: latency-svc-kqlxm
    Jul 19 00:49:23.949: INFO: Got endpoints: latency-svc-kqlxm [381.4083ms]
    Jul 19 00:49:23.953: INFO: Created: latency-svc-wt6lz
    Jul 19 00:49:24.016: INFO: Got endpoints: latency-svc-wt6lz [410.497486ms]
    Jul 19 00:49:24.018: INFO: Created: latency-svc-2ggjj
    Jul 19 00:49:24.022: INFO: Got endpoints: latency-svc-2ggjj [389.014268ms]
    Jul 19 00:49:24.026: INFO: Created: latency-svc-5q7qc
    Jul 19 00:49:24.044: INFO: Got endpoints: latency-svc-5q7qc [389.462134ms]
    Jul 19 00:49:24.069: INFO: Created: latency-svc-dn8k6
    Jul 19 00:49:24.087: INFO: Got endpoints: latency-svc-dn8k6 [409.846075ms]
    Jul 19 00:49:24.090: INFO: Created: latency-svc-8sjpq
    Jul 19 00:49:24.109: INFO: Got endpoints: latency-svc-8sjpq [409.502373ms]
    Jul 19 00:49:24.113: INFO: Created: latency-svc-2wk5x
    Jul 19 00:49:24.135: INFO: Got endpoints: latency-svc-2wk5x [401.989107ms]
    Jul 19 00:49:24.140: INFO: Created: latency-svc-vrqfj
    Jul 19 00:49:24.158: INFO: Got endpoints: latency-svc-vrqfj [404.196798ms]
    Jul 19 00:49:24.167: INFO: Created: latency-svc-6xx9f
    Jul 19 00:49:24.169: INFO: Got endpoints: latency-svc-6xx9f [386.778491ms]
    Jul 19 00:49:24.194: INFO: Created: latency-svc-c87lr
    Jul 19 00:49:24.199: INFO: Got endpoints: latency-svc-c87lr [391.250709ms]
    Jul 19 00:49:24.202: INFO: Created: latency-svc-nxzcc
    Jul 19 00:49:24.204: INFO: Got endpoints: latency-svc-nxzcc [387.98973ms]
    Jul 19 00:49:24.211: INFO: Created: latency-svc-fvvsp
    Jul 19 00:49:24.228: INFO: Got endpoints: latency-svc-fvvsp [388.952018ms]
    Jul 19 00:49:24.233: INFO: Created: latency-svc-4n8hg
    Jul 19 00:49:24.257: INFO: Got endpoints: latency-svc-4n8hg [390.801741ms]
    Jul 19 00:49:24.261: INFO: Created: latency-svc-9kbvw
    Jul 19 00:49:24.263: INFO: Got endpoints: latency-svc-9kbvw [357.678405ms]
    Jul 19 00:49:24.302: INFO: Created: latency-svc-kmdqq
    Jul 19 00:49:24.305: INFO: Got endpoints: latency-svc-kmdqq [376.909361ms]
    Jul 19 00:49:24.311: INFO: Created: latency-svc-svxwg
    Jul 19 00:49:24.313: INFO: Got endpoints: latency-svc-svxwg [363.790949ms]
    Jul 19 00:49:24.324: INFO: Created: latency-svc-xvpmg
    Jul 19 00:49:24.328: INFO: Got endpoints: latency-svc-xvpmg [312.134593ms]
    Jul 19 00:49:24.340: INFO: Created: latency-svc-2qn2j
    Jul 19 00:49:24.341: INFO: Got endpoints: latency-svc-2qn2j [319.910292ms]
    Jul 19 00:49:24.347: INFO: Created: latency-svc-nmfht
    Jul 19 00:49:24.350: INFO: Got endpoints: latency-svc-nmfht [305.407401ms]
    Jul 19 00:49:24.364: INFO: Created: latency-svc-rb64d
    Jul 19 00:49:24.367: INFO: Got endpoints: latency-svc-rb64d [279.897264ms]
    Jul 19 00:49:24.372: INFO: Created: latency-svc-xq6m7
    Jul 19 00:49:24.378: INFO: Got endpoints: latency-svc-xq6m7 [269.389795ms]
    Jul 19 00:49:24.381: INFO: Created: latency-svc-bj25x
    Jul 19 00:49:24.383: INFO: Got endpoints: latency-svc-bj25x [248.549521ms]
    Jul 19 00:49:24.388: INFO: Created: latency-svc-zn4mq
    Jul 19 00:49:24.390: INFO: Got endpoints: latency-svc-zn4mq [232.20813ms]
    Jul 19 00:49:24.396: INFO: Created: latency-svc-lclxx
    Jul 19 00:49:24.398: INFO: Got endpoints: latency-svc-lclxx [229.800097ms]
    Jul 19 00:49:24.407: INFO: Created: latency-svc-xl8d4
    Jul 19 00:49:24.410: INFO: Got endpoints: latency-svc-xl8d4 [211.502163ms]
    Jul 19 00:49:24.414: INFO: Created: latency-svc-qcpbl
    Jul 19 00:49:24.415: INFO: Got endpoints: latency-svc-qcpbl [211.242029ms]
    Jul 19 00:49:24.421: INFO: Created: latency-svc-fmwdc
    Jul 19 00:49:24.424: INFO: Got endpoints: latency-svc-fmwdc [195.364168ms]
    Jul 19 00:49:24.429: INFO: Created: latency-svc-2c559
    Jul 19 00:49:24.436: INFO: Created: latency-svc-sdvzj
    Jul 19 00:49:24.445: INFO: Created: latency-svc-h5st7
    Jul 19 00:49:24.463: INFO: Created: latency-svc-7ffhb
    Jul 19 00:49:24.499: INFO: Got endpoints: latency-svc-2c559 [241.901271ms]
    Jul 19 00:49:24.501: INFO: Created: latency-svc-lfgkn
    Jul 19 00:49:24.508: INFO: Created: latency-svc-scwwn
    Jul 19 00:49:24.514: INFO: Created: latency-svc-s2t9d
    Jul 19 00:49:24.515: INFO: Got endpoints: latency-svc-sdvzj [251.329843ms]
    Jul 19 00:49:24.520: INFO: Created: latency-svc-p9wln
    Jul 19 00:49:24.568: INFO: Got endpoints: latency-svc-h5st7 [263.602813ms]
    Jul 19 00:49:24.583: INFO: Created: latency-svc-5vx7r
    Jul 19 00:49:24.583: INFO: Created: latency-svc-k2vb9
    Jul 19 00:49:24.583: INFO: Created: latency-svc-6gg28
    Jul 19 00:49:24.583: INFO: Created: latency-svc-q8l5j
    Jul 19 00:49:24.583: INFO: Created: latency-svc-stk5w
    Jul 19 00:49:24.583: INFO: Created: latency-svc-tbhnz
    Jul 19 00:49:24.583: INFO: Created: latency-svc-5pfp5
    Jul 19 00:49:24.583: INFO: Created: latency-svc-mz9v2
    Jul 19 00:49:24.583: INFO: Created: latency-svc-78qpm
    Jul 19 00:49:24.583: INFO: Created: latency-svc-7cz6l
    Jul 19 00:49:24.641: INFO: Got endpoints: latency-svc-7ffhb [327.954623ms]
    Jul 19 00:49:24.650: INFO: Created: latency-svc-g8g8l
    Jul 19 00:49:24.664: INFO: Got endpoints: latency-svc-lfgkn [335.57739ms]
    Jul 19 00:49:24.678: INFO: Created: latency-svc-g862b
    Jul 19 00:49:24.715: INFO: Got endpoints: latency-svc-scwwn [373.81544ms]
    Jul 19 00:49:24.724: INFO: Created: latency-svc-8fdxt
    Jul 19 00:49:24.768: INFO: Got endpoints: latency-svc-s2t9d [418.406212ms]
    Jul 19 00:49:24.777: INFO: Created: latency-svc-sgmjx
    Jul 19 00:49:24.814: INFO: Got endpoints: latency-svc-p9wln [447.799328ms]
    Jul 19 00:49:24.824: INFO: Created: latency-svc-w72cr
    Jul 19 00:49:24.865: INFO: Got endpoints: latency-svc-q8l5j [296.573968ms]
    Jul 19 00:49:24.875: INFO: Created: latency-svc-m6w4m
    Jul 19 00:49:24.916: INFO: Got endpoints: latency-svc-6gg28 [400.946084ms]
    Jul 19 00:49:24.924: INFO: Created: latency-svc-kr7kd
    Jul 19 00:49:24.965: INFO: Got endpoints: latency-svc-stk5w [465.574952ms]
    Jul 19 00:49:24.976: INFO: Created: latency-svc-zqgsz
    Jul 19 00:49:25.014: INFO: Got endpoints: latency-svc-5vx7r [599.022327ms]
    Jul 19 00:49:25.022: INFO: Created: latency-svc-skl7s
    Jul 19 00:49:25.064: INFO: Got endpoints: latency-svc-mz9v2 [680.888268ms]
    Jul 19 00:49:25.074: INFO: Created: latency-svc-s65kk
    Jul 19 00:49:25.114: INFO: Got endpoints: latency-svc-7cz6l [704.004447ms]
    Jul 19 00:49:25.125: INFO: Created: latency-svc-vtsr5
    Jul 19 00:49:25.167: INFO: Got endpoints: latency-svc-tbhnz [768.955976ms]
    Jul 19 00:49:25.176: INFO: Created: latency-svc-689cg
    Jul 19 00:49:25.217: INFO: Got endpoints: latency-svc-5pfp5 [839.129384ms]
    Jul 19 00:49:25.226: INFO: Created: latency-svc-8tmsx
    Jul 19 00:49:25.264: INFO: Got endpoints: latency-svc-78qpm [840.286824ms]
    Jul 19 00:49:25.275: INFO: Created: latency-svc-96bdt
    Jul 19 00:49:25.314: INFO: Got endpoints: latency-svc-k2vb9 [924.090705ms]
    Jul 19 00:49:25.323: INFO: Created: latency-svc-xhlhz
    Jul 19 00:49:25.365: INFO: Got endpoints: latency-svc-g8g8l [723.746182ms]
    Jul 19 00:49:25.373: INFO: Created: latency-svc-65wjn
    Jul 19 00:49:25.419: INFO: Got endpoints: latency-svc-g862b [754.795864ms]
    Jul 19 00:49:25.428: INFO: Created: latency-svc-vn4wq
    Jul 19 00:49:25.464: INFO: Got endpoints: latency-svc-8fdxt [749.055081ms]
    Jul 19 00:49:25.475: INFO: Created: latency-svc-6fl2l
    Jul 19 00:49:25.514: INFO: Got endpoints: latency-svc-sgmjx [746.019443ms]
    Jul 19 00:49:25.522: INFO: Created: latency-svc-rxz5t
    Jul 19 00:49:25.564: INFO: Got endpoints: latency-svc-w72cr [749.170062ms]
    Jul 19 00:49:25.575: INFO: Created: latency-svc-x5c54
    Jul 19 00:49:25.614: INFO: Got endpoints: latency-svc-m6w4m [749.692227ms]
    Jul 19 00:49:25.625: INFO: Created: latency-svc-5bm25
    Jul 19 00:49:25.664: INFO: Got endpoints: latency-svc-kr7kd [748.858762ms]
    Jul 19 00:49:25.674: INFO: Created: latency-svc-twfh2
    Jul 19 00:49:25.714: INFO: Got endpoints: latency-svc-zqgsz [749.473331ms]
    Jul 19 00:49:25.727: INFO: Created: latency-svc-tbqxs
    Jul 19 00:49:25.765: INFO: Got endpoints: latency-svc-skl7s [750.282741ms]
    Jul 19 00:49:25.774: INFO: Created: latency-svc-lhvk7
    Jul 19 00:49:25.815: INFO: Got endpoints: latency-svc-s65kk [750.351333ms]
    Jul 19 00:49:25.825: INFO: Created: latency-svc-zv26w
    Jul 19 00:49:25.865: INFO: Got endpoints: latency-svc-vtsr5 [750.591476ms]
    Jul 19 00:49:25.877: INFO: Created: latency-svc-2b8zj
    Jul 19 00:49:25.917: INFO: Got endpoints: latency-svc-689cg [749.905313ms]
    Jul 19 00:49:25.927: INFO: Created: latency-svc-bfgql
    Jul 19 00:49:25.964: INFO: Got endpoints: latency-svc-8tmsx [746.189119ms]
    Jul 19 00:49:25.991: INFO: Created: latency-svc-t6jz8
    Jul 19 00:49:26.019: INFO: Got endpoints: latency-svc-96bdt [754.615769ms]
    Jul 19 00:49:26.027: INFO: Created: latency-svc-wn6pd
    Jul 19 00:49:26.064: INFO: Got endpoints: latency-svc-xhlhz [749.303231ms]
    Jul 19 00:49:26.076: INFO: Created: latency-svc-sckxx
    Jul 19 00:49:26.114: INFO: Got endpoints: latency-svc-65wjn [749.247846ms]
    Jul 19 00:49:26.127: INFO: Created: latency-svc-rx4dk
    Jul 19 00:49:26.165: INFO: Got endpoints: latency-svc-vn4wq [745.803693ms]
    Jul 19 00:49:26.176: INFO: Created: latency-svc-zt8c5
    Jul 19 00:49:26.214: INFO: Got endpoints: latency-svc-6fl2l [749.914495ms]
    Jul 19 00:49:26.227: INFO: Created: latency-svc-ww7df
    Jul 19 00:49:26.265: INFO: Got endpoints: latency-svc-rxz5t [750.824278ms]
    Jul 19 00:49:26.273: INFO: Created: latency-svc-fkbzq
    Jul 19 00:49:26.315: INFO: Got endpoints: latency-svc-x5c54 [751.506623ms]
    Jul 19 00:49:26.325: INFO: Created: latency-svc-g9qz6
    Jul 19 00:49:26.364: INFO: Got endpoints: latency-svc-5bm25 [749.54799ms]
    Jul 19 00:49:26.376: INFO: Created: latency-svc-c25d9
    Jul 19 00:49:26.415: INFO: Got endpoints: latency-svc-twfh2 [750.150404ms]
    Jul 19 00:49:26.426: INFO: Created: latency-svc-8kk8c
    Jul 19 00:49:26.465: INFO: Got endpoints: latency-svc-tbqxs [750.712592ms]
    Jul 19 00:49:26.474: INFO: Created: latency-svc-qwpz5
    Jul 19 00:49:26.516: INFO: Got endpoints: latency-svc-lhvk7 [751.478829ms]
    Jul 19 00:49:26.525: INFO: Created: latency-svc-86s7c
    Jul 19 00:49:26.564: INFO: Got endpoints: latency-svc-zv26w [749.449273ms]
    Jul 19 00:49:26.577: INFO: Created: latency-svc-j8phf
    Jul 19 00:49:26.616: INFO: Got endpoints: latency-svc-2b8zj [751.28116ms]
    Jul 19 00:49:26.625: INFO: Created: latency-svc-fv8sr
    Jul 19 00:49:26.665: INFO: Got endpoints: latency-svc-bfgql [747.386498ms]
    Jul 19 00:49:26.674: INFO: Created: latency-svc-gf2df
    Jul 19 00:49:26.718: INFO: Got endpoints: latency-svc-t6jz8 [754.492785ms]
    Jul 19 00:49:26.728: INFO: Created: latency-svc-4wgmb
    Jul 19 00:49:26.772: INFO: Got endpoints: latency-svc-wn6pd [753.416309ms]
    Jul 19 00:49:26.781: INFO: Created: latency-svc-xql5k
    Jul 19 00:49:26.814: INFO: Got endpoints: latency-svc-sckxx [750.087299ms]
    Jul 19 00:49:26.824: INFO: Created: latency-svc-ddr88
    Jul 19 00:49:26.864: INFO: Got endpoints: latency-svc-rx4dk [750.235113ms]
    Jul 19 00:49:26.917: INFO: Got endpoints: latency-svc-zt8c5 [752.337299ms]
    Jul 19 00:49:26.938: INFO: Created: latency-svc-86zjl
    Jul 19 00:49:26.949: INFO: Created: latency-svc-fmhpv
    Jul 19 00:49:26.965: INFO: Got endpoints: latency-svc-ww7df [750.403239ms]
    Jul 19 00:49:26.999: INFO: Created: latency-svc-9n4g8
    Jul 19 00:49:27.059: INFO: Got endpoints: latency-svc-fkbzq [794.351494ms]
    Jul 19 00:49:27.069: INFO: Got endpoints: latency-svc-g9qz6 [753.990957ms]
    Jul 19 00:49:27.073: INFO: Created: latency-svc-qkmpd
    Jul 19 00:49:27.095: INFO: Created: latency-svc-frcvd
    Jul 19 00:49:27.117: INFO: Got endpoints: latency-svc-c25d9 [753.323998ms]
    Jul 19 00:49:27.143: INFO: Created: latency-svc-strrt
    Jul 19 00:49:27.185: INFO: Got endpoints: latency-svc-8kk8c [769.905304ms]
    Jul 19 00:49:27.195: INFO: Created: latency-svc-p6dvk
    Jul 19 00:49:27.217: INFO: Got endpoints: latency-svc-qwpz5 [751.596173ms]
    Jul 19 00:49:27.248: INFO: Created: latency-svc-2tltt
    Jul 19 00:49:27.265: INFO: Got endpoints: latency-svc-86s7c [748.687434ms]
    Jul 19 00:49:27.316: INFO: Got endpoints: latency-svc-j8phf [751.722501ms]
    Jul 19 00:49:27.316: INFO: Created: latency-svc-hwwwx
    Jul 19 00:49:27.341: INFO: Created: latency-svc-tk8kx
    Jul 19 00:49:27.364: INFO: Got endpoints: latency-svc-fv8sr [748.417631ms]
    Jul 19 00:49:27.400: INFO: Created: latency-svc-gtkls
    Jul 19 00:49:27.447: INFO: Got endpoints: latency-svc-gf2df [781.798989ms]
    Jul 19 00:49:27.458: INFO: Created: latency-svc-n5nrj
    Jul 19 00:49:27.480: INFO: Got endpoints: latency-svc-4wgmb [761.681786ms]
    Jul 19 00:49:27.489: INFO: Created: latency-svc-vrrd5
    Jul 19 00:49:27.514: INFO: Got endpoints: latency-svc-xql5k [741.908672ms]
    Jul 19 00:49:27.540: INFO: Created: latency-svc-w2w5q
    Jul 19 00:49:27.581: INFO: Got endpoints: latency-svc-ddr88 [766.807398ms]
    Jul 19 00:49:27.592: INFO: Created: latency-svc-sqlc2
    Jul 19 00:49:27.616: INFO: Got endpoints: latency-svc-86zjl [751.229942ms]
    Jul 19 00:49:27.640: INFO: Created: latency-svc-xfbvh
    Jul 19 00:49:27.663: INFO: Got endpoints: latency-svc-fmhpv [746.400429ms]
    Jul 19 00:49:27.712: INFO: Created: latency-svc-jd67h
    Jul 19 00:49:27.714: INFO: Got endpoints: latency-svc-9n4g8 [749.697752ms]
    Jul 19 00:49:27.738: INFO: Created: latency-svc-kbnk7
    Jul 19 00:49:27.764: INFO: Got endpoints: latency-svc-qkmpd [704.319469ms]
    Jul 19 00:49:27.788: INFO: Created: latency-svc-m6nw5
    Jul 19 00:49:27.838: INFO: Got endpoints: latency-svc-frcvd [768.456527ms]
    Jul 19 00:49:27.848: INFO: Created: latency-svc-qk4wh
    Jul 19 00:49:27.867: INFO: Got endpoints: latency-svc-strrt [749.979551ms]
    Jul 19 00:49:27.898: INFO: Created: latency-svc-svgdk
    Jul 19 00:49:27.915: INFO: Got endpoints: latency-svc-p6dvk [730.200776ms]
    Jul 19 00:49:27.925: INFO: Created: latency-svc-fqptn
    Jul 19 00:49:27.964: INFO: Got endpoints: latency-svc-2tltt [747.345375ms]
    Jul 19 00:49:27.973: INFO: Created: latency-svc-mw6s5
    Jul 19 00:49:28.015: INFO: Got endpoints: latency-svc-hwwwx [749.663761ms]
    Jul 19 00:49:28.024: INFO: Created: latency-svc-hv7t4
    Jul 19 00:49:28.065: INFO: Got endpoints: latency-svc-tk8kx [748.810446ms]
    Jul 19 00:49:28.075: INFO: Created: latency-svc-g6frw
    Jul 19 00:49:28.114: INFO: Got endpoints: latency-svc-gtkls [749.900319ms]
    Jul 19 00:49:28.123: INFO: Created: latency-svc-f4gt8
    Jul 19 00:49:28.165: INFO: Got endpoints: latency-svc-n5nrj [718.133049ms]
    Jul 19 00:49:28.176: INFO: Created: latency-svc-pg92l
    Jul 19 00:49:28.216: INFO: Got endpoints: latency-svc-vrrd5 [735.940841ms]
    Jul 19 00:49:28.226: INFO: Created: latency-svc-vvqhl
    Jul 19 00:49:28.269: INFO: Got endpoints: latency-svc-w2w5q [754.579339ms]
    Jul 19 00:49:28.279: INFO: Created: latency-svc-zhx2q
    Jul 19 00:49:28.316: INFO: Got endpoints: latency-svc-sqlc2 [735.010424ms]
    Jul 19 00:49:28.327: INFO: Created: latency-svc-4pkxs
    Jul 19 00:49:28.365: INFO: Got endpoints: latency-svc-xfbvh [749.455617ms]
    Jul 19 00:49:28.384: INFO: Created: latency-svc-wbc6n
    Jul 19 00:49:28.415: INFO: Got endpoints: latency-svc-jd67h [751.57088ms]
    Jul 19 00:49:28.425: INFO: Created: latency-svc-kn8wb
    Jul 19 00:49:28.465: INFO: Got endpoints: latency-svc-kbnk7 [750.644542ms]
    Jul 19 00:49:28.484: INFO: Created: latency-svc-l27zb
    Jul 19 00:49:28.514: INFO: Got endpoints: latency-svc-m6nw5 [750.509983ms]
    Jul 19 00:49:28.532: INFO: Created: latency-svc-qgbbd
    Jul 19 00:49:28.566: INFO: Got endpoints: latency-svc-qk4wh [728.309064ms]
    Jul 19 00:49:28.574: INFO: Created: latency-svc-jd47q
    Jul 19 00:49:28.615: INFO: Got endpoints: latency-svc-svgdk [747.164317ms]
    Jul 19 00:49:28.625: INFO: Created: latency-svc-p9csc
    Jul 19 00:49:28.665: INFO: Got endpoints: latency-svc-fqptn [750.520445ms]
    Jul 19 00:49:28.674: INFO: Created: latency-svc-zzbtx
    Jul 19 00:49:28.716: INFO: Got endpoints: latency-svc-mw6s5 [752.498807ms]
    Jul 19 00:49:28.726: INFO: Created: latency-svc-s6gsj
    Jul 19 00:49:28.768: INFO: Got endpoints: latency-svc-hv7t4 [753.328485ms]
    Jul 19 00:49:28.777: INFO: Created: latency-svc-qnqgj
    Jul 19 00:49:28.814: INFO: Got endpoints: latency-svc-g6frw [749.532867ms]
    Jul 19 00:49:28.825: INFO: Created: latency-svc-wzgpq
    Jul 19 00:49:28.864: INFO: Got endpoints: latency-svc-f4gt8 [750.100595ms]
    Jul 19 00:49:28.882: INFO: Created: latency-svc-9jhjz
    Jul 19 00:49:28.915: INFO: Got endpoints: latency-svc-pg92l [750.270402ms]
    Jul 19 00:49:28.924: INFO: Created: latency-svc-mc9db
    Jul 19 00:49:28.964: INFO: Got endpoints: latency-svc-vvqhl [748.283102ms]
    Jul 19 00:49:28.974: INFO: Created: latency-svc-nz2xd
    Jul 19 00:49:29.027: INFO: Got endpoints: latency-svc-zhx2q [758.059483ms]
    Jul 19 00:49:29.035: INFO: Created: latency-svc-l954h
    Jul 19 00:49:29.065: INFO: Got endpoints: latency-svc-4pkxs [748.9042ms]
    Jul 19 00:49:29.074: INFO: Created: latency-svc-bbz44
    Jul 19 00:49:29.116: INFO: Got endpoints: latency-svc-wbc6n [750.566873ms]
    Jul 19 00:49:29.126: INFO: Created: latency-svc-dhqsp
    Jul 19 00:49:29.166: INFO: Got endpoints: latency-svc-kn8wb [751.000429ms]
    Jul 19 00:49:29.175: INFO: Created: latency-svc-mx5bz
    Jul 19 00:49:29.214: INFO: Got endpoints: latency-svc-l27zb [749.141645ms]
    Jul 19 00:49:29.222: INFO: Created: latency-svc-x6btz
    Jul 19 00:49:29.264: INFO: Got endpoints: latency-svc-qgbbd [749.821374ms]
    Jul 19 00:49:29.274: INFO: Created: latency-svc-pk4pc
    Jul 19 00:49:29.315: INFO: Got endpoints: latency-svc-jd47q [749.309616ms]
    Jul 19 00:49:29.324: INFO: Created: latency-svc-zpfg6
    Jul 19 00:49:29.364: INFO: Got endpoints: latency-svc-p9csc [749.628887ms]
    Jul 19 00:49:29.374: INFO: Created: latency-svc-r88km
    Jul 19 00:49:29.414: INFO: Got endpoints: latency-svc-zzbtx [749.046349ms]
    Jul 19 00:49:29.424: INFO: Created: latency-svc-4r5s5
    Jul 19 00:49:29.466: INFO: Got endpoints: latency-svc-s6gsj [749.071314ms]
    Jul 19 00:49:29.475: INFO: Created: latency-svc-rp6hx
    Jul 19 00:49:29.514: INFO: Got endpoints: latency-svc-qnqgj [745.785354ms]
    Jul 19 00:49:29.524: INFO: Created: latency-svc-fdwp5
    Jul 19 00:49:29.565: INFO: Got endpoints: latency-svc-wzgpq [750.640372ms]
    Jul 19 00:49:29.575: INFO: Created: latency-svc-mq5h7
    Jul 19 00:49:29.614: INFO: Got endpoints: latency-svc-9jhjz [749.218798ms]
    Jul 19 00:49:29.622: INFO: Created: latency-svc-fbvhp
    Jul 19 00:49:29.664: INFO: Got endpoints: latency-svc-mc9db [748.997624ms]
    Jul 19 00:49:29.679: INFO: Created: latency-svc-77fqp
    Jul 19 00:49:29.715: INFO: Got endpoints: latency-svc-nz2xd [750.939528ms]
    Jul 19 00:49:29.723: INFO: Created: latency-svc-bbr2j
    Jul 19 00:49:29.764: INFO: Got endpoints: latency-svc-l954h [737.652084ms]
    Jul 19 00:49:29.779: INFO: Created: latency-svc-xpwdl
    Jul 19 00:49:29.814: INFO: Got endpoints: latency-svc-bbz44 [749.853724ms]
    Jul 19 00:49:29.825: INFO: Created: latency-svc-ptdx5
    Jul 19 00:49:29.865: INFO: Got endpoints: latency-svc-dhqsp [748.86408ms]
    Jul 19 00:49:29.895: INFO: Created: latency-svc-tn45f
    Jul 19 00:49:29.915: INFO: Got endpoints: latency-svc-mx5bz [749.236649ms]
    Jul 19 00:49:29.924: INFO: Created: latency-svc-qm5r8
    Jul 19 00:49:29.968: INFO: Got endpoints: latency-svc-x6btz [753.394349ms]
    Jul 19 00:49:29.977: INFO: Created: latency-svc-9qq24
    Jul 19 00:49:30.015: INFO: Got endpoints: latency-svc-pk4pc [750.621421ms]
    Jul 19 00:49:30.025: INFO: Created: latency-svc-smmn8
    Jul 19 00:49:30.065: INFO: Got endpoints: latency-svc-zpfg6 [750.099344ms]
    Jul 19 00:49:30.075: INFO: Created: latency-svc-z2hxs
    Jul 19 00:49:30.114: INFO: Got endpoints: latency-svc-r88km [750.126871ms]
    Jul 19 00:49:30.183: INFO: Got endpoints: latency-svc-4r5s5 [768.715252ms]
    Jul 19 00:49:30.183: INFO: Created: latency-svc-4ncv9
    Jul 19 00:49:30.193: INFO: Created: latency-svc-xqhnq
    Jul 19 00:49:30.214: INFO: Got endpoints: latency-svc-rp6hx [748.392822ms]
    Jul 19 00:49:30.223: INFO: Created: latency-svc-mnvmg
    Jul 19 00:49:30.264: INFO: Got endpoints: latency-svc-fdwp5 [750.577207ms]
    Jul 19 00:49:30.274: INFO: Created: latency-svc-h7697
    Jul 19 00:49:30.319: INFO: Got endpoints: latency-svc-mq5h7 [754.057698ms]
    Jul 19 00:49:30.327: INFO: Created: latency-svc-sz8cr
    Jul 19 00:49:30.364: INFO: Got endpoints: latency-svc-fbvhp [749.899268ms]
    Jul 19 00:49:30.374: INFO: Created: latency-svc-xk8c9
    Jul 19 00:49:30.413: INFO: Got endpoints: latency-svc-77fqp [749.370217ms]
    Jul 19 00:49:30.430: INFO: Created: latency-svc-qkpl9
    Jul 19 00:49:30.464: INFO: Got endpoints: latency-svc-bbr2j [748.909136ms]
    Jul 19 00:49:30.473: INFO: Created: latency-svc-fdsjg
    Jul 19 00:49:30.516: INFO: Got endpoints: latency-svc-xpwdl [751.356711ms]
    Jul 19 00:49:30.528: INFO: Created: latency-svc-zchsw
    Jul 19 00:49:30.565: INFO: Got endpoints: latency-svc-ptdx5 [750.233395ms]
    Jul 19 00:49:30.574: INFO: Created: latency-svc-rd56h
    Jul 19 00:49:30.616: INFO: Got endpoints: latency-svc-tn45f [751.306392ms]
    Jul 19 00:49:30.624: INFO: Created: latency-svc-jqnj2
    Jul 19 00:49:30.665: INFO: Got endpoints: latency-svc-qm5r8 [749.423903ms]
    Jul 19 00:49:30.673: INFO: Created: latency-svc-p4p6p
    Jul 19 00:49:30.714: INFO: Got endpoints: latency-svc-9qq24 [745.926206ms]
    Jul 19 00:49:30.723: INFO: Created: latency-svc-rvgnx
    Jul 19 00:49:30.769: INFO: Got endpoints: latency-svc-smmn8 [754.140412ms]
    Jul 19 00:49:30.778: INFO: Created: latency-svc-5gwvt
    Jul 19 00:49:30.814: INFO: Got endpoints: latency-svc-z2hxs [748.979337ms]
    Jul 19 00:49:30.824: INFO: Created: latency-svc-xzs6p
    Jul 19 00:49:30.865: INFO: Got endpoints: latency-svc-4ncv9 [750.233585ms]
    Jul 19 00:49:30.874: INFO: Created: latency-svc-7l6b2
    Jul 19 00:49:30.914: INFO: Got endpoints: latency-svc-xqhnq [730.899693ms]
    Jul 19 00:49:30.923: INFO: Created: latency-svc-vk2tn
    Jul 19 00:49:30.969: INFO: Got endpoints: latency-svc-mnvmg [755.454495ms]
    Jul 19 00:49:30.977: INFO: Created: latency-svc-mzlsx
    Jul 19 00:49:31.014: INFO: Got endpoints: latency-svc-h7697 [749.793627ms]
    Jul 19 00:49:31.027: INFO: Created: latency-svc-dgw2c
    Jul 19 00:49:31.065: INFO: Got endpoints: latency-svc-sz8cr [745.740293ms]
    Jul 19 00:49:31.114: INFO: Got endpoints: latency-svc-xk8c9 [750.506009ms]
    Jul 19 00:49:31.164: INFO: Got endpoints: latency-svc-qkpl9 [750.988231ms]
    Jul 19 00:49:31.214: INFO: Got endpoints: latency-svc-fdsjg [749.749311ms]
    Jul 19 00:49:31.264: INFO: Got endpoints: latency-svc-zchsw [748.389656ms]
    Jul 19 00:49:31.315: INFO: Got endpoints: latency-svc-rd56h [750.644527ms]
    Jul 19 00:49:31.365: INFO: Got endpoints: latency-svc-jqnj2 [748.927892ms]
    Jul 19 00:49:31.414: INFO: Got endpoints: latency-svc-p4p6p [749.537519ms]
    Jul 19 00:49:31.464: INFO: Got endpoints: latency-svc-rvgnx [750.514756ms]
    Jul 19 00:49:31.516: INFO: Got endpoints: latency-svc-5gwvt [746.81507ms]
    Jul 19 00:49:31.564: INFO: Got endpoints: latency-svc-xzs6p [749.839993ms]
    Jul 19 00:49:31.613: INFO: Got endpoints: latency-svc-7l6b2 [748.773578ms]
    Jul 19 00:49:31.665: INFO: Got endpoints: latency-svc-vk2tn [750.984285ms]
    Jul 19 00:49:31.715: INFO: Got endpoints: latency-svc-mzlsx [745.783461ms]
    Jul 19 00:49:31.764: INFO: Got endpoints: latency-svc-dgw2c [749.565633ms]
    Jul 19 00:49:31.764: INFO: Latencies: [12.419536ms 46.942601ms 53.077868ms 64.10107ms 91.083315ms 132.262845ms 174.395894ms 195.364168ms 197.233547ms 211.242029ms 211.502163ms 229.800097ms 232.20813ms 237.652796ms 241.901271ms 248.549521ms 251.329843ms 263.602813ms 269.389795ms 279.897264ms 296.573968ms 303.300735ms 305.407401ms 307.320503ms 312.134593ms 319.910292ms 327.954623ms 335.57739ms 352.396187ms 357.678405ms 363.790949ms 373.81544ms 376.909361ms 381.4083ms 386.751825ms 386.778491ms 387.98973ms 388.952018ms 389.014268ms 389.462134ms 390.359432ms 390.801741ms 391.250709ms 400.946084ms 401.989107ms 404.196798ms 404.808517ms 409.502373ms 409.846075ms 410.497486ms 413.34805ms 417.026142ms 418.406212ms 426.303079ms 426.722325ms 437.109768ms 439.293213ms 447.799328ms 448.907706ms 459.554169ms 464.287033ms 465.574952ms 474.182982ms 475.410285ms 599.022327ms 680.888268ms 704.004447ms 704.319469ms 718.133049ms 723.746182ms 728.309064ms 730.200776ms 730.899693ms 735.010424ms 735.940841ms 737.652084ms 741.908672ms 745.740293ms 745.783461ms 745.785354ms 745.803693ms 745.926206ms 746.019443ms 746.189119ms 746.400429ms 746.81507ms 747.164317ms 747.345375ms 747.386498ms 748.283102ms 748.389656ms 748.392822ms 748.417631ms 748.687434ms 748.773578ms 748.810446ms 748.858762ms 748.86408ms 748.9042ms 748.909136ms 748.927892ms 748.979337ms 748.997624ms 749.046349ms 749.055081ms 749.071314ms 749.141645ms 749.170062ms 749.218798ms 749.236649ms 749.247846ms 749.303231ms 749.309616ms 749.370217ms 749.423903ms 749.449273ms 749.455617ms 749.473331ms 749.532867ms 749.537519ms 749.54799ms 749.565633ms 749.628887ms 749.663761ms 749.692227ms 749.697752ms 749.749311ms 749.793627ms 749.821374ms 749.839993ms 749.853724ms 749.899268ms 749.900319ms 749.905313ms 749.914495ms 749.979551ms 750.087299ms 750.099344ms 750.100595ms 750.126871ms 750.150404ms 750.233395ms 750.233585ms 750.235113ms 750.270402ms 750.282741ms 750.351333ms 750.403239ms 750.506009ms 750.509983ms 750.514756ms 750.520445ms 750.566873ms 750.577207ms 750.591476ms 750.621421ms 750.640372ms 750.644527ms 750.644542ms 750.712592ms 750.824278ms 750.939528ms 750.984285ms 750.988231ms 751.000429ms 751.229942ms 751.28116ms 751.306392ms 751.356711ms 751.478829ms 751.506623ms 751.57088ms 751.596173ms 751.722501ms 752.337299ms 752.498807ms 753.323998ms 753.328485ms 753.394349ms 753.416309ms 753.990957ms 754.057698ms 754.140412ms 754.492785ms 754.579339ms 754.615769ms 754.795864ms 755.454495ms 758.059483ms 761.681786ms 766.807398ms 768.456527ms 768.715252ms 768.955976ms 769.905304ms 781.798989ms 794.351494ms 839.129384ms 840.286824ms 924.090705ms]
    Jul 19 00:49:31.764: INFO: 50 %ile: 748.927892ms
    Jul 19 00:49:31.764: INFO: 90 %ile: 753.990957ms
    Jul 19 00:49:31.764: INFO: 99 %ile: 840.286824ms
    Jul 19 00:49:31.764: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:49:31.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-4830" for this suite. 07/19/23 00:49:31.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:49:31.77
Jul 19 00:49:31.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename events 07/19/23 00:49:31.771
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:49:31.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:49:31.781
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 07/19/23 00:49:31.782
Jul 19 00:49:31.786: INFO: created test-event-1
Jul 19 00:49:31.789: INFO: created test-event-2
Jul 19 00:49:31.794: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 07/19/23 00:49:31.794
STEP: delete collection of events 07/19/23 00:49:31.795
Jul 19 00:49:31.796: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 07/19/23 00:49:31.803
Jul 19 00:49:31.803: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jul 19 00:49:31.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7313" for this suite. 07/19/23 00:49:31.807
------------------------------
• [0.039 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:49:31.77
    Jul 19 00:49:31.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename events 07/19/23 00:49:31.771
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:49:31.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:49:31.781
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 07/19/23 00:49:31.782
    Jul 19 00:49:31.786: INFO: created test-event-1
    Jul 19 00:49:31.789: INFO: created test-event-2
    Jul 19 00:49:31.794: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 07/19/23 00:49:31.794
    STEP: delete collection of events 07/19/23 00:49:31.795
    Jul 19 00:49:31.796: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 07/19/23 00:49:31.803
    Jul 19 00:49:31.803: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:49:31.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7313" for this suite. 07/19/23 00:49:31.807
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:49:31.809
Jul 19 00:49:31.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename var-expansion 07/19/23 00:49:31.81
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:49:31.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:49:31.821
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 07/19/23 00:49:31.822
Jul 19 00:49:31.826: INFO: Waiting up to 5m0s for pod "var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f" in namespace "var-expansion-8478" to be "Succeeded or Failed"
Jul 19 00:49:31.829: INFO: Pod "var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20567ms
Jul 19 00:49:33.832: INFO: Pod "var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005817812s
Jul 19 00:49:35.832: INFO: Pod "var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00604989s
STEP: Saw pod success 07/19/23 00:49:35.832
Jul 19 00:49:35.832: INFO: Pod "var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f" satisfied condition "Succeeded or Failed"
Jul 19 00:49:35.833: INFO: Trying to get logs from node controller-1 pod var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f container dapi-container: <nil>
STEP: delete the pod 07/19/23 00:49:35.846
Jul 19 00:49:35.854: INFO: Waiting for pod var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f to disappear
Jul 19 00:49:35.855: INFO: Pod var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jul 19 00:49:35.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8478" for this suite. 07/19/23 00:49:35.858
------------------------------
• [4.051 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:49:31.809
    Jul 19 00:49:31.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename var-expansion 07/19/23 00:49:31.81
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:49:31.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:49:31.821
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 07/19/23 00:49:31.822
    Jul 19 00:49:31.826: INFO: Waiting up to 5m0s for pod "var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f" in namespace "var-expansion-8478" to be "Succeeded or Failed"
    Jul 19 00:49:31.829: INFO: Pod "var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20567ms
    Jul 19 00:49:33.832: INFO: Pod "var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005817812s
    Jul 19 00:49:35.832: INFO: Pod "var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00604989s
    STEP: Saw pod success 07/19/23 00:49:35.832
    Jul 19 00:49:35.832: INFO: Pod "var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f" satisfied condition "Succeeded or Failed"
    Jul 19 00:49:35.833: INFO: Trying to get logs from node controller-1 pod var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f container dapi-container: <nil>
    STEP: delete the pod 07/19/23 00:49:35.846
    Jul 19 00:49:35.854: INFO: Waiting for pod var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f to disappear
    Jul 19 00:49:35.855: INFO: Pod var-expansion-32e274e3-b0d9-4b1a-8535-d186491a194f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:49:35.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8478" for this suite. 07/19/23 00:49:35.858
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:49:35.86
Jul 19 00:49:35.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename daemonsets 07/19/23 00:49:35.861
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:49:35.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:49:35.873
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 07/19/23 00:49:35.904
STEP: Check that daemon pods launch on every node of the cluster. 07/19/23 00:49:35.906
Jul 19 00:49:35.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:49:35.912: INFO: Node controller-0 is running 0 daemon pod, expected 1
Jul 19 00:49:36.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 19 00:49:36.922: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 19 00:49:37.927: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 19 00:49:37.927: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 07/19/23 00:49:37.928
Jul 19 00:49:37.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 19 00:49:37.986: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 07/19/23 00:49:37.986
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:49:39.003
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1363, will wait for the garbage collector to delete the pods 07/19/23 00:49:39.003
Jul 19 00:49:39.064: INFO: Deleting DaemonSet.extensions daemon-set took: 8.654779ms
Jul 19 00:49:39.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.94081ms
Jul 19 00:49:41.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:49:41.967: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 19 00:49:41.969: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"80977"},"items":null}

Jul 19 00:49:41.970: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"80977"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:49:41.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1363" for this suite. 07/19/23 00:49:41.977
------------------------------
• [SLOW TEST] [6.119 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:49:35.86
    Jul 19 00:49:35.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename daemonsets 07/19/23 00:49:35.861
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:49:35.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:49:35.873
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 07/19/23 00:49:35.904
    STEP: Check that daemon pods launch on every node of the cluster. 07/19/23 00:49:35.906
    Jul 19 00:49:35.912: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:49:35.912: INFO: Node controller-0 is running 0 daemon pod, expected 1
    Jul 19 00:49:36.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 19 00:49:36.922: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 19 00:49:37.927: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 19 00:49:37.927: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 07/19/23 00:49:37.928
    Jul 19 00:49:37.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jul 19 00:49:37.986: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 07/19/23 00:49:37.986
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:49:39.003
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1363, will wait for the garbage collector to delete the pods 07/19/23 00:49:39.003
    Jul 19 00:49:39.064: INFO: Deleting DaemonSet.extensions daemon-set took: 8.654779ms
    Jul 19 00:49:39.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.94081ms
    Jul 19 00:49:41.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:49:41.967: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 19 00:49:41.969: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"80977"},"items":null}

    Jul 19 00:49:41.970: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"80977"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:49:41.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1363" for this suite. 07/19/23 00:49:41.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:49:41.98
Jul 19 00:49:41.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename init-container 07/19/23 00:49:41.98
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:49:41.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:49:41.998
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 07/19/23 00:49:42
Jul 19 00:49:42.000: INFO: PodSpec: initContainers in spec.initContainers
Jul 19 00:50:25.241: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-fb936951-31a0-4032-8a5d-4bf7ee9c2179", GenerateName:"", Namespace:"init-container-826", SelfLink:"", UID:"dfe0e7b5-7d55-4863-99ca-46ff47bf022e", ResourceVersion:"81326", Generation:0, CreationTimestamp:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"484796"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"69859985dde67395e8e912b76baabb684449187246b9748adb6cee0557a99818", "cni.projectcalico.org/podIP":"172.16.166.184/32", "cni.projectcalico.org/podIPs":"172.16.166.184/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.166.184\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.166.184\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f36618), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f366a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f366d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 19, 0, 50, 25, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f36720), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-47j2v", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0038c4640), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-47j2v", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-47j2v", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-47j2v", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00683d7d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"controller-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c65b20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00683d860)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00683d880)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00683d888), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00683d88c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0059e6b20), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.206.3", PodIP:"172.16.166.184", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.166.184"}}, StartTime:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c65c00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c65f10)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://e6efffd3dd2948174565a47f25641fe060c56e2694cffc025abf94d945192014", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038c46c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038c46a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00683d904)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:50:25.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-826" for this suite. 07/19/23 00:50:25.245
------------------------------
• [SLOW TEST] [43.268 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:49:41.98
    Jul 19 00:49:41.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename init-container 07/19/23 00:49:41.98
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:49:41.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:49:41.998
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 07/19/23 00:49:42
    Jul 19 00:49:42.000: INFO: PodSpec: initContainers in spec.initContainers
    Jul 19 00:50:25.241: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-fb936951-31a0-4032-8a5d-4bf7ee9c2179", GenerateName:"", Namespace:"init-container-826", SelfLink:"", UID:"dfe0e7b5-7d55-4863-99ca-46ff47bf022e", ResourceVersion:"81326", Generation:0, CreationTimestamp:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"484796"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"69859985dde67395e8e912b76baabb684449187246b9748adb6cee0557a99818", "cni.projectcalico.org/podIP":"172.16.166.184/32", "cni.projectcalico.org/podIPs":"172.16.166.184/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.166.184\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"chain\",\n    \"ips\": [\n        \"172.16.166.184\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f36618), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f366a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f366d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 19, 0, 50, 25, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f36720), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-47j2v", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0038c4640), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-47j2v", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-47j2v", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-47j2v", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00683d7d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"controller-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c65b20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00683d860)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00683d880)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00683d888), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00683d88c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0059e6b20), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.206.3", PodIP:"172.16.166.184", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.166.184"}}, StartTime:time.Date(2023, time.July, 19, 0, 49, 42, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c65c00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c65f10)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://e6efffd3dd2948174565a47f25641fe060c56e2694cffc025abf94d945192014", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038c46c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0038c46a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00683d904)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:50:25.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-826" for this suite. 07/19/23 00:50:25.245
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:50:25.248
Jul 19 00:50:25.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 00:50:25.249
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:25.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:25.261
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:50:25.263
Jul 19 00:50:25.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b" in namespace "downward-api-7555" to be "Succeeded or Failed"
Jul 19 00:50:25.269: INFO: Pod "downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.656973ms
Jul 19 00:50:27.272: INFO: Pod "downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005156502s
Jul 19 00:50:29.273: INFO: Pod "downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005992262s
STEP: Saw pod success 07/19/23 00:50:29.273
Jul 19 00:50:29.273: INFO: Pod "downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b" satisfied condition "Succeeded or Failed"
Jul 19 00:50:29.274: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b container client-container: <nil>
STEP: delete the pod 07/19/23 00:50:29.277
Jul 19 00:50:29.286: INFO: Waiting for pod downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b to disappear
Jul 19 00:50:29.287: INFO: Pod downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 19 00:50:29.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7555" for this suite. 07/19/23 00:50:29.29
------------------------------
• [4.044 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:50:25.248
    Jul 19 00:50:25.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 00:50:25.249
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:25.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:25.261
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:50:25.263
    Jul 19 00:50:25.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b" in namespace "downward-api-7555" to be "Succeeded or Failed"
    Jul 19 00:50:25.269: INFO: Pod "downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.656973ms
    Jul 19 00:50:27.272: INFO: Pod "downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005156502s
    Jul 19 00:50:29.273: INFO: Pod "downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005992262s
    STEP: Saw pod success 07/19/23 00:50:29.273
    Jul 19 00:50:29.273: INFO: Pod "downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b" satisfied condition "Succeeded or Failed"
    Jul 19 00:50:29.274: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b container client-container: <nil>
    STEP: delete the pod 07/19/23 00:50:29.277
    Jul 19 00:50:29.286: INFO: Waiting for pod downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b to disappear
    Jul 19 00:50:29.287: INFO: Pod downwardapi-volume-31a9a152-2678-4c05-970a-a02d2815ca0b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:50:29.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7555" for this suite. 07/19/23 00:50:29.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:50:29.293
Jul 19 00:50:29.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 00:50:29.294
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:29.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:29.302
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-f6d29b84-b45a-4e63-a3a3-3e3c9d3e6ab5 07/19/23 00:50:29.304
STEP: Creating a pod to test consume secrets 07/19/23 00:50:29.306
Jul 19 00:50:29.310: INFO: Waiting up to 5m0s for pod "pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f" in namespace "secrets-2342" to be "Succeeded or Failed"
Jul 19 00:50:29.311: INFO: Pod "pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.390211ms
Jul 19 00:50:31.314: INFO: Pod "pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004521422s
Jul 19 00:50:33.316: INFO: Pod "pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005706476s
STEP: Saw pod success 07/19/23 00:50:33.316
Jul 19 00:50:33.316: INFO: Pod "pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f" satisfied condition "Succeeded or Failed"
Jul 19 00:50:33.317: INFO: Trying to get logs from node controller-1 pod pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f container secret-volume-test: <nil>
STEP: delete the pod 07/19/23 00:50:33.321
Jul 19 00:50:33.337: INFO: Waiting for pod pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f to disappear
Jul 19 00:50:33.338: INFO: Pod pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 00:50:33.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2342" for this suite. 07/19/23 00:50:33.341
------------------------------
• [4.050 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:50:29.293
    Jul 19 00:50:29.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 00:50:29.294
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:29.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:29.302
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-f6d29b84-b45a-4e63-a3a3-3e3c9d3e6ab5 07/19/23 00:50:29.304
    STEP: Creating a pod to test consume secrets 07/19/23 00:50:29.306
    Jul 19 00:50:29.310: INFO: Waiting up to 5m0s for pod "pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f" in namespace "secrets-2342" to be "Succeeded or Failed"
    Jul 19 00:50:29.311: INFO: Pod "pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.390211ms
    Jul 19 00:50:31.314: INFO: Pod "pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004521422s
    Jul 19 00:50:33.316: INFO: Pod "pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005706476s
    STEP: Saw pod success 07/19/23 00:50:33.316
    Jul 19 00:50:33.316: INFO: Pod "pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f" satisfied condition "Succeeded or Failed"
    Jul 19 00:50:33.317: INFO: Trying to get logs from node controller-1 pod pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f container secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:50:33.321
    Jul 19 00:50:33.337: INFO: Waiting for pod pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f to disappear
    Jul 19 00:50:33.338: INFO: Pod pod-secrets-29424f88-80ac-416a-b28a-d4dd4496e29f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:50:33.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2342" for this suite. 07/19/23 00:50:33.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:50:33.344
Jul 19 00:50:33.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 00:50:33.345
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:33.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:33.359
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 00:50:33.368
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:50:34.05
STEP: Deploying the webhook pod 07/19/23 00:50:34.053
STEP: Wait for the deployment to be ready 07/19/23 00:50:34.06
Jul 19 00:50:34.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 00:50:36.072
STEP: Verifying the service has paired with the endpoint 07/19/23 00:50:36.08
Jul 19 00:50:37.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 07/19/23 00:50:37.083
STEP: Creating a configMap that does not comply to the validation webhook rules 07/19/23 00:50:37.092
STEP: Updating a validating webhook configuration's rules to not include the create operation 07/19/23 00:50:37.097
STEP: Creating a configMap that does not comply to the validation webhook rules 07/19/23 00:50:37.102
STEP: Patching a validating webhook configuration's rules to include the create operation 07/19/23 00:50:37.11
STEP: Creating a configMap that does not comply to the validation webhook rules 07/19/23 00:50:37.113
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:50:37.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5791" for this suite. 07/19/23 00:50:37.142
STEP: Destroying namespace "webhook-5791-markers" for this suite. 07/19/23 00:50:37.148
------------------------------
• [3.807 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:50:33.344
    Jul 19 00:50:33.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 00:50:33.345
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:33.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:33.359
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 00:50:33.368
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 00:50:34.05
    STEP: Deploying the webhook pod 07/19/23 00:50:34.053
    STEP: Wait for the deployment to be ready 07/19/23 00:50:34.06
    Jul 19 00:50:34.066: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 00:50:36.072
    STEP: Verifying the service has paired with the endpoint 07/19/23 00:50:36.08
    Jul 19 00:50:37.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 07/19/23 00:50:37.083
    STEP: Creating a configMap that does not comply to the validation webhook rules 07/19/23 00:50:37.092
    STEP: Updating a validating webhook configuration's rules to not include the create operation 07/19/23 00:50:37.097
    STEP: Creating a configMap that does not comply to the validation webhook rules 07/19/23 00:50:37.102
    STEP: Patching a validating webhook configuration's rules to include the create operation 07/19/23 00:50:37.11
    STEP: Creating a configMap that does not comply to the validation webhook rules 07/19/23 00:50:37.113
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:50:37.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5791" for this suite. 07/19/23 00:50:37.142
    STEP: Destroying namespace "webhook-5791-markers" for this suite. 07/19/23 00:50:37.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:50:37.152
Jul 19 00:50:37.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:50:37.152
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:37.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:37.164
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 07/19/23 00:50:37.166
Jul 19 00:50:37.173: INFO: Waiting up to 5m0s for pod "pod-113d46d3-fe97-444a-9b45-14b5a5b67438" in namespace "emptydir-226" to be "Succeeded or Failed"
Jul 19 00:50:37.174: INFO: Pod "pod-113d46d3-fe97-444a-9b45-14b5a5b67438": Phase="Pending", Reason="", readiness=false. Elapsed: 1.635328ms
Jul 19 00:50:39.177: INFO: Pod "pod-113d46d3-fe97-444a-9b45-14b5a5b67438": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004127559s
Jul 19 00:50:41.178: INFO: Pod "pod-113d46d3-fe97-444a-9b45-14b5a5b67438": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004988012s
STEP: Saw pod success 07/19/23 00:50:41.178
Jul 19 00:50:41.178: INFO: Pod "pod-113d46d3-fe97-444a-9b45-14b5a5b67438" satisfied condition "Succeeded or Failed"
Jul 19 00:50:41.179: INFO: Trying to get logs from node controller-1 pod pod-113d46d3-fe97-444a-9b45-14b5a5b67438 container test-container: <nil>
STEP: delete the pod 07/19/23 00:50:41.182
Jul 19 00:50:41.191: INFO: Waiting for pod pod-113d46d3-fe97-444a-9b45-14b5a5b67438 to disappear
Jul 19 00:50:41.192: INFO: Pod pod-113d46d3-fe97-444a-9b45-14b5a5b67438 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:50:41.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-226" for this suite. 07/19/23 00:50:41.194
------------------------------
• [4.046 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:50:37.152
    Jul 19 00:50:37.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:50:37.152
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:37.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:37.164
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 07/19/23 00:50:37.166
    Jul 19 00:50:37.173: INFO: Waiting up to 5m0s for pod "pod-113d46d3-fe97-444a-9b45-14b5a5b67438" in namespace "emptydir-226" to be "Succeeded or Failed"
    Jul 19 00:50:37.174: INFO: Pod "pod-113d46d3-fe97-444a-9b45-14b5a5b67438": Phase="Pending", Reason="", readiness=false. Elapsed: 1.635328ms
    Jul 19 00:50:39.177: INFO: Pod "pod-113d46d3-fe97-444a-9b45-14b5a5b67438": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004127559s
    Jul 19 00:50:41.178: INFO: Pod "pod-113d46d3-fe97-444a-9b45-14b5a5b67438": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004988012s
    STEP: Saw pod success 07/19/23 00:50:41.178
    Jul 19 00:50:41.178: INFO: Pod "pod-113d46d3-fe97-444a-9b45-14b5a5b67438" satisfied condition "Succeeded or Failed"
    Jul 19 00:50:41.179: INFO: Trying to get logs from node controller-1 pod pod-113d46d3-fe97-444a-9b45-14b5a5b67438 container test-container: <nil>
    STEP: delete the pod 07/19/23 00:50:41.182
    Jul 19 00:50:41.191: INFO: Waiting for pod pod-113d46d3-fe97-444a-9b45-14b5a5b67438 to disappear
    Jul 19 00:50:41.192: INFO: Pod pod-113d46d3-fe97-444a-9b45-14b5a5b67438 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:50:41.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-226" for this suite. 07/19/23 00:50:41.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:50:41.198
Jul 19 00:50:41.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:50:41.199
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:41.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:41.207
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jul 19 00:50:41.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/19/23 00:50:43.253
Jul 19 00:50:43.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4355 --namespace=crd-publish-openapi-4355 create -f -'
Jul 19 00:50:43.981: INFO: stderr: ""
Jul 19 00:50:43.981: INFO: stdout: "e2e-test-crd-publish-openapi-7459-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 19 00:50:43.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4355 --namespace=crd-publish-openapi-4355 delete e2e-test-crd-publish-openapi-7459-crds test-cr'
Jul 19 00:50:44.044: INFO: stderr: ""
Jul 19 00:50:44.044: INFO: stdout: "e2e-test-crd-publish-openapi-7459-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 19 00:50:44.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4355 --namespace=crd-publish-openapi-4355 apply -f -'
Jul 19 00:50:44.738: INFO: stderr: ""
Jul 19 00:50:44.738: INFO: stdout: "e2e-test-crd-publish-openapi-7459-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 19 00:50:44.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4355 --namespace=crd-publish-openapi-4355 delete e2e-test-crd-publish-openapi-7459-crds test-cr'
Jul 19 00:50:44.800: INFO: stderr: ""
Jul 19 00:50:44.800: INFO: stdout: "e2e-test-crd-publish-openapi-7459-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 07/19/23 00:50:44.8
Jul 19 00:50:44.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4355 explain e2e-test-crd-publish-openapi-7459-crds'
Jul 19 00:50:45.002: INFO: stderr: ""
Jul 19 00:50:45.002: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7459-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:50:47.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4355" for this suite. 07/19/23 00:50:47.021
------------------------------
• [SLOW TEST] [5.825 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:50:41.198
    Jul 19 00:50:41.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 00:50:41.199
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:41.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:41.207
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jul 19 00:50:41.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/19/23 00:50:43.253
    Jul 19 00:50:43.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4355 --namespace=crd-publish-openapi-4355 create -f -'
    Jul 19 00:50:43.981: INFO: stderr: ""
    Jul 19 00:50:43.981: INFO: stdout: "e2e-test-crd-publish-openapi-7459-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jul 19 00:50:43.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4355 --namespace=crd-publish-openapi-4355 delete e2e-test-crd-publish-openapi-7459-crds test-cr'
    Jul 19 00:50:44.044: INFO: stderr: ""
    Jul 19 00:50:44.044: INFO: stdout: "e2e-test-crd-publish-openapi-7459-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jul 19 00:50:44.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4355 --namespace=crd-publish-openapi-4355 apply -f -'
    Jul 19 00:50:44.738: INFO: stderr: ""
    Jul 19 00:50:44.738: INFO: stdout: "e2e-test-crd-publish-openapi-7459-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jul 19 00:50:44.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4355 --namespace=crd-publish-openapi-4355 delete e2e-test-crd-publish-openapi-7459-crds test-cr'
    Jul 19 00:50:44.800: INFO: stderr: ""
    Jul 19 00:50:44.800: INFO: stdout: "e2e-test-crd-publish-openapi-7459-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 07/19/23 00:50:44.8
    Jul 19 00:50:44.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-4355 explain e2e-test-crd-publish-openapi-7459-crds'
    Jul 19 00:50:45.002: INFO: stderr: ""
    Jul 19 00:50:45.002: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7459-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:50:47.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4355" for this suite. 07/19/23 00:50:47.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:50:47.025
Jul 19 00:50:47.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-runtime 07/19/23 00:50:47.025
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:47.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:47.033
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 07/19/23 00:50:47.039
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 07/19/23 00:51:07.091
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 07/19/23 00:51:07.093
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 07/19/23 00:51:07.096
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 07/19/23 00:51:07.096
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 07/19/23 00:51:07.11
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 07/19/23 00:51:10.119
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 07/19/23 00:51:12.142
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 07/19/23 00:51:12.146
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 07/19/23 00:51:12.146
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 07/19/23 00:51:12.159
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 07/19/23 00:51:13.165
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 07/19/23 00:51:16.175
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 07/19/23 00:51:16.178
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 07/19/23 00:51:16.178
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jul 19 00:51:16.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4159" for this suite. 07/19/23 00:51:16.192
------------------------------
• [SLOW TEST] [29.170 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:50:47.025
    Jul 19 00:50:47.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-runtime 07/19/23 00:50:47.025
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:50:47.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:50:47.033
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 07/19/23 00:50:47.039
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 07/19/23 00:51:07.091
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 07/19/23 00:51:07.093
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 07/19/23 00:51:07.096
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 07/19/23 00:51:07.096
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 07/19/23 00:51:07.11
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 07/19/23 00:51:10.119
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 07/19/23 00:51:12.142
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 07/19/23 00:51:12.146
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 07/19/23 00:51:12.146
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 07/19/23 00:51:12.159
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 07/19/23 00:51:13.165
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 07/19/23 00:51:16.175
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 07/19/23 00:51:16.178
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 07/19/23 00:51:16.178
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:51:16.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4159" for this suite. 07/19/23 00:51:16.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:51:16.195
Jul 19 00:51:16.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename containers 07/19/23 00:51:16.196
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:16.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:16.205
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 07/19/23 00:51:16.207
Jul 19 00:51:16.211: INFO: Waiting up to 5m0s for pod "client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3" in namespace "containers-4922" to be "Succeeded or Failed"
Jul 19 00:51:16.214: INFO: Pod "client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305736ms
Jul 19 00:51:18.216: INFO: Pod "client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005782921s
Jul 19 00:51:20.216: INFO: Pod "client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005384183s
STEP: Saw pod success 07/19/23 00:51:20.216
Jul 19 00:51:20.216: INFO: Pod "client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3" satisfied condition "Succeeded or Failed"
Jul 19 00:51:20.218: INFO: Trying to get logs from node controller-1 pod client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:51:20.221
Jul 19 00:51:20.228: INFO: Waiting for pod client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3 to disappear
Jul 19 00:51:20.229: INFO: Pod client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jul 19 00:51:20.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4922" for this suite. 07/19/23 00:51:20.231
------------------------------
• [4.038 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:51:16.195
    Jul 19 00:51:16.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename containers 07/19/23 00:51:16.196
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:16.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:16.205
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 07/19/23 00:51:16.207
    Jul 19 00:51:16.211: INFO: Waiting up to 5m0s for pod "client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3" in namespace "containers-4922" to be "Succeeded or Failed"
    Jul 19 00:51:16.214: INFO: Pod "client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305736ms
    Jul 19 00:51:18.216: INFO: Pod "client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005782921s
    Jul 19 00:51:20.216: INFO: Pod "client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005384183s
    STEP: Saw pod success 07/19/23 00:51:20.216
    Jul 19 00:51:20.216: INFO: Pod "client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3" satisfied condition "Succeeded or Failed"
    Jul 19 00:51:20.218: INFO: Trying to get logs from node controller-1 pod client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:51:20.221
    Jul 19 00:51:20.228: INFO: Waiting for pod client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3 to disappear
    Jul 19 00:51:20.229: INFO: Pod client-containers-bf95a603-9f7d-43c3-a38e-93f6ab8c3fb3 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:51:20.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4922" for this suite. 07/19/23 00:51:20.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:51:20.234
Jul 19 00:51:20.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 00:51:20.235
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:20.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:20.243
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 07/19/23 00:51:20.245
Jul 19 00:51:20.249: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9" in namespace "emptydir-9749" to be "running"
Jul 19 00:51:20.252: INFO: Pod "pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.714634ms
Jul 19 00:51:22.254: INFO: Pod "pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9": Phase="Running", Reason="", readiness=false. Elapsed: 2.005471996s
Jul 19 00:51:22.254: INFO: Pod "pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9" satisfied condition "running"
STEP: Reading file content from the nginx-container 07/19/23 00:51:22.254
Jul 19 00:51:22.254: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9749 PodName:pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 00:51:22.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 00:51:22.255: INFO: ExecWithOptions: Clientset creation
Jul 19 00:51:22.255: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9749/pods/pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jul 19 00:51:22.301: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 00:51:22.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9749" for this suite. 07/19/23 00:51:22.303
------------------------------
• [2.072 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:51:20.234
    Jul 19 00:51:20.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 00:51:20.235
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:20.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:20.243
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 07/19/23 00:51:20.245
    Jul 19 00:51:20.249: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9" in namespace "emptydir-9749" to be "running"
    Jul 19 00:51:20.252: INFO: Pod "pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.714634ms
    Jul 19 00:51:22.254: INFO: Pod "pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9": Phase="Running", Reason="", readiness=false. Elapsed: 2.005471996s
    Jul 19 00:51:22.254: INFO: Pod "pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9" satisfied condition "running"
    STEP: Reading file content from the nginx-container 07/19/23 00:51:22.254
    Jul 19 00:51:22.254: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9749 PodName:pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 00:51:22.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 00:51:22.255: INFO: ExecWithOptions: Clientset creation
    Jul 19 00:51:22.255: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9749/pods/pod-sharedvolume-62a4ee0d-d226-4c2a-94af-b21b1108f3c9/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jul 19 00:51:22.301: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:51:22.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9749" for this suite. 07/19/23 00:51:22.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:51:22.307
Jul 19 00:51:22.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/19/23 00:51:22.308
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:22.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:22.318
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jul 19 00:51:22.323: INFO: Waiting up to 5m0s for pod "server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8" in namespace "pods-6045" to be "running and ready"
Jul 19 00:51:22.327: INFO: Pod "server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.03444ms
Jul 19 00:51:22.327: INFO: The phase of Pod server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:51:24.329: INFO: Pod "server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005723607s
Jul 19 00:51:24.329: INFO: The phase of Pod server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8 is Running (Ready = true)
Jul 19 00:51:24.329: INFO: Pod "server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8" satisfied condition "running and ready"
Jul 19 00:51:24.344: INFO: Waiting up to 5m0s for pod "client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0" in namespace "pods-6045" to be "Succeeded or Failed"
Jul 19 00:51:24.346: INFO: Pod "client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.682383ms
Jul 19 00:51:26.348: INFO: Pod "client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003669485s
Jul 19 00:51:28.349: INFO: Pod "client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004478148s
STEP: Saw pod success 07/19/23 00:51:28.349
Jul 19 00:51:28.349: INFO: Pod "client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0" satisfied condition "Succeeded or Failed"
Jul 19 00:51:28.350: INFO: Trying to get logs from node controller-1 pod client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0 container env3cont: <nil>
STEP: delete the pod 07/19/23 00:51:28.354
Jul 19 00:51:28.360: INFO: Waiting for pod client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0 to disappear
Jul 19 00:51:28.364: INFO: Pod client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jul 19 00:51:28.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6045" for this suite. 07/19/23 00:51:28.366
------------------------------
• [SLOW TEST] [6.062 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:51:22.307
    Jul 19 00:51:22.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/19/23 00:51:22.308
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:22.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:22.318
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jul 19 00:51:22.323: INFO: Waiting up to 5m0s for pod "server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8" in namespace "pods-6045" to be "running and ready"
    Jul 19 00:51:22.327: INFO: Pod "server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.03444ms
    Jul 19 00:51:22.327: INFO: The phase of Pod server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:51:24.329: INFO: Pod "server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005723607s
    Jul 19 00:51:24.329: INFO: The phase of Pod server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8 is Running (Ready = true)
    Jul 19 00:51:24.329: INFO: Pod "server-envvars-76bda57a-4c7f-4d40-8ad8-848e0f067da8" satisfied condition "running and ready"
    Jul 19 00:51:24.344: INFO: Waiting up to 5m0s for pod "client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0" in namespace "pods-6045" to be "Succeeded or Failed"
    Jul 19 00:51:24.346: INFO: Pod "client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.682383ms
    Jul 19 00:51:26.348: INFO: Pod "client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003669485s
    Jul 19 00:51:28.349: INFO: Pod "client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004478148s
    STEP: Saw pod success 07/19/23 00:51:28.349
    Jul 19 00:51:28.349: INFO: Pod "client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0" satisfied condition "Succeeded or Failed"
    Jul 19 00:51:28.350: INFO: Trying to get logs from node controller-1 pod client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0 container env3cont: <nil>
    STEP: delete the pod 07/19/23 00:51:28.354
    Jul 19 00:51:28.360: INFO: Waiting for pod client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0 to disappear
    Jul 19 00:51:28.364: INFO: Pod client-envvars-850d6a37-d225-412c-8812-b6a9ff85bfb0 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:51:28.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6045" for this suite. 07/19/23 00:51:28.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:51:28.37
Jul 19 00:51:28.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-probe 07/19/23 00:51:28.37
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:28.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:28.379
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d in namespace container-probe-2575 07/19/23 00:51:28.38
Jul 19 00:51:28.384: INFO: Waiting up to 5m0s for pod "liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d" in namespace "container-probe-2575" to be "not pending"
Jul 19 00:51:28.386: INFO: Pod "liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556359ms
Jul 19 00:51:30.389: INFO: Pod "liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004610587s
Jul 19 00:51:30.389: INFO: Pod "liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d" satisfied condition "not pending"
Jul 19 00:51:30.389: INFO: Started pod liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d in namespace container-probe-2575
STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 00:51:30.389
Jul 19 00:51:30.391: INFO: Initial restart count of pod liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d is 0
Jul 19 00:51:50.418: INFO: Restart count of pod container-probe-2575/liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d is now 1 (20.027535881s elapsed)
STEP: deleting the pod 07/19/23 00:51:50.418
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jul 19 00:51:50.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2575" for this suite. 07/19/23 00:51:50.427
------------------------------
• [SLOW TEST] [22.060 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:51:28.37
    Jul 19 00:51:28.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-probe 07/19/23 00:51:28.37
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:28.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:28.379
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d in namespace container-probe-2575 07/19/23 00:51:28.38
    Jul 19 00:51:28.384: INFO: Waiting up to 5m0s for pod "liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d" in namespace "container-probe-2575" to be "not pending"
    Jul 19 00:51:28.386: INFO: Pod "liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556359ms
    Jul 19 00:51:30.389: INFO: Pod "liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004610587s
    Jul 19 00:51:30.389: INFO: Pod "liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d" satisfied condition "not pending"
    Jul 19 00:51:30.389: INFO: Started pod liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d in namespace container-probe-2575
    STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 00:51:30.389
    Jul 19 00:51:30.391: INFO: Initial restart count of pod liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d is 0
    Jul 19 00:51:50.418: INFO: Restart count of pod container-probe-2575/liveness-9d5f7d08-c794-4de4-84ee-362cdbbd255d is now 1 (20.027535881s elapsed)
    STEP: deleting the pod 07/19/23 00:51:50.418
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:51:50.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2575" for this suite. 07/19/23 00:51:50.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:51:50.435
Jul 19 00:51:50.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replicaset 07/19/23 00:51:50.436
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:50.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:50.445
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jul 19 00:51:50.455: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 19 00:51:55.457: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/19/23 00:51:55.457
STEP: Scaling up "test-rs" replicaset  07/19/23 00:51:55.457
Jul 19 00:51:55.461: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 07/19/23 00:51:55.462
W0719 00:51:55.467314      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jul 19 00:51:55.468: INFO: observed ReplicaSet test-rs in namespace replicaset-3613 with ReadyReplicas 1, AvailableReplicas 1
Jul 19 00:51:55.473: INFO: observed ReplicaSet test-rs in namespace replicaset-3613 with ReadyReplicas 1, AvailableReplicas 1
Jul 19 00:51:55.480: INFO: observed ReplicaSet test-rs in namespace replicaset-3613 with ReadyReplicas 1, AvailableReplicas 1
Jul 19 00:51:55.484: INFO: observed ReplicaSet test-rs in namespace replicaset-3613 with ReadyReplicas 1, AvailableReplicas 1
Jul 19 00:51:56.249: INFO: observed ReplicaSet test-rs in namespace replicaset-3613 with ReadyReplicas 2, AvailableReplicas 2
Jul 19 00:51:56.511: INFO: observed Replicaset test-rs in namespace replicaset-3613 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jul 19 00:51:56.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3613" for this suite. 07/19/23 00:51:56.513
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:51:50.435
    Jul 19 00:51:50.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replicaset 07/19/23 00:51:50.436
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:50.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:50.445
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jul 19 00:51:50.455: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jul 19 00:51:55.457: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/19/23 00:51:55.457
    STEP: Scaling up "test-rs" replicaset  07/19/23 00:51:55.457
    Jul 19 00:51:55.461: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 07/19/23 00:51:55.462
    W0719 00:51:55.467314      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jul 19 00:51:55.468: INFO: observed ReplicaSet test-rs in namespace replicaset-3613 with ReadyReplicas 1, AvailableReplicas 1
    Jul 19 00:51:55.473: INFO: observed ReplicaSet test-rs in namespace replicaset-3613 with ReadyReplicas 1, AvailableReplicas 1
    Jul 19 00:51:55.480: INFO: observed ReplicaSet test-rs in namespace replicaset-3613 with ReadyReplicas 1, AvailableReplicas 1
    Jul 19 00:51:55.484: INFO: observed ReplicaSet test-rs in namespace replicaset-3613 with ReadyReplicas 1, AvailableReplicas 1
    Jul 19 00:51:56.249: INFO: observed ReplicaSet test-rs in namespace replicaset-3613 with ReadyReplicas 2, AvailableReplicas 2
    Jul 19 00:51:56.511: INFO: observed Replicaset test-rs in namespace replicaset-3613 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:51:56.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3613" for this suite. 07/19/23 00:51:56.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:51:56.517
Jul 19 00:51:56.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:51:56.518
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:56.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:56.526
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 07/19/23 00:51:56.528
Jul 19 00:51:56.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 create -f -'
Jul 19 00:51:57.289: INFO: stderr: ""
Jul 19 00:51:57.289: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 07/19/23 00:51:57.289
Jul 19 00:51:57.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 19 00:51:57.352: INFO: stderr: ""
Jul 19 00:51:57.352: INFO: stdout: "update-demo-nautilus-f4kds update-demo-nautilus-sk78g "
Jul 19 00:51:57.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-f4kds -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 19 00:51:57.412: INFO: stderr: ""
Jul 19 00:51:57.412: INFO: stdout: ""
Jul 19 00:51:57.412: INFO: update-demo-nautilus-f4kds is created but not running
Jul 19 00:52:02.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 19 00:52:02.476: INFO: stderr: ""
Jul 19 00:52:02.476: INFO: stdout: "update-demo-nautilus-f4kds update-demo-nautilus-sk78g "
Jul 19 00:52:02.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-f4kds -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 19 00:52:02.536: INFO: stderr: ""
Jul 19 00:52:02.536: INFO: stdout: ""
Jul 19 00:52:02.536: INFO: update-demo-nautilus-f4kds is created but not running
Jul 19 00:52:07.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 19 00:52:07.599: INFO: stderr: ""
Jul 19 00:52:07.599: INFO: stdout: "update-demo-nautilus-f4kds update-demo-nautilus-sk78g "
Jul 19 00:52:07.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-f4kds -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 19 00:52:07.659: INFO: stderr: ""
Jul 19 00:52:07.659: INFO: stdout: "true"
Jul 19 00:52:07.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-f4kds -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 19 00:52:07.718: INFO: stderr: ""
Jul 19 00:52:07.718: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jul 19 00:52:07.718: INFO: validating pod update-demo-nautilus-f4kds
Jul 19 00:52:07.721: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 00:52:07.721: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 00:52:07.721: INFO: update-demo-nautilus-f4kds is verified up and running
Jul 19 00:52:07.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-sk78g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 19 00:52:07.782: INFO: stderr: ""
Jul 19 00:52:07.782: INFO: stdout: "true"
Jul 19 00:52:07.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-sk78g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 19 00:52:07.842: INFO: stderr: ""
Jul 19 00:52:07.842: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jul 19 00:52:07.842: INFO: validating pod update-demo-nautilus-sk78g
Jul 19 00:52:07.845: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 00:52:07.845: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 00:52:07.845: INFO: update-demo-nautilus-sk78g is verified up and running
STEP: using delete to clean up resources 07/19/23 00:52:07.845
Jul 19 00:52:07.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 delete --grace-period=0 --force -f -'
Jul 19 00:52:07.928: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 00:52:07.929: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 19 00:52:07.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get rc,svc -l name=update-demo --no-headers'
Jul 19 00:52:07.993: INFO: stderr: "No resources found in kubectl-2889 namespace.\n"
Jul 19 00:52:07.993: INFO: stdout: ""
Jul 19 00:52:07.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 19 00:52:08.064: INFO: stderr: ""
Jul 19 00:52:08.064: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:52:08.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2889" for this suite. 07/19/23 00:52:08.066
------------------------------
• [SLOW TEST] [11.552 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:51:56.517
    Jul 19 00:51:56.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:51:56.518
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:51:56.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:51:56.526
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 07/19/23 00:51:56.528
    Jul 19 00:51:56.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 create -f -'
    Jul 19 00:51:57.289: INFO: stderr: ""
    Jul 19 00:51:57.289: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 07/19/23 00:51:57.289
    Jul 19 00:51:57.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 19 00:51:57.352: INFO: stderr: ""
    Jul 19 00:51:57.352: INFO: stdout: "update-demo-nautilus-f4kds update-demo-nautilus-sk78g "
    Jul 19 00:51:57.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-f4kds -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 19 00:51:57.412: INFO: stderr: ""
    Jul 19 00:51:57.412: INFO: stdout: ""
    Jul 19 00:51:57.412: INFO: update-demo-nautilus-f4kds is created but not running
    Jul 19 00:52:02.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 19 00:52:02.476: INFO: stderr: ""
    Jul 19 00:52:02.476: INFO: stdout: "update-demo-nautilus-f4kds update-demo-nautilus-sk78g "
    Jul 19 00:52:02.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-f4kds -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 19 00:52:02.536: INFO: stderr: ""
    Jul 19 00:52:02.536: INFO: stdout: ""
    Jul 19 00:52:02.536: INFO: update-demo-nautilus-f4kds is created but not running
    Jul 19 00:52:07.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 19 00:52:07.599: INFO: stderr: ""
    Jul 19 00:52:07.599: INFO: stdout: "update-demo-nautilus-f4kds update-demo-nautilus-sk78g "
    Jul 19 00:52:07.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-f4kds -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 19 00:52:07.659: INFO: stderr: ""
    Jul 19 00:52:07.659: INFO: stdout: "true"
    Jul 19 00:52:07.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-f4kds -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 19 00:52:07.718: INFO: stderr: ""
    Jul 19 00:52:07.718: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jul 19 00:52:07.718: INFO: validating pod update-demo-nautilus-f4kds
    Jul 19 00:52:07.721: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 19 00:52:07.721: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 19 00:52:07.721: INFO: update-demo-nautilus-f4kds is verified up and running
    Jul 19 00:52:07.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-sk78g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 19 00:52:07.782: INFO: stderr: ""
    Jul 19 00:52:07.782: INFO: stdout: "true"
    Jul 19 00:52:07.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods update-demo-nautilus-sk78g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 19 00:52:07.842: INFO: stderr: ""
    Jul 19 00:52:07.842: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jul 19 00:52:07.842: INFO: validating pod update-demo-nautilus-sk78g
    Jul 19 00:52:07.845: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 19 00:52:07.845: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 19 00:52:07.845: INFO: update-demo-nautilus-sk78g is verified up and running
    STEP: using delete to clean up resources 07/19/23 00:52:07.845
    Jul 19 00:52:07.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 delete --grace-period=0 --force -f -'
    Jul 19 00:52:07.928: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 19 00:52:07.929: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jul 19 00:52:07.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get rc,svc -l name=update-demo --no-headers'
    Jul 19 00:52:07.993: INFO: stderr: "No resources found in kubectl-2889 namespace.\n"
    Jul 19 00:52:07.993: INFO: stdout: ""
    Jul 19 00:52:07.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2889 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jul 19 00:52:08.064: INFO: stderr: ""
    Jul 19 00:52:08.064: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:52:08.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2889" for this suite. 07/19/23 00:52:08.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:52:08.069
Jul 19 00:52:08.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename deployment 07/19/23 00:52:08.07
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:08.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:08.121
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jul 19 00:52:08.122: INFO: Creating deployment "test-recreate-deployment"
Jul 19 00:52:08.157: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 19 00:52:08.185: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 19 00:52:10.189: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 19 00:52:10.191: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 19 00:52:10.196: INFO: Updating deployment test-recreate-deployment
Jul 19 00:52:10.196: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 19 00:52:10.239: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2265  5a5a195f-1930-4601-8ad2-10f9d5e3d9b2 82628 2 2023-07-19 00:52:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d109f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-19 00:52:10 +0000 UTC,LastTransitionTime:2023-07-19 00:52:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-07-19 00:52:10 +0000 UTC,LastTransitionTime:2023-07-19 00:52:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 19 00:52:10.243: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2265  290ecf4c-5f61-4404-a1f1-5d46709529b3 82627 1 2023-07-19 00:52:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5a5a195f-1930-4601-8ad2-10f9d5e3d9b2 0xc00401f930 0xc00401f931}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a5a195f-1930-4601-8ad2-10f9d5e3d9b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401f9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 00:52:10.243: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 19 00:52:10.243: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2265  7315ab13-ef78-47c5-bd66-8428b5fda886 82617 2 2023-07-19 00:52:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5a5a195f-1930-4601-8ad2-10f9d5e3d9b2 0xc00401f807 0xc00401f808}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a5a195f-1930-4601-8ad2-10f9d5e3d9b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401f8c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 00:52:10.245: INFO: Pod "test-recreate-deployment-cff6dc657-qttpb" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-qttpb test-recreate-deployment-cff6dc657- deployment-2265  5eb9588a-edf8-47a6-8703-169793e2816b 82629 0 2023-07-19 00:52:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 290ecf4c-5f61-4404-a1f1-5d46709529b3 0xc0050f84a0 0xc0050f84a1}] [] [{kube-controller-manager Update v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"290ecf4c-5f61-4404-a1f1-5d46709529b3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57rm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57rm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:52:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:52:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:52:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:52:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:,StartTime:2023-07-19 00:52:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jul 19 00:52:10.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2265" for this suite. 07/19/23 00:52:10.248
------------------------------
• [2.180 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:52:08.069
    Jul 19 00:52:08.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename deployment 07/19/23 00:52:08.07
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:08.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:08.121
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jul 19 00:52:08.122: INFO: Creating deployment "test-recreate-deployment"
    Jul 19 00:52:08.157: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jul 19 00:52:08.185: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jul 19 00:52:10.189: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jul 19 00:52:10.191: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jul 19 00:52:10.196: INFO: Updating deployment test-recreate-deployment
    Jul 19 00:52:10.196: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 19 00:52:10.239: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2265  5a5a195f-1930-4601-8ad2-10f9d5e3d9b2 82628 2 2023-07-19 00:52:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d109f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-19 00:52:10 +0000 UTC,LastTransitionTime:2023-07-19 00:52:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-07-19 00:52:10 +0000 UTC,LastTransitionTime:2023-07-19 00:52:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jul 19 00:52:10.243: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2265  290ecf4c-5f61-4404-a1f1-5d46709529b3 82627 1 2023-07-19 00:52:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 5a5a195f-1930-4601-8ad2-10f9d5e3d9b2 0xc00401f930 0xc00401f931}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a5a195f-1930-4601-8ad2-10f9d5e3d9b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401f9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 00:52:10.243: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jul 19 00:52:10.243: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2265  7315ab13-ef78-47c5-bd66-8428b5fda886 82617 2 2023-07-19 00:52:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 5a5a195f-1930-4601-8ad2-10f9d5e3d9b2 0xc00401f807 0xc00401f808}] [] [{kube-controller-manager Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a5a195f-1930-4601-8ad2-10f9d5e3d9b2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401f8c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 00:52:10.245: INFO: Pod "test-recreate-deployment-cff6dc657-qttpb" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-qttpb test-recreate-deployment-cff6dc657- deployment-2265  5eb9588a-edf8-47a6-8703-169793e2816b 82629 0 2023-07-19 00:52:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 290ecf4c-5f61-4404-a1f1-5d46709529b3 0xc0050f84a0 0xc0050f84a1}] [] [{kube-controller-manager Update v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"290ecf4c-5f61-4404-a1f1-5d46709529b3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-19 00:52:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57rm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57rm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:52:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:52:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:52:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 00:52:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:,StartTime:2023-07-19 00:52:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:52:10.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2265" for this suite. 07/19/23 00:52:10.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:52:10.25
Jul 19 00:52:10.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:52:10.251
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:10.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:10.262
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jul 19 00:52:10.266: INFO: Got root ca configmap in namespace "svcaccounts-7985"
Jul 19 00:52:10.268: INFO: Deleted root ca configmap in namespace "svcaccounts-7985"
STEP: waiting for a new root ca configmap created 07/19/23 00:52:10.769
Jul 19 00:52:10.771: INFO: Recreated root ca configmap in namespace "svcaccounts-7985"
Jul 19 00:52:10.773: INFO: Updated root ca configmap in namespace "svcaccounts-7985"
STEP: waiting for the root ca configmap reconciled 07/19/23 00:52:11.274
Jul 19 00:52:11.282: INFO: Reconciled root ca configmap in namespace "svcaccounts-7985"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jul 19 00:52:11.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7985" for this suite. 07/19/23 00:52:11.285
------------------------------
• [1.037 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:52:10.25
    Jul 19 00:52:10.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename svcaccounts 07/19/23 00:52:10.251
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:10.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:10.262
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jul 19 00:52:10.266: INFO: Got root ca configmap in namespace "svcaccounts-7985"
    Jul 19 00:52:10.268: INFO: Deleted root ca configmap in namespace "svcaccounts-7985"
    STEP: waiting for a new root ca configmap created 07/19/23 00:52:10.769
    Jul 19 00:52:10.771: INFO: Recreated root ca configmap in namespace "svcaccounts-7985"
    Jul 19 00:52:10.773: INFO: Updated root ca configmap in namespace "svcaccounts-7985"
    STEP: waiting for the root ca configmap reconciled 07/19/23 00:52:11.274
    Jul 19 00:52:11.282: INFO: Reconciled root ca configmap in namespace "svcaccounts-7985"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:52:11.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7985" for this suite. 07/19/23 00:52:11.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:52:11.288
Jul 19 00:52:11.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:52:11.289
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:11.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:11.297
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 07/19/23 00:52:11.299
Jul 19 00:52:11.302: INFO: Waiting up to 5m0s for pod "labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846" in namespace "projected-393" to be "running and ready"
Jul 19 00:52:11.304: INFO: Pod "labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489039ms
Jul 19 00:52:11.304: INFO: The phase of Pod labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:52:13.306: INFO: Pod "labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846": Phase="Running", Reason="", readiness=true. Elapsed: 2.004129931s
Jul 19 00:52:13.307: INFO: The phase of Pod labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846 is Running (Ready = true)
Jul 19 00:52:13.307: INFO: Pod "labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846" satisfied condition "running and ready"
Jul 19 00:52:13.819: INFO: Successfully updated pod "labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 19 00:52:17.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-393" for this suite. 07/19/23 00:52:17.84
------------------------------
• [SLOW TEST] [6.555 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:52:11.288
    Jul 19 00:52:11.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:52:11.289
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:11.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:11.297
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 07/19/23 00:52:11.299
    Jul 19 00:52:11.302: INFO: Waiting up to 5m0s for pod "labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846" in namespace "projected-393" to be "running and ready"
    Jul 19 00:52:11.304: INFO: Pod "labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489039ms
    Jul 19 00:52:11.304: INFO: The phase of Pod labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:52:13.306: INFO: Pod "labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846": Phase="Running", Reason="", readiness=true. Elapsed: 2.004129931s
    Jul 19 00:52:13.307: INFO: The phase of Pod labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846 is Running (Ready = true)
    Jul 19 00:52:13.307: INFO: Pod "labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846" satisfied condition "running and ready"
    Jul 19 00:52:13.819: INFO: Successfully updated pod "labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:52:17.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-393" for this suite. 07/19/23 00:52:17.84
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:52:17.843
Jul 19 00:52:17.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sched-pred 07/19/23 00:52:17.845
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:17.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:17.857
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jul 19 00:52:17.859: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 19 00:52:17.863: INFO: Waiting for terminating namespaces to be deleted...
Jul 19 00:52:17.865: INFO: 
Logging pods the apiserver thinks is on node controller-0 before test
Jul 19 00:52:17.874: INFO: cm-cert-manager-7fb65857f5-5mnw8 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container cert-manager-controller ready: true, restart count 2
Jul 19 00:52:17.874: INFO: cm-cert-manager-cainjector-86b69d7d69-6czpr from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container cert-manager-cainjector ready: true, restart count 1
Jul 19 00:52:17.874: INFO: cm-cert-manager-webhook-98ddcd5cb-2q6p9 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container cert-manager-webhook ready: true, restart count 1
Jul 19 00:52:17.874: INFO: helm-controller-5fb8ccb85d-nl9lf from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container manager ready: true, restart count 1
Jul 19 00:52:17.874: INFO: source-controller-69b5d8f7d8-66tt6 from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container manager ready: true, restart count 1
Jul 19 00:52:17.874: INFO: calico-kube-controllers-7f5cd5f684-5jblg from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jul 19 00:52:17.874: INFO: calico-node-hmrnj from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container calico-node ready: true, restart count 1
Jul 19 00:52:17.874: INFO: cephfs-nodeplugin-n2vfj from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:52:17.874: INFO: cephfs-provisioner-5f69fbf97-q8lg6 from kube-system started at 2023-07-18 22:35:12 +0000 UTC (4 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:52:17.874: INFO: coredns-66856967f4-whd5z from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container coredns ready: true, restart count 1
Jul 19 00:52:17.874: INFO: ic-nginx-ingress-ingress-nginx-controller-6hpm5 from kube-system started at 2023-07-18 21:49:16 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container controller ready: true, restart count 1
Jul 19 00:52:17.874: INFO: kube-apiserver-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container kube-apiserver ready: true, restart count 2
Jul 19 00:52:17.874: INFO: kube-controller-manager-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 19 00:52:17.874: INFO: kube-multus-ds-amd64-bcmw4 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container kube-multus ready: true, restart count 1
Jul 19 00:52:17.874: INFO: kube-proxy-kxjqb from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 19 00:52:17.874: INFO: kube-scheduler-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container kube-scheduler ready: true, restart count 1
Jul 19 00:52:17.874: INFO: kube-sriov-cni-ds-amd64-69r58 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container kube-sriov-cni ready: true, restart count 1
Jul 19 00:52:17.874: INFO: rbd-nodeplugin-j7slt from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:52:17.874: INFO: rbd-provisioner-54c6c894f7-sggw9 from kube-system started at 2023-07-18 22:34:50 +0000 UTC (6 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:52:17.874: INFO: dm-monitor-84b75cf89c-vnb6c from platform-deployment-manager started at 2023-07-18 21:53:06 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container dm-monitor ready: true, restart count 1
Jul 19 00:52:17.874: INFO: platform-deployment-manager-7ff76b89d-qg7g9 from platform-deployment-manager started at 2023-07-18 21:52:11 +0000 UTC (2 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jul 19 00:52:17.874: INFO: 	Container manager ready: true, restart count 1
Jul 19 00:52:17.874: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 00:52:17.874: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 19 00:52:17.874: INFO: 
Logging pods the apiserver thinks is on node controller-1 before test
Jul 19 00:52:17.882: INFO: cm-cert-manager-7fb65857f5-g94zp from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container cert-manager-controller ready: true, restart count 0
Jul 19 00:52:17.883: INFO: cm-cert-manager-cainjector-86b69d7d69-vl4v2 from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
Jul 19 00:52:17.883: INFO: cm-cert-manager-webhook-98ddcd5cb-tq28v from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container cert-manager-webhook ready: true, restart count 0
Jul 19 00:52:17.883: INFO: calico-node-hz2hv from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container calico-node ready: true, restart count 0
Jul 19 00:52:17.883: INFO: ceph-pools-audit-28162125-cqlst from kube-system started at 2023-07-19 00:45:00 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jul 19 00:52:17.883: INFO: ceph-pools-audit-28162130-rr8lg from kube-system started at 2023-07-19 00:50:00 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jul 19 00:52:17.883: INFO: cephfs-nodeplugin-xn85z from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:52:17.883: INFO: cephfs-provisioner-5f69fbf97-vxdxv from kube-system started at 2023-07-19 00:41:43 +0000 UTC (4 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:52:17.883: INFO: coredns-66856967f4-vcphq from kube-system started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container coredns ready: true, restart count 0
Jul 19 00:52:17.883: INFO: ic-nginx-ingress-ingress-nginx-controller-vfpd9 from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container controller ready: true, restart count 0
Jul 19 00:52:17.883: INFO: kube-apiserver-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container kube-apiserver ready: true, restart count 1
Jul 19 00:52:17.883: INFO: kube-controller-manager-controller-1 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 19 00:52:17.883: INFO: kube-multus-ds-amd64-k6nb2 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container kube-multus ready: true, restart count 1
Jul 19 00:52:17.883: INFO: kube-proxy-66jhd from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 19 00:52:17.883: INFO: kube-scheduler-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container kube-scheduler ready: true, restart count 1
Jul 19 00:52:17.883: INFO: kube-sriov-cni-ds-amd64-rpb6b from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jul 19 00:52:17.883: INFO: rbd-nodeplugin-9mw7d from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:52:17.883: INFO: rbd-provisioner-54c6c894f7-6k98h from kube-system started at 2023-07-19 00:41:43 +0000 UTC (6 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:52:17.883: INFO: labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846 from projected-393 started at 2023-07-19 00:52:11 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container client-container ready: true, restart count 0
Jul 19 00:52:17.883: INFO: sonobuoy from sonobuoy started at 2023-07-18 23:44:15 +0000 UTC (1 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 19 00:52:17.883: INFO: sonobuoy-e2e-job-6b28f3e332364b76 from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container e2e ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 00:52:17.883: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-ckzmn from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 00:52:17.883: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 00:52:17.883: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 07/19/23 00:52:17.883
Jul 19 00:52:17.887: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-537" to be "running"
Jul 19 00:52:17.889: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.509623ms
Jul 19 00:52:19.891: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004452394s
Jul 19 00:52:19.891: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 07/19/23 00:52:19.893
STEP: Trying to apply a random label on the found node. 07/19/23 00:52:19.899
STEP: verifying the node has the label kubernetes.io/e2e-f24e6c9b-2bce-4c05-ad6e-83b4f9ffdeb8 42 07/19/23 00:52:19.909
STEP: Trying to relaunch the pod, now with labels. 07/19/23 00:52:19.912
Jul 19 00:52:19.916: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-537" to be "not pending"
Jul 19 00:52:19.917: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.652743ms
Jul 19 00:52:21.931: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.015128636s
Jul 19 00:52:21.931: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-f24e6c9b-2bce-4c05-ad6e-83b4f9ffdeb8 off the node controller-1 07/19/23 00:52:21.932
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f24e6c9b-2bce-4c05-ad6e-83b4f9ffdeb8 07/19/23 00:52:21.939
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:52:21.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-537" for this suite. 07/19/23 00:52:21.945
------------------------------
• [4.104 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:52:17.843
    Jul 19 00:52:17.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sched-pred 07/19/23 00:52:17.845
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:17.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:17.857
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jul 19 00:52:17.859: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jul 19 00:52:17.863: INFO: Waiting for terminating namespaces to be deleted...
    Jul 19 00:52:17.865: INFO: 
    Logging pods the apiserver thinks is on node controller-0 before test
    Jul 19 00:52:17.874: INFO: cm-cert-manager-7fb65857f5-5mnw8 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container cert-manager-controller ready: true, restart count 2
    Jul 19 00:52:17.874: INFO: cm-cert-manager-cainjector-86b69d7d69-6czpr from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container cert-manager-cainjector ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: cm-cert-manager-webhook-98ddcd5cb-2q6p9 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container cert-manager-webhook ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: helm-controller-5fb8ccb85d-nl9lf from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container manager ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: source-controller-69b5d8f7d8-66tt6 from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container manager ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: calico-kube-controllers-7f5cd5f684-5jblg from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container calico-kube-controllers ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: calico-node-hmrnj from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container calico-node ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: cephfs-nodeplugin-n2vfj from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: cephfs-provisioner-5f69fbf97-q8lg6 from kube-system started at 2023-07-18 22:35:12 +0000 UTC (4 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: coredns-66856967f4-whd5z from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container coredns ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: ic-nginx-ingress-ingress-nginx-controller-6hpm5 from kube-system started at 2023-07-18 21:49:16 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container controller ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: kube-apiserver-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container kube-apiserver ready: true, restart count 2
    Jul 19 00:52:17.874: INFO: kube-controller-manager-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: kube-multus-ds-amd64-bcmw4 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container kube-multus ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: kube-proxy-kxjqb from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: kube-scheduler-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: kube-sriov-cni-ds-amd64-69r58 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container kube-sriov-cni ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: rbd-nodeplugin-j7slt from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: rbd-provisioner-54c6c894f7-sggw9 from kube-system started at 2023-07-18 22:34:50 +0000 UTC (6 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container csi-attacher ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: dm-monitor-84b75cf89c-vnb6c from platform-deployment-manager started at 2023-07-18 21:53:06 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container dm-monitor ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: platform-deployment-manager-7ff76b89d-qg7g9 from platform-deployment-manager started at 2023-07-18 21:52:11 +0000 UTC (2 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: 	Container manager ready: true, restart count 1
    Jul 19 00:52:17.874: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 00:52:17.874: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 19 00:52:17.874: INFO: 
    Logging pods the apiserver thinks is on node controller-1 before test
    Jul 19 00:52:17.882: INFO: cm-cert-manager-7fb65857f5-g94zp from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container cert-manager-controller ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: cm-cert-manager-cainjector-86b69d7d69-vl4v2 from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: cm-cert-manager-webhook-98ddcd5cb-tq28v from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: calico-node-hz2hv from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container calico-node ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: ceph-pools-audit-28162125-cqlst from kube-system started at 2023-07-19 00:45:00 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
    Jul 19 00:52:17.883: INFO: ceph-pools-audit-28162130-rr8lg from kube-system started at 2023-07-19 00:50:00 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
    Jul 19 00:52:17.883: INFO: cephfs-nodeplugin-xn85z from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: cephfs-provisioner-5f69fbf97-vxdxv from kube-system started at 2023-07-19 00:41:43 +0000 UTC (4 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: coredns-66856967f4-vcphq from kube-system started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container coredns ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: ic-nginx-ingress-ingress-nginx-controller-vfpd9 from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container controller ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: kube-apiserver-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jul 19 00:52:17.883: INFO: kube-controller-manager-controller-1 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jul 19 00:52:17.883: INFO: kube-multus-ds-amd64-k6nb2 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container kube-multus ready: true, restart count 1
    Jul 19 00:52:17.883: INFO: kube-proxy-66jhd from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 19 00:52:17.883: INFO: kube-scheduler-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jul 19 00:52:17.883: INFO: kube-sriov-cni-ds-amd64-rpb6b from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container kube-sriov-cni ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: rbd-nodeplugin-9mw7d from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: rbd-provisioner-54c6c894f7-6k98h from kube-system started at 2023-07-19 00:41:43 +0000 UTC (6 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container csi-attacher ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: labelsupdate3b70da7f-7842-4e7a-abdf-d99ba0d1d846 from projected-393 started at 2023-07-19 00:52:11 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container client-container ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: sonobuoy from sonobuoy started at 2023-07-18 23:44:15 +0000 UTC (1 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: sonobuoy-e2e-job-6b28f3e332364b76 from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container e2e ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-ckzmn from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 00:52:17.883: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 00:52:17.883: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 07/19/23 00:52:17.883
    Jul 19 00:52:17.887: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-537" to be "running"
    Jul 19 00:52:17.889: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.509623ms
    Jul 19 00:52:19.891: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.004452394s
    Jul 19 00:52:19.891: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 07/19/23 00:52:19.893
    STEP: Trying to apply a random label on the found node. 07/19/23 00:52:19.899
    STEP: verifying the node has the label kubernetes.io/e2e-f24e6c9b-2bce-4c05-ad6e-83b4f9ffdeb8 42 07/19/23 00:52:19.909
    STEP: Trying to relaunch the pod, now with labels. 07/19/23 00:52:19.912
    Jul 19 00:52:19.916: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-537" to be "not pending"
    Jul 19 00:52:19.917: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 1.652743ms
    Jul 19 00:52:21.931: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.015128636s
    Jul 19 00:52:21.931: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-f24e6c9b-2bce-4c05-ad6e-83b4f9ffdeb8 off the node controller-1 07/19/23 00:52:21.932
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f24e6c9b-2bce-4c05-ad6e-83b4f9ffdeb8 07/19/23 00:52:21.939
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:52:21.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-537" for this suite. 07/19/23 00:52:21.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:52:21.949
Jul 19 00:52:21.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename disruption 07/19/23 00:52:21.949
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:21.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:21.958
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 07/19/23 00:52:21.96
STEP: Waiting for the pdb to be processed 07/19/23 00:52:21.962
STEP: updating the pdb 07/19/23 00:52:23.966
STEP: Waiting for the pdb to be processed 07/19/23 00:52:23.97
STEP: patching the pdb 07/19/23 00:52:25.974
STEP: Waiting for the pdb to be processed 07/19/23 00:52:25.979
STEP: Waiting for the pdb to be deleted 07/19/23 00:52:27.987
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jul 19 00:52:27.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2054" for this suite. 07/19/23 00:52:27.99
------------------------------
• [SLOW TEST] [6.044 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:52:21.949
    Jul 19 00:52:21.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename disruption 07/19/23 00:52:21.949
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:21.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:21.958
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 07/19/23 00:52:21.96
    STEP: Waiting for the pdb to be processed 07/19/23 00:52:21.962
    STEP: updating the pdb 07/19/23 00:52:23.966
    STEP: Waiting for the pdb to be processed 07/19/23 00:52:23.97
    STEP: patching the pdb 07/19/23 00:52:25.974
    STEP: Waiting for the pdb to be processed 07/19/23 00:52:25.979
    STEP: Waiting for the pdb to be deleted 07/19/23 00:52:27.987
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:52:27.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2054" for this suite. 07/19/23 00:52:27.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:52:27.994
Jul 19 00:52:27.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename resourcequota 07/19/23 00:52:27.995
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:28.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:28.003
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 07/19/23 00:52:28.005
STEP: Creating a ResourceQuota 07/19/23 00:52:33.007
STEP: Ensuring resource quota status is calculated 07/19/23 00:52:33.009
STEP: Creating a Service 07/19/23 00:52:35.011
STEP: Creating a NodePort Service 07/19/23 00:52:35.021
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 07/19/23 00:52:35.042
STEP: Ensuring resource quota status captures service creation 07/19/23 00:52:35.059
STEP: Deleting Services 07/19/23 00:52:37.061
STEP: Ensuring resource quota status released usage 07/19/23 00:52:37.084
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jul 19 00:52:39.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8549" for this suite. 07/19/23 00:52:39.09
------------------------------
• [SLOW TEST] [11.098 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:52:27.994
    Jul 19 00:52:27.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename resourcequota 07/19/23 00:52:27.995
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:28.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:28.003
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 07/19/23 00:52:28.005
    STEP: Creating a ResourceQuota 07/19/23 00:52:33.007
    STEP: Ensuring resource quota status is calculated 07/19/23 00:52:33.009
    STEP: Creating a Service 07/19/23 00:52:35.011
    STEP: Creating a NodePort Service 07/19/23 00:52:35.021
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 07/19/23 00:52:35.042
    STEP: Ensuring resource quota status captures service creation 07/19/23 00:52:35.059
    STEP: Deleting Services 07/19/23 00:52:37.061
    STEP: Ensuring resource quota status released usage 07/19/23 00:52:37.084
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:52:39.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8549" for this suite. 07/19/23 00:52:39.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:52:39.094
Jul 19 00:52:39.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename subpath 07/19/23 00:52:39.095
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:39.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:39.104
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 07/19/23 00:52:39.106
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-cx9t 07/19/23 00:52:39.11
STEP: Creating a pod to test atomic-volume-subpath 07/19/23 00:52:39.11
Jul 19 00:52:39.114: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cx9t" in namespace "subpath-1353" to be "Succeeded or Failed"
Jul 19 00:52:39.121: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.893949ms
Jul 19 00:52:41.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.009482886s
Jul 19 00:52:43.124: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 4.010610121s
Jul 19 00:52:45.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 6.009364793s
Jul 19 00:52:47.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 8.009781901s
Jul 19 00:52:49.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 10.009801911s
Jul 19 00:52:51.124: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 12.010056328s
Jul 19 00:52:53.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 14.009467562s
Jul 19 00:52:55.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 16.009193943s
Jul 19 00:52:57.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 18.009719214s
Jul 19 00:52:59.124: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 20.0106787s
Jul 19 00:53:01.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=false. Elapsed: 22.009296887s
Jul 19 00:53:03.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009458149s
STEP: Saw pod success 07/19/23 00:53:03.123
Jul 19 00:53:03.123: INFO: Pod "pod-subpath-test-projected-cx9t" satisfied condition "Succeeded or Failed"
Jul 19 00:53:03.125: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-projected-cx9t container test-container-subpath-projected-cx9t: <nil>
STEP: delete the pod 07/19/23 00:53:03.129
Jul 19 00:53:03.136: INFO: Waiting for pod pod-subpath-test-projected-cx9t to disappear
Jul 19 00:53:03.138: INFO: Pod pod-subpath-test-projected-cx9t no longer exists
STEP: Deleting pod pod-subpath-test-projected-cx9t 07/19/23 00:53:03.138
Jul 19 00:53:03.138: INFO: Deleting pod "pod-subpath-test-projected-cx9t" in namespace "subpath-1353"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jul 19 00:53:03.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1353" for this suite. 07/19/23 00:53:03.142
------------------------------
• [SLOW TEST] [24.051 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:52:39.094
    Jul 19 00:52:39.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename subpath 07/19/23 00:52:39.095
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:52:39.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:52:39.104
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 07/19/23 00:52:39.106
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-cx9t 07/19/23 00:52:39.11
    STEP: Creating a pod to test atomic-volume-subpath 07/19/23 00:52:39.11
    Jul 19 00:52:39.114: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cx9t" in namespace "subpath-1353" to be "Succeeded or Failed"
    Jul 19 00:52:39.121: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.893949ms
    Jul 19 00:52:41.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 2.009482886s
    Jul 19 00:52:43.124: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 4.010610121s
    Jul 19 00:52:45.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 6.009364793s
    Jul 19 00:52:47.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 8.009781901s
    Jul 19 00:52:49.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 10.009801911s
    Jul 19 00:52:51.124: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 12.010056328s
    Jul 19 00:52:53.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 14.009467562s
    Jul 19 00:52:55.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 16.009193943s
    Jul 19 00:52:57.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 18.009719214s
    Jul 19 00:52:59.124: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=true. Elapsed: 20.0106787s
    Jul 19 00:53:01.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Running", Reason="", readiness=false. Elapsed: 22.009296887s
    Jul 19 00:53:03.123: INFO: Pod "pod-subpath-test-projected-cx9t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009458149s
    STEP: Saw pod success 07/19/23 00:53:03.123
    Jul 19 00:53:03.123: INFO: Pod "pod-subpath-test-projected-cx9t" satisfied condition "Succeeded or Failed"
    Jul 19 00:53:03.125: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-projected-cx9t container test-container-subpath-projected-cx9t: <nil>
    STEP: delete the pod 07/19/23 00:53:03.129
    Jul 19 00:53:03.136: INFO: Waiting for pod pod-subpath-test-projected-cx9t to disappear
    Jul 19 00:53:03.138: INFO: Pod pod-subpath-test-projected-cx9t no longer exists
    STEP: Deleting pod pod-subpath-test-projected-cx9t 07/19/23 00:53:03.138
    Jul 19 00:53:03.138: INFO: Deleting pod "pod-subpath-test-projected-cx9t" in namespace "subpath-1353"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:53:03.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1353" for this suite. 07/19/23 00:53:03.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:53:03.146
Jul 19 00:53:03.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 00:53:03.146
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:03.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:03.155
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 07/19/23 00:53:03.157
STEP: fetching the ConfigMap 07/19/23 00:53:03.159
STEP: patching the ConfigMap 07/19/23 00:53:03.163
STEP: listing all ConfigMaps in all namespaces with a label selector 07/19/23 00:53:03.17
STEP: deleting the ConfigMap by collection with a label selector 07/19/23 00:53:03.172
STEP: listing all ConfigMaps in test namespace 07/19/23 00:53:03.175
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:53:03.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-327" for this suite. 07/19/23 00:53:03.179
------------------------------
• [0.035 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:53:03.146
    Jul 19 00:53:03.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 00:53:03.146
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:03.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:03.155
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 07/19/23 00:53:03.157
    STEP: fetching the ConfigMap 07/19/23 00:53:03.159
    STEP: patching the ConfigMap 07/19/23 00:53:03.163
    STEP: listing all ConfigMaps in all namespaces with a label selector 07/19/23 00:53:03.17
    STEP: deleting the ConfigMap by collection with a label selector 07/19/23 00:53:03.172
    STEP: listing all ConfigMaps in test namespace 07/19/23 00:53:03.175
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:53:03.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-327" for this suite. 07/19/23 00:53:03.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:53:03.181
Jul 19 00:53:03.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename subpath 07/19/23 00:53:03.182
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:03.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:03.194
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 07/19/23 00:53:03.195
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-mr9q 07/19/23 00:53:03.199
STEP: Creating a pod to test atomic-volume-subpath 07/19/23 00:53:03.199
Jul 19 00:53:03.204: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mr9q" in namespace "subpath-2788" to be "Succeeded or Failed"
Jul 19 00:53:03.208: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59167ms
Jul 19 00:53:05.210: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.006445302s
Jul 19 00:53:07.211: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 4.006998386s
Jul 19 00:53:09.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 6.008281308s
Jul 19 00:53:11.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 8.008042461s
Jul 19 00:53:13.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 10.007989052s
Jul 19 00:53:15.211: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 12.007305191s
Jul 19 00:53:17.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 14.008275236s
Jul 19 00:53:19.211: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 16.007374859s
Jul 19 00:53:21.211: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 18.006959083s
Jul 19 00:53:23.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 20.008059647s
Jul 19 00:53:25.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=false. Elapsed: 22.007727474s
Jul 19 00:53:27.210: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006671142s
STEP: Saw pod success 07/19/23 00:53:27.21
Jul 19 00:53:27.211: INFO: Pod "pod-subpath-test-configmap-mr9q" satisfied condition "Succeeded or Failed"
Jul 19 00:53:27.212: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-configmap-mr9q container test-container-subpath-configmap-mr9q: <nil>
STEP: delete the pod 07/19/23 00:53:27.216
Jul 19 00:53:27.223: INFO: Waiting for pod pod-subpath-test-configmap-mr9q to disappear
Jul 19 00:53:27.224: INFO: Pod pod-subpath-test-configmap-mr9q no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mr9q 07/19/23 00:53:27.224
Jul 19 00:53:27.224: INFO: Deleting pod "pod-subpath-test-configmap-mr9q" in namespace "subpath-2788"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jul 19 00:53:27.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2788" for this suite. 07/19/23 00:53:27.228
------------------------------
• [SLOW TEST] [24.049 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:53:03.181
    Jul 19 00:53:03.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename subpath 07/19/23 00:53:03.182
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:03.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:03.194
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 07/19/23 00:53:03.195
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-mr9q 07/19/23 00:53:03.199
    STEP: Creating a pod to test atomic-volume-subpath 07/19/23 00:53:03.199
    Jul 19 00:53:03.204: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mr9q" in namespace "subpath-2788" to be "Succeeded or Failed"
    Jul 19 00:53:03.208: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.59167ms
    Jul 19 00:53:05.210: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.006445302s
    Jul 19 00:53:07.211: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 4.006998386s
    Jul 19 00:53:09.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 6.008281308s
    Jul 19 00:53:11.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 8.008042461s
    Jul 19 00:53:13.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 10.007989052s
    Jul 19 00:53:15.211: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 12.007305191s
    Jul 19 00:53:17.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 14.008275236s
    Jul 19 00:53:19.211: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 16.007374859s
    Jul 19 00:53:21.211: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 18.006959083s
    Jul 19 00:53:23.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=true. Elapsed: 20.008059647s
    Jul 19 00:53:25.212: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Running", Reason="", readiness=false. Elapsed: 22.007727474s
    Jul 19 00:53:27.210: INFO: Pod "pod-subpath-test-configmap-mr9q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006671142s
    STEP: Saw pod success 07/19/23 00:53:27.21
    Jul 19 00:53:27.211: INFO: Pod "pod-subpath-test-configmap-mr9q" satisfied condition "Succeeded or Failed"
    Jul 19 00:53:27.212: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-configmap-mr9q container test-container-subpath-configmap-mr9q: <nil>
    STEP: delete the pod 07/19/23 00:53:27.216
    Jul 19 00:53:27.223: INFO: Waiting for pod pod-subpath-test-configmap-mr9q to disappear
    Jul 19 00:53:27.224: INFO: Pod pod-subpath-test-configmap-mr9q no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-mr9q 07/19/23 00:53:27.224
    Jul 19 00:53:27.224: INFO: Deleting pod "pod-subpath-test-configmap-mr9q" in namespace "subpath-2788"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:53:27.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2788" for this suite. 07/19/23 00:53:27.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:53:27.231
Jul 19 00:53:27.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename dns 07/19/23 00:53:27.232
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:27.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:27.24
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-386.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-386.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 07/19/23 00:53:27.242
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-386.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-386.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 07/19/23 00:53:27.242
STEP: creating a pod to probe /etc/hosts 07/19/23 00:53:27.242
STEP: submitting the pod to kubernetes 07/19/23 00:53:27.242
Jul 19 00:53:27.246: INFO: Waiting up to 15m0s for pod "dns-test-e627d7f9-0386-4c57-95d1-d712102610f7" in namespace "dns-386" to be "running"
Jul 19 00:53:27.251: INFO: Pod "dns-test-e627d7f9-0386-4c57-95d1-d712102610f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.510573ms
Jul 19 00:53:29.253: INFO: Pod "dns-test-e627d7f9-0386-4c57-95d1-d712102610f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006794064s
Jul 19 00:53:29.253: INFO: Pod "dns-test-e627d7f9-0386-4c57-95d1-d712102610f7" satisfied condition "running"
STEP: retrieving the pod 07/19/23 00:53:29.253
STEP: looking for the results for each expected name from probers 07/19/23 00:53:29.255
Jul 19 00:53:29.262: INFO: DNS probes using dns-386/dns-test-e627d7f9-0386-4c57-95d1-d712102610f7 succeeded

STEP: deleting the pod 07/19/23 00:53:29.262
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jul 19 00:53:29.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-386" for this suite. 07/19/23 00:53:29.274
------------------------------
• [2.046 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:53:27.231
    Jul 19 00:53:27.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename dns 07/19/23 00:53:27.232
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:27.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:27.24
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-386.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-386.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     07/19/23 00:53:27.242
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-386.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-386.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     07/19/23 00:53:27.242
    STEP: creating a pod to probe /etc/hosts 07/19/23 00:53:27.242
    STEP: submitting the pod to kubernetes 07/19/23 00:53:27.242
    Jul 19 00:53:27.246: INFO: Waiting up to 15m0s for pod "dns-test-e627d7f9-0386-4c57-95d1-d712102610f7" in namespace "dns-386" to be "running"
    Jul 19 00:53:27.251: INFO: Pod "dns-test-e627d7f9-0386-4c57-95d1-d712102610f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.510573ms
    Jul 19 00:53:29.253: INFO: Pod "dns-test-e627d7f9-0386-4c57-95d1-d712102610f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006794064s
    Jul 19 00:53:29.253: INFO: Pod "dns-test-e627d7f9-0386-4c57-95d1-d712102610f7" satisfied condition "running"
    STEP: retrieving the pod 07/19/23 00:53:29.253
    STEP: looking for the results for each expected name from probers 07/19/23 00:53:29.255
    Jul 19 00:53:29.262: INFO: DNS probes using dns-386/dns-test-e627d7f9-0386-4c57-95d1-d712102610f7 succeeded

    STEP: deleting the pod 07/19/23 00:53:29.262
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:53:29.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-386" for this suite. 07/19/23 00:53:29.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:53:29.277
Jul 19 00:53:29.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename proxy 07/19/23 00:53:29.278
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:29.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:29.288
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jul 19 00:53:29.289: INFO: Creating pod...
Jul 19 00:53:29.293: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6549" to be "running"
Jul 19 00:53:29.295: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.393289ms
Jul 19 00:53:31.297: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.003486241s
Jul 19 00:53:31.297: INFO: Pod "agnhost" satisfied condition "running"
Jul 19 00:53:31.297: INFO: Creating service...
Jul 19 00:53:31.305: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/DELETE
Jul 19 00:53:31.307: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 19 00:53:31.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/GET
Jul 19 00:53:31.312: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul 19 00:53:31.312: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/HEAD
Jul 19 00:53:31.313: INFO: http.Client request:HEAD | StatusCode:200
Jul 19 00:53:31.313: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/OPTIONS
Jul 19 00:53:31.315: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 19 00:53:31.315: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/PATCH
Jul 19 00:53:31.322: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 19 00:53:31.322: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/POST
Jul 19 00:53:31.323: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 19 00:53:31.323: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/PUT
Jul 19 00:53:31.325: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul 19 00:53:31.325: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/DELETE
Jul 19 00:53:31.327: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 19 00:53:31.327: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/GET
Jul 19 00:53:31.328: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul 19 00:53:31.328: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/HEAD
Jul 19 00:53:31.330: INFO: http.Client request:HEAD | StatusCode:200
Jul 19 00:53:31.330: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/OPTIONS
Jul 19 00:53:31.332: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 19 00:53:31.332: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/PATCH
Jul 19 00:53:31.334: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 19 00:53:31.334: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/POST
Jul 19 00:53:31.336: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 19 00:53:31.336: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/PUT
Jul 19 00:53:31.338: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jul 19 00:53:31.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-6549" for this suite. 07/19/23 00:53:31.34
------------------------------
• [2.066 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:53:29.277
    Jul 19 00:53:29.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename proxy 07/19/23 00:53:29.278
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:29.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:29.288
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jul 19 00:53:29.289: INFO: Creating pod...
    Jul 19 00:53:29.293: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6549" to be "running"
    Jul 19 00:53:29.295: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.393289ms
    Jul 19 00:53:31.297: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.003486241s
    Jul 19 00:53:31.297: INFO: Pod "agnhost" satisfied condition "running"
    Jul 19 00:53:31.297: INFO: Creating service...
    Jul 19 00:53:31.305: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/DELETE
    Jul 19 00:53:31.307: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jul 19 00:53:31.307: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/GET
    Jul 19 00:53:31.312: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jul 19 00:53:31.312: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/HEAD
    Jul 19 00:53:31.313: INFO: http.Client request:HEAD | StatusCode:200
    Jul 19 00:53:31.313: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/OPTIONS
    Jul 19 00:53:31.315: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jul 19 00:53:31.315: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/PATCH
    Jul 19 00:53:31.322: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jul 19 00:53:31.322: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/POST
    Jul 19 00:53:31.323: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jul 19 00:53:31.323: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/pods/agnhost/proxy/some/path/with/PUT
    Jul 19 00:53:31.325: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jul 19 00:53:31.325: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/DELETE
    Jul 19 00:53:31.327: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jul 19 00:53:31.327: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/GET
    Jul 19 00:53:31.328: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jul 19 00:53:31.328: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/HEAD
    Jul 19 00:53:31.330: INFO: http.Client request:HEAD | StatusCode:200
    Jul 19 00:53:31.330: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/OPTIONS
    Jul 19 00:53:31.332: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jul 19 00:53:31.332: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/PATCH
    Jul 19 00:53:31.334: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jul 19 00:53:31.334: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/POST
    Jul 19 00:53:31.336: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jul 19 00:53:31.336: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-6549/services/test-service/proxy/some/path/with/PUT
    Jul 19 00:53:31.338: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:53:31.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-6549" for this suite. 07/19/23 00:53:31.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:53:31.343
Jul 19 00:53:31.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:53:31.344
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:31.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:31.352
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:53:31.354
Jul 19 00:53:31.358: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996" in namespace "projected-5150" to be "Succeeded or Failed"
Jul 19 00:53:31.360: INFO: Pod "downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996": Phase="Pending", Reason="", readiness=false. Elapsed: 1.428436ms
Jul 19 00:53:33.362: INFO: Pod "downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003559849s
Jul 19 00:53:35.362: INFO: Pod "downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003781556s
STEP: Saw pod success 07/19/23 00:53:35.362
Jul 19 00:53:35.362: INFO: Pod "downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996" satisfied condition "Succeeded or Failed"
Jul 19 00:53:35.364: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996 container client-container: <nil>
STEP: delete the pod 07/19/23 00:53:35.367
Jul 19 00:53:35.374: INFO: Waiting for pod downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996 to disappear
Jul 19 00:53:35.375: INFO: Pod downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 19 00:53:35.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5150" for this suite. 07/19/23 00:53:35.377
------------------------------
• [4.036 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:53:31.343
    Jul 19 00:53:31.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:53:31.344
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:31.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:31.352
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:53:31.354
    Jul 19 00:53:31.358: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996" in namespace "projected-5150" to be "Succeeded or Failed"
    Jul 19 00:53:31.360: INFO: Pod "downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996": Phase="Pending", Reason="", readiness=false. Elapsed: 1.428436ms
    Jul 19 00:53:33.362: INFO: Pod "downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003559849s
    Jul 19 00:53:35.362: INFO: Pod "downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003781556s
    STEP: Saw pod success 07/19/23 00:53:35.362
    Jul 19 00:53:35.362: INFO: Pod "downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996" satisfied condition "Succeeded or Failed"
    Jul 19 00:53:35.364: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996 container client-container: <nil>
    STEP: delete the pod 07/19/23 00:53:35.367
    Jul 19 00:53:35.374: INFO: Waiting for pod downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996 to disappear
    Jul 19 00:53:35.375: INFO: Pod downwardapi-volume-4f355883-0db6-4a5e-86ed-cd18b6814996 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:53:35.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5150" for this suite. 07/19/23 00:53:35.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:53:35.38
Jul 19 00:53:35.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename daemonsets 07/19/23 00:53:35.381
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:35.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:35.389
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jul 19 00:53:35.399: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 07/19/23 00:53:35.401
Jul 19 00:53:35.404: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:53:35.404: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 07/19/23 00:53:35.404
Jul 19 00:53:35.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:53:35.415: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 19 00:53:36.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:53:36.418: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 19 00:53:37.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 19 00:53:37.418: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 07/19/23 00:53:37.42
Jul 19 00:53:37.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 19 00:53:37.430: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jul 19 00:53:38.432: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:53:38.432: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 07/19/23 00:53:38.432
Jul 19 00:53:38.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:53:38.471: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 19 00:53:39.473: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:53:39.473: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 19 00:53:40.473: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:53:40.473: INFO: Node controller-1 is running 0 daemon pod, expected 1
Jul 19 00:53:41.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 19 00:53:41.474: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:53:41.477
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5636, will wait for the garbage collector to delete the pods 07/19/23 00:53:41.477
Jul 19 00:53:41.535: INFO: Deleting DaemonSet.extensions daemon-set took: 6.272383ms
Jul 19 00:53:41.636: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.930985ms
Jul 19 00:53:43.839: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 19 00:53:43.839: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 19 00:53:43.840: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"83650"},"items":null}

Jul 19 00:53:43.842: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"83650"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:53:43.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5636" for this suite. 07/19/23 00:53:43.857
------------------------------
• [SLOW TEST] [8.485 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:53:35.38
    Jul 19 00:53:35.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename daemonsets 07/19/23 00:53:35.381
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:35.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:35.389
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jul 19 00:53:35.399: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 07/19/23 00:53:35.401
    Jul 19 00:53:35.404: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:53:35.404: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 07/19/23 00:53:35.404
    Jul 19 00:53:35.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:53:35.415: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 19 00:53:36.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:53:36.418: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 19 00:53:37.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 19 00:53:37.418: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 07/19/23 00:53:37.42
    Jul 19 00:53:37.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 19 00:53:37.430: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jul 19 00:53:38.432: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:53:38.432: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 07/19/23 00:53:38.432
    Jul 19 00:53:38.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:53:38.471: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 19 00:53:39.473: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:53:39.473: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 19 00:53:40.473: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:53:40.473: INFO: Node controller-1 is running 0 daemon pod, expected 1
    Jul 19 00:53:41.474: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jul 19 00:53:41.474: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 07/19/23 00:53:41.477
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5636, will wait for the garbage collector to delete the pods 07/19/23 00:53:41.477
    Jul 19 00:53:41.535: INFO: Deleting DaemonSet.extensions daemon-set took: 6.272383ms
    Jul 19 00:53:41.636: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.930985ms
    Jul 19 00:53:43.839: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jul 19 00:53:43.839: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jul 19 00:53:43.840: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"83650"},"items":null}

    Jul 19 00:53:43.842: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"83650"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:53:43.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5636" for this suite. 07/19/23 00:53:43.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:53:43.866
Jul 19 00:53:43.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sched-pred 07/19/23 00:53:43.867
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:43.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:43.876
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jul 19 00:53:43.877: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 19 00:53:43.881: INFO: Waiting for terminating namespaces to be deleted...
Jul 19 00:53:43.883: INFO: 
Logging pods the apiserver thinks is on node controller-0 before test
Jul 19 00:53:43.891: INFO: cm-cert-manager-7fb65857f5-5mnw8 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container cert-manager-controller ready: true, restart count 2
Jul 19 00:53:43.891: INFO: cm-cert-manager-cainjector-86b69d7d69-6czpr from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container cert-manager-cainjector ready: true, restart count 1
Jul 19 00:53:43.891: INFO: cm-cert-manager-webhook-98ddcd5cb-2q6p9 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container cert-manager-webhook ready: true, restart count 1
Jul 19 00:53:43.891: INFO: helm-controller-5fb8ccb85d-nl9lf from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container manager ready: true, restart count 1
Jul 19 00:53:43.891: INFO: source-controller-69b5d8f7d8-66tt6 from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container manager ready: true, restart count 1
Jul 19 00:53:43.891: INFO: calico-kube-controllers-7f5cd5f684-5jblg from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jul 19 00:53:43.891: INFO: calico-node-hmrnj from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container calico-node ready: true, restart count 1
Jul 19 00:53:43.891: INFO: cephfs-nodeplugin-n2vfj from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:53:43.891: INFO: cephfs-provisioner-5f69fbf97-q8lg6 from kube-system started at 2023-07-18 22:35:12 +0000 UTC (4 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:53:43.891: INFO: coredns-66856967f4-whd5z from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container coredns ready: true, restart count 1
Jul 19 00:53:43.891: INFO: ic-nginx-ingress-ingress-nginx-controller-6hpm5 from kube-system started at 2023-07-18 21:49:16 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container controller ready: true, restart count 1
Jul 19 00:53:43.891: INFO: kube-apiserver-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container kube-apiserver ready: true, restart count 2
Jul 19 00:53:43.891: INFO: kube-controller-manager-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 19 00:53:43.891: INFO: kube-multus-ds-amd64-bcmw4 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container kube-multus ready: true, restart count 1
Jul 19 00:53:43.891: INFO: kube-proxy-kxjqb from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 19 00:53:43.891: INFO: kube-scheduler-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container kube-scheduler ready: true, restart count 1
Jul 19 00:53:43.891: INFO: kube-sriov-cni-ds-amd64-69r58 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container kube-sriov-cni ready: true, restart count 1
Jul 19 00:53:43.891: INFO: rbd-nodeplugin-j7slt from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:53:43.891: INFO: rbd-provisioner-54c6c894f7-sggw9 from kube-system started at 2023-07-18 22:34:50 +0000 UTC (6 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:53:43.891: INFO: dm-monitor-84b75cf89c-vnb6c from platform-deployment-manager started at 2023-07-18 21:53:06 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container dm-monitor ready: true, restart count 1
Jul 19 00:53:43.891: INFO: platform-deployment-manager-7ff76b89d-qg7g9 from platform-deployment-manager started at 2023-07-18 21:52:11 +0000 UTC (2 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jul 19 00:53:43.891: INFO: 	Container manager ready: true, restart count 1
Jul 19 00:53:43.891: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 00:53:43.891: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 19 00:53:43.891: INFO: 
Logging pods the apiserver thinks is on node controller-1 before test
Jul 19 00:53:43.900: INFO: cm-cert-manager-7fb65857f5-g94zp from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container cert-manager-controller ready: true, restart count 0
Jul 19 00:53:43.900: INFO: cm-cert-manager-cainjector-86b69d7d69-vl4v2 from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
Jul 19 00:53:43.900: INFO: cm-cert-manager-webhook-98ddcd5cb-tq28v from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container cert-manager-webhook ready: true, restart count 0
Jul 19 00:53:43.900: INFO: calico-node-hz2hv from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container calico-node ready: true, restart count 0
Jul 19 00:53:43.900: INFO: ceph-pools-audit-28162125-cqlst from kube-system started at 2023-07-19 00:45:00 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jul 19 00:53:43.900: INFO: ceph-pools-audit-28162130-rr8lg from kube-system started at 2023-07-19 00:50:00 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jul 19 00:53:43.900: INFO: cephfs-nodeplugin-xn85z from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:53:43.900: INFO: cephfs-provisioner-5f69fbf97-vxdxv from kube-system started at 2023-07-19 00:41:43 +0000 UTC (4 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:53:43.900: INFO: coredns-66856967f4-vcphq from kube-system started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container coredns ready: true, restart count 0
Jul 19 00:53:43.900: INFO: ic-nginx-ingress-ingress-nginx-controller-vfpd9 from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container controller ready: true, restart count 0
Jul 19 00:53:43.900: INFO: kube-apiserver-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container kube-apiserver ready: true, restart count 1
Jul 19 00:53:43.900: INFO: kube-controller-manager-controller-1 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 19 00:53:43.900: INFO: kube-multus-ds-amd64-k6nb2 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container kube-multus ready: true, restart count 1
Jul 19 00:53:43.900: INFO: kube-proxy-66jhd from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 19 00:53:43.900: INFO: kube-scheduler-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container kube-scheduler ready: true, restart count 1
Jul 19 00:53:43.900: INFO: kube-sriov-cni-ds-amd64-rpb6b from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jul 19 00:53:43.900: INFO: rbd-nodeplugin-9mw7d from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:53:43.900: INFO: rbd-provisioner-54c6c894f7-6k98h from kube-system started at 2023-07-19 00:41:43 +0000 UTC (6 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:53:43.900: INFO: sonobuoy from sonobuoy started at 2023-07-18 23:44:15 +0000 UTC (1 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 19 00:53:43.900: INFO: sonobuoy-e2e-job-6b28f3e332364b76 from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container e2e ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 00:53:43.900: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-ckzmn from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 00:53:43.900: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 00:53:43.900: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 07/19/23 00:53:43.9
Jul 19 00:53:43.904: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2102" to be "running"
Jul 19 00:53:43.906: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.361002ms
Jul 19 00:53:45.907: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003141347s
Jul 19 00:53:45.907: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 07/19/23 00:53:45.909
STEP: Trying to apply a random label on the found node. 07/19/23 00:53:45.917
STEP: verifying the node has the label kubernetes.io/e2e-74dc4a1c-0642-4780-ac39-48354e08db89 95 07/19/23 00:53:45.925
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 07/19/23 00:53:45.927
Jul 19 00:53:45.931: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2102" to be "not pending"
Jul 19 00:53:45.936: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.922563ms
Jul 19 00:53:47.938: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007323793s
Jul 19 00:53:47.938: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.206.3 on the node which pod4 resides and expect not scheduled 07/19/23 00:53:47.938
Jul 19 00:53:47.941: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2102" to be "not pending"
Jul 19 00:53:47.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.71256ms
Jul 19 00:53:49.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004288087s
Jul 19 00:53:51.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006170276s
Jul 19 00:53:53.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004204149s
Jul 19 00:53:55.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003637318s
Jul 19 00:53:57.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004233139s
Jul 19 00:53:59.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004392854s
Jul 19 00:54:01.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004587316s
Jul 19 00:54:03.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005419425s
Jul 19 00:54:05.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004420186s
Jul 19 00:54:07.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00505485s
Jul 19 00:54:09.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004642414s
Jul 19 00:54:11.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004897306s
Jul 19 00:54:13.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004196248s
Jul 19 00:54:15.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004075008s
Jul 19 00:54:17.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004278046s
Jul 19 00:54:19.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004496774s
Jul 19 00:54:21.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.003572188s
Jul 19 00:54:23.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00517186s
Jul 19 00:54:25.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004278621s
Jul 19 00:54:27.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004417271s
Jul 19 00:54:29.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004349444s
Jul 19 00:54:31.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.004344961s
Jul 19 00:54:33.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004423703s
Jul 19 00:54:35.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.003512199s
Jul 19 00:54:37.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004273385s
Jul 19 00:54:39.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.003905187s
Jul 19 00:54:41.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.003960287s
Jul 19 00:54:43.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004227979s
Jul 19 00:54:45.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004478153s
Jul 19 00:54:47.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.003821386s
Jul 19 00:54:49.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004428995s
Jul 19 00:54:51.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005007899s
Jul 19 00:54:53.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00397395s
Jul 19 00:54:55.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004463435s
Jul 19 00:54:57.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004057243s
Jul 19 00:54:59.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004799285s
Jul 19 00:55:01.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.003779982s
Jul 19 00:55:03.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00405883s
Jul 19 00:55:05.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.003826782s
Jul 19 00:55:07.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005137794s
Jul 19 00:55:09.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004416631s
Jul 19 00:55:11.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005462672s
Jul 19 00:55:13.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004207864s
Jul 19 00:55:15.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004394913s
Jul 19 00:55:17.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004808077s
Jul 19 00:55:19.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004518669s
Jul 19 00:55:21.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00485966s
Jul 19 00:55:23.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005386122s
Jul 19 00:55:25.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.003975131s
Jul 19 00:55:27.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004029712s
Jul 19 00:55:29.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.003831745s
Jul 19 00:55:31.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.003911942s
Jul 19 00:55:33.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.003888551s
Jul 19 00:55:35.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.003857609s
Jul 19 00:55:37.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.003923417s
Jul 19 00:55:39.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004730857s
Jul 19 00:55:41.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005030355s
Jul 19 00:55:43.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004525271s
Jul 19 00:55:45.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004428796s
Jul 19 00:55:47.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004049663s
Jul 19 00:55:49.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005388385s
Jul 19 00:55:51.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.003987086s
Jul 19 00:55:53.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.004061435s
Jul 19 00:55:55.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.004039042s
Jul 19 00:55:57.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004396129s
Jul 19 00:55:59.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.003698095s
Jul 19 00:56:01.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.00391129s
Jul 19 00:56:03.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005002264s
Jul 19 00:56:05.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.003866205s
Jul 19 00:56:07.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.004165239s
Jul 19 00:56:09.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.004695018s
Jul 19 00:56:11.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004287642s
Jul 19 00:56:13.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.004557449s
Jul 19 00:56:15.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004179307s
Jul 19 00:56:17.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.003772787s
Jul 19 00:56:19.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005110169s
Jul 19 00:56:21.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.004608018s
Jul 19 00:56:23.952: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.010804807s
Jul 19 00:56:25.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004112736s
Jul 19 00:56:27.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.003820377s
Jul 19 00:56:29.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.004943613s
Jul 19 00:56:31.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.00434449s
Jul 19 00:56:33.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005188367s
Jul 19 00:56:35.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.00391304s
Jul 19 00:56:37.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004915594s
Jul 19 00:56:39.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.003750825s
Jul 19 00:56:41.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.005804263s
Jul 19 00:56:43.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.004100966s
Jul 19 00:56:45.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.003920689s
Jul 19 00:56:47.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.004043647s
Jul 19 00:56:49.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004458246s
Jul 19 00:56:51.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00418445s
Jul 19 00:56:53.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005180707s
Jul 19 00:56:55.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.003778991s
Jul 19 00:56:57.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.003955517s
Jul 19 00:56:59.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.004438279s
Jul 19 00:57:01.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.003708401s
Jul 19 00:57:03.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.005068213s
Jul 19 00:57:05.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.004250119s
Jul 19 00:57:07.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005153295s
Jul 19 00:57:09.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004381675s
Jul 19 00:57:11.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005191672s
Jul 19 00:57:13.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004964159s
Jul 19 00:57:15.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.004730481s
Jul 19 00:57:17.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.003840929s
Jul 19 00:57:19.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.004700406s
Jul 19 00:57:21.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005001019s
Jul 19 00:57:23.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004147855s
Jul 19 00:57:25.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.003467783s
Jul 19 00:57:27.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004155371s
Jul 19 00:57:29.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.004394006s
Jul 19 00:57:31.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.003731816s
Jul 19 00:57:33.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005063985s
Jul 19 00:57:35.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.003737627s
Jul 19 00:57:37.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.004805965s
Jul 19 00:57:39.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.004554197s
Jul 19 00:57:41.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.004138151s
Jul 19 00:57:43.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004974736s
Jul 19 00:57:45.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004401188s
Jul 19 00:57:47.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.003925041s
Jul 19 00:57:49.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004618417s
Jul 19 00:57:51.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005615257s
Jul 19 00:57:53.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.004706607s
Jul 19 00:57:55.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00385291s
Jul 19 00:57:57.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004249244s
Jul 19 00:57:59.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.003925044s
Jul 19 00:58:01.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.004129472s
Jul 19 00:58:03.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005368634s
Jul 19 00:58:05.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.003874319s
Jul 19 00:58:07.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00392639s
Jul 19 00:58:09.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.004766171s
Jul 19 00:58:11.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004839932s
Jul 19 00:58:13.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.004840364s
Jul 19 00:58:15.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.004339349s
Jul 19 00:58:17.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.003642589s
Jul 19 00:58:19.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005619823s
Jul 19 00:58:21.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.004713473s
Jul 19 00:58:23.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.005757883s
Jul 19 00:58:25.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.003467477s
Jul 19 00:58:27.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004104472s
Jul 19 00:58:29.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004276461s
Jul 19 00:58:31.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.004132389s
Jul 19 00:58:33.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004889416s
Jul 19 00:58:35.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.004656392s
Jul 19 00:58:37.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.005081545s
Jul 19 00:58:39.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.00390193s
Jul 19 00:58:41.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.003952152s
Jul 19 00:58:43.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.004277001s
Jul 19 00:58:45.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.00404262s
Jul 19 00:58:47.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.004518715s
Jul 19 00:58:47.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006131222s
STEP: removing the label kubernetes.io/e2e-74dc4a1c-0642-4780-ac39-48354e08db89 off the node controller-1 07/19/23 00:58:47.947
STEP: verifying the node doesn't have the label kubernetes.io/e2e-74dc4a1c-0642-4780-ac39-48354e08db89 07/19/23 00:58:47.954
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:58:47.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2102" for this suite. 07/19/23 00:58:47.962
------------------------------
• [SLOW TEST] [304.099 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:53:43.866
    Jul 19 00:53:43.866: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sched-pred 07/19/23 00:53:43.867
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:53:43.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:53:43.876
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jul 19 00:53:43.877: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jul 19 00:53:43.881: INFO: Waiting for terminating namespaces to be deleted...
    Jul 19 00:53:43.883: INFO: 
    Logging pods the apiserver thinks is on node controller-0 before test
    Jul 19 00:53:43.891: INFO: cm-cert-manager-7fb65857f5-5mnw8 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container cert-manager-controller ready: true, restart count 2
    Jul 19 00:53:43.891: INFO: cm-cert-manager-cainjector-86b69d7d69-6czpr from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container cert-manager-cainjector ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: cm-cert-manager-webhook-98ddcd5cb-2q6p9 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container cert-manager-webhook ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: helm-controller-5fb8ccb85d-nl9lf from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container manager ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: source-controller-69b5d8f7d8-66tt6 from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container manager ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: calico-kube-controllers-7f5cd5f684-5jblg from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container calico-kube-controllers ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: calico-node-hmrnj from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container calico-node ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: cephfs-nodeplugin-n2vfj from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: cephfs-provisioner-5f69fbf97-q8lg6 from kube-system started at 2023-07-18 22:35:12 +0000 UTC (4 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: coredns-66856967f4-whd5z from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container coredns ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: ic-nginx-ingress-ingress-nginx-controller-6hpm5 from kube-system started at 2023-07-18 21:49:16 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container controller ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: kube-apiserver-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container kube-apiserver ready: true, restart count 2
    Jul 19 00:53:43.891: INFO: kube-controller-manager-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: kube-multus-ds-amd64-bcmw4 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container kube-multus ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: kube-proxy-kxjqb from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: kube-scheduler-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: kube-sriov-cni-ds-amd64-69r58 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container kube-sriov-cni ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: rbd-nodeplugin-j7slt from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: rbd-provisioner-54c6c894f7-sggw9 from kube-system started at 2023-07-18 22:34:50 +0000 UTC (6 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container csi-attacher ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: dm-monitor-84b75cf89c-vnb6c from platform-deployment-manager started at 2023-07-18 21:53:06 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container dm-monitor ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: platform-deployment-manager-7ff76b89d-qg7g9 from platform-deployment-manager started at 2023-07-18 21:52:11 +0000 UTC (2 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: 	Container manager ready: true, restart count 1
    Jul 19 00:53:43.891: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 00:53:43.891: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 19 00:53:43.891: INFO: 
    Logging pods the apiserver thinks is on node controller-1 before test
    Jul 19 00:53:43.900: INFO: cm-cert-manager-7fb65857f5-g94zp from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container cert-manager-controller ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: cm-cert-manager-cainjector-86b69d7d69-vl4v2 from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: cm-cert-manager-webhook-98ddcd5cb-tq28v from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: calico-node-hz2hv from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container calico-node ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: ceph-pools-audit-28162125-cqlst from kube-system started at 2023-07-19 00:45:00 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
    Jul 19 00:53:43.900: INFO: ceph-pools-audit-28162130-rr8lg from kube-system started at 2023-07-19 00:50:00 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
    Jul 19 00:53:43.900: INFO: cephfs-nodeplugin-xn85z from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: cephfs-provisioner-5f69fbf97-vxdxv from kube-system started at 2023-07-19 00:41:43 +0000 UTC (4 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: coredns-66856967f4-vcphq from kube-system started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container coredns ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: ic-nginx-ingress-ingress-nginx-controller-vfpd9 from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container controller ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: kube-apiserver-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jul 19 00:53:43.900: INFO: kube-controller-manager-controller-1 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jul 19 00:53:43.900: INFO: kube-multus-ds-amd64-k6nb2 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container kube-multus ready: true, restart count 1
    Jul 19 00:53:43.900: INFO: kube-proxy-66jhd from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 19 00:53:43.900: INFO: kube-scheduler-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jul 19 00:53:43.900: INFO: kube-sriov-cni-ds-amd64-rpb6b from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container kube-sriov-cni ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: rbd-nodeplugin-9mw7d from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: rbd-provisioner-54c6c894f7-6k98h from kube-system started at 2023-07-19 00:41:43 +0000 UTC (6 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container csi-attacher ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: sonobuoy from sonobuoy started at 2023-07-18 23:44:15 +0000 UTC (1 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: sonobuoy-e2e-job-6b28f3e332364b76 from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container e2e ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-ckzmn from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 00:53:43.900: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 00:53:43.900: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 07/19/23 00:53:43.9
    Jul 19 00:53:43.904: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2102" to be "running"
    Jul 19 00:53:43.906: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.361002ms
    Jul 19 00:53:45.907: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003141347s
    Jul 19 00:53:45.907: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 07/19/23 00:53:45.909
    STEP: Trying to apply a random label on the found node. 07/19/23 00:53:45.917
    STEP: verifying the node has the label kubernetes.io/e2e-74dc4a1c-0642-4780-ac39-48354e08db89 95 07/19/23 00:53:45.925
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 07/19/23 00:53:45.927
    Jul 19 00:53:45.931: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2102" to be "not pending"
    Jul 19 00:53:45.936: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.922563ms
    Jul 19 00:53:47.938: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007323793s
    Jul 19 00:53:47.938: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.206.3 on the node which pod4 resides and expect not scheduled 07/19/23 00:53:47.938
    Jul 19 00:53:47.941: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2102" to be "not pending"
    Jul 19 00:53:47.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.71256ms
    Jul 19 00:53:49.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004288087s
    Jul 19 00:53:51.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006170276s
    Jul 19 00:53:53.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004204149s
    Jul 19 00:53:55.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.003637318s
    Jul 19 00:53:57.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004233139s
    Jul 19 00:53:59.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004392854s
    Jul 19 00:54:01.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004587316s
    Jul 19 00:54:03.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005419425s
    Jul 19 00:54:05.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.004420186s
    Jul 19 00:54:07.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00505485s
    Jul 19 00:54:09.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004642414s
    Jul 19 00:54:11.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004897306s
    Jul 19 00:54:13.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.004196248s
    Jul 19 00:54:15.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.004075008s
    Jul 19 00:54:17.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004278046s
    Jul 19 00:54:19.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004496774s
    Jul 19 00:54:21.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.003572188s
    Jul 19 00:54:23.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00517186s
    Jul 19 00:54:25.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004278621s
    Jul 19 00:54:27.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004417271s
    Jul 19 00:54:29.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.004349444s
    Jul 19 00:54:31.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.004344961s
    Jul 19 00:54:33.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.004423703s
    Jul 19 00:54:35.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.003512199s
    Jul 19 00:54:37.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.004273385s
    Jul 19 00:54:39.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.003905187s
    Jul 19 00:54:41.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.003960287s
    Jul 19 00:54:43.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004227979s
    Jul 19 00:54:45.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.004478153s
    Jul 19 00:54:47.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.003821386s
    Jul 19 00:54:49.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.004428995s
    Jul 19 00:54:51.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005007899s
    Jul 19 00:54:53.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00397395s
    Jul 19 00:54:55.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004463435s
    Jul 19 00:54:57.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.004057243s
    Jul 19 00:54:59.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004799285s
    Jul 19 00:55:01.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.003779982s
    Jul 19 00:55:03.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00405883s
    Jul 19 00:55:05.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.003826782s
    Jul 19 00:55:07.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005137794s
    Jul 19 00:55:09.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004416631s
    Jul 19 00:55:11.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005462672s
    Jul 19 00:55:13.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004207864s
    Jul 19 00:55:15.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004394913s
    Jul 19 00:55:17.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004808077s
    Jul 19 00:55:19.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004518669s
    Jul 19 00:55:21.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00485966s
    Jul 19 00:55:23.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005386122s
    Jul 19 00:55:25.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.003975131s
    Jul 19 00:55:27.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004029712s
    Jul 19 00:55:29.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.003831745s
    Jul 19 00:55:31.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.003911942s
    Jul 19 00:55:33.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.003888551s
    Jul 19 00:55:35.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.003857609s
    Jul 19 00:55:37.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.003923417s
    Jul 19 00:55:39.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.004730857s
    Jul 19 00:55:41.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005030355s
    Jul 19 00:55:43.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004525271s
    Jul 19 00:55:45.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004428796s
    Jul 19 00:55:47.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004049663s
    Jul 19 00:55:49.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005388385s
    Jul 19 00:55:51.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.003987086s
    Jul 19 00:55:53.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.004061435s
    Jul 19 00:55:55.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.004039042s
    Jul 19 00:55:57.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.004396129s
    Jul 19 00:55:59.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.003698095s
    Jul 19 00:56:01.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.00391129s
    Jul 19 00:56:03.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005002264s
    Jul 19 00:56:05.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.003866205s
    Jul 19 00:56:07.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.004165239s
    Jul 19 00:56:09.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.004695018s
    Jul 19 00:56:11.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.004287642s
    Jul 19 00:56:13.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.004557449s
    Jul 19 00:56:15.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004179307s
    Jul 19 00:56:17.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.003772787s
    Jul 19 00:56:19.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.005110169s
    Jul 19 00:56:21.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.004608018s
    Jul 19 00:56:23.952: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.010804807s
    Jul 19 00:56:25.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.004112736s
    Jul 19 00:56:27.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.003820377s
    Jul 19 00:56:29.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.004943613s
    Jul 19 00:56:31.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.00434449s
    Jul 19 00:56:33.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005188367s
    Jul 19 00:56:35.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.00391304s
    Jul 19 00:56:37.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.004915594s
    Jul 19 00:56:39.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.003750825s
    Jul 19 00:56:41.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.005804263s
    Jul 19 00:56:43.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.004100966s
    Jul 19 00:56:45.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.003920689s
    Jul 19 00:56:47.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.004043647s
    Jul 19 00:56:49.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004458246s
    Jul 19 00:56:51.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00418445s
    Jul 19 00:56:53.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005180707s
    Jul 19 00:56:55.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.003778991s
    Jul 19 00:56:57.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.003955517s
    Jul 19 00:56:59.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.004438279s
    Jul 19 00:57:01.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.003708401s
    Jul 19 00:57:03.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.005068213s
    Jul 19 00:57:05.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.004250119s
    Jul 19 00:57:07.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.005153295s
    Jul 19 00:57:09.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.004381675s
    Jul 19 00:57:11.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005191672s
    Jul 19 00:57:13.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004964159s
    Jul 19 00:57:15.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.004730481s
    Jul 19 00:57:17.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.003840929s
    Jul 19 00:57:19.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.004700406s
    Jul 19 00:57:21.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005001019s
    Jul 19 00:57:23.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.004147855s
    Jul 19 00:57:25.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.003467783s
    Jul 19 00:57:27.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.004155371s
    Jul 19 00:57:29.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.004394006s
    Jul 19 00:57:31.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.003731816s
    Jul 19 00:57:33.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.005063985s
    Jul 19 00:57:35.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.003737627s
    Jul 19 00:57:37.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.004805965s
    Jul 19 00:57:39.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.004554197s
    Jul 19 00:57:41.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.004138151s
    Jul 19 00:57:43.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004974736s
    Jul 19 00:57:45.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004401188s
    Jul 19 00:57:47.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.003925041s
    Jul 19 00:57:49.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.004618417s
    Jul 19 00:57:51.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005615257s
    Jul 19 00:57:53.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.004706607s
    Jul 19 00:57:55.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.00385291s
    Jul 19 00:57:57.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.004249244s
    Jul 19 00:57:59.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.003925044s
    Jul 19 00:58:01.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.004129472s
    Jul 19 00:58:03.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005368634s
    Jul 19 00:58:05.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.003874319s
    Jul 19 00:58:07.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00392639s
    Jul 19 00:58:09.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.004766171s
    Jul 19 00:58:11.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004839932s
    Jul 19 00:58:13.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.004840364s
    Jul 19 00:58:15.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.004339349s
    Jul 19 00:58:17.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.003642589s
    Jul 19 00:58:19.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.005619823s
    Jul 19 00:58:21.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.004713473s
    Jul 19 00:58:23.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.005757883s
    Jul 19 00:58:25.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.003467477s
    Jul 19 00:58:27.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004104472s
    Jul 19 00:58:29.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.004276461s
    Jul 19 00:58:31.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.004132389s
    Jul 19 00:58:33.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.004889416s
    Jul 19 00:58:35.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.004656392s
    Jul 19 00:58:37.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.005081545s
    Jul 19 00:58:39.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.00390193s
    Jul 19 00:58:41.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.003952152s
    Jul 19 00:58:43.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.004277001s
    Jul 19 00:58:45.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.00404262s
    Jul 19 00:58:47.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.004518715s
    Jul 19 00:58:47.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006131222s
    STEP: removing the label kubernetes.io/e2e-74dc4a1c-0642-4780-ac39-48354e08db89 off the node controller-1 07/19/23 00:58:47.947
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-74dc4a1c-0642-4780-ac39-48354e08db89 07/19/23 00:58:47.954
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:58:47.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2102" for this suite. 07/19/23 00:58:47.962
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:58:47.966
Jul 19 00:58:47.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename dns 07/19/23 00:58:47.967
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:58:47.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:58:47.977
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 07/19/23 00:58:47.978
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7246.svc.cluster.local;sleep 1; done
 07/19/23 00:58:47.981
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7246.svc.cluster.local;sleep 1; done
 07/19/23 00:58:47.981
STEP: creating a pod to probe DNS 07/19/23 00:58:47.981
STEP: submitting the pod to kubernetes 07/19/23 00:58:47.981
Jul 19 00:58:47.989: INFO: Waiting up to 15m0s for pod "dns-test-7c619ced-5639-4550-b169-c2944a09fe07" in namespace "dns-7246" to be "running"
Jul 19 00:58:47.992: INFO: Pod "dns-test-7c619ced-5639-4550-b169-c2944a09fe07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197721ms
Jul 19 00:58:49.995: INFO: Pod "dns-test-7c619ced-5639-4550-b169-c2944a09fe07": Phase="Running", Reason="", readiness=true. Elapsed: 2.005491799s
Jul 19 00:58:49.995: INFO: Pod "dns-test-7c619ced-5639-4550-b169-c2944a09fe07" satisfied condition "running"
STEP: retrieving the pod 07/19/23 00:58:49.995
STEP: looking for the results for each expected name from probers 07/19/23 00:58:49.997
Jul 19 00:58:49.998: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
Jul 19 00:58:50.000: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
Jul 19 00:58:50.002: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
Jul 19 00:58:50.003: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
Jul 19 00:58:50.005: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
Jul 19 00:58:50.006: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
Jul 19 00:58:50.008: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
Jul 19 00:58:50.009: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
Jul 19 00:58:50.009: INFO: Lookups using dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7246.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7246.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local jessie_udp@dns-test-service-2.dns-7246.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7246.svc.cluster.local]

Jul 19 00:58:55.024: INFO: DNS probes using dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07 succeeded

STEP: deleting the pod 07/19/23 00:58:55.024
STEP: deleting the test headless service 07/19/23 00:58:55.042
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jul 19 00:58:55.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7246" for this suite. 07/19/23 00:58:55.054
------------------------------
• [SLOW TEST] [7.090 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:58:47.966
    Jul 19 00:58:47.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename dns 07/19/23 00:58:47.967
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:58:47.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:58:47.977
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 07/19/23 00:58:47.978
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7246.svc.cluster.local;sleep 1; done
     07/19/23 00:58:47.981
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7246.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7246.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7246.svc.cluster.local;sleep 1; done
     07/19/23 00:58:47.981
    STEP: creating a pod to probe DNS 07/19/23 00:58:47.981
    STEP: submitting the pod to kubernetes 07/19/23 00:58:47.981
    Jul 19 00:58:47.989: INFO: Waiting up to 15m0s for pod "dns-test-7c619ced-5639-4550-b169-c2944a09fe07" in namespace "dns-7246" to be "running"
    Jul 19 00:58:47.992: INFO: Pod "dns-test-7c619ced-5639-4550-b169-c2944a09fe07": Phase="Pending", Reason="", readiness=false. Elapsed: 3.197721ms
    Jul 19 00:58:49.995: INFO: Pod "dns-test-7c619ced-5639-4550-b169-c2944a09fe07": Phase="Running", Reason="", readiness=true. Elapsed: 2.005491799s
    Jul 19 00:58:49.995: INFO: Pod "dns-test-7c619ced-5639-4550-b169-c2944a09fe07" satisfied condition "running"
    STEP: retrieving the pod 07/19/23 00:58:49.995
    STEP: looking for the results for each expected name from probers 07/19/23 00:58:49.997
    Jul 19 00:58:49.998: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
    Jul 19 00:58:50.000: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
    Jul 19 00:58:50.002: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
    Jul 19 00:58:50.003: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
    Jul 19 00:58:50.005: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
    Jul 19 00:58:50.006: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
    Jul 19 00:58:50.008: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
    Jul 19 00:58:50.009: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7246.svc.cluster.local from pod dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07: the server could not find the requested resource (get pods dns-test-7c619ced-5639-4550-b169-c2944a09fe07)
    Jul 19 00:58:50.009: INFO: Lookups using dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7246.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7246.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7246.svc.cluster.local jessie_udp@dns-test-service-2.dns-7246.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7246.svc.cluster.local]

    Jul 19 00:58:55.024: INFO: DNS probes using dns-7246/dns-test-7c619ced-5639-4550-b169-c2944a09fe07 succeeded

    STEP: deleting the pod 07/19/23 00:58:55.024
    STEP: deleting the test headless service 07/19/23 00:58:55.042
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:58:55.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7246" for this suite. 07/19/23 00:58:55.054
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:58:55.057
Jul 19 00:58:55.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sched-pred 07/19/23 00:58:55.057
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:58:55.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:58:55.068
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jul 19 00:58:55.070: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 19 00:58:55.073: INFO: Waiting for terminating namespaces to be deleted...
Jul 19 00:58:55.075: INFO: 
Logging pods the apiserver thinks is on node controller-0 before test
Jul 19 00:58:55.095: INFO: cm-cert-manager-7fb65857f5-5mnw8 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container cert-manager-controller ready: true, restart count 2
Jul 19 00:58:55.095: INFO: cm-cert-manager-cainjector-86b69d7d69-6czpr from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container cert-manager-cainjector ready: true, restart count 1
Jul 19 00:58:55.095: INFO: cm-cert-manager-webhook-98ddcd5cb-2q6p9 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container cert-manager-webhook ready: true, restart count 1
Jul 19 00:58:55.095: INFO: helm-controller-5fb8ccb85d-nl9lf from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container manager ready: true, restart count 1
Jul 19 00:58:55.095: INFO: source-controller-69b5d8f7d8-66tt6 from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container manager ready: true, restart count 1
Jul 19 00:58:55.095: INFO: calico-kube-controllers-7f5cd5f684-5jblg from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jul 19 00:58:55.095: INFO: calico-node-hmrnj from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container calico-node ready: true, restart count 1
Jul 19 00:58:55.095: INFO: cephfs-nodeplugin-n2vfj from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:58:55.095: INFO: cephfs-provisioner-5f69fbf97-q8lg6 from kube-system started at 2023-07-18 22:35:12 +0000 UTC (4 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:58:55.095: INFO: coredns-66856967f4-whd5z from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container coredns ready: true, restart count 1
Jul 19 00:58:55.095: INFO: ic-nginx-ingress-ingress-nginx-controller-6hpm5 from kube-system started at 2023-07-18 21:49:16 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container controller ready: true, restart count 1
Jul 19 00:58:55.095: INFO: kube-apiserver-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container kube-apiserver ready: true, restart count 2
Jul 19 00:58:55.095: INFO: kube-controller-manager-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 19 00:58:55.095: INFO: kube-multus-ds-amd64-bcmw4 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container kube-multus ready: true, restart count 1
Jul 19 00:58:55.095: INFO: kube-proxy-kxjqb from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 19 00:58:55.095: INFO: kube-scheduler-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container kube-scheduler ready: true, restart count 1
Jul 19 00:58:55.095: INFO: kube-sriov-cni-ds-amd64-69r58 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container kube-sriov-cni ready: true, restart count 1
Jul 19 00:58:55.095: INFO: rbd-nodeplugin-j7slt from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:58:55.095: INFO: rbd-provisioner-54c6c894f7-sggw9 from kube-system started at 2023-07-18 22:34:50 +0000 UTC (6 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:58:55.095: INFO: dm-monitor-84b75cf89c-vnb6c from platform-deployment-manager started at 2023-07-18 21:53:06 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container dm-monitor ready: true, restart count 1
Jul 19 00:58:55.095: INFO: platform-deployment-manager-7ff76b89d-qg7g9 from platform-deployment-manager started at 2023-07-18 21:52:11 +0000 UTC (2 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jul 19 00:58:55.095: INFO: 	Container manager ready: true, restart count 1
Jul 19 00:58:55.095: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 00:58:55.095: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 19 00:58:55.095: INFO: 
Logging pods the apiserver thinks is on node controller-1 before test
Jul 19 00:58:55.104: INFO: cm-cert-manager-7fb65857f5-g94zp from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container cert-manager-controller ready: true, restart count 0
Jul 19 00:58:55.104: INFO: cm-cert-manager-cainjector-86b69d7d69-vl4v2 from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
Jul 19 00:58:55.104: INFO: cm-cert-manager-webhook-98ddcd5cb-tq28v from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container cert-manager-webhook ready: true, restart count 0
Jul 19 00:58:55.104: INFO: calico-node-hz2hv from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container calico-node ready: true, restart count 0
Jul 19 00:58:55.104: INFO: ceph-pools-audit-28162125-cqlst from kube-system started at 2023-07-19 00:45:00 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jul 19 00:58:55.104: INFO: ceph-pools-audit-28162130-rr8lg from kube-system started at 2023-07-19 00:50:00 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jul 19 00:58:55.104: INFO: ceph-pools-audit-28162135-gtznm from kube-system started at 2023-07-19 00:55:00 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jul 19 00:58:55.104: INFO: cephfs-nodeplugin-xn85z from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:58:55.104: INFO: cephfs-provisioner-5f69fbf97-vxdxv from kube-system started at 2023-07-19 00:41:43 +0000 UTC (4 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:58:55.104: INFO: coredns-66856967f4-vcphq from kube-system started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container coredns ready: true, restart count 0
Jul 19 00:58:55.104: INFO: ic-nginx-ingress-ingress-nginx-controller-vfpd9 from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container controller ready: true, restart count 0
Jul 19 00:58:55.104: INFO: kube-apiserver-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container kube-apiserver ready: true, restart count 1
Jul 19 00:58:55.104: INFO: kube-controller-manager-controller-1 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 19 00:58:55.104: INFO: kube-multus-ds-amd64-k6nb2 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container kube-multus ready: true, restart count 1
Jul 19 00:58:55.104: INFO: kube-proxy-66jhd from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 19 00:58:55.104: INFO: kube-scheduler-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container kube-scheduler ready: true, restart count 1
Jul 19 00:58:55.104: INFO: kube-sriov-cni-ds-amd64-rpb6b from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jul 19 00:58:55.104: INFO: rbd-nodeplugin-9mw7d from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 00:58:55.104: INFO: rbd-provisioner-54c6c894f7-6k98h from kube-system started at 2023-07-19 00:41:43 +0000 UTC (6 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 00:58:55.104: INFO: sonobuoy from sonobuoy started at 2023-07-18 23:44:15 +0000 UTC (1 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 19 00:58:55.104: INFO: sonobuoy-e2e-job-6b28f3e332364b76 from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container e2e ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 00:58:55.104: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-ckzmn from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 00:58:55.104: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 00:58:55.104: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node controller-0 07/19/23 00:58:55.122
STEP: verifying the node has the label node controller-1 07/19/23 00:58:55.131
Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-7fb65857f5-5mnw8 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-7fb65857f5-g94zp requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-cainjector-86b69d7d69-6czpr requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-cainjector-86b69d7d69-vl4v2 requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-webhook-98ddcd5cb-2q6p9 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-webhook-98ddcd5cb-tq28v requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod helm-controller-5fb8ccb85d-nl9lf requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod source-controller-69b5d8f7d8-66tt6 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod calico-kube-controllers-7f5cd5f684-5jblg requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod calico-node-hmrnj requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod calico-node-hz2hv requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod cephfs-nodeplugin-n2vfj requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod cephfs-nodeplugin-xn85z requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod cephfs-provisioner-5f69fbf97-q8lg6 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod cephfs-provisioner-5f69fbf97-vxdxv requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod coredns-66856967f4-vcphq requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod coredns-66856967f4-whd5z requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod ic-nginx-ingress-ingress-nginx-controller-6hpm5 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod ic-nginx-ingress-ingress-nginx-controller-vfpd9 requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod kube-apiserver-controller-0 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod kube-apiserver-controller-1 requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod kube-controller-manager-controller-0 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod kube-controller-manager-controller-1 requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod kube-multus-ds-amd64-bcmw4 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod kube-multus-ds-amd64-k6nb2 requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod kube-proxy-66jhd requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod kube-proxy-kxjqb requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod kube-scheduler-controller-0 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod kube-scheduler-controller-1 requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod kube-sriov-cni-ds-amd64-69r58 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod kube-sriov-cni-ds-amd64-rpb6b requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod rbd-nodeplugin-9mw7d requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod rbd-nodeplugin-j7slt requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod rbd-provisioner-54c6c894f7-6k98h requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod rbd-provisioner-54c6c894f7-sggw9 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod dm-monitor-84b75cf89c-vnb6c requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod platform-deployment-manager-7ff76b89d-qg7g9 requesting resource cpu=0m on Node controller-0
Jul 19 00:58:55.146: INFO: Pod sonobuoy requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod sonobuoy-e2e-job-6b28f3e332364b76 requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-ckzmn requesting resource cpu=0m on Node controller-1
Jul 19 00:58:55.146: INFO: Pod sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j requesting resource cpu=0m on Node controller-0
STEP: Starting Pods to consume most of the cluster CPU. 07/19/23 00:58:55.146
Jul 19 00:58:55.146: INFO: Creating a pod which consumes cpu=8400m on Node controller-0
Jul 19 00:58:55.150: INFO: Creating a pod which consumes cpu=8400m on Node controller-1
Jul 19 00:58:55.155: INFO: Waiting up to 5m0s for pod "filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31" in namespace "sched-pred-8153" to be "running"
Jul 19 00:58:55.157: INFO: Pod "filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.326334ms
Jul 19 00:58:57.160: INFO: Pod "filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31": Phase="Running", Reason="", readiness=true. Elapsed: 2.004828386s
Jul 19 00:58:57.160: INFO: Pod "filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31" satisfied condition "running"
Jul 19 00:58:57.160: INFO: Waiting up to 5m0s for pod "filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231" in namespace "sched-pred-8153" to be "running"
Jul 19 00:58:57.161: INFO: Pod "filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231": Phase="Running", Reason="", readiness=true. Elapsed: 1.533517ms
Jul 19 00:58:57.161: INFO: Pod "filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 07/19/23 00:58:57.161
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31.17731ef7dfef8c56], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8153/filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31 to controller-0] 07/19/23 00:58:57.163
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31.17731ef7ff3c57ff], Reason = [AddedInterface], Message = [Add eth0 [172.16.192.105/32] from chain] 07/19/23 00:58:57.163
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31.17731ef804cd7d52], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 07/19/23 00:58:57.164
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31.17731ef805e221dd], Reason = [Created], Message = [Created container filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31] 07/19/23 00:58:57.164
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31.17731ef809daead8], Reason = [Started], Message = [Started container filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31] 07/19/23 00:58:57.164
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231.17731ef7e0268a37], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8153/filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231 to controller-1] 07/19/23 00:58:57.164
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231.17731ef80035c94e], Reason = [AddedInterface], Message = [Add eth0 [172.16.166.191/32] from chain] 07/19/23 00:58:57.164
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231.17731ef805064ea4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 07/19/23 00:58:57.164
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231.17731ef80640ceee], Reason = [Created], Message = [Created container filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231] 07/19/23 00:58:57.164
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231.17731ef80a4564ff], Reason = [Started], Message = [Started container filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231] 07/19/23 00:58:57.164
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17731ef857e2a831], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 07/19/23 00:58:57.17
STEP: removing the label node off the node controller-0 07/19/23 00:58:58.17
STEP: verifying the node doesn't have the label node 07/19/23 00:58:58.177
STEP: removing the label node off the node controller-1 07/19/23 00:58:58.181
STEP: verifying the node doesn't have the label node 07/19/23 00:58:58.188
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:58:58.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8153" for this suite. 07/19/23 00:58:58.194
------------------------------
• [3.141 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:58:55.057
    Jul 19 00:58:55.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sched-pred 07/19/23 00:58:55.057
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:58:55.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:58:55.068
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jul 19 00:58:55.070: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jul 19 00:58:55.073: INFO: Waiting for terminating namespaces to be deleted...
    Jul 19 00:58:55.075: INFO: 
    Logging pods the apiserver thinks is on node controller-0 before test
    Jul 19 00:58:55.095: INFO: cm-cert-manager-7fb65857f5-5mnw8 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container cert-manager-controller ready: true, restart count 2
    Jul 19 00:58:55.095: INFO: cm-cert-manager-cainjector-86b69d7d69-6czpr from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container cert-manager-cainjector ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: cm-cert-manager-webhook-98ddcd5cb-2q6p9 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container cert-manager-webhook ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: helm-controller-5fb8ccb85d-nl9lf from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container manager ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: source-controller-69b5d8f7d8-66tt6 from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container manager ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: calico-kube-controllers-7f5cd5f684-5jblg from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container calico-kube-controllers ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: calico-node-hmrnj from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container calico-node ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: cephfs-nodeplugin-n2vfj from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: cephfs-provisioner-5f69fbf97-q8lg6 from kube-system started at 2023-07-18 22:35:12 +0000 UTC (4 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: coredns-66856967f4-whd5z from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container coredns ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: ic-nginx-ingress-ingress-nginx-controller-6hpm5 from kube-system started at 2023-07-18 21:49:16 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container controller ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: kube-apiserver-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container kube-apiserver ready: true, restart count 2
    Jul 19 00:58:55.095: INFO: kube-controller-manager-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: kube-multus-ds-amd64-bcmw4 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container kube-multus ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: kube-proxy-kxjqb from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: kube-scheduler-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: kube-sriov-cni-ds-amd64-69r58 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container kube-sriov-cni ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: rbd-nodeplugin-j7slt from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: rbd-provisioner-54c6c894f7-sggw9 from kube-system started at 2023-07-18 22:34:50 +0000 UTC (6 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container csi-attacher ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: dm-monitor-84b75cf89c-vnb6c from platform-deployment-manager started at 2023-07-18 21:53:06 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container dm-monitor ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: platform-deployment-manager-7ff76b89d-qg7g9 from platform-deployment-manager started at 2023-07-18 21:52:11 +0000 UTC (2 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: 	Container manager ready: true, restart count 1
    Jul 19 00:58:55.095: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 00:58:55.095: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 19 00:58:55.095: INFO: 
    Logging pods the apiserver thinks is on node controller-1 before test
    Jul 19 00:58:55.104: INFO: cm-cert-manager-7fb65857f5-g94zp from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container cert-manager-controller ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: cm-cert-manager-cainjector-86b69d7d69-vl4v2 from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: cm-cert-manager-webhook-98ddcd5cb-tq28v from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: calico-node-hz2hv from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container calico-node ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: ceph-pools-audit-28162125-cqlst from kube-system started at 2023-07-19 00:45:00 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
    Jul 19 00:58:55.104: INFO: ceph-pools-audit-28162130-rr8lg from kube-system started at 2023-07-19 00:50:00 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
    Jul 19 00:58:55.104: INFO: ceph-pools-audit-28162135-gtznm from kube-system started at 2023-07-19 00:55:00 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
    Jul 19 00:58:55.104: INFO: cephfs-nodeplugin-xn85z from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: cephfs-provisioner-5f69fbf97-vxdxv from kube-system started at 2023-07-19 00:41:43 +0000 UTC (4 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: coredns-66856967f4-vcphq from kube-system started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container coredns ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: ic-nginx-ingress-ingress-nginx-controller-vfpd9 from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container controller ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: kube-apiserver-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jul 19 00:58:55.104: INFO: kube-controller-manager-controller-1 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jul 19 00:58:55.104: INFO: kube-multus-ds-amd64-k6nb2 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container kube-multus ready: true, restart count 1
    Jul 19 00:58:55.104: INFO: kube-proxy-66jhd from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 19 00:58:55.104: INFO: kube-scheduler-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jul 19 00:58:55.104: INFO: kube-sriov-cni-ds-amd64-rpb6b from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container kube-sriov-cni ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: rbd-nodeplugin-9mw7d from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: rbd-provisioner-54c6c894f7-6k98h from kube-system started at 2023-07-19 00:41:43 +0000 UTC (6 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container csi-attacher ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: sonobuoy from sonobuoy started at 2023-07-18 23:44:15 +0000 UTC (1 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: sonobuoy-e2e-job-6b28f3e332364b76 from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container e2e ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-ckzmn from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 00:58:55.104: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 00:58:55.104: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node controller-0 07/19/23 00:58:55.122
    STEP: verifying the node has the label node controller-1 07/19/23 00:58:55.131
    Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-7fb65857f5-5mnw8 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-7fb65857f5-g94zp requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-cainjector-86b69d7d69-6czpr requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-cainjector-86b69d7d69-vl4v2 requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-webhook-98ddcd5cb-2q6p9 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod cm-cert-manager-webhook-98ddcd5cb-tq28v requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod helm-controller-5fb8ccb85d-nl9lf requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod source-controller-69b5d8f7d8-66tt6 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod calico-kube-controllers-7f5cd5f684-5jblg requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod calico-node-hmrnj requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod calico-node-hz2hv requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod cephfs-nodeplugin-n2vfj requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod cephfs-nodeplugin-xn85z requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod cephfs-provisioner-5f69fbf97-q8lg6 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod cephfs-provisioner-5f69fbf97-vxdxv requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod coredns-66856967f4-vcphq requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod coredns-66856967f4-whd5z requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod ic-nginx-ingress-ingress-nginx-controller-6hpm5 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod ic-nginx-ingress-ingress-nginx-controller-vfpd9 requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod kube-apiserver-controller-0 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod kube-apiserver-controller-1 requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod kube-controller-manager-controller-0 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod kube-controller-manager-controller-1 requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod kube-multus-ds-amd64-bcmw4 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod kube-multus-ds-amd64-k6nb2 requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod kube-proxy-66jhd requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod kube-proxy-kxjqb requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod kube-scheduler-controller-0 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod kube-scheduler-controller-1 requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod kube-sriov-cni-ds-amd64-69r58 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod kube-sriov-cni-ds-amd64-rpb6b requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod rbd-nodeplugin-9mw7d requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod rbd-nodeplugin-j7slt requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod rbd-provisioner-54c6c894f7-6k98h requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod rbd-provisioner-54c6c894f7-sggw9 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod dm-monitor-84b75cf89c-vnb6c requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod platform-deployment-manager-7ff76b89d-qg7g9 requesting resource cpu=0m on Node controller-0
    Jul 19 00:58:55.146: INFO: Pod sonobuoy requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod sonobuoy-e2e-job-6b28f3e332364b76 requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-ckzmn requesting resource cpu=0m on Node controller-1
    Jul 19 00:58:55.146: INFO: Pod sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j requesting resource cpu=0m on Node controller-0
    STEP: Starting Pods to consume most of the cluster CPU. 07/19/23 00:58:55.146
    Jul 19 00:58:55.146: INFO: Creating a pod which consumes cpu=8400m on Node controller-0
    Jul 19 00:58:55.150: INFO: Creating a pod which consumes cpu=8400m on Node controller-1
    Jul 19 00:58:55.155: INFO: Waiting up to 5m0s for pod "filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31" in namespace "sched-pred-8153" to be "running"
    Jul 19 00:58:55.157: INFO: Pod "filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.326334ms
    Jul 19 00:58:57.160: INFO: Pod "filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31": Phase="Running", Reason="", readiness=true. Elapsed: 2.004828386s
    Jul 19 00:58:57.160: INFO: Pod "filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31" satisfied condition "running"
    Jul 19 00:58:57.160: INFO: Waiting up to 5m0s for pod "filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231" in namespace "sched-pred-8153" to be "running"
    Jul 19 00:58:57.161: INFO: Pod "filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231": Phase="Running", Reason="", readiness=true. Elapsed: 1.533517ms
    Jul 19 00:58:57.161: INFO: Pod "filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 07/19/23 00:58:57.161
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31.17731ef7dfef8c56], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8153/filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31 to controller-0] 07/19/23 00:58:57.163
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31.17731ef7ff3c57ff], Reason = [AddedInterface], Message = [Add eth0 [172.16.192.105/32] from chain] 07/19/23 00:58:57.163
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31.17731ef804cd7d52], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 07/19/23 00:58:57.164
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31.17731ef805e221dd], Reason = [Created], Message = [Created container filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31] 07/19/23 00:58:57.164
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31.17731ef809daead8], Reason = [Started], Message = [Started container filler-pod-62f3aae2-c461-4117-8cbb-6701c0f6de31] 07/19/23 00:58:57.164
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231.17731ef7e0268a37], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8153/filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231 to controller-1] 07/19/23 00:58:57.164
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231.17731ef80035c94e], Reason = [AddedInterface], Message = [Add eth0 [172.16.166.191/32] from chain] 07/19/23 00:58:57.164
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231.17731ef805064ea4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 07/19/23 00:58:57.164
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231.17731ef80640ceee], Reason = [Created], Message = [Created container filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231] 07/19/23 00:58:57.164
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231.17731ef80a4564ff], Reason = [Started], Message = [Started container filler-pod-c85486ab-e235-4ea3-ae2d-e45c50c00231] 07/19/23 00:58:57.164
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17731ef857e2a831], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 07/19/23 00:58:57.17
    STEP: removing the label node off the node controller-0 07/19/23 00:58:58.17
    STEP: verifying the node doesn't have the label node 07/19/23 00:58:58.177
    STEP: removing the label node off the node controller-1 07/19/23 00:58:58.181
    STEP: verifying the node doesn't have the label node 07/19/23 00:58:58.188
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:58:58.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8153" for this suite. 07/19/23 00:58:58.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:58:58.198
Jul 19 00:58:58.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replication-controller 07/19/23 00:58:58.199
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:58:58.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:58:58.209
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 07/19/23 00:58:58.211
Jul 19 00:58:58.215: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-5274" to be "running and ready"
Jul 19 00:58:58.217: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573678ms
Jul 19 00:58:58.217: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:59:00.219: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.004377297s
Jul 19 00:59:00.219: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jul 19 00:59:00.219: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 07/19/23 00:59:00.221
STEP: Then the orphan pod is adopted 07/19/23 00:59:00.223
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:01.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5274" for this suite. 07/19/23 00:59:01.229
------------------------------
• [3.033 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:58:58.198
    Jul 19 00:58:58.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replication-controller 07/19/23 00:58:58.199
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:58:58.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:58:58.209
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 07/19/23 00:58:58.211
    Jul 19 00:58:58.215: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-5274" to be "running and ready"
    Jul 19 00:58:58.217: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573678ms
    Jul 19 00:58:58.217: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:59:00.219: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.004377297s
    Jul 19 00:59:00.219: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jul 19 00:59:00.219: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 07/19/23 00:59:00.221
    STEP: Then the orphan pod is adopted 07/19/23 00:59:00.223
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:01.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5274" for this suite. 07/19/23 00:59:01.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:01.232
Jul 19 00:59:01.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:59:01.233
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:01.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:01.245
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-9231 07/19/23 00:59:01.246
STEP: creating service affinity-nodeport in namespace services-9231 07/19/23 00:59:01.247
STEP: creating replication controller affinity-nodeport in namespace services-9231 07/19/23 00:59:01.261
I0719 00:59:01.264937      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9231, replica count: 3
I0719 00:59:04.316420      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 00:59:04.321: INFO: Creating new exec pod
Jul 19 00:59:04.325: INFO: Waiting up to 5m0s for pod "execpod-affinityljdsc" in namespace "services-9231" to be "running"
Jul 19 00:59:04.326: INFO: Pod "execpod-affinityljdsc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.265413ms
Jul 19 00:59:06.328: INFO: Pod "execpod-affinityljdsc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003564386s
Jul 19 00:59:06.328: INFO: Pod "execpod-affinityljdsc" satisfied condition "running"
Jul 19 00:59:07.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9231 exec execpod-affinityljdsc -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jul 19 00:59:07.450: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul 19 00:59:07.450: INFO: stdout: ""
Jul 19 00:59:07.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9231 exec execpod-affinityljdsc -- /bin/sh -x -c nc -v -z -w 2 10.103.109.6 80'
Jul 19 00:59:07.562: INFO: stderr: "+ nc -v -z -w 2 10.103.109.6 80\nConnection to 10.103.109.6 80 port [tcp/http] succeeded!\n"
Jul 19 00:59:07.562: INFO: stdout: ""
Jul 19 00:59:07.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9231 exec execpod-affinityljdsc -- /bin/sh -x -c nc -v -z -w 2 192.168.206.2 32362'
Jul 19 00:59:07.672: INFO: stderr: "+ nc -v -z -w 2 192.168.206.2 32362\nConnection to 192.168.206.2 32362 port [tcp/*] succeeded!\n"
Jul 19 00:59:07.672: INFO: stdout: ""
Jul 19 00:59:07.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9231 exec execpod-affinityljdsc -- /bin/sh -x -c nc -v -z -w 2 192.168.206.3 32362'
Jul 19 00:59:07.772: INFO: stderr: "+ nc -v -z -w 2 192.168.206.3 32362\nConnection to 192.168.206.3 32362 port [tcp/*] succeeded!\n"
Jul 19 00:59:07.772: INFO: stdout: ""
Jul 19 00:59:07.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9231 exec execpod-affinityljdsc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.206.2:32362/ ; done'
Jul 19 00:59:07.933: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n"
Jul 19 00:59:07.933: INFO: stdout: "\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk"
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
Jul 19 00:59:07.933: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9231, will wait for the garbage collector to delete the pods 07/19/23 00:59:07.942
Jul 19 00:59:07.998: INFO: Deleting ReplicationController affinity-nodeport took: 2.626584ms
Jul 19 00:59:08.098: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.582166ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:09.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9231" for this suite. 07/19/23 00:59:09.716
------------------------------
• [SLOW TEST] [8.487 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:01.232
    Jul 19 00:59:01.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:59:01.233
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:01.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:01.245
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-9231 07/19/23 00:59:01.246
    STEP: creating service affinity-nodeport in namespace services-9231 07/19/23 00:59:01.247
    STEP: creating replication controller affinity-nodeport in namespace services-9231 07/19/23 00:59:01.261
    I0719 00:59:01.264937      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9231, replica count: 3
    I0719 00:59:04.316420      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 19 00:59:04.321: INFO: Creating new exec pod
    Jul 19 00:59:04.325: INFO: Waiting up to 5m0s for pod "execpod-affinityljdsc" in namespace "services-9231" to be "running"
    Jul 19 00:59:04.326: INFO: Pod "execpod-affinityljdsc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.265413ms
    Jul 19 00:59:06.328: INFO: Pod "execpod-affinityljdsc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003564386s
    Jul 19 00:59:06.328: INFO: Pod "execpod-affinityljdsc" satisfied condition "running"
    Jul 19 00:59:07.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9231 exec execpod-affinityljdsc -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jul 19 00:59:07.450: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jul 19 00:59:07.450: INFO: stdout: ""
    Jul 19 00:59:07.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9231 exec execpod-affinityljdsc -- /bin/sh -x -c nc -v -z -w 2 10.103.109.6 80'
    Jul 19 00:59:07.562: INFO: stderr: "+ nc -v -z -w 2 10.103.109.6 80\nConnection to 10.103.109.6 80 port [tcp/http] succeeded!\n"
    Jul 19 00:59:07.562: INFO: stdout: ""
    Jul 19 00:59:07.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9231 exec execpod-affinityljdsc -- /bin/sh -x -c nc -v -z -w 2 192.168.206.2 32362'
    Jul 19 00:59:07.672: INFO: stderr: "+ nc -v -z -w 2 192.168.206.2 32362\nConnection to 192.168.206.2 32362 port [tcp/*] succeeded!\n"
    Jul 19 00:59:07.672: INFO: stdout: ""
    Jul 19 00:59:07.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9231 exec execpod-affinityljdsc -- /bin/sh -x -c nc -v -z -w 2 192.168.206.3 32362'
    Jul 19 00:59:07.772: INFO: stderr: "+ nc -v -z -w 2 192.168.206.3 32362\nConnection to 192.168.206.3 32362 port [tcp/*] succeeded!\n"
    Jul 19 00:59:07.772: INFO: stdout: ""
    Jul 19 00:59:07.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9231 exec execpod-affinityljdsc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.206.2:32362/ ; done'
    Jul 19 00:59:07.933: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.206.2:32362/\n"
    Jul 19 00:59:07.933: INFO: stdout: "\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk\naffinity-nodeport-gsnjk"
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Received response from host: affinity-nodeport-gsnjk
    Jul 19 00:59:07.933: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-9231, will wait for the garbage collector to delete the pods 07/19/23 00:59:07.942
    Jul 19 00:59:07.998: INFO: Deleting ReplicationController affinity-nodeport took: 2.626584ms
    Jul 19 00:59:08.098: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.582166ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:09.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9231" for this suite. 07/19/23 00:59:09.716
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:09.72
Jul 19 00:59:09.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:59:09.72
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:09.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:09.728
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 07/19/23 00:59:09.731
Jul 19 00:59:09.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 19 00:59:09.794: INFO: stderr: ""
Jul 19 00:59:09.794: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 07/19/23 00:59:09.794
Jul 19 00:59:09.794: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 19 00:59:09.794: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6218" to be "running and ready, or succeeded"
Jul 19 00:59:09.797: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.818784ms
Jul 19 00:59:09.797: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'controller-1' to be 'Running' but was 'Pending'
Jul 19 00:59:11.800: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006050943s
Jul 19 00:59:11.801: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 19 00:59:11.801: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 07/19/23 00:59:11.801
Jul 19 00:59:11.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator'
Jul 19 00:59:11.871: INFO: stderr: ""
Jul 19 00:59:11.871: INFO: stdout: "I0719 00:59:10.480540       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/jcm 388\nI0719 00:59:10.680686       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/xzmf 267\nI0719 00:59:10.881282       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/qrx 410\nI0719 00:59:11.081552       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/ndnx 453\nI0719 00:59:11.280785       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/t4bg 362\nI0719 00:59:11.481101       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/l877 468\nI0719 00:59:11.681509       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/mv5n 551\n"
STEP: limiting log lines 07/19/23 00:59:11.871
Jul 19 00:59:11.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator --tail=1'
Jul 19 00:59:11.934: INFO: stderr: ""
Jul 19 00:59:11.934: INFO: stdout: "I0719 00:59:11.880613       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/5vb6 505\n"
Jul 19 00:59:11.934: INFO: got output "I0719 00:59:11.880613       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/5vb6 505\n"
STEP: limiting log bytes 07/19/23 00:59:11.934
Jul 19 00:59:11.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator --limit-bytes=1'
Jul 19 00:59:11.997: INFO: stderr: ""
Jul 19 00:59:11.997: INFO: stdout: "I"
Jul 19 00:59:11.997: INFO: got output "I"
STEP: exposing timestamps 07/19/23 00:59:11.997
Jul 19 00:59:11.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator --tail=1 --timestamps'
Jul 19 00:59:12.061: INFO: stderr: ""
Jul 19 00:59:12.061: INFO: stdout: "2023-07-19T00:59:11.880675738Z I0719 00:59:11.880613       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/5vb6 505\n"
Jul 19 00:59:12.061: INFO: got output "2023-07-19T00:59:11.880675738Z I0719 00:59:11.880613       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/5vb6 505\n"
STEP: restricting to a time range 07/19/23 00:59:12.061
Jul 19 00:59:14.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator --since=1s'
Jul 19 00:59:14.632: INFO: stderr: ""
Jul 19 00:59:14.632: INFO: stdout: "I0719 00:59:13.680872       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/qc6 214\nI0719 00:59:13.881378       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/dzv8 241\nI0719 00:59:14.081581       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/j7f 272\nI0719 00:59:14.281066       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/t8dc 504\nI0719 00:59:14.481545       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/mhvn 542\n"
Jul 19 00:59:14.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator --since=24h'
Jul 19 00:59:14.698: INFO: stderr: ""
Jul 19 00:59:14.698: INFO: stdout: "I0719 00:59:10.480540       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/jcm 388\nI0719 00:59:10.680686       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/xzmf 267\nI0719 00:59:10.881282       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/qrx 410\nI0719 00:59:11.081552       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/ndnx 453\nI0719 00:59:11.280785       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/t4bg 362\nI0719 00:59:11.481101       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/l877 468\nI0719 00:59:11.681509       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/mv5n 551\nI0719 00:59:11.880613       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/5vb6 505\nI0719 00:59:12.081048       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/fc6k 375\nI0719 00:59:12.281180       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/lnx 587\nI0719 00:59:12.481645       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/lrx 373\nI0719 00:59:12.680955       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/2cvb 257\nI0719 00:59:12.881499       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/242d 222\nI0719 00:59:13.080758       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/zfkh 425\nI0719 00:59:13.281296       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/4mt 477\nI0719 00:59:13.481524       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/jbm 577\nI0719 00:59:13.680872       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/qc6 214\nI0719 00:59:13.881378       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/dzv8 241\nI0719 00:59:14.081581       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/j7f 272\nI0719 00:59:14.281066       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/t8dc 504\nI0719 00:59:14.481545       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/mhvn 542\nI0719 00:59:14.680899       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/h7cw 308\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jul 19 00:59:14.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 delete pod logs-generator'
Jul 19 00:59:15.651: INFO: stderr: ""
Jul 19 00:59:15.651: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:15.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6218" for this suite. 07/19/23 00:59:15.654
------------------------------
• [SLOW TEST] [5.937 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:09.72
    Jul 19 00:59:09.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:59:09.72
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:09.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:09.728
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 07/19/23 00:59:09.731
    Jul 19 00:59:09.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jul 19 00:59:09.794: INFO: stderr: ""
    Jul 19 00:59:09.794: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 07/19/23 00:59:09.794
    Jul 19 00:59:09.794: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jul 19 00:59:09.794: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6218" to be "running and ready, or succeeded"
    Jul 19 00:59:09.797: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.818784ms
    Jul 19 00:59:09.797: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'controller-1' to be 'Running' but was 'Pending'
    Jul 19 00:59:11.800: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006050943s
    Jul 19 00:59:11.801: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jul 19 00:59:11.801: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 07/19/23 00:59:11.801
    Jul 19 00:59:11.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator'
    Jul 19 00:59:11.871: INFO: stderr: ""
    Jul 19 00:59:11.871: INFO: stdout: "I0719 00:59:10.480540       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/jcm 388\nI0719 00:59:10.680686       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/xzmf 267\nI0719 00:59:10.881282       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/qrx 410\nI0719 00:59:11.081552       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/ndnx 453\nI0719 00:59:11.280785       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/t4bg 362\nI0719 00:59:11.481101       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/l877 468\nI0719 00:59:11.681509       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/mv5n 551\n"
    STEP: limiting log lines 07/19/23 00:59:11.871
    Jul 19 00:59:11.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator --tail=1'
    Jul 19 00:59:11.934: INFO: stderr: ""
    Jul 19 00:59:11.934: INFO: stdout: "I0719 00:59:11.880613       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/5vb6 505\n"
    Jul 19 00:59:11.934: INFO: got output "I0719 00:59:11.880613       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/5vb6 505\n"
    STEP: limiting log bytes 07/19/23 00:59:11.934
    Jul 19 00:59:11.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator --limit-bytes=1'
    Jul 19 00:59:11.997: INFO: stderr: ""
    Jul 19 00:59:11.997: INFO: stdout: "I"
    Jul 19 00:59:11.997: INFO: got output "I"
    STEP: exposing timestamps 07/19/23 00:59:11.997
    Jul 19 00:59:11.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator --tail=1 --timestamps'
    Jul 19 00:59:12.061: INFO: stderr: ""
    Jul 19 00:59:12.061: INFO: stdout: "2023-07-19T00:59:11.880675738Z I0719 00:59:11.880613       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/5vb6 505\n"
    Jul 19 00:59:12.061: INFO: got output "2023-07-19T00:59:11.880675738Z I0719 00:59:11.880613       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/5vb6 505\n"
    STEP: restricting to a time range 07/19/23 00:59:12.061
    Jul 19 00:59:14.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator --since=1s'
    Jul 19 00:59:14.632: INFO: stderr: ""
    Jul 19 00:59:14.632: INFO: stdout: "I0719 00:59:13.680872       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/qc6 214\nI0719 00:59:13.881378       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/dzv8 241\nI0719 00:59:14.081581       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/j7f 272\nI0719 00:59:14.281066       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/t8dc 504\nI0719 00:59:14.481545       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/mhvn 542\n"
    Jul 19 00:59:14.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 logs logs-generator logs-generator --since=24h'
    Jul 19 00:59:14.698: INFO: stderr: ""
    Jul 19 00:59:14.698: INFO: stdout: "I0719 00:59:10.480540       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/jcm 388\nI0719 00:59:10.680686       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/xzmf 267\nI0719 00:59:10.881282       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/qrx 410\nI0719 00:59:11.081552       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/ndnx 453\nI0719 00:59:11.280785       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/t4bg 362\nI0719 00:59:11.481101       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/l877 468\nI0719 00:59:11.681509       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/mv5n 551\nI0719 00:59:11.880613       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/5vb6 505\nI0719 00:59:12.081048       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/fc6k 375\nI0719 00:59:12.281180       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/lnx 587\nI0719 00:59:12.481645       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/lrx 373\nI0719 00:59:12.680955       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/2cvb 257\nI0719 00:59:12.881499       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/242d 222\nI0719 00:59:13.080758       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/zfkh 425\nI0719 00:59:13.281296       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/4mt 477\nI0719 00:59:13.481524       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/jbm 577\nI0719 00:59:13.680872       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/qc6 214\nI0719 00:59:13.881378       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/dzv8 241\nI0719 00:59:14.081581       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/j7f 272\nI0719 00:59:14.281066       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/t8dc 504\nI0719 00:59:14.481545       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/mhvn 542\nI0719 00:59:14.680899       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/h7cw 308\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jul 19 00:59:14.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-6218 delete pod logs-generator'
    Jul 19 00:59:15.651: INFO: stderr: ""
    Jul 19 00:59:15.651: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:15.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6218" for this suite. 07/19/23 00:59:15.654
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:15.657
Jul 19 00:59:15.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:59:15.657
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:15.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:15.666
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-b8fa3a71-6b26-4b36-a169-853a9381da50 07/19/23 00:59:15.667
STEP: Creating a pod to test consume configMaps 07/19/23 00:59:15.669
Jul 19 00:59:15.673: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf" in namespace "projected-6851" to be "Succeeded or Failed"
Jul 19 00:59:15.675: INFO: Pod "pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.403759ms
Jul 19 00:59:17.677: INFO: Pod "pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003991843s
Jul 19 00:59:19.677: INFO: Pod "pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003391234s
STEP: Saw pod success 07/19/23 00:59:19.677
Jul 19 00:59:19.677: INFO: Pod "pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf" satisfied condition "Succeeded or Failed"
Jul 19 00:59:19.678: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf container agnhost-container: <nil>
STEP: delete the pod 07/19/23 00:59:19.682
Jul 19 00:59:19.689: INFO: Waiting for pod pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf to disappear
Jul 19 00:59:19.691: INFO: Pod pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:19.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6851" for this suite. 07/19/23 00:59:19.693
------------------------------
• [4.039 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:15.657
    Jul 19 00:59:15.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:59:15.657
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:15.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:15.666
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-b8fa3a71-6b26-4b36-a169-853a9381da50 07/19/23 00:59:15.667
    STEP: Creating a pod to test consume configMaps 07/19/23 00:59:15.669
    Jul 19 00:59:15.673: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf" in namespace "projected-6851" to be "Succeeded or Failed"
    Jul 19 00:59:15.675: INFO: Pod "pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.403759ms
    Jul 19 00:59:17.677: INFO: Pod "pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003991843s
    Jul 19 00:59:19.677: INFO: Pod "pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003391234s
    STEP: Saw pod success 07/19/23 00:59:19.677
    Jul 19 00:59:19.677: INFO: Pod "pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf" satisfied condition "Succeeded or Failed"
    Jul 19 00:59:19.678: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 00:59:19.682
    Jul 19 00:59:19.689: INFO: Waiting for pod pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf to disappear
    Jul 19 00:59:19.691: INFO: Pod pod-projected-configmaps-7338f9de-366d-4411-8063-960fcb2750cf no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:19.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6851" for this suite. 07/19/23 00:59:19.693
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:19.696
Jul 19 00:59:19.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:59:19.696
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:19.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:19.707
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-e8603e6c-8b11-49e7-bc5d-d441c4bcd79e 07/19/23 00:59:19.709
STEP: Creating a pod to test consume secrets 07/19/23 00:59:19.711
Jul 19 00:59:19.715: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88" in namespace "projected-8869" to be "Succeeded or Failed"
Jul 19 00:59:19.717: INFO: Pod "pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481736ms
Jul 19 00:59:21.719: INFO: Pod "pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003657651s
Jul 19 00:59:23.719: INFO: Pod "pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003874149s
STEP: Saw pod success 07/19/23 00:59:23.719
Jul 19 00:59:23.719: INFO: Pod "pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88" satisfied condition "Succeeded or Failed"
Jul 19 00:59:23.721: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88 container projected-secret-volume-test: <nil>
STEP: delete the pod 07/19/23 00:59:23.724
Jul 19 00:59:23.732: INFO: Waiting for pod pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88 to disappear
Jul 19 00:59:23.734: INFO: Pod pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:23.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8869" for this suite. 07/19/23 00:59:23.736
------------------------------
• [4.043 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:19.696
    Jul 19 00:59:19.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:59:19.696
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:19.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:19.707
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-e8603e6c-8b11-49e7-bc5d-d441c4bcd79e 07/19/23 00:59:19.709
    STEP: Creating a pod to test consume secrets 07/19/23 00:59:19.711
    Jul 19 00:59:19.715: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88" in namespace "projected-8869" to be "Succeeded or Failed"
    Jul 19 00:59:19.717: INFO: Pod "pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481736ms
    Jul 19 00:59:21.719: INFO: Pod "pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003657651s
    Jul 19 00:59:23.719: INFO: Pod "pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003874149s
    STEP: Saw pod success 07/19/23 00:59:23.719
    Jul 19 00:59:23.719: INFO: Pod "pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88" satisfied condition "Succeeded or Failed"
    Jul 19 00:59:23.721: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88 container projected-secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 00:59:23.724
    Jul 19 00:59:23.732: INFO: Waiting for pod pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88 to disappear
    Jul 19 00:59:23.734: INFO: Pod pod-projected-secrets-1213aa47-8552-4844-9659-2bb3e81d8b88 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:23.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8869" for this suite. 07/19/23 00:59:23.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:23.738
Jul 19 00:59:23.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:59:23.739
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:23.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:23.747
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:59:23.749
Jul 19 00:59:23.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705" in namespace "projected-4416" to be "Succeeded or Failed"
Jul 19 00:59:23.754: INFO: Pod "downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705": Phase="Pending", Reason="", readiness=false. Elapsed: 1.371286ms
Jul 19 00:59:25.756: INFO: Pod "downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00346696s
Jul 19 00:59:27.758: INFO: Pod "downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00538965s
STEP: Saw pod success 07/19/23 00:59:27.758
Jul 19 00:59:27.758: INFO: Pod "downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705" satisfied condition "Succeeded or Failed"
Jul 19 00:59:27.760: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705 container client-container: <nil>
STEP: delete the pod 07/19/23 00:59:27.764
Jul 19 00:59:27.784: INFO: Waiting for pod downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705 to disappear
Jul 19 00:59:27.786: INFO: Pod downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:27.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4416" for this suite. 07/19/23 00:59:27.789
------------------------------
• [4.053 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:23.738
    Jul 19 00:59:23.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:59:23.739
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:23.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:23.747
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:59:23.749
    Jul 19 00:59:23.753: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705" in namespace "projected-4416" to be "Succeeded or Failed"
    Jul 19 00:59:23.754: INFO: Pod "downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705": Phase="Pending", Reason="", readiness=false. Elapsed: 1.371286ms
    Jul 19 00:59:25.756: INFO: Pod "downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00346696s
    Jul 19 00:59:27.758: INFO: Pod "downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00538965s
    STEP: Saw pod success 07/19/23 00:59:27.758
    Jul 19 00:59:27.758: INFO: Pod "downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705" satisfied condition "Succeeded or Failed"
    Jul 19 00:59:27.760: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705 container client-container: <nil>
    STEP: delete the pod 07/19/23 00:59:27.764
    Jul 19 00:59:27.784: INFO: Waiting for pod downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705 to disappear
    Jul 19 00:59:27.786: INFO: Pod downwardapi-volume-e6a43b04-55cf-487f-b8da-5963c2dc1705 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:27.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4416" for this suite. 07/19/23 00:59:27.789
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:27.791
Jul 19 00:59:27.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/19/23 00:59:27.792
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:27.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:27.805
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 07/19/23 00:59:27.806
STEP: submitting the pod to kubernetes 07/19/23 00:59:27.806
Jul 19 00:59:27.810: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06" in namespace "pods-773" to be "running and ready"
Jul 19 00:59:27.812: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06": Phase="Pending", Reason="", readiness=false. Elapsed: 1.338604ms
Jul 19 00:59:27.812: INFO: The phase of Pod pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 00:59:29.814: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06": Phase="Running", Reason="", readiness=true. Elapsed: 2.00386913s
Jul 19 00:59:29.814: INFO: The phase of Pod pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06 is Running (Ready = true)
Jul 19 00:59:29.814: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 07/19/23 00:59:29.816
STEP: updating the pod 07/19/23 00:59:29.817
Jul 19 00:59:30.325: INFO: Successfully updated pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06"
Jul 19 00:59:30.325: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06" in namespace "pods-773" to be "terminated with reason DeadlineExceeded"
Jul 19 00:59:30.328: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06": Phase="Running", Reason="", readiness=true. Elapsed: 3.663088ms
Jul 19 00:59:32.332: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06": Phase="Running", Reason="", readiness=true. Elapsed: 2.007231215s
Jul 19 00:59:34.331: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.006231901s
Jul 19 00:59:34.331: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:34.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-773" for this suite. 07/19/23 00:59:34.334
------------------------------
• [SLOW TEST] [6.545 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:27.791
    Jul 19 00:59:27.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/19/23 00:59:27.792
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:27.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:27.805
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 07/19/23 00:59:27.806
    STEP: submitting the pod to kubernetes 07/19/23 00:59:27.806
    Jul 19 00:59:27.810: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06" in namespace "pods-773" to be "running and ready"
    Jul 19 00:59:27.812: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06": Phase="Pending", Reason="", readiness=false. Elapsed: 1.338604ms
    Jul 19 00:59:27.812: INFO: The phase of Pod pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 00:59:29.814: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06": Phase="Running", Reason="", readiness=true. Elapsed: 2.00386913s
    Jul 19 00:59:29.814: INFO: The phase of Pod pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06 is Running (Ready = true)
    Jul 19 00:59:29.814: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 07/19/23 00:59:29.816
    STEP: updating the pod 07/19/23 00:59:29.817
    Jul 19 00:59:30.325: INFO: Successfully updated pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06"
    Jul 19 00:59:30.325: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06" in namespace "pods-773" to be "terminated with reason DeadlineExceeded"
    Jul 19 00:59:30.328: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06": Phase="Running", Reason="", readiness=true. Elapsed: 3.663088ms
    Jul 19 00:59:32.332: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06": Phase="Running", Reason="", readiness=true. Elapsed: 2.007231215s
    Jul 19 00:59:34.331: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.006231901s
    Jul 19 00:59:34.331: INFO: Pod "pod-update-activedeadlineseconds-447a5079-dc09-4129-a3ab-1aa6f9e36c06" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:34.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-773" for this suite. 07/19/23 00:59:34.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:34.338
Jul 19 00:59:34.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename dns 07/19/23 00:59:34.338
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:34.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:34.348
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 07/19/23 00:59:34.35
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 07/19/23 00:59:34.351
STEP: creating a pod to probe DNS 07/19/23 00:59:34.351
STEP: submitting the pod to kubernetes 07/19/23 00:59:34.351
Jul 19 00:59:34.355: INFO: Waiting up to 15m0s for pod "dns-test-e58b46e3-c8ef-4579-b150-2fcfd4399f67" in namespace "dns-7739" to be "running"
Jul 19 00:59:34.359: INFO: Pod "dns-test-e58b46e3-c8ef-4579-b150-2fcfd4399f67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.168209ms
Jul 19 00:59:36.361: INFO: Pod "dns-test-e58b46e3-c8ef-4579-b150-2fcfd4399f67": Phase="Running", Reason="", readiness=true. Elapsed: 2.006055655s
Jul 19 00:59:36.361: INFO: Pod "dns-test-e58b46e3-c8ef-4579-b150-2fcfd4399f67" satisfied condition "running"
STEP: retrieving the pod 07/19/23 00:59:36.361
STEP: looking for the results for each expected name from probers 07/19/23 00:59:36.365
Jul 19 00:59:36.372: INFO: DNS probes using dns-7739/dns-test-e58b46e3-c8ef-4579-b150-2fcfd4399f67 succeeded

STEP: deleting the pod 07/19/23 00:59:36.372
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:36.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7739" for this suite. 07/19/23 00:59:36.381
------------------------------
• [2.046 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:34.338
    Jul 19 00:59:34.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename dns 07/19/23 00:59:34.338
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:34.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:34.348
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     07/19/23 00:59:34.35
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     07/19/23 00:59:34.351
    STEP: creating a pod to probe DNS 07/19/23 00:59:34.351
    STEP: submitting the pod to kubernetes 07/19/23 00:59:34.351
    Jul 19 00:59:34.355: INFO: Waiting up to 15m0s for pod "dns-test-e58b46e3-c8ef-4579-b150-2fcfd4399f67" in namespace "dns-7739" to be "running"
    Jul 19 00:59:34.359: INFO: Pod "dns-test-e58b46e3-c8ef-4579-b150-2fcfd4399f67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.168209ms
    Jul 19 00:59:36.361: INFO: Pod "dns-test-e58b46e3-c8ef-4579-b150-2fcfd4399f67": Phase="Running", Reason="", readiness=true. Elapsed: 2.006055655s
    Jul 19 00:59:36.361: INFO: Pod "dns-test-e58b46e3-c8ef-4579-b150-2fcfd4399f67" satisfied condition "running"
    STEP: retrieving the pod 07/19/23 00:59:36.361
    STEP: looking for the results for each expected name from probers 07/19/23 00:59:36.365
    Jul 19 00:59:36.372: INFO: DNS probes using dns-7739/dns-test-e58b46e3-c8ef-4579-b150-2fcfd4399f67 succeeded

    STEP: deleting the pod 07/19/23 00:59:36.372
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:36.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7739" for this suite. 07/19/23 00:59:36.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:36.384
Jul 19 00:59:36.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename custom-resource-definition 07/19/23 00:59:36.385
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:36.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:36.4
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jul 19 00:59:36.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:36.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4084" for this suite. 07/19/23 00:59:36.924
------------------------------
• [0.550 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:36.384
    Jul 19 00:59:36.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename custom-resource-definition 07/19/23 00:59:36.385
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:36.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:36.4
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jul 19 00:59:36.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:36.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4084" for this suite. 07/19/23 00:59:36.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:36.935
Jul 19 00:59:36.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 00:59:36.936
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:36.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:36.948
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 07/19/23 00:59:36.949
Jul 19 00:59:36.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071" in namespace "projected-7843" to be "Succeeded or Failed"
Jul 19 00:59:36.956: INFO: Pod "downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071": Phase="Pending", Reason="", readiness=false. Elapsed: 3.346724ms
Jul 19 00:59:38.960: INFO: Pod "downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007239009s
Jul 19 00:59:40.959: INFO: Pod "downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006232623s
STEP: Saw pod success 07/19/23 00:59:40.959
Jul 19 00:59:40.959: INFO: Pod "downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071" satisfied condition "Succeeded or Failed"
Jul 19 00:59:40.961: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071 container client-container: <nil>
STEP: delete the pod 07/19/23 00:59:40.964
Jul 19 00:59:40.971: INFO: Waiting for pod downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071 to disappear
Jul 19 00:59:40.972: INFO: Pod downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:40.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7843" for this suite. 07/19/23 00:59:40.974
------------------------------
• [4.041 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:36.935
    Jul 19 00:59:36.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 00:59:36.936
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:36.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:36.948
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 07/19/23 00:59:36.949
    Jul 19 00:59:36.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071" in namespace "projected-7843" to be "Succeeded or Failed"
    Jul 19 00:59:36.956: INFO: Pod "downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071": Phase="Pending", Reason="", readiness=false. Elapsed: 3.346724ms
    Jul 19 00:59:38.960: INFO: Pod "downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007239009s
    Jul 19 00:59:40.959: INFO: Pod "downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006232623s
    STEP: Saw pod success 07/19/23 00:59:40.959
    Jul 19 00:59:40.959: INFO: Pod "downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071" satisfied condition "Succeeded or Failed"
    Jul 19 00:59:40.961: INFO: Trying to get logs from node controller-1 pod downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071 container client-container: <nil>
    STEP: delete the pod 07/19/23 00:59:40.964
    Jul 19 00:59:40.971: INFO: Waiting for pod downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071 to disappear
    Jul 19 00:59:40.972: INFO: Pod downwardapi-volume-d69479fe-58e6-4d29-9cfe-8a18a3e46071 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:40.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7843" for this suite. 07/19/23 00:59:40.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:40.977
Jul 19 00:59:40.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 00:59:40.978
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:40.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:40.986
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9027 07/19/23 00:59:40.988
STEP: changing the ExternalName service to type=NodePort 07/19/23 00:59:40.99
STEP: creating replication controller externalname-service in namespace services-9027 07/19/23 00:59:41.006
I0719 00:59:41.010864      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9027, replica count: 2
I0719 00:59:44.061581      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 00:59:44.061: INFO: Creating new exec pod
Jul 19 00:59:44.066: INFO: Waiting up to 5m0s for pod "execpodfbcb7" in namespace "services-9027" to be "running"
Jul 19 00:59:44.068: INFO: Pod "execpodfbcb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.763855ms
Jul 19 00:59:46.072: INFO: Pod "execpodfbcb7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006431315s
Jul 19 00:59:46.072: INFO: Pod "execpodfbcb7" satisfied condition "running"
Jul 19 00:59:47.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9027 exec execpodfbcb7 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jul 19 00:59:47.191: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 19 00:59:47.191: INFO: stdout: ""
Jul 19 00:59:47.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9027 exec execpodfbcb7 -- /bin/sh -x -c nc -v -z -w 2 10.111.198.73 80'
Jul 19 00:59:47.292: INFO: stderr: "+ nc -v -z -w 2 10.111.198.73 80\nConnection to 10.111.198.73 80 port [tcp/http] succeeded!\n"
Jul 19 00:59:47.292: INFO: stdout: ""
Jul 19 00:59:47.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9027 exec execpodfbcb7 -- /bin/sh -x -c nc -v -z -w 2 192.168.206.2 30542'
Jul 19 00:59:47.391: INFO: stderr: "+ nc -v -z -w 2 192.168.206.2 30542\nConnection to 192.168.206.2 30542 port [tcp/*] succeeded!\n"
Jul 19 00:59:47.391: INFO: stdout: ""
Jul 19 00:59:47.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9027 exec execpodfbcb7 -- /bin/sh -x -c nc -v -z -w 2 192.168.206.3 30542'
Jul 19 00:59:47.494: INFO: stderr: "+ nc -v -z -w 2 192.168.206.3 30542\nConnection to 192.168.206.3 30542 port [tcp/*] succeeded!\n"
Jul 19 00:59:47.494: INFO: stdout: ""
Jul 19 00:59:47.494: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:47.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9027" for this suite. 07/19/23 00:59:47.512
------------------------------
• [SLOW TEST] [6.537 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:40.977
    Jul 19 00:59:40.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 00:59:40.978
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:40.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:40.986
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9027 07/19/23 00:59:40.988
    STEP: changing the ExternalName service to type=NodePort 07/19/23 00:59:40.99
    STEP: creating replication controller externalname-service in namespace services-9027 07/19/23 00:59:41.006
    I0719 00:59:41.010864      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9027, replica count: 2
    I0719 00:59:44.061581      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 19 00:59:44.061: INFO: Creating new exec pod
    Jul 19 00:59:44.066: INFO: Waiting up to 5m0s for pod "execpodfbcb7" in namespace "services-9027" to be "running"
    Jul 19 00:59:44.068: INFO: Pod "execpodfbcb7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.763855ms
    Jul 19 00:59:46.072: INFO: Pod "execpodfbcb7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006431315s
    Jul 19 00:59:46.072: INFO: Pod "execpodfbcb7" satisfied condition "running"
    Jul 19 00:59:47.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9027 exec execpodfbcb7 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jul 19 00:59:47.191: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jul 19 00:59:47.191: INFO: stdout: ""
    Jul 19 00:59:47.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9027 exec execpodfbcb7 -- /bin/sh -x -c nc -v -z -w 2 10.111.198.73 80'
    Jul 19 00:59:47.292: INFO: stderr: "+ nc -v -z -w 2 10.111.198.73 80\nConnection to 10.111.198.73 80 port [tcp/http] succeeded!\n"
    Jul 19 00:59:47.292: INFO: stdout: ""
    Jul 19 00:59:47.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9027 exec execpodfbcb7 -- /bin/sh -x -c nc -v -z -w 2 192.168.206.2 30542'
    Jul 19 00:59:47.391: INFO: stderr: "+ nc -v -z -w 2 192.168.206.2 30542\nConnection to 192.168.206.2 30542 port [tcp/*] succeeded!\n"
    Jul 19 00:59:47.391: INFO: stdout: ""
    Jul 19 00:59:47.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-9027 exec execpodfbcb7 -- /bin/sh -x -c nc -v -z -w 2 192.168.206.3 30542'
    Jul 19 00:59:47.494: INFO: stderr: "+ nc -v -z -w 2 192.168.206.3 30542\nConnection to 192.168.206.3 30542 port [tcp/*] succeeded!\n"
    Jul 19 00:59:47.494: INFO: stdout: ""
    Jul 19 00:59:47.494: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:47.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9027" for this suite. 07/19/23 00:59:47.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:47.515
Jul 19 00:59:47.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename proxy 07/19/23 00:59:47.516
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:47.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:47.525
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 07/19/23 00:59:47.536
STEP: creating replication controller proxy-service-9sgzr in namespace proxy-2684 07/19/23 00:59:47.536
I0719 00:59:47.540399      21 runners.go:193] Created replication controller with name: proxy-service-9sgzr, namespace: proxy-2684, replica count: 1
I0719 00:59:48.591612      21 runners.go:193] proxy-service-9sgzr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0719 00:59:49.591862      21 runners.go:193] proxy-service-9sgzr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0719 00:59:50.592637      21 runners.go:193] proxy-service-9sgzr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 00:59:50.594: INFO: setup took 3.06736704s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 07/19/23 00:59:50.594
Jul 19 00:59:50.597: INFO: (0) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.258531ms)
Jul 19 00:59:50.597: INFO: (0) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.26262ms)
Jul 19 00:59:50.597: INFO: (0) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.334679ms)
Jul 19 00:59:50.597: INFO: (0) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.28876ms)
Jul 19 00:59:50.599: INFO: (0) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.842278ms)
Jul 19 00:59:50.599: INFO: (0) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 5.130722ms)
Jul 19 00:59:50.599: INFO: (0) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 5.167539ms)
Jul 19 00:59:50.601: INFO: (0) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 6.71297ms)
Jul 19 00:59:50.603: INFO: (0) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 8.492225ms)
Jul 19 00:59:50.603: INFO: (0) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 8.59304ms)
Jul 19 00:59:50.603: INFO: (0) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 8.672007ms)
Jul 19 00:59:50.603: INFO: (0) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 8.791289ms)
Jul 19 00:59:50.604: INFO: (0) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 9.321732ms)
Jul 19 00:59:50.605: INFO: (0) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 10.673256ms)
Jul 19 00:59:50.605: INFO: (0) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 11.190016ms)
Jul 19 00:59:50.605: INFO: (0) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 11.192684ms)
Jul 19 00:59:50.609: INFO: (1) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.698729ms)
Jul 19 00:59:50.609: INFO: (1) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 3.841626ms)
Jul 19 00:59:50.610: INFO: (1) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 4.645252ms)
Jul 19 00:59:50.610: INFO: (1) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.713716ms)
Jul 19 00:59:50.610: INFO: (1) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 4.777596ms)
Jul 19 00:59:50.610: INFO: (1) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 4.775353ms)
Jul 19 00:59:50.610: INFO: (1) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.762694ms)
Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 5.01672ms)
Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 5.097729ms)
Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 5.09447ms)
Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 5.165697ms)
Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 5.089702ms)
Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 5.165925ms)
Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 5.122988ms)
Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 5.195691ms)
Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 5.144141ms)
Jul 19 00:59:50.613: INFO: (2) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.154564ms)
Jul 19 00:59:50.613: INFO: (2) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.174754ms)
Jul 19 00:59:50.613: INFO: (2) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 2.239698ms)
Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 6.138427ms)
Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 6.090512ms)
Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 6.163254ms)
Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 6.277478ms)
Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 6.321735ms)
Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 6.283752ms)
Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 6.28771ms)
Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 6.350001ms)
Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 6.381385ms)
Jul 19 00:59:50.619: INFO: (2) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 7.738981ms)
Jul 19 00:59:50.619: INFO: (2) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 7.769784ms)
Jul 19 00:59:50.619: INFO: (2) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 7.78713ms)
Jul 19 00:59:50.619: INFO: (2) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 7.804585ms)
Jul 19 00:59:50.621: INFO: (3) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 2.322335ms)
Jul 19 00:59:50.622: INFO: (3) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.808272ms)
Jul 19 00:59:50.623: INFO: (3) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.360919ms)
Jul 19 00:59:50.623: INFO: (3) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.357203ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.903869ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.873415ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.898316ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 4.89612ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.969381ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 4.944326ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 5.04193ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 5.000596ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 5.072169ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 5.02173ms)
Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 5.013541ms)
Jul 19 00:59:50.627: INFO: (3) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 8.650315ms)
Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.127817ms)
Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.53005ms)
Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.481601ms)
Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.482198ms)
Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.498733ms)
Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.468499ms)
Jul 19 00:59:50.632: INFO: (4) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.595385ms)
Jul 19 00:59:50.633: INFO: (4) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 5.405028ms)
Jul 19 00:59:50.633: INFO: (4) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 5.545403ms)
Jul 19 00:59:50.633: INFO: (4) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 5.767926ms)
Jul 19 00:59:50.634: INFO: (4) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 7.020866ms)
Jul 19 00:59:50.635: INFO: (4) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 7.077804ms)
Jul 19 00:59:50.635: INFO: (4) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 7.098653ms)
Jul 19 00:59:50.635: INFO: (4) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 7.694692ms)
Jul 19 00:59:50.635: INFO: (4) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 7.74369ms)
Jul 19 00:59:50.635: INFO: (4) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 7.838445ms)
Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.947673ms)
Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.943644ms)
Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 2.967335ms)
Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 2.939898ms)
Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 2.922412ms)
Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 2.932204ms)
Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 2.969718ms)
Jul 19 00:59:50.639: INFO: (5) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 4.122553ms)
Jul 19 00:59:50.639: INFO: (5) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.153596ms)
Jul 19 00:59:50.639: INFO: (5) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.172271ms)
Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.129925ms)
Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.137911ms)
Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.168278ms)
Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.26312ms)
Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 4.178931ms)
Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.191557ms)
Jul 19 00:59:50.643: INFO: (6) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.019225ms)
Jul 19 00:59:50.643: INFO: (6) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 2.999434ms)
Jul 19 00:59:50.643: INFO: (6) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.011ms)
Jul 19 00:59:50.643: INFO: (6) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.393058ms)
Jul 19 00:59:50.643: INFO: (6) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.647305ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.982142ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 4.013104ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 4.349248ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.390193ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.379825ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.493519ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.46902ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.562447ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.587344ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.699093ms)
Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.759462ms)
Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.555475ms)
Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 2.535931ms)
Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 2.493152ms)
Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 2.570959ms)
Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 2.610873ms)
Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.784736ms)
Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 2.80068ms)
Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.921608ms)
Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.897515ms)
Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 2.872323ms)
Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.467851ms)
Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.798924ms)
Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.814528ms)
Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.88076ms)
Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.790756ms)
Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.904924ms)
Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.382308ms)
Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 3.609764ms)
Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.7017ms)
Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.736115ms)
Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.732323ms)
Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 3.793602ms)
Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 7.276713ms)
Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 7.317355ms)
Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 7.484474ms)
Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 7.520736ms)
Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 7.59572ms)
Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 7.535789ms)
Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 7.567724ms)
Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 7.531837ms)
Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 7.54983ms)
Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 7.826541ms)
Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 4.16107ms)
Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 4.357587ms)
Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 4.412336ms)
Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 4.351774ms)
Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 4.373545ms)
Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 4.470317ms)
Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.483176ms)
Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 4.446927ms)
Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.486264ms)
Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.442769ms)
Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 5.662387ms)
Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 5.646335ms)
Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 5.654376ms)
Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 5.817622ms)
Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 5.832245ms)
Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 5.760535ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 3.799266ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 3.865043ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 3.920815ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 3.835468ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.824854ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.807514ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.829122ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.875951ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 3.829734ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.8624ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 3.912628ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 3.931618ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 3.97047ms)
Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.924986ms)
Jul 19 00:59:50.668: INFO: (10) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 5.069404ms)
Jul 19 00:59:50.668: INFO: (10) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 5.129343ms)
Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 2.566197ms)
Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.641408ms)
Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.711645ms)
Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 2.91952ms)
Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 2.987837ms)
Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 2.964381ms)
Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.925472ms)
Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.023895ms)
Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 3.422326ms)
Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.502719ms)
Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 3.423183ms)
Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 3.557411ms)
Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 3.5408ms)
Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 3.600704ms)
Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 3.603818ms)
Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 3.584132ms)
Jul 19 00:59:50.675: INFO: (12) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.698207ms)
Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.037992ms)
Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.024161ms)
Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 4.119645ms)
Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 4.114572ms)
Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.137698ms)
Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 4.136863ms)
Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.462255ms)
Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.423718ms)
Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 4.480954ms)
Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 4.521494ms)
Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.529848ms)
Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 4.50494ms)
Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.599645ms)
Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.557113ms)
Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.576525ms)
Jul 19 00:59:50.679: INFO: (13) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.326947ms)
Jul 19 00:59:50.679: INFO: (13) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 2.466977ms)
Jul 19 00:59:50.680: INFO: (13) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.361655ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 3.716922ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.805506ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.83424ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 3.868137ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 3.895201ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 3.934748ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.874121ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 3.93823ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.866349ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.879591ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 3.961523ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 3.875913ms)
Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 4.141642ms)
Jul 19 00:59:50.683: INFO: (14) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 2.395252ms)
Jul 19 00:59:50.683: INFO: (14) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 2.412321ms)
Jul 19 00:59:50.683: INFO: (14) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.318227ms)
Jul 19 00:59:50.684: INFO: (14) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 2.830179ms)
Jul 19 00:59:50.684: INFO: (14) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.125384ms)
Jul 19 00:59:50.684: INFO: (14) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.170146ms)
Jul 19 00:59:50.684: INFO: (14) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.208458ms)
Jul 19 00:59:50.684: INFO: (14) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.216975ms)
Jul 19 00:59:50.686: INFO: (14) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 4.377595ms)
Jul 19 00:59:50.686: INFO: (14) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.713737ms)
Jul 19 00:59:50.686: INFO: (14) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.796596ms)
Jul 19 00:59:50.686: INFO: (14) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.917265ms)
Jul 19 00:59:50.687: INFO: (14) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 5.508271ms)
Jul 19 00:59:50.687: INFO: (14) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 5.599909ms)
Jul 19 00:59:50.687: INFO: (14) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 5.891504ms)
Jul 19 00:59:50.687: INFO: (14) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 5.952782ms)
Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 3.077466ms)
Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 3.160039ms)
Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 3.269691ms)
Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.239639ms)
Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.339119ms)
Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.278001ms)
Jul 19 00:59:50.691: INFO: (15) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.302967ms)
Jul 19 00:59:50.691: INFO: (15) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.361263ms)
Jul 19 00:59:50.691: INFO: (15) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.331382ms)
Jul 19 00:59:50.691: INFO: (15) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.407856ms)
Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.438698ms)
Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.41202ms)
Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.509053ms)
Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.624307ms)
Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.911765ms)
Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.918436ms)
Jul 19 00:59:50.699: INFO: (16) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 7.207324ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 7.674445ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 7.836425ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 7.756143ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 7.792773ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 7.753308ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 7.720644ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 7.805084ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 7.892945ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 7.880039ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 7.950778ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 7.942313ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 8.015327ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 8.025746ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 8.052169ms)
Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 8.132681ms)
Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 5.738749ms)
Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 5.715636ms)
Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 5.655694ms)
Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 5.735695ms)
Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 5.68484ms)
Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 5.767116ms)
Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 6.091931ms)
Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 6.21914ms)
Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 6.232016ms)
Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 6.297043ms)
Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 6.348295ms)
Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 6.29028ms)
Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 6.321632ms)
Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 6.249887ms)
Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 6.274278ms)
Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 6.380514ms)
Jul 19 00:59:50.713: INFO: (18) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 6.352452ms)
Jul 19 00:59:50.713: INFO: (18) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 6.397064ms)
Jul 19 00:59:50.713: INFO: (18) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 6.397794ms)
Jul 19 00:59:50.713: INFO: (18) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 6.444794ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 6.478882ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 6.708937ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 7.076182ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 7.137026ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 7.180119ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 7.172677ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 7.215418ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 7.236172ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 7.220166ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 7.247137ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 7.341215ms)
Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 7.290859ms)
Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 2.72145ms)
Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 2.695686ms)
Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.961808ms)
Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.962705ms)
Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.1091ms)
Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 3.14347ms)
Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 3.141621ms)
Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.229365ms)
Jul 19 00:59:50.718: INFO: (19) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.315068ms)
Jul 19 00:59:50.718: INFO: (19) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 3.453253ms)
Jul 19 00:59:50.718: INFO: (19) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.394744ms)
Jul 19 00:59:50.719: INFO: (19) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.503533ms)
Jul 19 00:59:50.719: INFO: (19) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.571079ms)
Jul 19 00:59:50.719: INFO: (19) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.582845ms)
Jul 19 00:59:50.719: INFO: (19) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.58698ms)
Jul 19 00:59:50.719: INFO: (19) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.877462ms)
STEP: deleting ReplicationController proxy-service-9sgzr in namespace proxy-2684, will wait for the garbage collector to delete the pods 07/19/23 00:59:50.719
Jul 19 00:59:50.775: INFO: Deleting ReplicationController proxy-service-9sgzr took: 2.945225ms
Jul 19 00:59:50.876: INFO: Terminating ReplicationController proxy-service-9sgzr pods took: 100.959301ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:52.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2684" for this suite. 07/19/23 00:59:52.879
------------------------------
• [SLOW TEST] [5.366 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:47.515
    Jul 19 00:59:47.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename proxy 07/19/23 00:59:47.516
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:47.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:47.525
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 07/19/23 00:59:47.536
    STEP: creating replication controller proxy-service-9sgzr in namespace proxy-2684 07/19/23 00:59:47.536
    I0719 00:59:47.540399      21 runners.go:193] Created replication controller with name: proxy-service-9sgzr, namespace: proxy-2684, replica count: 1
    I0719 00:59:48.591612      21 runners.go:193] proxy-service-9sgzr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0719 00:59:49.591862      21 runners.go:193] proxy-service-9sgzr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0719 00:59:50.592637      21 runners.go:193] proxy-service-9sgzr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 19 00:59:50.594: INFO: setup took 3.06736704s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 07/19/23 00:59:50.594
    Jul 19 00:59:50.597: INFO: (0) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.258531ms)
    Jul 19 00:59:50.597: INFO: (0) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.26262ms)
    Jul 19 00:59:50.597: INFO: (0) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.334679ms)
    Jul 19 00:59:50.597: INFO: (0) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.28876ms)
    Jul 19 00:59:50.599: INFO: (0) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.842278ms)
    Jul 19 00:59:50.599: INFO: (0) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 5.130722ms)
    Jul 19 00:59:50.599: INFO: (0) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 5.167539ms)
    Jul 19 00:59:50.601: INFO: (0) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 6.71297ms)
    Jul 19 00:59:50.603: INFO: (0) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 8.492225ms)
    Jul 19 00:59:50.603: INFO: (0) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 8.59304ms)
    Jul 19 00:59:50.603: INFO: (0) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 8.672007ms)
    Jul 19 00:59:50.603: INFO: (0) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 8.791289ms)
    Jul 19 00:59:50.604: INFO: (0) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 9.321732ms)
    Jul 19 00:59:50.605: INFO: (0) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 10.673256ms)
    Jul 19 00:59:50.605: INFO: (0) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 11.190016ms)
    Jul 19 00:59:50.605: INFO: (0) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 11.192684ms)
    Jul 19 00:59:50.609: INFO: (1) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.698729ms)
    Jul 19 00:59:50.609: INFO: (1) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 3.841626ms)
    Jul 19 00:59:50.610: INFO: (1) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 4.645252ms)
    Jul 19 00:59:50.610: INFO: (1) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.713716ms)
    Jul 19 00:59:50.610: INFO: (1) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 4.777596ms)
    Jul 19 00:59:50.610: INFO: (1) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 4.775353ms)
    Jul 19 00:59:50.610: INFO: (1) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.762694ms)
    Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 5.01672ms)
    Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 5.097729ms)
    Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 5.09447ms)
    Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 5.165697ms)
    Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 5.089702ms)
    Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 5.165925ms)
    Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 5.122988ms)
    Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 5.195691ms)
    Jul 19 00:59:50.611: INFO: (1) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 5.144141ms)
    Jul 19 00:59:50.613: INFO: (2) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.154564ms)
    Jul 19 00:59:50.613: INFO: (2) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.174754ms)
    Jul 19 00:59:50.613: INFO: (2) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 2.239698ms)
    Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 6.138427ms)
    Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 6.090512ms)
    Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 6.163254ms)
    Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 6.277478ms)
    Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 6.321735ms)
    Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 6.283752ms)
    Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 6.28771ms)
    Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 6.350001ms)
    Jul 19 00:59:50.617: INFO: (2) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 6.381385ms)
    Jul 19 00:59:50.619: INFO: (2) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 7.738981ms)
    Jul 19 00:59:50.619: INFO: (2) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 7.769784ms)
    Jul 19 00:59:50.619: INFO: (2) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 7.78713ms)
    Jul 19 00:59:50.619: INFO: (2) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 7.804585ms)
    Jul 19 00:59:50.621: INFO: (3) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 2.322335ms)
    Jul 19 00:59:50.622: INFO: (3) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.808272ms)
    Jul 19 00:59:50.623: INFO: (3) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.360919ms)
    Jul 19 00:59:50.623: INFO: (3) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.357203ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.903869ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.873415ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.898316ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 4.89612ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.969381ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 4.944326ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 5.04193ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 5.000596ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 5.072169ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 5.02173ms)
    Jul 19 00:59:50.624: INFO: (3) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 5.013541ms)
    Jul 19 00:59:50.627: INFO: (3) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 8.650315ms)
    Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.127817ms)
    Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.53005ms)
    Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.481601ms)
    Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.482198ms)
    Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.498733ms)
    Jul 19 00:59:50.631: INFO: (4) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.468499ms)
    Jul 19 00:59:50.632: INFO: (4) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.595385ms)
    Jul 19 00:59:50.633: INFO: (4) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 5.405028ms)
    Jul 19 00:59:50.633: INFO: (4) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 5.545403ms)
    Jul 19 00:59:50.633: INFO: (4) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 5.767926ms)
    Jul 19 00:59:50.634: INFO: (4) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 7.020866ms)
    Jul 19 00:59:50.635: INFO: (4) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 7.077804ms)
    Jul 19 00:59:50.635: INFO: (4) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 7.098653ms)
    Jul 19 00:59:50.635: INFO: (4) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 7.694692ms)
    Jul 19 00:59:50.635: INFO: (4) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 7.74369ms)
    Jul 19 00:59:50.635: INFO: (4) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 7.838445ms)
    Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.947673ms)
    Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.943644ms)
    Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 2.967335ms)
    Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 2.939898ms)
    Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 2.922412ms)
    Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 2.932204ms)
    Jul 19 00:59:50.638: INFO: (5) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 2.969718ms)
    Jul 19 00:59:50.639: INFO: (5) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 4.122553ms)
    Jul 19 00:59:50.639: INFO: (5) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.153596ms)
    Jul 19 00:59:50.639: INFO: (5) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.172271ms)
    Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.129925ms)
    Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.137911ms)
    Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.168278ms)
    Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.26312ms)
    Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 4.178931ms)
    Jul 19 00:59:50.640: INFO: (5) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.191557ms)
    Jul 19 00:59:50.643: INFO: (6) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.019225ms)
    Jul 19 00:59:50.643: INFO: (6) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 2.999434ms)
    Jul 19 00:59:50.643: INFO: (6) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.011ms)
    Jul 19 00:59:50.643: INFO: (6) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.393058ms)
    Jul 19 00:59:50.643: INFO: (6) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.647305ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.982142ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 4.013104ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 4.349248ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.390193ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.379825ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.493519ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.46902ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.562447ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.587344ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.699093ms)
    Jul 19 00:59:50.644: INFO: (6) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.759462ms)
    Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.555475ms)
    Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 2.535931ms)
    Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 2.493152ms)
    Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 2.570959ms)
    Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 2.610873ms)
    Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.784736ms)
    Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 2.80068ms)
    Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.921608ms)
    Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.897515ms)
    Jul 19 00:59:50.647: INFO: (7) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 2.872323ms)
    Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.467851ms)
    Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.798924ms)
    Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.814528ms)
    Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.88076ms)
    Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.790756ms)
    Jul 19 00:59:50.649: INFO: (7) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.904924ms)
    Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.382308ms)
    Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 3.609764ms)
    Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.7017ms)
    Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.736115ms)
    Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.732323ms)
    Jul 19 00:59:50.653: INFO: (8) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 3.793602ms)
    Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 7.276713ms)
    Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 7.317355ms)
    Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 7.484474ms)
    Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 7.520736ms)
    Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 7.59572ms)
    Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 7.535789ms)
    Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 7.567724ms)
    Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 7.531837ms)
    Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 7.54983ms)
    Jul 19 00:59:50.657: INFO: (8) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 7.826541ms)
    Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 4.16107ms)
    Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 4.357587ms)
    Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 4.412336ms)
    Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 4.351774ms)
    Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 4.373545ms)
    Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 4.470317ms)
    Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.483176ms)
    Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 4.446927ms)
    Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.486264ms)
    Jul 19 00:59:50.662: INFO: (9) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.442769ms)
    Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 5.662387ms)
    Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 5.646335ms)
    Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 5.654376ms)
    Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 5.817622ms)
    Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 5.832245ms)
    Jul 19 00:59:50.663: INFO: (9) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 5.760535ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 3.799266ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 3.865043ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 3.920815ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 3.835468ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.824854ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.807514ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.829122ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.875951ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 3.829734ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.8624ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 3.912628ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 3.931618ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 3.97047ms)
    Jul 19 00:59:50.667: INFO: (10) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.924986ms)
    Jul 19 00:59:50.668: INFO: (10) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 5.069404ms)
    Jul 19 00:59:50.668: INFO: (10) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 5.129343ms)
    Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 2.566197ms)
    Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.641408ms)
    Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.711645ms)
    Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 2.91952ms)
    Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 2.987837ms)
    Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 2.964381ms)
    Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.925472ms)
    Jul 19 00:59:50.671: INFO: (11) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.023895ms)
    Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 3.422326ms)
    Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.502719ms)
    Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 3.423183ms)
    Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 3.557411ms)
    Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 3.5408ms)
    Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 3.600704ms)
    Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 3.603818ms)
    Jul 19 00:59:50.672: INFO: (11) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 3.584132ms)
    Jul 19 00:59:50.675: INFO: (12) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.698207ms)
    Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.037992ms)
    Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.024161ms)
    Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 4.119645ms)
    Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 4.114572ms)
    Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 4.137698ms)
    Jul 19 00:59:50.676: INFO: (12) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 4.136863ms)
    Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.462255ms)
    Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.423718ms)
    Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 4.480954ms)
    Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 4.521494ms)
    Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.529848ms)
    Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 4.50494ms)
    Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.599645ms)
    Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.557113ms)
    Jul 19 00:59:50.677: INFO: (12) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 4.576525ms)
    Jul 19 00:59:50.679: INFO: (13) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.326947ms)
    Jul 19 00:59:50.679: INFO: (13) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 2.466977ms)
    Jul 19 00:59:50.680: INFO: (13) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.361655ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 3.716922ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.805506ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.83424ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 3.868137ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 3.895201ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 3.934748ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.874121ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 3.93823ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.866349ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.879591ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 3.961523ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 3.875913ms)
    Jul 19 00:59:50.681: INFO: (13) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 4.141642ms)
    Jul 19 00:59:50.683: INFO: (14) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 2.395252ms)
    Jul 19 00:59:50.683: INFO: (14) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 2.412321ms)
    Jul 19 00:59:50.683: INFO: (14) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.318227ms)
    Jul 19 00:59:50.684: INFO: (14) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 2.830179ms)
    Jul 19 00:59:50.684: INFO: (14) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.125384ms)
    Jul 19 00:59:50.684: INFO: (14) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.170146ms)
    Jul 19 00:59:50.684: INFO: (14) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.208458ms)
    Jul 19 00:59:50.684: INFO: (14) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.216975ms)
    Jul 19 00:59:50.686: INFO: (14) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 4.377595ms)
    Jul 19 00:59:50.686: INFO: (14) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.713737ms)
    Jul 19 00:59:50.686: INFO: (14) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.796596ms)
    Jul 19 00:59:50.686: INFO: (14) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.917265ms)
    Jul 19 00:59:50.687: INFO: (14) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 5.508271ms)
    Jul 19 00:59:50.687: INFO: (14) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 5.599909ms)
    Jul 19 00:59:50.687: INFO: (14) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 5.891504ms)
    Jul 19 00:59:50.687: INFO: (14) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 5.952782ms)
    Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 3.077466ms)
    Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 3.160039ms)
    Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 3.269691ms)
    Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.239639ms)
    Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.339119ms)
    Jul 19 00:59:50.690: INFO: (15) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 3.278001ms)
    Jul 19 00:59:50.691: INFO: (15) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.302967ms)
    Jul 19 00:59:50.691: INFO: (15) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.361263ms)
    Jul 19 00:59:50.691: INFO: (15) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.331382ms)
    Jul 19 00:59:50.691: INFO: (15) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.407856ms)
    Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 4.438698ms)
    Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.41202ms)
    Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.509053ms)
    Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.624307ms)
    Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.911765ms)
    Jul 19 00:59:50.692: INFO: (15) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.918436ms)
    Jul 19 00:59:50.699: INFO: (16) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 7.207324ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 7.674445ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 7.836425ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 7.756143ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 7.792773ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 7.753308ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 7.720644ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 7.805084ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 7.892945ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 7.880039ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 7.950778ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 7.942313ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 8.015327ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 8.025746ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 8.052169ms)
    Jul 19 00:59:50.700: INFO: (16) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 8.132681ms)
    Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 5.738749ms)
    Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 5.715636ms)
    Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 5.655694ms)
    Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 5.735695ms)
    Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 5.68484ms)
    Jul 19 00:59:50.706: INFO: (17) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 5.767116ms)
    Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 6.091931ms)
    Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 6.21914ms)
    Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 6.232016ms)
    Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 6.297043ms)
    Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 6.348295ms)
    Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 6.29028ms)
    Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 6.321632ms)
    Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 6.249887ms)
    Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 6.274278ms)
    Jul 19 00:59:50.707: INFO: (17) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 6.380514ms)
    Jul 19 00:59:50.713: INFO: (18) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 6.352452ms)
    Jul 19 00:59:50.713: INFO: (18) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 6.397064ms)
    Jul 19 00:59:50.713: INFO: (18) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 6.397794ms)
    Jul 19 00:59:50.713: INFO: (18) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 6.444794ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 6.478882ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 6.708937ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 7.076182ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 7.137026ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 7.180119ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 7.172677ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 7.215418ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 7.236172ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 7.220166ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 7.247137ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 7.341215ms)
    Jul 19 00:59:50.714: INFO: (18) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 7.290859ms)
    Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname2/proxy/: bar (200; 2.72145ms)
    Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:1080/proxy/rewriteme">test<... (200; 2.695686ms)
    Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:162/proxy/: bar (200; 2.961808ms)
    Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:160/proxy/: foo (200; 2.962705ms)
    Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:462/proxy/: tls qux (200; 3.1091ms)
    Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:460/proxy/: tls baz (200; 3.14347ms)
    Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/https:proxy-service-9sgzr-5k987:443/proxy/tlsrewritem... (200; 3.141621ms)
    Jul 19 00:59:50.717: INFO: (19) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:1080/proxy/rewriteme">... (200; 3.229365ms)
    Jul 19 00:59:50.718: INFO: (19) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987:160/proxy/: foo (200; 3.315068ms)
    Jul 19 00:59:50.718: INFO: (19) /api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/: <a href="/api/v1/namespaces/proxy-2684/pods/proxy-service-9sgzr-5k987/proxy/rewriteme">test</a> (200; 3.453253ms)
    Jul 19 00:59:50.718: INFO: (19) /api/v1/namespaces/proxy-2684/pods/http:proxy-service-9sgzr-5k987:162/proxy/: bar (200; 3.394744ms)
    Jul 19 00:59:50.719: INFO: (19) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname2/proxy/: bar (200; 4.503533ms)
    Jul 19 00:59:50.719: INFO: (19) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname1/proxy/: tls baz (200; 4.571079ms)
    Jul 19 00:59:50.719: INFO: (19) /api/v1/namespaces/proxy-2684/services/https:proxy-service-9sgzr:tlsportname2/proxy/: tls qux (200; 4.582845ms)
    Jul 19 00:59:50.719: INFO: (19) /api/v1/namespaces/proxy-2684/services/http:proxy-service-9sgzr:portname1/proxy/: foo (200; 4.58698ms)
    Jul 19 00:59:50.719: INFO: (19) /api/v1/namespaces/proxy-2684/services/proxy-service-9sgzr:portname1/proxy/: foo (200; 4.877462ms)
    STEP: deleting ReplicationController proxy-service-9sgzr in namespace proxy-2684, will wait for the garbage collector to delete the pods 07/19/23 00:59:50.719
    Jul 19 00:59:50.775: INFO: Deleting ReplicationController proxy-service-9sgzr took: 2.945225ms
    Jul 19 00:59:50.876: INFO: Terminating ReplicationController proxy-service-9sgzr pods took: 100.959301ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:52.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2684" for this suite. 07/19/23 00:59:52.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:52.882
Jul 19 00:59:52.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename var-expansion 07/19/23 00:59:52.883
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:52.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:52.891
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 07/19/23 00:59:52.893
Jul 19 00:59:52.898: INFO: Waiting up to 5m0s for pod "var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3" in namespace "var-expansion-664" to be "Succeeded or Failed"
Jul 19 00:59:52.903: INFO: Pod "var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.192159ms
Jul 19 00:59:54.905: INFO: Pod "var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007740691s
Jul 19 00:59:56.906: INFO: Pod "var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008254623s
STEP: Saw pod success 07/19/23 00:59:56.906
Jul 19 00:59:56.906: INFO: Pod "var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3" satisfied condition "Succeeded or Failed"
Jul 19 00:59:56.907: INFO: Trying to get logs from node controller-1 pod var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3 container dapi-container: <nil>
STEP: delete the pod 07/19/23 00:59:56.911
Jul 19 00:59:56.919: INFO: Waiting for pod var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3 to disappear
Jul 19 00:59:56.921: INFO: Pod var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:56.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-664" for this suite. 07/19/23 00:59:56.923
------------------------------
• [4.044 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:52.882
    Jul 19 00:59:52.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename var-expansion 07/19/23 00:59:52.883
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:52.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:52.891
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 07/19/23 00:59:52.893
    Jul 19 00:59:52.898: INFO: Waiting up to 5m0s for pod "var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3" in namespace "var-expansion-664" to be "Succeeded or Failed"
    Jul 19 00:59:52.903: INFO: Pod "var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.192159ms
    Jul 19 00:59:54.905: INFO: Pod "var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007740691s
    Jul 19 00:59:56.906: INFO: Pod "var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008254623s
    STEP: Saw pod success 07/19/23 00:59:56.906
    Jul 19 00:59:56.906: INFO: Pod "var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3" satisfied condition "Succeeded or Failed"
    Jul 19 00:59:56.907: INFO: Trying to get logs from node controller-1 pod var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3 container dapi-container: <nil>
    STEP: delete the pod 07/19/23 00:59:56.911
    Jul 19 00:59:56.919: INFO: Waiting for pod var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3 to disappear
    Jul 19 00:59:56.921: INFO: Pod var-expansion-a759be8b-2f5c-432b-bd7c-590d7116b1f3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:56.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-664" for this suite. 07/19/23 00:59:56.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:56.927
Jul 19 00:59:56.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename namespaces 07/19/23 00:59:56.928
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:56.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:56.936
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-7973" 07/19/23 00:59:56.938
Jul 19 00:59:56.941: INFO: Namespace "namespaces-7973" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"cc259ba7-b4dd-4a59-af4b-b9b08bdad088", "kubernetes.io/metadata.name":"namespaces-7973", "namespaces-7973":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 00:59:56.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7973" for this suite. 07/19/23 00:59:56.943
------------------------------
• [0.019 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:56.927
    Jul 19 00:59:56.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename namespaces 07/19/23 00:59:56.928
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:56.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:56.936
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-7973" 07/19/23 00:59:56.938
    Jul 19 00:59:56.941: INFO: Namespace "namespaces-7973" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"cc259ba7-b4dd-4a59-af4b-b9b08bdad088", "kubernetes.io/metadata.name":"namespaces-7973", "namespaces-7973":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 00:59:56.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7973" for this suite. 07/19/23 00:59:56.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 00:59:56.947
Jul 19 00:59:56.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 00:59:56.948
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:56.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:56.964
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 07/19/23 00:59:56.966
Jul 19 00:59:56.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 create -f -'
Jul 19 00:59:57.178: INFO: stderr: ""
Jul 19 00:59:57.178: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 07/19/23 00:59:57.179
Jul 19 00:59:57.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 19 00:59:57.242: INFO: stderr: ""
Jul 19 00:59:57.242: INFO: stdout: "update-demo-nautilus-bq27p update-demo-nautilus-h94ps "
Jul 19 00:59:57.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 19 00:59:57.308: INFO: stderr: ""
Jul 19 00:59:57.308: INFO: stdout: ""
Jul 19 00:59:57.308: INFO: update-demo-nautilus-bq27p is created but not running
Jul 19 01:00:02.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 19 01:00:02.373: INFO: stderr: ""
Jul 19 01:00:02.373: INFO: stdout: "update-demo-nautilus-bq27p update-demo-nautilus-h94ps "
Jul 19 01:00:02.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 19 01:00:02.434: INFO: stderr: ""
Jul 19 01:00:02.434: INFO: stdout: "true"
Jul 19 01:00:02.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 19 01:00:02.498: INFO: stderr: ""
Jul 19 01:00:02.498: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jul 19 01:00:02.498: INFO: validating pod update-demo-nautilus-bq27p
Jul 19 01:00:02.500: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 01:00:02.500: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 01:00:02.500: INFO: update-demo-nautilus-bq27p is verified up and running
Jul 19 01:00:02.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-h94ps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 19 01:00:02.561: INFO: stderr: ""
Jul 19 01:00:02.561: INFO: stdout: "true"
Jul 19 01:00:02.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-h94ps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 19 01:00:02.632: INFO: stderr: ""
Jul 19 01:00:02.632: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jul 19 01:00:02.632: INFO: validating pod update-demo-nautilus-h94ps
Jul 19 01:00:02.634: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 01:00:02.634: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 01:00:02.634: INFO: update-demo-nautilus-h94ps is verified up and running
STEP: scaling down the replication controller 07/19/23 01:00:02.634
Jul 19 01:00:02.636: INFO: scanned /root for discovery docs: <nil>
Jul 19 01:00:02.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jul 19 01:00:03.705: INFO: stderr: ""
Jul 19 01:00:03.705: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 07/19/23 01:00:03.705
Jul 19 01:00:03.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 19 01:00:03.767: INFO: stderr: ""
Jul 19 01:00:03.767: INFO: stdout: "update-demo-nautilus-bq27p update-demo-nautilus-h94ps "
STEP: Replicas for name=update-demo: expected=1 actual=2 07/19/23 01:00:03.767
Jul 19 01:00:08.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 19 01:00:08.830: INFO: stderr: ""
Jul 19 01:00:08.830: INFO: stdout: "update-demo-nautilus-bq27p "
Jul 19 01:00:08.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 19 01:00:08.889: INFO: stderr: ""
Jul 19 01:00:08.889: INFO: stdout: "true"
Jul 19 01:00:08.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 19 01:00:08.949: INFO: stderr: ""
Jul 19 01:00:08.949: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jul 19 01:00:08.949: INFO: validating pod update-demo-nautilus-bq27p
Jul 19 01:00:08.951: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 01:00:08.951: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 01:00:08.951: INFO: update-demo-nautilus-bq27p is verified up and running
STEP: scaling up the replication controller 07/19/23 01:00:08.951
Jul 19 01:00:08.952: INFO: scanned /root for discovery docs: <nil>
Jul 19 01:00:08.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jul 19 01:00:10.026: INFO: stderr: ""
Jul 19 01:00:10.026: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 07/19/23 01:00:10.026
Jul 19 01:00:10.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 19 01:00:10.089: INFO: stderr: ""
Jul 19 01:00:10.089: INFO: stdout: "update-demo-nautilus-bq27p update-demo-nautilus-gp8pt "
Jul 19 01:00:10.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 19 01:00:10.149: INFO: stderr: ""
Jul 19 01:00:10.149: INFO: stdout: "true"
Jul 19 01:00:10.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 19 01:00:10.208: INFO: stderr: ""
Jul 19 01:00:10.208: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jul 19 01:00:10.208: INFO: validating pod update-demo-nautilus-bq27p
Jul 19 01:00:10.210: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 01:00:10.210: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 01:00:10.210: INFO: update-demo-nautilus-bq27p is verified up and running
Jul 19 01:00:10.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-gp8pt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 19 01:00:10.274: INFO: stderr: ""
Jul 19 01:00:10.274: INFO: stdout: "true"
Jul 19 01:00:10.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-gp8pt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 19 01:00:10.337: INFO: stderr: ""
Jul 19 01:00:10.337: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jul 19 01:00:10.337: INFO: validating pod update-demo-nautilus-gp8pt
Jul 19 01:00:10.340: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 19 01:00:10.340: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 19 01:00:10.340: INFO: update-demo-nautilus-gp8pt is verified up and running
STEP: using delete to clean up resources 07/19/23 01:00:10.34
Jul 19 01:00:10.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 delete --grace-period=0 --force -f -'
Jul 19 01:00:10.401: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 01:00:10.401: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 19 01:00:10.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get rc,svc -l name=update-demo --no-headers'
Jul 19 01:00:10.468: INFO: stderr: "No resources found in kubectl-51 namespace.\n"
Jul 19 01:00:10.468: INFO: stdout: ""
Jul 19 01:00:10.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 19 01:00:10.535: INFO: stderr: ""
Jul 19 01:00:10.535: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:10.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-51" for this suite. 07/19/23 01:00:10.541
------------------------------
• [SLOW TEST] [13.598 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 00:59:56.947
    Jul 19 00:59:56.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 00:59:56.948
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 00:59:56.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 00:59:56.964
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 07/19/23 00:59:56.966
    Jul 19 00:59:56.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 create -f -'
    Jul 19 00:59:57.178: INFO: stderr: ""
    Jul 19 00:59:57.178: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 07/19/23 00:59:57.179
    Jul 19 00:59:57.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 19 00:59:57.242: INFO: stderr: ""
    Jul 19 00:59:57.242: INFO: stdout: "update-demo-nautilus-bq27p update-demo-nautilus-h94ps "
    Jul 19 00:59:57.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 19 00:59:57.308: INFO: stderr: ""
    Jul 19 00:59:57.308: INFO: stdout: ""
    Jul 19 00:59:57.308: INFO: update-demo-nautilus-bq27p is created but not running
    Jul 19 01:00:02.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 19 01:00:02.373: INFO: stderr: ""
    Jul 19 01:00:02.373: INFO: stdout: "update-demo-nautilus-bq27p update-demo-nautilus-h94ps "
    Jul 19 01:00:02.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 19 01:00:02.434: INFO: stderr: ""
    Jul 19 01:00:02.434: INFO: stdout: "true"
    Jul 19 01:00:02.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 19 01:00:02.498: INFO: stderr: ""
    Jul 19 01:00:02.498: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jul 19 01:00:02.498: INFO: validating pod update-demo-nautilus-bq27p
    Jul 19 01:00:02.500: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 19 01:00:02.500: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 19 01:00:02.500: INFO: update-demo-nautilus-bq27p is verified up and running
    Jul 19 01:00:02.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-h94ps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 19 01:00:02.561: INFO: stderr: ""
    Jul 19 01:00:02.561: INFO: stdout: "true"
    Jul 19 01:00:02.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-h94ps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 19 01:00:02.632: INFO: stderr: ""
    Jul 19 01:00:02.632: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jul 19 01:00:02.632: INFO: validating pod update-demo-nautilus-h94ps
    Jul 19 01:00:02.634: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 19 01:00:02.634: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 19 01:00:02.634: INFO: update-demo-nautilus-h94ps is verified up and running
    STEP: scaling down the replication controller 07/19/23 01:00:02.634
    Jul 19 01:00:02.636: INFO: scanned /root for discovery docs: <nil>
    Jul 19 01:00:02.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jul 19 01:00:03.705: INFO: stderr: ""
    Jul 19 01:00:03.705: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 07/19/23 01:00:03.705
    Jul 19 01:00:03.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 19 01:00:03.767: INFO: stderr: ""
    Jul 19 01:00:03.767: INFO: stdout: "update-demo-nautilus-bq27p update-demo-nautilus-h94ps "
    STEP: Replicas for name=update-demo: expected=1 actual=2 07/19/23 01:00:03.767
    Jul 19 01:00:08.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 19 01:00:08.830: INFO: stderr: ""
    Jul 19 01:00:08.830: INFO: stdout: "update-demo-nautilus-bq27p "
    Jul 19 01:00:08.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 19 01:00:08.889: INFO: stderr: ""
    Jul 19 01:00:08.889: INFO: stdout: "true"
    Jul 19 01:00:08.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 19 01:00:08.949: INFO: stderr: ""
    Jul 19 01:00:08.949: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jul 19 01:00:08.949: INFO: validating pod update-demo-nautilus-bq27p
    Jul 19 01:00:08.951: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 19 01:00:08.951: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 19 01:00:08.951: INFO: update-demo-nautilus-bq27p is verified up and running
    STEP: scaling up the replication controller 07/19/23 01:00:08.951
    Jul 19 01:00:08.952: INFO: scanned /root for discovery docs: <nil>
    Jul 19 01:00:08.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jul 19 01:00:10.026: INFO: stderr: ""
    Jul 19 01:00:10.026: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 07/19/23 01:00:10.026
    Jul 19 01:00:10.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jul 19 01:00:10.089: INFO: stderr: ""
    Jul 19 01:00:10.089: INFO: stdout: "update-demo-nautilus-bq27p update-demo-nautilus-gp8pt "
    Jul 19 01:00:10.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 19 01:00:10.149: INFO: stderr: ""
    Jul 19 01:00:10.149: INFO: stdout: "true"
    Jul 19 01:00:10.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-bq27p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 19 01:00:10.208: INFO: stderr: ""
    Jul 19 01:00:10.208: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jul 19 01:00:10.208: INFO: validating pod update-demo-nautilus-bq27p
    Jul 19 01:00:10.210: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 19 01:00:10.210: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 19 01:00:10.210: INFO: update-demo-nautilus-bq27p is verified up and running
    Jul 19 01:00:10.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-gp8pt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jul 19 01:00:10.274: INFO: stderr: ""
    Jul 19 01:00:10.274: INFO: stdout: "true"
    Jul 19 01:00:10.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods update-demo-nautilus-gp8pt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jul 19 01:00:10.337: INFO: stderr: ""
    Jul 19 01:00:10.337: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jul 19 01:00:10.337: INFO: validating pod update-demo-nautilus-gp8pt
    Jul 19 01:00:10.340: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jul 19 01:00:10.340: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jul 19 01:00:10.340: INFO: update-demo-nautilus-gp8pt is verified up and running
    STEP: using delete to clean up resources 07/19/23 01:00:10.34
    Jul 19 01:00:10.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 delete --grace-period=0 --force -f -'
    Jul 19 01:00:10.401: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 19 01:00:10.401: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jul 19 01:00:10.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get rc,svc -l name=update-demo --no-headers'
    Jul 19 01:00:10.468: INFO: stderr: "No resources found in kubectl-51 namespace.\n"
    Jul 19 01:00:10.468: INFO: stdout: ""
    Jul 19 01:00:10.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-51 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jul 19 01:00:10.535: INFO: stderr: ""
    Jul 19 01:00:10.535: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:10.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-51" for this suite. 07/19/23 01:00:10.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:10.546
Jul 19 01:00:10.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 01:00:10.546
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:10.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:10.555
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 07/19/23 01:00:10.557
Jul 19 01:00:10.561: INFO: Waiting up to 5m0s for pod "downward-api-ded74fee-7d86-407f-875b-ac136d5783a6" in namespace "downward-api-5086" to be "Succeeded or Failed"
Jul 19 01:00:10.562: INFO: Pod "downward-api-ded74fee-7d86-407f-875b-ac136d5783a6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.406861ms
Jul 19 01:00:12.565: INFO: Pod "downward-api-ded74fee-7d86-407f-875b-ac136d5783a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004588532s
Jul 19 01:00:14.565: INFO: Pod "downward-api-ded74fee-7d86-407f-875b-ac136d5783a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004111272s
STEP: Saw pod success 07/19/23 01:00:14.565
Jul 19 01:00:14.565: INFO: Pod "downward-api-ded74fee-7d86-407f-875b-ac136d5783a6" satisfied condition "Succeeded or Failed"
Jul 19 01:00:14.566: INFO: Trying to get logs from node controller-1 pod downward-api-ded74fee-7d86-407f-875b-ac136d5783a6 container dapi-container: <nil>
STEP: delete the pod 07/19/23 01:00:14.57
Jul 19 01:00:14.577: INFO: Waiting for pod downward-api-ded74fee-7d86-407f-875b-ac136d5783a6 to disappear
Jul 19 01:00:14.578: INFO: Pod downward-api-ded74fee-7d86-407f-875b-ac136d5783a6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:14.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5086" for this suite. 07/19/23 01:00:14.581
------------------------------
• [4.039 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:10.546
    Jul 19 01:00:10.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 01:00:10.546
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:10.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:10.555
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 07/19/23 01:00:10.557
    Jul 19 01:00:10.561: INFO: Waiting up to 5m0s for pod "downward-api-ded74fee-7d86-407f-875b-ac136d5783a6" in namespace "downward-api-5086" to be "Succeeded or Failed"
    Jul 19 01:00:10.562: INFO: Pod "downward-api-ded74fee-7d86-407f-875b-ac136d5783a6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.406861ms
    Jul 19 01:00:12.565: INFO: Pod "downward-api-ded74fee-7d86-407f-875b-ac136d5783a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004588532s
    Jul 19 01:00:14.565: INFO: Pod "downward-api-ded74fee-7d86-407f-875b-ac136d5783a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004111272s
    STEP: Saw pod success 07/19/23 01:00:14.565
    Jul 19 01:00:14.565: INFO: Pod "downward-api-ded74fee-7d86-407f-875b-ac136d5783a6" satisfied condition "Succeeded or Failed"
    Jul 19 01:00:14.566: INFO: Trying to get logs from node controller-1 pod downward-api-ded74fee-7d86-407f-875b-ac136d5783a6 container dapi-container: <nil>
    STEP: delete the pod 07/19/23 01:00:14.57
    Jul 19 01:00:14.577: INFO: Waiting for pod downward-api-ded74fee-7d86-407f-875b-ac136d5783a6 to disappear
    Jul 19 01:00:14.578: INFO: Pod downward-api-ded74fee-7d86-407f-875b-ac136d5783a6 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:14.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5086" for this suite. 07/19/23 01:00:14.581
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:14.585
Jul 19 01:00:14.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 01:00:14.586
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:14.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:14.594
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 07/19/23 01:00:14.596
Jul 19 01:00:14.600: INFO: Waiting up to 5m0s for pod "pod-d57e916f-fcc2-4157-af5c-f29676378d97" in namespace "emptydir-5045" to be "Succeeded or Failed"
Jul 19 01:00:14.601: INFO: Pod "pod-d57e916f-fcc2-4157-af5c-f29676378d97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.237417ms
Jul 19 01:00:16.604: INFO: Pod "pod-d57e916f-fcc2-4157-af5c-f29676378d97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004168546s
Jul 19 01:00:18.604: INFO: Pod "pod-d57e916f-fcc2-4157-af5c-f29676378d97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003843907s
STEP: Saw pod success 07/19/23 01:00:18.604
Jul 19 01:00:18.604: INFO: Pod "pod-d57e916f-fcc2-4157-af5c-f29676378d97" satisfied condition "Succeeded or Failed"
Jul 19 01:00:18.605: INFO: Trying to get logs from node controller-1 pod pod-d57e916f-fcc2-4157-af5c-f29676378d97 container test-container: <nil>
STEP: delete the pod 07/19/23 01:00:18.609
Jul 19 01:00:18.615: INFO: Waiting for pod pod-d57e916f-fcc2-4157-af5c-f29676378d97 to disappear
Jul 19 01:00:18.617: INFO: Pod pod-d57e916f-fcc2-4157-af5c-f29676378d97 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:18.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5045" for this suite. 07/19/23 01:00:18.619
------------------------------
• [4.037 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:14.585
    Jul 19 01:00:14.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 01:00:14.586
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:14.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:14.594
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 07/19/23 01:00:14.596
    Jul 19 01:00:14.600: INFO: Waiting up to 5m0s for pod "pod-d57e916f-fcc2-4157-af5c-f29676378d97" in namespace "emptydir-5045" to be "Succeeded or Failed"
    Jul 19 01:00:14.601: INFO: Pod "pod-d57e916f-fcc2-4157-af5c-f29676378d97": Phase="Pending", Reason="", readiness=false. Elapsed: 1.237417ms
    Jul 19 01:00:16.604: INFO: Pod "pod-d57e916f-fcc2-4157-af5c-f29676378d97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004168546s
    Jul 19 01:00:18.604: INFO: Pod "pod-d57e916f-fcc2-4157-af5c-f29676378d97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003843907s
    STEP: Saw pod success 07/19/23 01:00:18.604
    Jul 19 01:00:18.604: INFO: Pod "pod-d57e916f-fcc2-4157-af5c-f29676378d97" satisfied condition "Succeeded or Failed"
    Jul 19 01:00:18.605: INFO: Trying to get logs from node controller-1 pod pod-d57e916f-fcc2-4157-af5c-f29676378d97 container test-container: <nil>
    STEP: delete the pod 07/19/23 01:00:18.609
    Jul 19 01:00:18.615: INFO: Waiting for pod pod-d57e916f-fcc2-4157-af5c-f29676378d97 to disappear
    Jul 19 01:00:18.617: INFO: Pod pod-d57e916f-fcc2-4157-af5c-f29676378d97 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:18.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5045" for this suite. 07/19/23 01:00:18.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:18.621
Jul 19 01:00:18.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename runtimeclass 07/19/23 01:00:18.622
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:18.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:18.636
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-6207-delete-me 07/19/23 01:00:18.641
STEP: Waiting for the RuntimeClass to disappear 07/19/23 01:00:18.643
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:18.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6207" for this suite. 07/19/23 01:00:18.65
------------------------------
• [0.031 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:18.621
    Jul 19 01:00:18.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename runtimeclass 07/19/23 01:00:18.622
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:18.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:18.636
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-6207-delete-me 07/19/23 01:00:18.641
    STEP: Waiting for the RuntimeClass to disappear 07/19/23 01:00:18.643
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:18.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6207" for this suite. 07/19/23 01:00:18.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:18.654
Jul 19 01:00:18.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename proxy 07/19/23 01:00:18.655
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:18.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:18.667
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jul 19 01:00:18.668: INFO: Creating pod...
Jul 19 01:00:18.672: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-160" to be "running"
Jul 19 01:00:18.674: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.799089ms
Jul 19 01:00:20.677: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004981495s
Jul 19 01:00:20.677: INFO: Pod "agnhost" satisfied condition "running"
Jul 19 01:00:20.677: INFO: Creating service...
Jul 19 01:00:20.685: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=DELETE
Jul 19 01:00:20.688: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 19 01:00:20.688: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=OPTIONS
Jul 19 01:00:20.690: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 19 01:00:20.690: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=PATCH
Jul 19 01:00:20.691: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 19 01:00:20.691: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=POST
Jul 19 01:00:20.693: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 19 01:00:20.693: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=PUT
Jul 19 01:00:20.694: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul 19 01:00:20.694: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=DELETE
Jul 19 01:00:20.696: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 19 01:00:20.696: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jul 19 01:00:20.698: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 19 01:00:20.698: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=PATCH
Jul 19 01:00:20.700: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 19 01:00:20.700: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=POST
Jul 19 01:00:20.702: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 19 01:00:20.702: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=PUT
Jul 19 01:00:20.704: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul 19 01:00:20.704: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=GET
Jul 19 01:00:20.705: INFO: http.Client request:GET StatusCode:301
Jul 19 01:00:20.705: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=GET
Jul 19 01:00:20.707: INFO: http.Client request:GET StatusCode:301
Jul 19 01:00:20.707: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=HEAD
Jul 19 01:00:20.708: INFO: http.Client request:HEAD StatusCode:301
Jul 19 01:00:20.708: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=HEAD
Jul 19 01:00:20.709: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:20.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-160" for this suite. 07/19/23 01:00:20.712
------------------------------
• [2.060 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:18.654
    Jul 19 01:00:18.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename proxy 07/19/23 01:00:18.655
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:18.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:18.667
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jul 19 01:00:18.668: INFO: Creating pod...
    Jul 19 01:00:18.672: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-160" to be "running"
    Jul 19 01:00:18.674: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.799089ms
    Jul 19 01:00:20.677: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004981495s
    Jul 19 01:00:20.677: INFO: Pod "agnhost" satisfied condition "running"
    Jul 19 01:00:20.677: INFO: Creating service...
    Jul 19 01:00:20.685: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=DELETE
    Jul 19 01:00:20.688: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jul 19 01:00:20.688: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=OPTIONS
    Jul 19 01:00:20.690: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jul 19 01:00:20.690: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=PATCH
    Jul 19 01:00:20.691: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jul 19 01:00:20.691: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=POST
    Jul 19 01:00:20.693: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jul 19 01:00:20.693: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=PUT
    Jul 19 01:00:20.694: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jul 19 01:00:20.694: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=DELETE
    Jul 19 01:00:20.696: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jul 19 01:00:20.696: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jul 19 01:00:20.698: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jul 19 01:00:20.698: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=PATCH
    Jul 19 01:00:20.700: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jul 19 01:00:20.700: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=POST
    Jul 19 01:00:20.702: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jul 19 01:00:20.702: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=PUT
    Jul 19 01:00:20.704: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jul 19 01:00:20.704: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=GET
    Jul 19 01:00:20.705: INFO: http.Client request:GET StatusCode:301
    Jul 19 01:00:20.705: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=GET
    Jul 19 01:00:20.707: INFO: http.Client request:GET StatusCode:301
    Jul 19 01:00:20.707: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/pods/agnhost/proxy?method=HEAD
    Jul 19 01:00:20.708: INFO: http.Client request:HEAD StatusCode:301
    Jul 19 01:00:20.708: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-160/services/e2e-proxy-test-service/proxy?method=HEAD
    Jul 19 01:00:20.709: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:20.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-160" for this suite. 07/19/23 01:00:20.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:20.715
Jul 19 01:00:20.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename job 07/19/23 01:00:20.715
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:20.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:20.726
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 07/19/23 01:00:20.727
STEP: Ensure pods equal to parallelism count is attached to the job 07/19/23 01:00:20.73
STEP: patching /status 07/19/23 01:00:22.733
STEP: updating /status 07/19/23 01:00:22.738
STEP: get /status 07/19/23 01:00:22.76
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:22.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7157" for this suite. 07/19/23 01:00:22.764
------------------------------
• [2.052 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:20.715
    Jul 19 01:00:20.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename job 07/19/23 01:00:20.715
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:20.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:20.726
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 07/19/23 01:00:20.727
    STEP: Ensure pods equal to parallelism count is attached to the job 07/19/23 01:00:20.73
    STEP: patching /status 07/19/23 01:00:22.733
    STEP: updating /status 07/19/23 01:00:22.738
    STEP: get /status 07/19/23 01:00:22.76
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:22.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7157" for this suite. 07/19/23 01:00:22.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:22.768
Jul 19 01:00:22.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 01:00:22.769
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:22.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:22.777
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 07/19/23 01:00:22.779
Jul 19 01:00:22.783: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3" in namespace "downward-api-2817" to be "Succeeded or Failed"
Jul 19 01:00:22.785: INFO: Pod "downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.473225ms
Jul 19 01:00:24.787: INFO: Pod "downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003570491s
Jul 19 01:00:26.787: INFO: Pod "downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003732286s
STEP: Saw pod success 07/19/23 01:00:26.787
Jul 19 01:00:26.787: INFO: Pod "downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3" satisfied condition "Succeeded or Failed"
Jul 19 01:00:26.789: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3 container client-container: <nil>
STEP: delete the pod 07/19/23 01:00:26.799
Jul 19 01:00:26.807: INFO: Waiting for pod downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3 to disappear
Jul 19 01:00:26.808: INFO: Pod downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:26.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2817" for this suite. 07/19/23 01:00:26.81
------------------------------
• [4.045 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:22.768
    Jul 19 01:00:22.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 01:00:22.769
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:22.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:22.777
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 07/19/23 01:00:22.779
    Jul 19 01:00:22.783: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3" in namespace "downward-api-2817" to be "Succeeded or Failed"
    Jul 19 01:00:22.785: INFO: Pod "downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.473225ms
    Jul 19 01:00:24.787: INFO: Pod "downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003570491s
    Jul 19 01:00:26.787: INFO: Pod "downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003732286s
    STEP: Saw pod success 07/19/23 01:00:26.787
    Jul 19 01:00:26.787: INFO: Pod "downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3" satisfied condition "Succeeded or Failed"
    Jul 19 01:00:26.789: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3 container client-container: <nil>
    STEP: delete the pod 07/19/23 01:00:26.799
    Jul 19 01:00:26.807: INFO: Waiting for pod downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3 to disappear
    Jul 19 01:00:26.808: INFO: Pod downwardapi-volume-c23d4f61-5c26-4cb9-906f-253c44f617e3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:26.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2817" for this suite. 07/19/23 01:00:26.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:26.816
Jul 19 01:00:26.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 01:00:26.817
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:26.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:26.828
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 07/19/23 01:00:26.83
Jul 19 01:00:26.830: INFO: Creating e2e-svc-a-qxdlw
Jul 19 01:00:26.838: INFO: Creating e2e-svc-b-z2vt8
Jul 19 01:00:26.846: INFO: Creating e2e-svc-c-m26kb
STEP: deleting service collection 07/19/23 01:00:26.856
Jul 19 01:00:26.877: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:26.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7952" for this suite. 07/19/23 01:00:26.879
------------------------------
• [0.066 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:26.816
    Jul 19 01:00:26.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 01:00:26.817
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:26.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:26.828
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 07/19/23 01:00:26.83
    Jul 19 01:00:26.830: INFO: Creating e2e-svc-a-qxdlw
    Jul 19 01:00:26.838: INFO: Creating e2e-svc-b-z2vt8
    Jul 19 01:00:26.846: INFO: Creating e2e-svc-c-m26kb
    STEP: deleting service collection 07/19/23 01:00:26.856
    Jul 19 01:00:26.877: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:26.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7952" for this suite. 07/19/23 01:00:26.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:26.882
Jul 19 01:00:26.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename downward-api 07/19/23 01:00:26.883
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:26.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:26.895
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 07/19/23 01:00:26.897
Jul 19 01:00:26.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c" in namespace "downward-api-8946" to be "Succeeded or Failed"
Jul 19 01:00:26.913: INFO: Pod "downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.600574ms
Jul 19 01:00:28.915: INFO: Pod "downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006580893s
Jul 19 01:00:30.915: INFO: Pod "downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006400296s
STEP: Saw pod success 07/19/23 01:00:30.915
Jul 19 01:00:30.915: INFO: Pod "downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c" satisfied condition "Succeeded or Failed"
Jul 19 01:00:30.917: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c container client-container: <nil>
STEP: delete the pod 07/19/23 01:00:30.921
Jul 19 01:00:30.929: INFO: Waiting for pod downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c to disappear
Jul 19 01:00:30.930: INFO: Pod downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:30.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8946" for this suite. 07/19/23 01:00:30.932
------------------------------
• [4.053 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:26.882
    Jul 19 01:00:26.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename downward-api 07/19/23 01:00:26.883
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:26.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:26.895
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 07/19/23 01:00:26.897
    Jul 19 01:00:26.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c" in namespace "downward-api-8946" to be "Succeeded or Failed"
    Jul 19 01:00:26.913: INFO: Pod "downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.600574ms
    Jul 19 01:00:28.915: INFO: Pod "downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006580893s
    Jul 19 01:00:30.915: INFO: Pod "downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006400296s
    STEP: Saw pod success 07/19/23 01:00:30.915
    Jul 19 01:00:30.915: INFO: Pod "downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c" satisfied condition "Succeeded or Failed"
    Jul 19 01:00:30.917: INFO: Trying to get logs from node controller-0 pod downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c container client-container: <nil>
    STEP: delete the pod 07/19/23 01:00:30.921
    Jul 19 01:00:30.929: INFO: Waiting for pod downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c to disappear
    Jul 19 01:00:30.930: INFO: Pod downwardapi-volume-ba6fcf53-4e49-4d36-aa14-70c0ea3f221c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:30.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8946" for this suite. 07/19/23 01:00:30.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:30.935
Jul 19 01:00:30.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 01:00:30.936
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:30.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:30.945
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-5f1be7fa-14df-4513-8d38-be596514b65c 07/19/23 01:00:30.947
STEP: Creating a pod to test consume configMaps 07/19/23 01:00:30.949
Jul 19 01:00:30.952: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445" in namespace "projected-6658" to be "Succeeded or Failed"
Jul 19 01:00:30.956: INFO: Pod "pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445": Phase="Pending", Reason="", readiness=false. Elapsed: 3.662063ms
Jul 19 01:00:32.959: INFO: Pod "pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006681467s
Jul 19 01:00:34.958: INFO: Pod "pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006132343s
STEP: Saw pod success 07/19/23 01:00:34.958
Jul 19 01:00:34.958: INFO: Pod "pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445" satisfied condition "Succeeded or Failed"
Jul 19 01:00:34.960: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 01:00:34.964
Jul 19 01:00:34.970: INFO: Waiting for pod pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445 to disappear
Jul 19 01:00:34.971: INFO: Pod pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:34.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6658" for this suite. 07/19/23 01:00:34.974
------------------------------
• [4.041 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:30.935
    Jul 19 01:00:30.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 01:00:30.936
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:30.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:30.945
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-5f1be7fa-14df-4513-8d38-be596514b65c 07/19/23 01:00:30.947
    STEP: Creating a pod to test consume configMaps 07/19/23 01:00:30.949
    Jul 19 01:00:30.952: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445" in namespace "projected-6658" to be "Succeeded or Failed"
    Jul 19 01:00:30.956: INFO: Pod "pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445": Phase="Pending", Reason="", readiness=false. Elapsed: 3.662063ms
    Jul 19 01:00:32.959: INFO: Pod "pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006681467s
    Jul 19 01:00:34.958: INFO: Pod "pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006132343s
    STEP: Saw pod success 07/19/23 01:00:34.958
    Jul 19 01:00:34.958: INFO: Pod "pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445" satisfied condition "Succeeded or Failed"
    Jul 19 01:00:34.960: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 01:00:34.964
    Jul 19 01:00:34.970: INFO: Waiting for pod pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445 to disappear
    Jul 19 01:00:34.971: INFO: Pod pod-projected-configmaps-b4a36440-d45d-40a9-9e17-1a614ed2c445 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:34.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6658" for this suite. 07/19/23 01:00:34.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:34.977
Jul 19 01:00:34.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 01:00:34.978
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:34.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:34.988
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-4a792bf0-d5a5-4882-9a59-ac2ed64e7a2c 07/19/23 01:00:34.99
STEP: Creating a pod to test consume configMaps 07/19/23 01:00:34.992
Jul 19 01:00:34.996: INFO: Waiting up to 5m0s for pod "pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443" in namespace "configmap-8966" to be "Succeeded or Failed"
Jul 19 01:00:34.999: INFO: Pod "pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443": Phase="Pending", Reason="", readiness=false. Elapsed: 2.759074ms
Jul 19 01:00:37.002: INFO: Pod "pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006054834s
Jul 19 01:00:39.001: INFO: Pod "pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005066079s
STEP: Saw pod success 07/19/23 01:00:39.001
Jul 19 01:00:39.001: INFO: Pod "pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443" satisfied condition "Succeeded or Failed"
Jul 19 01:00:39.003: INFO: Trying to get logs from node controller-0 pod pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 01:00:39.006
Jul 19 01:00:39.013: INFO: Waiting for pod pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443 to disappear
Jul 19 01:00:39.014: INFO: Pod pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:39.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8966" for this suite. 07/19/23 01:00:39.016
------------------------------
• [4.041 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:34.977
    Jul 19 01:00:34.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 01:00:34.978
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:34.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:34.988
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-4a792bf0-d5a5-4882-9a59-ac2ed64e7a2c 07/19/23 01:00:34.99
    STEP: Creating a pod to test consume configMaps 07/19/23 01:00:34.992
    Jul 19 01:00:34.996: INFO: Waiting up to 5m0s for pod "pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443" in namespace "configmap-8966" to be "Succeeded or Failed"
    Jul 19 01:00:34.999: INFO: Pod "pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443": Phase="Pending", Reason="", readiness=false. Elapsed: 2.759074ms
    Jul 19 01:00:37.002: INFO: Pod "pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006054834s
    Jul 19 01:00:39.001: INFO: Pod "pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005066079s
    STEP: Saw pod success 07/19/23 01:00:39.001
    Jul 19 01:00:39.001: INFO: Pod "pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443" satisfied condition "Succeeded or Failed"
    Jul 19 01:00:39.003: INFO: Trying to get logs from node controller-0 pod pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 01:00:39.006
    Jul 19 01:00:39.013: INFO: Waiting for pod pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443 to disappear
    Jul 19 01:00:39.014: INFO: Pod pod-configmaps-2215cd14-a0d9-4ba2-8b99-83d8bbc12443 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:39.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8966" for this suite. 07/19/23 01:00:39.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:39.02
Jul 19 01:00:39.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 01:00:39.02
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:39.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:39.028
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 01:00:39.035
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 01:00:39.499
STEP: Deploying the webhook pod 07/19/23 01:00:39.503
STEP: Wait for the deployment to be ready 07/19/23 01:00:39.509
Jul 19 01:00:39.515: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 01:00:41.521
STEP: Verifying the service has paired with the endpoint 07/19/23 01:00:41.53
Jul 19 01:00:42.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 07/19/23 01:00:42.568
STEP: Creating a configMap that does not comply to the validation webhook rules 07/19/23 01:00:42.59
STEP: Deleting the collection of validation webhooks 07/19/23 01:00:42.611
STEP: Creating a configMap that does not comply to the validation webhook rules 07/19/23 01:00:42.636
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 01:00:42.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8725" for this suite. 07/19/23 01:00:42.669
STEP: Destroying namespace "webhook-8725-markers" for this suite. 07/19/23 01:00:42.673
------------------------------
• [3.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:39.02
    Jul 19 01:00:39.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 01:00:39.02
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:39.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:39.028
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 01:00:39.035
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 01:00:39.499
    STEP: Deploying the webhook pod 07/19/23 01:00:39.503
    STEP: Wait for the deployment to be ready 07/19/23 01:00:39.509
    Jul 19 01:00:39.515: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 01:00:41.521
    STEP: Verifying the service has paired with the endpoint 07/19/23 01:00:41.53
    Jul 19 01:00:42.531: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 07/19/23 01:00:42.568
    STEP: Creating a configMap that does not comply to the validation webhook rules 07/19/23 01:00:42.59
    STEP: Deleting the collection of validation webhooks 07/19/23 01:00:42.611
    STEP: Creating a configMap that does not comply to the validation webhook rules 07/19/23 01:00:42.636
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:00:42.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8725" for this suite. 07/19/23 01:00:42.669
    STEP: Destroying namespace "webhook-8725-markers" for this suite. 07/19/23 01:00:42.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:00:42.68
Jul 19 01:00:42.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sched-preemption 07/19/23 01:00:42.681
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:42.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:42.692
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jul 19 01:00:42.701: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 19 01:01:42.732: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 07/19/23 01:01:42.734
Jul 19 01:01:42.746: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jul 19 01:01:42.750: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jul 19 01:01:42.765: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jul 19 01:01:42.771: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 07/19/23 01:01:42.771
Jul 19 01:01:42.771: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3744" to be "running"
Jul 19 01:01:42.775: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.988244ms
Jul 19 01:01:44.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.00631907s
Jul 19 01:01:44.778: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jul 19 01:01:44.778: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
Jul 19 01:01:44.779: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.516911ms
Jul 19 01:01:44.779: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jul 19 01:01:44.779: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
Jul 19 01:01:44.781: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.325764ms
Jul 19 01:01:44.781: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jul 19 01:01:44.781: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
Jul 19 01:01:44.782: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.393962ms
Jul 19 01:01:44.782: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 07/19/23 01:01:44.782
Jul 19 01:01:44.785: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3744" to be "running"
Jul 19 01:01:44.789: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.670517ms
Jul 19 01:01:46.792: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007380613s
Jul 19 01:01:48.792: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006961057s
Jul 19 01:01:50.792: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007187487s
Jul 19 01:01:50.792: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 01:01:50.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3744" for this suite. 07/19/23 01:01:50.835
------------------------------
• [SLOW TEST] [68.158 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:00:42.68
    Jul 19 01:00:42.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sched-preemption 07/19/23 01:00:42.681
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:00:42.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:00:42.692
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jul 19 01:00:42.701: INFO: Waiting up to 1m0s for all nodes to be ready
    Jul 19 01:01:42.732: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 07/19/23 01:01:42.734
    Jul 19 01:01:42.746: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jul 19 01:01:42.750: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jul 19 01:01:42.765: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jul 19 01:01:42.771: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 07/19/23 01:01:42.771
    Jul 19 01:01:42.771: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3744" to be "running"
    Jul 19 01:01:42.775: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.988244ms
    Jul 19 01:01:44.778: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.00631907s
    Jul 19 01:01:44.778: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jul 19 01:01:44.778: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
    Jul 19 01:01:44.779: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.516911ms
    Jul 19 01:01:44.779: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jul 19 01:01:44.779: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
    Jul 19 01:01:44.781: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.325764ms
    Jul 19 01:01:44.781: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jul 19 01:01:44.781: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3744" to be "running"
    Jul 19 01:01:44.782: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.393962ms
    Jul 19 01:01:44.782: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 07/19/23 01:01:44.782
    Jul 19 01:01:44.785: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3744" to be "running"
    Jul 19 01:01:44.789: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.670517ms
    Jul 19 01:01:46.792: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007380613s
    Jul 19 01:01:48.792: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006961057s
    Jul 19 01:01:50.792: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007187487s
    Jul 19 01:01:50.792: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:01:50.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3744" for this suite. 07/19/23 01:01:50.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:01:50.838
Jul 19 01:01:50.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename namespaces 07/19/23 01:01:50.839
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:01:50.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:01:50.847
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 07/19/23 01:01:50.849
Jul 19 01:01:50.851: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 07/19/23 01:01:50.851
Jul 19 01:01:50.854: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 07/19/23 01:01:50.854
Jul 19 01:01:50.859: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 01:01:50.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-470" for this suite. 07/19/23 01:01:50.861
------------------------------
• [0.025 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:01:50.838
    Jul 19 01:01:50.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename namespaces 07/19/23 01:01:50.839
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:01:50.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:01:50.847
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 07/19/23 01:01:50.849
    Jul 19 01:01:50.851: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 07/19/23 01:01:50.851
    Jul 19 01:01:50.854: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 07/19/23 01:01:50.854
    Jul 19 01:01:50.859: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:01:50.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-470" for this suite. 07/19/23 01:01:50.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:01:50.865
Jul 19 01:01:50.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename pods 07/19/23 01:01:50.866
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:01:50.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:01:50.874
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jul 19 01:01:50.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: creating the pod 07/19/23 01:01:50.876
STEP: submitting the pod to kubernetes 07/19/23 01:01:50.877
Jul 19 01:01:50.883: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23" in namespace "pods-6237" to be "running and ready"
Jul 19 01:01:50.888: INFO: Pod "pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.586319ms
Jul 19 01:01:50.888: INFO: The phase of Pod pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 01:01:52.890: INFO: Pod "pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23": Phase="Running", Reason="", readiness=true. Elapsed: 2.006572969s
Jul 19 01:01:52.890: INFO: The phase of Pod pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23 is Running (Ready = true)
Jul 19 01:01:52.890: INFO: Pod "pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jul 19 01:01:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6237" for this suite. 07/19/23 01:01:52.953
------------------------------
• [2.090 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:01:50.865
    Jul 19 01:01:50.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename pods 07/19/23 01:01:50.866
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:01:50.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:01:50.874
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jul 19 01:01:50.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: creating the pod 07/19/23 01:01:50.876
    STEP: submitting the pod to kubernetes 07/19/23 01:01:50.877
    Jul 19 01:01:50.883: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23" in namespace "pods-6237" to be "running and ready"
    Jul 19 01:01:50.888: INFO: Pod "pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.586319ms
    Jul 19 01:01:50.888: INFO: The phase of Pod pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 01:01:52.890: INFO: Pod "pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23": Phase="Running", Reason="", readiness=true. Elapsed: 2.006572969s
    Jul 19 01:01:52.890: INFO: The phase of Pod pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23 is Running (Ready = true)
    Jul 19 01:01:52.890: INFO: Pod "pod-exec-websocket-fdf1f016-54f5-4f1f-96af-7efe10c2fe23" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:01:52.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6237" for this suite. 07/19/23 01:01:52.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:01:52.956
Jul 19 01:01:52.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename job 07/19/23 01:01:52.957
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:01:52.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:01:52.968
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 07/19/23 01:01:52.97
STEP: Ensuring active pods == parallelism 07/19/23 01:01:52.972
STEP: Orphaning one of the Job's Pods 07/19/23 01:01:54.975
Jul 19 01:01:55.484: INFO: Successfully updated pod "adopt-release-2ldqq"
STEP: Checking that the Job readopts the Pod 07/19/23 01:01:55.484
Jul 19 01:01:55.484: INFO: Waiting up to 15m0s for pod "adopt-release-2ldqq" in namespace "job-9703" to be "adopted"
Jul 19 01:01:55.488: INFO: Pod "adopt-release-2ldqq": Phase="Running", Reason="", readiness=true. Elapsed: 4.303206ms
Jul 19 01:01:57.491: INFO: Pod "adopt-release-2ldqq": Phase="Running", Reason="", readiness=true. Elapsed: 2.006690502s
Jul 19 01:01:57.491: INFO: Pod "adopt-release-2ldqq" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 07/19/23 01:01:57.491
Jul 19 01:01:57.997: INFO: Successfully updated pod "adopt-release-2ldqq"
STEP: Checking that the Job releases the Pod 07/19/23 01:01:57.997
Jul 19 01:01:57.997: INFO: Waiting up to 15m0s for pod "adopt-release-2ldqq" in namespace "job-9703" to be "released"
Jul 19 01:01:58.001: INFO: Pod "adopt-release-2ldqq": Phase="Running", Reason="", readiness=true. Elapsed: 3.702636ms
Jul 19 01:02:00.007: INFO: Pod "adopt-release-2ldqq": Phase="Running", Reason="", readiness=true. Elapsed: 2.009138234s
Jul 19 01:02:00.007: INFO: Pod "adopt-release-2ldqq" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:00.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9703" for this suite. 07/19/23 01:02:00.009
------------------------------
• [SLOW TEST] [7.056 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:01:52.956
    Jul 19 01:01:52.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename job 07/19/23 01:01:52.957
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:01:52.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:01:52.968
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 07/19/23 01:01:52.97
    STEP: Ensuring active pods == parallelism 07/19/23 01:01:52.972
    STEP: Orphaning one of the Job's Pods 07/19/23 01:01:54.975
    Jul 19 01:01:55.484: INFO: Successfully updated pod "adopt-release-2ldqq"
    STEP: Checking that the Job readopts the Pod 07/19/23 01:01:55.484
    Jul 19 01:01:55.484: INFO: Waiting up to 15m0s for pod "adopt-release-2ldqq" in namespace "job-9703" to be "adopted"
    Jul 19 01:01:55.488: INFO: Pod "adopt-release-2ldqq": Phase="Running", Reason="", readiness=true. Elapsed: 4.303206ms
    Jul 19 01:01:57.491: INFO: Pod "adopt-release-2ldqq": Phase="Running", Reason="", readiness=true. Elapsed: 2.006690502s
    Jul 19 01:01:57.491: INFO: Pod "adopt-release-2ldqq" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 07/19/23 01:01:57.491
    Jul 19 01:01:57.997: INFO: Successfully updated pod "adopt-release-2ldqq"
    STEP: Checking that the Job releases the Pod 07/19/23 01:01:57.997
    Jul 19 01:01:57.997: INFO: Waiting up to 15m0s for pod "adopt-release-2ldqq" in namespace "job-9703" to be "released"
    Jul 19 01:01:58.001: INFO: Pod "adopt-release-2ldqq": Phase="Running", Reason="", readiness=true. Elapsed: 3.702636ms
    Jul 19 01:02:00.007: INFO: Pod "adopt-release-2ldqq": Phase="Running", Reason="", readiness=true. Elapsed: 2.009138234s
    Jul 19 01:02:00.007: INFO: Pod "adopt-release-2ldqq" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:00.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9703" for this suite. 07/19/23 01:02:00.009
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:00.012
Jul 19 01:02:00.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename dns 07/19/23 01:02:00.013
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:00.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:00.023
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 07/19/23 01:02:00.025
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8248 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8248;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8248 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8248;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8248.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8248.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8248.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8248.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8248.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8248.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8248.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8248.svc;check="$$(dig +notcp +noall +answer +search 45.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.45_udp@PTR;check="$$(dig +tcp +noall +answer +search 45.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.45_tcp@PTR;sleep 1; done
 07/19/23 01:02:00.041
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8248 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8248;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8248 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8248;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8248.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8248.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8248.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8248.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8248.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8248.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8248.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8248.svc;check="$$(dig +notcp +noall +answer +search 45.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.45_udp@PTR;check="$$(dig +tcp +noall +answer +search 45.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.45_tcp@PTR;sleep 1; done
 07/19/23 01:02:00.041
STEP: creating a pod to probe DNS 07/19/23 01:02:00.041
STEP: submitting the pod to kubernetes 07/19/23 01:02:00.041
Jul 19 01:02:00.047: INFO: Waiting up to 15m0s for pod "dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5" in namespace "dns-8248" to be "running"
Jul 19 01:02:00.050: INFO: Pod "dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.13104ms
Jul 19 01:02:02.052: INFO: Pod "dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005406828s
Jul 19 01:02:02.052: INFO: Pod "dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5" satisfied condition "running"
STEP: retrieving the pod 07/19/23 01:02:02.052
STEP: looking for the results for each expected name from probers 07/19/23 01:02:02.054
Jul 19 01:02:02.056: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.057: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.059: INFO: Unable to read wheezy_udp@dns-test-service.dns-8248 from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.061: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8248 from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.062: INFO: Unable to read wheezy_udp@dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.064: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.066: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.067: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.075: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.077: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.079: INFO: Unable to read jessie_udp@dns-test-service.dns-8248 from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.081: INFO: Unable to read jessie_tcp@dns-test-service.dns-8248 from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.084: INFO: Unable to read jessie_udp@dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.085: INFO: Unable to read jessie_tcp@dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
Jul 19 01:02:02.097: INFO: Lookups using dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8248 wheezy_tcp@dns-test-service.dns-8248 wheezy_udp@dns-test-service.dns-8248.svc wheezy_tcp@dns-test-service.dns-8248.svc wheezy_udp@_http._tcp.dns-test-service.dns-8248.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8248.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8248 jessie_tcp@dns-test-service.dns-8248 jessie_udp@dns-test-service.dns-8248.svc jessie_tcp@dns-test-service.dns-8248.svc]

Jul 19 01:02:07.138: INFO: DNS probes using dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5 succeeded

STEP: deleting the pod 07/19/23 01:02:07.138
STEP: deleting the test service 07/19/23 01:02:07.149
STEP: deleting the test headless service 07/19/23 01:02:07.162
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:07.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8248" for this suite. 07/19/23 01:02:07.17
------------------------------
• [SLOW TEST] [7.161 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:00.012
    Jul 19 01:02:00.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename dns 07/19/23 01:02:00.013
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:00.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:00.023
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 07/19/23 01:02:00.025
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8248 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8248;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8248 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8248;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8248.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8248.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8248.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8248.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8248.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8248.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8248.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8248.svc;check="$$(dig +notcp +noall +answer +search 45.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.45_udp@PTR;check="$$(dig +tcp +noall +answer +search 45.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.45_tcp@PTR;sleep 1; done
     07/19/23 01:02:00.041
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8248 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8248;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8248 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8248;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8248.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8248.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8248.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8248.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8248.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8248.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8248.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8248.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8248.svc;check="$$(dig +notcp +noall +answer +search 45.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.45_udp@PTR;check="$$(dig +tcp +noall +answer +search 45.177.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.177.45_tcp@PTR;sleep 1; done
     07/19/23 01:02:00.041
    STEP: creating a pod to probe DNS 07/19/23 01:02:00.041
    STEP: submitting the pod to kubernetes 07/19/23 01:02:00.041
    Jul 19 01:02:00.047: INFO: Waiting up to 15m0s for pod "dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5" in namespace "dns-8248" to be "running"
    Jul 19 01:02:00.050: INFO: Pod "dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.13104ms
    Jul 19 01:02:02.052: INFO: Pod "dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005406828s
    Jul 19 01:02:02.052: INFO: Pod "dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5" satisfied condition "running"
    STEP: retrieving the pod 07/19/23 01:02:02.052
    STEP: looking for the results for each expected name from probers 07/19/23 01:02:02.054
    Jul 19 01:02:02.056: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.057: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.059: INFO: Unable to read wheezy_udp@dns-test-service.dns-8248 from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.061: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8248 from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.062: INFO: Unable to read wheezy_udp@dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.064: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.066: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.067: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.075: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.077: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.079: INFO: Unable to read jessie_udp@dns-test-service.dns-8248 from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.081: INFO: Unable to read jessie_tcp@dns-test-service.dns-8248 from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.084: INFO: Unable to read jessie_udp@dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.085: INFO: Unable to read jessie_tcp@dns-test-service.dns-8248.svc from pod dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5: the server could not find the requested resource (get pods dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5)
    Jul 19 01:02:02.097: INFO: Lookups using dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8248 wheezy_tcp@dns-test-service.dns-8248 wheezy_udp@dns-test-service.dns-8248.svc wheezy_tcp@dns-test-service.dns-8248.svc wheezy_udp@_http._tcp.dns-test-service.dns-8248.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8248.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8248 jessie_tcp@dns-test-service.dns-8248 jessie_udp@dns-test-service.dns-8248.svc jessie_tcp@dns-test-service.dns-8248.svc]

    Jul 19 01:02:07.138: INFO: DNS probes using dns-8248/dns-test-f3619383-9b92-4c35-89b2-0c720aa0bdb5 succeeded

    STEP: deleting the pod 07/19/23 01:02:07.138
    STEP: deleting the test service 07/19/23 01:02:07.149
    STEP: deleting the test headless service 07/19/23 01:02:07.162
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:07.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8248" for this suite. 07/19/23 01:02:07.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:07.173
Jul 19 01:02:07.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 01:02:07.174
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:07.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:07.183
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 07/19/23 01:02:07.185
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:07.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3436" for this suite. 07/19/23 01:02:07.189
------------------------------
• [0.019 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:07.173
    Jul 19 01:02:07.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 01:02:07.174
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:07.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:07.183
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 07/19/23 01:02:07.185
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:07.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3436" for this suite. 07/19/23 01:02:07.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:07.193
Jul 19 01:02:07.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 01:02:07.193
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:07.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:07.208
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-a258694f-14c1-4d8a-a619-2bce4e955d81 07/19/23 01:02:07.21
STEP: Creating a pod to test consume secrets 07/19/23 01:02:07.212
Jul 19 01:02:07.216: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76" in namespace "projected-8348" to be "Succeeded or Failed"
Jul 19 01:02:07.220: INFO: Pod "pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.744258ms
Jul 19 01:02:09.222: INFO: Pod "pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006155635s
Jul 19 01:02:11.222: INFO: Pod "pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006608945s
STEP: Saw pod success 07/19/23 01:02:11.222
Jul 19 01:02:11.223: INFO: Pod "pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76" satisfied condition "Succeeded or Failed"
Jul 19 01:02:11.224: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76 container projected-secret-volume-test: <nil>
STEP: delete the pod 07/19/23 01:02:11.233
Jul 19 01:02:11.239: INFO: Waiting for pod pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76 to disappear
Jul 19 01:02:11.242: INFO: Pod pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:11.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8348" for this suite. 07/19/23 01:02:11.244
------------------------------
• [4.054 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:07.193
    Jul 19 01:02:07.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 01:02:07.193
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:07.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:07.208
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-a258694f-14c1-4d8a-a619-2bce4e955d81 07/19/23 01:02:07.21
    STEP: Creating a pod to test consume secrets 07/19/23 01:02:07.212
    Jul 19 01:02:07.216: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76" in namespace "projected-8348" to be "Succeeded or Failed"
    Jul 19 01:02:07.220: INFO: Pod "pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.744258ms
    Jul 19 01:02:09.222: INFO: Pod "pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006155635s
    Jul 19 01:02:11.222: INFO: Pod "pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006608945s
    STEP: Saw pod success 07/19/23 01:02:11.222
    Jul 19 01:02:11.223: INFO: Pod "pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76" satisfied condition "Succeeded or Failed"
    Jul 19 01:02:11.224: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76 container projected-secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 01:02:11.233
    Jul 19 01:02:11.239: INFO: Waiting for pod pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76 to disappear
    Jul 19 01:02:11.242: INFO: Pod pod-projected-secrets-fc0367bf-1bb9-490c-881c-85c098699d76 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:11.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8348" for this suite. 07/19/23 01:02:11.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:11.247
Jul 19 01:02:11.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 01:02:11.248
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:11.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:11.255
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-0e9550ea-9db5-4910-adc9-069a17a73643 07/19/23 01:02:11.257
STEP: Creating a pod to test consume configMaps 07/19/23 01:02:11.259
Jul 19 01:02:11.263: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7" in namespace "projected-5623" to be "Succeeded or Failed"
Jul 19 01:02:11.264: INFO: Pod "pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.332238ms
Jul 19 01:02:13.267: INFO: Pod "pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004094172s
Jul 19 01:02:15.267: INFO: Pod "pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004334664s
STEP: Saw pod success 07/19/23 01:02:15.267
Jul 19 01:02:15.267: INFO: Pod "pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7" satisfied condition "Succeeded or Failed"
Jul 19 01:02:15.269: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 01:02:15.278
Jul 19 01:02:15.286: INFO: Waiting for pod pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7 to disappear
Jul 19 01:02:15.288: INFO: Pod pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:15.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5623" for this suite. 07/19/23 01:02:15.291
------------------------------
• [4.046 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:11.247
    Jul 19 01:02:11.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 01:02:11.248
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:11.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:11.255
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-0e9550ea-9db5-4910-adc9-069a17a73643 07/19/23 01:02:11.257
    STEP: Creating a pod to test consume configMaps 07/19/23 01:02:11.259
    Jul 19 01:02:11.263: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7" in namespace "projected-5623" to be "Succeeded or Failed"
    Jul 19 01:02:11.264: INFO: Pod "pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.332238ms
    Jul 19 01:02:13.267: INFO: Pod "pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004094172s
    Jul 19 01:02:15.267: INFO: Pod "pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004334664s
    STEP: Saw pod success 07/19/23 01:02:15.267
    Jul 19 01:02:15.267: INFO: Pod "pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7" satisfied condition "Succeeded or Failed"
    Jul 19 01:02:15.269: INFO: Trying to get logs from node controller-0 pod pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 01:02:15.278
    Jul 19 01:02:15.286: INFO: Waiting for pod pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7 to disappear
    Jul 19 01:02:15.288: INFO: Pod pod-projected-configmaps-77bff1f7-b647-4a2c-abcd-db69970300b7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:15.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5623" for this suite. 07/19/23 01:02:15.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:15.295
Jul 19 01:02:15.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename ingressclass 07/19/23 01:02:15.295
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:15.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:15.307
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 07/19/23 01:02:15.309
STEP: getting /apis/networking.k8s.io 07/19/23 01:02:15.31
STEP: getting /apis/networking.k8s.iov1 07/19/23 01:02:15.311
STEP: creating 07/19/23 01:02:15.312
STEP: getting 07/19/23 01:02:15.317
STEP: listing 07/19/23 01:02:15.323
STEP: watching 07/19/23 01:02:15.324
Jul 19 01:02:15.324: INFO: starting watch
STEP: patching 07/19/23 01:02:15.325
STEP: updating 07/19/23 01:02:15.327
Jul 19 01:02:15.329: INFO: waiting for watch events with expected annotations
Jul 19 01:02:15.329: INFO: saw patched and updated annotations
STEP: deleting 07/19/23 01:02:15.329
STEP: deleting a collection 07/19/23 01:02:15.335
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:15.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-4317" for this suite. 07/19/23 01:02:15.344
------------------------------
• [0.052 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:15.295
    Jul 19 01:02:15.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename ingressclass 07/19/23 01:02:15.295
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:15.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:15.307
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 07/19/23 01:02:15.309
    STEP: getting /apis/networking.k8s.io 07/19/23 01:02:15.31
    STEP: getting /apis/networking.k8s.iov1 07/19/23 01:02:15.311
    STEP: creating 07/19/23 01:02:15.312
    STEP: getting 07/19/23 01:02:15.317
    STEP: listing 07/19/23 01:02:15.323
    STEP: watching 07/19/23 01:02:15.324
    Jul 19 01:02:15.324: INFO: starting watch
    STEP: patching 07/19/23 01:02:15.325
    STEP: updating 07/19/23 01:02:15.327
    Jul 19 01:02:15.329: INFO: waiting for watch events with expected annotations
    Jul 19 01:02:15.329: INFO: saw patched and updated annotations
    STEP: deleting 07/19/23 01:02:15.329
    STEP: deleting a collection 07/19/23 01:02:15.335
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:15.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-4317" for this suite. 07/19/23 01:02:15.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:15.347
Jul 19 01:02:15.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir 07/19/23 01:02:15.348
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:15.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:15.358
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 07/19/23 01:02:15.36
Jul 19 01:02:15.363: INFO: Waiting up to 5m0s for pod "pod-da9c2d4f-fe6b-478d-b993-ef955e661f76" in namespace "emptydir-5436" to be "Succeeded or Failed"
Jul 19 01:02:15.367: INFO: Pod "pod-da9c2d4f-fe6b-478d-b993-ef955e661f76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542156ms
Jul 19 01:02:17.370: INFO: Pod "pod-da9c2d4f-fe6b-478d-b993-ef955e661f76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006406688s
Jul 19 01:02:19.370: INFO: Pod "pod-da9c2d4f-fe6b-478d-b993-ef955e661f76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006510517s
STEP: Saw pod success 07/19/23 01:02:19.37
Jul 19 01:02:19.370: INFO: Pod "pod-da9c2d4f-fe6b-478d-b993-ef955e661f76" satisfied condition "Succeeded or Failed"
Jul 19 01:02:19.371: INFO: Trying to get logs from node controller-0 pod pod-da9c2d4f-fe6b-478d-b993-ef955e661f76 container test-container: <nil>
STEP: delete the pod 07/19/23 01:02:19.375
Jul 19 01:02:19.381: INFO: Waiting for pod pod-da9c2d4f-fe6b-478d-b993-ef955e661f76 to disappear
Jul 19 01:02:19.382: INFO: Pod pod-da9c2d4f-fe6b-478d-b993-ef955e661f76 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:19.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5436" for this suite. 07/19/23 01:02:19.384
------------------------------
• [4.041 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:15.347
    Jul 19 01:02:15.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir 07/19/23 01:02:15.348
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:15.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:15.358
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 07/19/23 01:02:15.36
    Jul 19 01:02:15.363: INFO: Waiting up to 5m0s for pod "pod-da9c2d4f-fe6b-478d-b993-ef955e661f76" in namespace "emptydir-5436" to be "Succeeded or Failed"
    Jul 19 01:02:15.367: INFO: Pod "pod-da9c2d4f-fe6b-478d-b993-ef955e661f76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.542156ms
    Jul 19 01:02:17.370: INFO: Pod "pod-da9c2d4f-fe6b-478d-b993-ef955e661f76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006406688s
    Jul 19 01:02:19.370: INFO: Pod "pod-da9c2d4f-fe6b-478d-b993-ef955e661f76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006510517s
    STEP: Saw pod success 07/19/23 01:02:19.37
    Jul 19 01:02:19.370: INFO: Pod "pod-da9c2d4f-fe6b-478d-b993-ef955e661f76" satisfied condition "Succeeded or Failed"
    Jul 19 01:02:19.371: INFO: Trying to get logs from node controller-0 pod pod-da9c2d4f-fe6b-478d-b993-ef955e661f76 container test-container: <nil>
    STEP: delete the pod 07/19/23 01:02:19.375
    Jul 19 01:02:19.381: INFO: Waiting for pod pod-da9c2d4f-fe6b-478d-b993-ef955e661f76 to disappear
    Jul 19 01:02:19.382: INFO: Pod pod-da9c2d4f-fe6b-478d-b993-ef955e661f76 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:19.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5436" for this suite. 07/19/23 01:02:19.384
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:19.388
Jul 19 01:02:19.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename deployment 07/19/23 01:02:19.388
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:19.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:19.399
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 07/19/23 01:02:19.403
STEP: waiting for Deployment to be created 07/19/23 01:02:19.408
STEP: waiting for all Replicas to be Ready 07/19/23 01:02:19.409
Jul 19 01:02:19.410: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 19 01:02:19.410: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 19 01:02:19.414: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 19 01:02:19.414: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 19 01:02:19.423: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 19 01:02:19.423: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 19 01:02:19.437: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 19 01:02:19.437: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 19 01:02:20.210: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 19 01:02:20.210: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 19 01:02:20.976: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 07/19/23 01:02:20.976
W0719 01:02:20.985069      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jul 19 01:02:20.986: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 07/19/23 01:02:20.986
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:20.992: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:20.992: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:21.004: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:21.004: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:21.009: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:21.009: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:21.017: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:21.017: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:21.986: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:21.986: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:21.999: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
STEP: listing Deployments 07/19/23 01:02:21.999
Jul 19 01:02:22.002: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 07/19/23 01:02:22.002
Jul 19 01:02:22.011: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 07/19/23 01:02:22.011
Jul 19 01:02:22.015: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 19 01:02:22.016: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 19 01:02:22.030: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 19 01:02:22.040: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 19 01:02:22.044: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 19 01:02:22.995: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 19 01:02:23.007: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 19 01:02:23.014: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 19 01:02:23.023: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 19 01:02:24.230: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 07/19/23 01:02:24.246
STEP: fetching the DeploymentStatus 07/19/23 01:02:24.25
Jul 19 01:02:24.253: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 3
STEP: deleting the Deployment 07/19/23 01:02:24.254
Jul 19 01:02:24.258: INFO: observed event type MODIFIED
Jul 19 01:02:24.258: INFO: observed event type MODIFIED
Jul 19 01:02:24.258: INFO: observed event type MODIFIED
Jul 19 01:02:24.258: INFO: observed event type MODIFIED
Jul 19 01:02:24.258: INFO: observed event type MODIFIED
Jul 19 01:02:24.258: INFO: observed event type MODIFIED
Jul 19 01:02:24.258: INFO: observed event type MODIFIED
Jul 19 01:02:24.258: INFO: observed event type MODIFIED
Jul 19 01:02:24.258: INFO: observed event type MODIFIED
Jul 19 01:02:24.259: INFO: observed event type MODIFIED
Jul 19 01:02:24.259: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 19 01:02:24.260: INFO: Log out all the ReplicaSets if there is no deployment created
Jul 19 01:02:24.262: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-6799  6a3357f2-bec4-4955-8e54-3f449735370e 88789 2 2023-07-19 01:02:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 4f6782d3-5de4-43ca-8b1c-12442ecd595c 0xc0034c9567 0xc0034c9568}] [] [{kube-controller-manager Update apps/v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f6782d3-5de4-43ca-8b1c-12442ecd595c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:02:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034c95f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jul 19 01:02:24.265: INFO: pod: "test-deployment-7b7876f9d6-7cvd5":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-7cvd5 test-deployment-7b7876f9d6- deployment-6799  839a671f-c080-465a-bad6-396ed5309611 88742 0 2023-07-19 01:02:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:52054ecb24572aa710bc22f4fba83841eff64ed880061e90470d343788e0c854 cni.projectcalico.org/podIP:172.16.192.97/32 cni.projectcalico.org/podIPs:172.16.192.97/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.97"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.97"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6a3357f2-bec4-4955-8e54-3f449735370e 0xc0034c9ab7 0xc0034c9ab8}] [] [{calico Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a3357f2-bec4-4955-8e54-3f449735370e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g68th,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g68th,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.97,StartTime:2023-07-19 01:02:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 01:02:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6a0789722cad7df24a03feaca4199516a0995c7211a83f5714b947e0b3658b88,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul 19 01:02:24.265: INFO: pod: "test-deployment-7b7876f9d6-c697q":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-c697q test-deployment-7b7876f9d6- deployment-6799  d95cb2d1-5914-4199-97d9-cb1d983afd8f 88788 0 2023-07-19 01:02:23 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:95eb3358333520505f3d85e29627ba2e79d352c5c79d6a953dbc3b752e0c5306 cni.projectcalico.org/podIP:172.16.166.181/32 cni.projectcalico.org/podIPs:172.16.166.181/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.181"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.181"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6a3357f2-bec4-4955-8e54-3f449735370e 0xc0034c9ce7 0xc0034c9ce8}] [] [{calico Update v1 2023-07-19 01:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 01:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a3357f2-bec4-4955-8e54-3f449735370e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 01:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 01:02:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78xnb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78xnb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.181,StartTime:2023-07-19 01:02:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 01:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://32f6061c2a79750315e2291e2e9b9f1c8aab016d54041a568487db0f09e044cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul 19 01:02:24.265: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-6799  e8c2db00-30b5-42ac-8ce1-92820d5dedc5 88797 4 2023-07-19 01:02:20 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 4f6782d3-5de4-43ca-8b1c-12442ecd595c 0xc0034c9657 0xc0034c9658}] [] [{kube-controller-manager Update apps/v1 2023-07-19 01:02:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f6782d3-5de4-43ca-8b1c-12442ecd595c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:02:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034c96e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jul 19 01:02:24.267: INFO: pod: "test-deployment-7df74c55ff-5lks5":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-5lks5 test-deployment-7df74c55ff- deployment-6799  558a753a-e093-4944-9bb1-6961febb9871 88793 0 2023-07-19 01:02:20 +0000 UTC 2023-07-19 01:02:25 +0000 UTC 0xc00865b478 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:7c14fa564f606c15941316dc36dde36474eff2577a9d4c5cdfd1930bd303b3f3 cni.projectcalico.org/podIP:172.16.192.95/32 cni.projectcalico.org/podIPs:172.16.192.95/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.95"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.192.95"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-7df74c55ff e8c2db00-30b5-42ac-8ce1-92820d5dedc5 0xc00865b4c7 0xc00865b4c8}] [] [{kube-controller-manager Update v1 2023-07-19 01:02:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8c2db00-30b5-42ac-8ce1-92820d5dedc5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 01:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-07-19 01:02:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-07-19 01:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2l6bd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2l6bd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.95,StartTime:2023-07-19 01:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 01:02:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://9fbe03194166a487c664c864f6c8ced7b092dabc1eaf5c6b3f2e36c02f93624b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul 19 01:02:24.267: INFO: pod: "test-deployment-7df74c55ff-gz8hq":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-gz8hq test-deployment-7df74c55ff- deployment-6799  2cbbccf9-f18f-4614-9ccb-b7206632bf13 88765 0 2023-07-19 01:02:22 +0000 UTC 2023-07-19 01:02:23 +0000 UTC 0xc00865b6d0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:70c689e0319db7a6f36cf54fde76bfe66f75b4be0f42706d42442d3edbd8b33a cni.projectcalico.org/podIP:172.16.166.163/32 cni.projectcalico.org/podIPs:172.16.166.163/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.163"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.163"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-deployment-7df74c55ff e8c2db00-30b5-42ac-8ce1-92820d5dedc5 0xc00865b707 0xc00865b708}] [] [{calico Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8c2db00-30b5-42ac-8ce1-92820d5dedc5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 01:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7stc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7stc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.163,StartTime:2023-07-19 01:02:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 01:02:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://8d50466e3e0b5a41c187970f629d355dd23c4475784d7e45d6718372e75186d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul 19 01:02:24.267: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-6799  f669dd2a-d198-477b-b01e-47ef577c5ca9 88688 3 2023-07-19 01:02:19 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 4f6782d3-5de4-43ca-8b1c-12442ecd595c 0xc0034c9747 0xc0034c9748}] [] [{kube-controller-manager Update apps/v1 2023-07-19 01:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f6782d3-5de4-43ca-8b1c-12442ecd595c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:02:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034c97e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:24.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6799" for this suite. 07/19/23 01:02:24.274
------------------------------
• [4.889 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:19.388
    Jul 19 01:02:19.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename deployment 07/19/23 01:02:19.388
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:19.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:19.399
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 07/19/23 01:02:19.403
    STEP: waiting for Deployment to be created 07/19/23 01:02:19.408
    STEP: waiting for all Replicas to be Ready 07/19/23 01:02:19.409
    Jul 19 01:02:19.410: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 19 01:02:19.410: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 19 01:02:19.414: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 19 01:02:19.414: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 19 01:02:19.423: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 19 01:02:19.423: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 19 01:02:19.437: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 19 01:02:19.437: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jul 19 01:02:20.210: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jul 19 01:02:20.210: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jul 19 01:02:20.976: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 07/19/23 01:02:20.976
    W0719 01:02:20.985069      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jul 19 01:02:20.986: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 07/19/23 01:02:20.986
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 0
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:20.987: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:20.992: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:20.992: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:21.004: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:21.004: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:21.009: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:21.009: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:21.017: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:21.017: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:21.986: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:21.986: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:21.999: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    STEP: listing Deployments 07/19/23 01:02:21.999
    Jul 19 01:02:22.002: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 07/19/23 01:02:22.002
    Jul 19 01:02:22.011: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 07/19/23 01:02:22.011
    Jul 19 01:02:22.015: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 19 01:02:22.016: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 19 01:02:22.030: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 19 01:02:22.040: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 19 01:02:22.044: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 19 01:02:22.995: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 19 01:02:23.007: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 19 01:02:23.014: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 19 01:02:23.023: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jul 19 01:02:24.230: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 07/19/23 01:02:24.246
    STEP: fetching the DeploymentStatus 07/19/23 01:02:24.25
    Jul 19 01:02:24.253: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 1
    Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 2
    Jul 19 01:02:24.254: INFO: observed Deployment test-deployment in namespace deployment-6799 with ReadyReplicas 3
    STEP: deleting the Deployment 07/19/23 01:02:24.254
    Jul 19 01:02:24.258: INFO: observed event type MODIFIED
    Jul 19 01:02:24.258: INFO: observed event type MODIFIED
    Jul 19 01:02:24.258: INFO: observed event type MODIFIED
    Jul 19 01:02:24.258: INFO: observed event type MODIFIED
    Jul 19 01:02:24.258: INFO: observed event type MODIFIED
    Jul 19 01:02:24.258: INFO: observed event type MODIFIED
    Jul 19 01:02:24.258: INFO: observed event type MODIFIED
    Jul 19 01:02:24.258: INFO: observed event type MODIFIED
    Jul 19 01:02:24.258: INFO: observed event type MODIFIED
    Jul 19 01:02:24.259: INFO: observed event type MODIFIED
    Jul 19 01:02:24.259: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 19 01:02:24.260: INFO: Log out all the ReplicaSets if there is no deployment created
    Jul 19 01:02:24.262: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-6799  6a3357f2-bec4-4955-8e54-3f449735370e 88789 2 2023-07-19 01:02:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 4f6782d3-5de4-43ca-8b1c-12442ecd595c 0xc0034c9567 0xc0034c9568}] [] [{kube-controller-manager Update apps/v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f6782d3-5de4-43ca-8b1c-12442ecd595c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:02:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034c95f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jul 19 01:02:24.265: INFO: pod: "test-deployment-7b7876f9d6-7cvd5":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-7cvd5 test-deployment-7b7876f9d6- deployment-6799  839a671f-c080-465a-bad6-396ed5309611 88742 0 2023-07-19 01:02:22 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:52054ecb24572aa710bc22f4fba83841eff64ed880061e90470d343788e0c854 cni.projectcalico.org/podIP:172.16.192.97/32 cni.projectcalico.org/podIPs:172.16.192.97/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.97"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.97"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6a3357f2-bec4-4955-8e54-3f449735370e 0xc0034c9ab7 0xc0034c9ab8}] [] [{calico Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a3357f2-bec4-4955-8e54-3f449735370e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g68th,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g68th,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.97,StartTime:2023-07-19 01:02:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 01:02:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6a0789722cad7df24a03feaca4199516a0995c7211a83f5714b947e0b3658b88,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jul 19 01:02:24.265: INFO: pod: "test-deployment-7b7876f9d6-c697q":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-c697q test-deployment-7b7876f9d6- deployment-6799  d95cb2d1-5914-4199-97d9-cb1d983afd8f 88788 0 2023-07-19 01:02:23 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:95eb3358333520505f3d85e29627ba2e79d352c5c79d6a953dbc3b752e0c5306 cni.projectcalico.org/podIP:172.16.166.181/32 cni.projectcalico.org/podIPs:172.16.166.181/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.181"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.181"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6a3357f2-bec4-4955-8e54-3f449735370e 0xc0034c9ce7 0xc0034c9ce8}] [] [{calico Update v1 2023-07-19 01:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 01:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a3357f2-bec4-4955-8e54-3f449735370e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 01:02:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 01:02:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78xnb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78xnb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.181,StartTime:2023-07-19 01:02:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 01:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://32f6061c2a79750315e2291e2e9b9f1c8aab016d54041a568487db0f09e044cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jul 19 01:02:24.265: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-6799  e8c2db00-30b5-42ac-8ce1-92820d5dedc5 88797 4 2023-07-19 01:02:20 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 4f6782d3-5de4-43ca-8b1c-12442ecd595c 0xc0034c9657 0xc0034c9658}] [] [{kube-controller-manager Update apps/v1 2023-07-19 01:02:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f6782d3-5de4-43ca-8b1c-12442ecd595c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:02:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034c96e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jul 19 01:02:24.267: INFO: pod: "test-deployment-7df74c55ff-5lks5":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-5lks5 test-deployment-7df74c55ff- deployment-6799  558a753a-e093-4944-9bb1-6961febb9871 88793 0 2023-07-19 01:02:20 +0000 UTC 2023-07-19 01:02:25 +0000 UTC 0xc00865b478 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:7c14fa564f606c15941316dc36dde36474eff2577a9d4c5cdfd1930bd303b3f3 cni.projectcalico.org/podIP:172.16.192.95/32 cni.projectcalico.org/podIPs:172.16.192.95/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.95"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.192.95"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-7df74c55ff e8c2db00-30b5-42ac-8ce1-92820d5dedc5 0xc00865b4c7 0xc00865b4c8}] [] [{kube-controller-manager Update v1 2023-07-19 01:02:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8c2db00-30b5-42ac-8ce1-92820d5dedc5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-07-19 01:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-07-19 01:02:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.192.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-07-19 01:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2l6bd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2l6bd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.2,PodIP:172.16.192.95,StartTime:2023-07-19 01:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 01:02:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://9fbe03194166a487c664c864f6c8ced7b092dabc1eaf5c6b3f2e36c02f93624b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.192.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jul 19 01:02:24.267: INFO: pod: "test-deployment-7df74c55ff-gz8hq":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-gz8hq test-deployment-7df74c55ff- deployment-6799  2cbbccf9-f18f-4614-9ccb-b7206632bf13 88765 0 2023-07-19 01:02:22 +0000 UTC 2023-07-19 01:02:23 +0000 UTC 0xc00865b6d0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:70c689e0319db7a6f36cf54fde76bfe66f75b4be0f42706d42442d3edbd8b33a cni.projectcalico.org/podIP:172.16.166.163/32 cni.projectcalico.org/podIPs:172.16.166.163/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.163"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.163"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-deployment-7df74c55ff e8c2db00-30b5-42ac-8ce1-92820d5dedc5 0xc00865b707 0xc00865b708}] [] [{calico Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e8c2db00-30b5-42ac-8ce1-92820d5dedc5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 01:02:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 01:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7stc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7stc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:02:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.163,StartTime:2023-07-19 01:02:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 01:02:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://8d50466e3e0b5a41c187970f629d355dd23c4475784d7e45d6718372e75186d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jul 19 01:02:24.267: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-6799  f669dd2a-d198-477b-b01e-47ef577c5ca9 88688 3 2023-07-19 01:02:19 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 4f6782d3-5de4-43ca-8b1c-12442ecd595c 0xc0034c9747 0xc0034c9748}] [] [{kube-controller-manager Update apps/v1 2023-07-19 01:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f6782d3-5de4-43ca-8b1c-12442ecd595c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:02:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034c97e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:24.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6799" for this suite. 07/19/23 01:02:24.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:24.277
Jul 19 01:02:24.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 01:02:24.278
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:24.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:24.286
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9d9c0e87-32de-4059-a57d-baab986cd591 07/19/23 01:02:24.29
STEP: Creating the pod 07/19/23 01:02:24.292
Jul 19 01:02:24.296: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12" in namespace "projected-7341" to be "running and ready"
Jul 19 01:02:24.299: INFO: Pod "pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12": Phase="Pending", Reason="", readiness=false. Elapsed: 3.471636ms
Jul 19 01:02:24.300: INFO: The phase of Pod pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 01:02:26.302: INFO: Pod "pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12": Phase="Running", Reason="", readiness=true. Elapsed: 2.005630315s
Jul 19 01:02:26.302: INFO: The phase of Pod pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12 is Running (Ready = true)
Jul 19 01:02:26.302: INFO: Pod "pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-9d9c0e87-32de-4059-a57d-baab986cd591 07/19/23 01:02:26.306
STEP: waiting to observe update in volume 07/19/23 01:02:26.308
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:30.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7341" for this suite. 07/19/23 01:02:30.322
------------------------------
• [SLOW TEST] [6.047 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:24.277
    Jul 19 01:02:24.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 01:02:24.278
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:24.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:24.286
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-9d9c0e87-32de-4059-a57d-baab986cd591 07/19/23 01:02:24.29
    STEP: Creating the pod 07/19/23 01:02:24.292
    Jul 19 01:02:24.296: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12" in namespace "projected-7341" to be "running and ready"
    Jul 19 01:02:24.299: INFO: Pod "pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12": Phase="Pending", Reason="", readiness=false. Elapsed: 3.471636ms
    Jul 19 01:02:24.300: INFO: The phase of Pod pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 01:02:26.302: INFO: Pod "pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12": Phase="Running", Reason="", readiness=true. Elapsed: 2.005630315s
    Jul 19 01:02:26.302: INFO: The phase of Pod pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12 is Running (Ready = true)
    Jul 19 01:02:26.302: INFO: Pod "pod-projected-configmaps-43fe89ad-516f-40b1-887f-30adb411ab12" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-9d9c0e87-32de-4059-a57d-baab986cd591 07/19/23 01:02:26.306
    STEP: waiting to observe update in volume 07/19/23 01:02:26.308
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:30.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7341" for this suite. 07/19/23 01:02:30.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:30.325
Jul 19 01:02:30.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 01:02:30.325
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:30.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:30.334
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 07/19/23 01:02:30.335
Jul 19 01:02:30.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2581 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jul 19 01:02:30.398: INFO: stderr: ""
Jul 19 01:02:30.398: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 07/19/23 01:02:30.398
Jul 19 01:02:30.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2581 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jul 19 01:02:30.597: INFO: stderr: ""
Jul 19 01:02:30.597: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 07/19/23 01:02:30.597
Jul 19 01:02:30.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2581 delete pods e2e-test-httpd-pod'
Jul 19 01:02:32.253: INFO: stderr: ""
Jul 19 01:02:32.254: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:32.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2581" for this suite. 07/19/23 01:02:32.256
------------------------------
• [1.934 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:30.325
    Jul 19 01:02:30.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 01:02:30.325
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:30.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:30.334
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 07/19/23 01:02:30.335
    Jul 19 01:02:30.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2581 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jul 19 01:02:30.398: INFO: stderr: ""
    Jul 19 01:02:30.398: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 07/19/23 01:02:30.398
    Jul 19 01:02:30.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2581 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jul 19 01:02:30.597: INFO: stderr: ""
    Jul 19 01:02:30.597: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 07/19/23 01:02:30.597
    Jul 19 01:02:30.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2581 delete pods e2e-test-httpd-pod'
    Jul 19 01:02:32.253: INFO: stderr: ""
    Jul 19 01:02:32.254: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:32.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2581" for this suite. 07/19/23 01:02:32.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:32.26
Jul 19 01:02:32.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename csistoragecapacity 07/19/23 01:02:32.26
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:32.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:32.268
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 07/19/23 01:02:32.27
STEP: getting /apis/storage.k8s.io 07/19/23 01:02:32.271
STEP: getting /apis/storage.k8s.io/v1 07/19/23 01:02:32.272
STEP: creating 07/19/23 01:02:32.273
STEP: watching 07/19/23 01:02:32.282
Jul 19 01:02:32.282: INFO: starting watch
STEP: getting 07/19/23 01:02:32.286
STEP: listing in namespace 07/19/23 01:02:32.287
STEP: listing across namespaces 07/19/23 01:02:32.289
STEP: patching 07/19/23 01:02:32.29
STEP: updating 07/19/23 01:02:32.292
Jul 19 01:02:32.294: INFO: waiting for watch events with expected annotations in namespace
Jul 19 01:02:32.294: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 07/19/23 01:02:32.294
STEP: deleting a collection 07/19/23 01:02:32.3
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:32.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-167" for this suite. 07/19/23 01:02:32.308
------------------------------
• [0.051 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:32.26
    Jul 19 01:02:32.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename csistoragecapacity 07/19/23 01:02:32.26
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:32.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:32.268
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 07/19/23 01:02:32.27
    STEP: getting /apis/storage.k8s.io 07/19/23 01:02:32.271
    STEP: getting /apis/storage.k8s.io/v1 07/19/23 01:02:32.272
    STEP: creating 07/19/23 01:02:32.273
    STEP: watching 07/19/23 01:02:32.282
    Jul 19 01:02:32.282: INFO: starting watch
    STEP: getting 07/19/23 01:02:32.286
    STEP: listing in namespace 07/19/23 01:02:32.287
    STEP: listing across namespaces 07/19/23 01:02:32.289
    STEP: patching 07/19/23 01:02:32.29
    STEP: updating 07/19/23 01:02:32.292
    Jul 19 01:02:32.294: INFO: waiting for watch events with expected annotations in namespace
    Jul 19 01:02:32.294: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 07/19/23 01:02:32.294
    STEP: deleting a collection 07/19/23 01:02:32.3
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:32.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-167" for this suite. 07/19/23 01:02:32.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:32.311
Jul 19 01:02:32.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 01:02:32.312
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:32.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:32.324
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-7eb97236-5b8a-4a87-9ab4-d8fcd06e113c 07/19/23 01:02:32.327
STEP: Creating secret with name s-test-opt-upd-6abac0a1-912c-47f9-b0cf-5c8abadbc37d 07/19/23 01:02:32.329
STEP: Creating the pod 07/19/23 01:02:32.331
Jul 19 01:02:32.335: INFO: Waiting up to 5m0s for pod "pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2" in namespace "secrets-9986" to be "running and ready"
Jul 19 01:02:32.338: INFO: Pod "pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778527ms
Jul 19 01:02:32.338: INFO: The phase of Pod pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2 is Pending, waiting for it to be Running (with Ready = true)
Jul 19 01:02:34.340: INFO: Pod "pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004840044s
Jul 19 01:02:34.340: INFO: The phase of Pod pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2 is Running (Ready = true)
Jul 19 01:02:34.340: INFO: Pod "pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-7eb97236-5b8a-4a87-9ab4-d8fcd06e113c 07/19/23 01:02:34.351
STEP: Updating secret s-test-opt-upd-6abac0a1-912c-47f9-b0cf-5c8abadbc37d 07/19/23 01:02:34.354
STEP: Creating secret with name s-test-opt-create-6bb869b8-ea13-4a51-8b6e-f2957667f3f2 07/19/23 01:02:34.356
STEP: waiting to observe update in volume 07/19/23 01:02:34.36
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:36.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9986" for this suite. 07/19/23 01:02:36.378
------------------------------
• [4.069 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:32.311
    Jul 19 01:02:32.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 01:02:32.312
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:32.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:32.324
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-7eb97236-5b8a-4a87-9ab4-d8fcd06e113c 07/19/23 01:02:32.327
    STEP: Creating secret with name s-test-opt-upd-6abac0a1-912c-47f9-b0cf-5c8abadbc37d 07/19/23 01:02:32.329
    STEP: Creating the pod 07/19/23 01:02:32.331
    Jul 19 01:02:32.335: INFO: Waiting up to 5m0s for pod "pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2" in namespace "secrets-9986" to be "running and ready"
    Jul 19 01:02:32.338: INFO: Pod "pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778527ms
    Jul 19 01:02:32.338: INFO: The phase of Pod pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2 is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 01:02:34.340: INFO: Pod "pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004840044s
    Jul 19 01:02:34.340: INFO: The phase of Pod pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2 is Running (Ready = true)
    Jul 19 01:02:34.340: INFO: Pod "pod-secrets-cd786210-2856-4505-88d5-9cd2e25b28c2" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-7eb97236-5b8a-4a87-9ab4-d8fcd06e113c 07/19/23 01:02:34.351
    STEP: Updating secret s-test-opt-upd-6abac0a1-912c-47f9-b0cf-5c8abadbc37d 07/19/23 01:02:34.354
    STEP: Creating secret with name s-test-opt-create-6bb869b8-ea13-4a51-8b6e-f2957667f3f2 07/19/23 01:02:34.356
    STEP: waiting to observe update in volume 07/19/23 01:02:34.36
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:36.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9986" for this suite. 07/19/23 01:02:36.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:36.381
Jul 19 01:02:36.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 01:02:36.382
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:36.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:36.394
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4905 07/19/23 01:02:36.396
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 07/19/23 01:02:36.406
STEP: creating service externalsvc in namespace services-4905 07/19/23 01:02:36.406
STEP: creating replication controller externalsvc in namespace services-4905 07/19/23 01:02:36.415
I0719 01:02:36.418989      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4905, replica count: 2
I0719 01:02:39.469485      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 07/19/23 01:02:39.471
Jul 19 01:02:39.481: INFO: Creating new exec pod
Jul 19 01:02:39.485: INFO: Waiting up to 5m0s for pod "execpodndb7j" in namespace "services-4905" to be "running"
Jul 19 01:02:39.486: INFO: Pod "execpodndb7j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.363557ms
Jul 19 01:02:41.489: INFO: Pod "execpodndb7j": Phase="Running", Reason="", readiness=true. Elapsed: 2.00371111s
Jul 19 01:02:41.489: INFO: Pod "execpodndb7j" satisfied condition "running"
Jul 19 01:02:41.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4905 exec execpodndb7j -- /bin/sh -x -c nslookup clusterip-service.services-4905.svc.cluster.local'
Jul 19 01:02:41.625: INFO: stderr: "+ nslookup clusterip-service.services-4905.svc.cluster.local\n"
Jul 19 01:02:41.625: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-4905.svc.cluster.local\tcanonical name = externalsvc.services-4905.svc.cluster.local.\nName:\texternalsvc.services-4905.svc.cluster.local\nAddress: 10.97.183.20\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4905, will wait for the garbage collector to delete the pods 07/19/23 01:02:41.625
Jul 19 01:02:41.679: INFO: Deleting ReplicationController externalsvc took: 2.635317ms
Jul 19 01:02:41.780: INFO: Terminating ReplicationController externalsvc pods took: 100.44301ms
Jul 19 01:02:43.394: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:43.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4905" for this suite. 07/19/23 01:02:43.403
------------------------------
• [SLOW TEST] [7.027 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:36.381
    Jul 19 01:02:36.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 01:02:36.382
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:36.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:36.394
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4905 07/19/23 01:02:36.396
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 07/19/23 01:02:36.406
    STEP: creating service externalsvc in namespace services-4905 07/19/23 01:02:36.406
    STEP: creating replication controller externalsvc in namespace services-4905 07/19/23 01:02:36.415
    I0719 01:02:36.418989      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4905, replica count: 2
    I0719 01:02:39.469485      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 07/19/23 01:02:39.471
    Jul 19 01:02:39.481: INFO: Creating new exec pod
    Jul 19 01:02:39.485: INFO: Waiting up to 5m0s for pod "execpodndb7j" in namespace "services-4905" to be "running"
    Jul 19 01:02:39.486: INFO: Pod "execpodndb7j": Phase="Pending", Reason="", readiness=false. Elapsed: 1.363557ms
    Jul 19 01:02:41.489: INFO: Pod "execpodndb7j": Phase="Running", Reason="", readiness=true. Elapsed: 2.00371111s
    Jul 19 01:02:41.489: INFO: Pod "execpodndb7j" satisfied condition "running"
    Jul 19 01:02:41.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-4905 exec execpodndb7j -- /bin/sh -x -c nslookup clusterip-service.services-4905.svc.cluster.local'
    Jul 19 01:02:41.625: INFO: stderr: "+ nslookup clusterip-service.services-4905.svc.cluster.local\n"
    Jul 19 01:02:41.625: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-4905.svc.cluster.local\tcanonical name = externalsvc.services-4905.svc.cluster.local.\nName:\texternalsvc.services-4905.svc.cluster.local\nAddress: 10.97.183.20\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4905, will wait for the garbage collector to delete the pods 07/19/23 01:02:41.625
    Jul 19 01:02:41.679: INFO: Deleting ReplicationController externalsvc took: 2.635317ms
    Jul 19 01:02:41.780: INFO: Terminating ReplicationController externalsvc pods took: 100.44301ms
    Jul 19 01:02:43.394: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:43.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4905" for this suite. 07/19/23 01:02:43.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:43.409
Jul 19 01:02:43.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 01:02:43.41
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:43.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:43.417
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-435c4758-426c-4083-a87f-60e75090de51 07/19/23 01:02:43.419
STEP: Creating a pod to test consume configMaps 07/19/23 01:02:43.421
Jul 19 01:02:43.425: INFO: Waiting up to 5m0s for pod "pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88" in namespace "configmap-6669" to be "Succeeded or Failed"
Jul 19 01:02:43.426: INFO: Pod "pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.226635ms
Jul 19 01:02:45.429: INFO: Pod "pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004195878s
Jul 19 01:02:47.429: INFO: Pod "pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004011601s
STEP: Saw pod success 07/19/23 01:02:47.429
Jul 19 01:02:47.429: INFO: Pod "pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88" satisfied condition "Succeeded or Failed"
Jul 19 01:02:47.431: INFO: Trying to get logs from node controller-1 pod pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 01:02:47.435
Jul 19 01:02:47.441: INFO: Waiting for pod pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88 to disappear
Jul 19 01:02:47.442: INFO: Pod pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:47.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6669" for this suite. 07/19/23 01:02:47.445
------------------------------
• [4.042 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:43.409
    Jul 19 01:02:43.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 01:02:43.41
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:43.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:43.417
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-435c4758-426c-4083-a87f-60e75090de51 07/19/23 01:02:43.419
    STEP: Creating a pod to test consume configMaps 07/19/23 01:02:43.421
    Jul 19 01:02:43.425: INFO: Waiting up to 5m0s for pod "pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88" in namespace "configmap-6669" to be "Succeeded or Failed"
    Jul 19 01:02:43.426: INFO: Pod "pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.226635ms
    Jul 19 01:02:45.429: INFO: Pod "pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004195878s
    Jul 19 01:02:47.429: INFO: Pod "pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004011601s
    STEP: Saw pod success 07/19/23 01:02:47.429
    Jul 19 01:02:47.429: INFO: Pod "pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88" satisfied condition "Succeeded or Failed"
    Jul 19 01:02:47.431: INFO: Trying to get logs from node controller-1 pod pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 01:02:47.435
    Jul 19 01:02:47.441: INFO: Waiting for pod pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88 to disappear
    Jul 19 01:02:47.442: INFO: Pod pod-configmaps-1cc6a8fb-6048-489c-9010-eba788b62d88 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:47.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6669" for this suite. 07/19/23 01:02:47.445
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:47.451
Jul 19 01:02:47.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 01:02:47.452
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:47.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:47.473
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 07/19/23 01:02:47.475
Jul 19 01:02:47.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 create -f -'
Jul 19 01:02:48.159: INFO: stderr: ""
Jul 19 01:02:48.159: INFO: stdout: "pod/pause created\n"
Jul 19 01:02:48.159: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 19 01:02:48.159: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2884" to be "running and ready"
Jul 19 01:02:48.161: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.741515ms
Jul 19 01:02:48.161: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'controller-1' to be 'Running' but was 'Pending'
Jul 19 01:02:50.163: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.003696792s
Jul 19 01:02:50.163: INFO: Pod "pause" satisfied condition "running and ready"
Jul 19 01:02:50.163: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 07/19/23 01:02:50.163
Jul 19 01:02:50.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 label pods pause testing-label=testing-label-value'
Jul 19 01:02:50.229: INFO: stderr: ""
Jul 19 01:02:50.229: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 07/19/23 01:02:50.229
Jul 19 01:02:50.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 get pod pause -L testing-label'
Jul 19 01:02:50.307: INFO: stderr: ""
Jul 19 01:02:50.307: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 07/19/23 01:02:50.307
Jul 19 01:02:50.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 label pods pause testing-label-'
Jul 19 01:02:50.376: INFO: stderr: ""
Jul 19 01:02:50.376: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 07/19/23 01:02:50.376
Jul 19 01:02:50.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 get pod pause -L testing-label'
Jul 19 01:02:50.438: INFO: stderr: ""
Jul 19 01:02:50.438: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 07/19/23 01:02:50.438
Jul 19 01:02:50.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 delete --grace-period=0 --force -f -'
Jul 19 01:02:50.501: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 19 01:02:50.501: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 19 01:02:50.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 get rc,svc -l name=pause --no-headers'
Jul 19 01:02:50.568: INFO: stderr: "No resources found in kubectl-2884 namespace.\n"
Jul 19 01:02:50.568: INFO: stdout: ""
Jul 19 01:02:50.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 19 01:02:50.627: INFO: stderr: ""
Jul 19 01:02:50.627: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:50.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2884" for this suite. 07/19/23 01:02:50.63
------------------------------
• [3.181 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:47.451
    Jul 19 01:02:47.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 01:02:47.452
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:47.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:47.473
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 07/19/23 01:02:47.475
    Jul 19 01:02:47.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 create -f -'
    Jul 19 01:02:48.159: INFO: stderr: ""
    Jul 19 01:02:48.159: INFO: stdout: "pod/pause created\n"
    Jul 19 01:02:48.159: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jul 19 01:02:48.159: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2884" to be "running and ready"
    Jul 19 01:02:48.161: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.741515ms
    Jul 19 01:02:48.161: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'controller-1' to be 'Running' but was 'Pending'
    Jul 19 01:02:50.163: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.003696792s
    Jul 19 01:02:50.163: INFO: Pod "pause" satisfied condition "running and ready"
    Jul 19 01:02:50.163: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 07/19/23 01:02:50.163
    Jul 19 01:02:50.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 label pods pause testing-label=testing-label-value'
    Jul 19 01:02:50.229: INFO: stderr: ""
    Jul 19 01:02:50.229: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 07/19/23 01:02:50.229
    Jul 19 01:02:50.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 get pod pause -L testing-label'
    Jul 19 01:02:50.307: INFO: stderr: ""
    Jul 19 01:02:50.307: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 07/19/23 01:02:50.307
    Jul 19 01:02:50.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 label pods pause testing-label-'
    Jul 19 01:02:50.376: INFO: stderr: ""
    Jul 19 01:02:50.376: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 07/19/23 01:02:50.376
    Jul 19 01:02:50.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 get pod pause -L testing-label'
    Jul 19 01:02:50.438: INFO: stderr: ""
    Jul 19 01:02:50.438: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 07/19/23 01:02:50.438
    Jul 19 01:02:50.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 delete --grace-period=0 --force -f -'
    Jul 19 01:02:50.501: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jul 19 01:02:50.501: INFO: stdout: "pod \"pause\" force deleted\n"
    Jul 19 01:02:50.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 get rc,svc -l name=pause --no-headers'
    Jul 19 01:02:50.568: INFO: stderr: "No resources found in kubectl-2884 namespace.\n"
    Jul 19 01:02:50.568: INFO: stdout: ""
    Jul 19 01:02:50.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-2884 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jul 19 01:02:50.627: INFO: stderr: ""
    Jul 19 01:02:50.627: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:50.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2884" for this suite. 07/19/23 01:02:50.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:50.633
Jul 19 01:02:50.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-lifecycle-hook 07/19/23 01:02:50.634
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:50.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:50.646
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 07/19/23 01:02:50.65
Jul 19 01:02:50.654: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7180" to be "running and ready"
Jul 19 01:02:50.655: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.311596ms
Jul 19 01:02:50.655: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 19 01:02:52.658: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004517772s
Jul 19 01:02:52.658: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jul 19 01:02:52.658: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 07/19/23 01:02:52.66
Jul 19 01:02:52.662: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7180" to be "running and ready"
Jul 19 01:02:52.667: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231212ms
Jul 19 01:02:52.667: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 19 01:02:54.669: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00690145s
Jul 19 01:02:54.669: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jul 19 01:02:54.669: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 07/19/23 01:02:54.671
STEP: delete the pod with lifecycle hook 07/19/23 01:02:54.675
Jul 19 01:02:54.678: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 01:02:54.682: INFO: Pod pod-with-poststart-http-hook still exists
Jul 19 01:02:56.683: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 01:02:56.685: INFO: Pod pod-with-poststart-http-hook still exists
Jul 19 01:02:58.682: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 19 01:02:58.684: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jul 19 01:02:58.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7180" for this suite. 07/19/23 01:02:58.686
------------------------------
• [SLOW TEST] [8.056 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:50.633
    Jul 19 01:02:50.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-lifecycle-hook 07/19/23 01:02:50.634
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:50.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:50.646
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 07/19/23 01:02:50.65
    Jul 19 01:02:50.654: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7180" to be "running and ready"
    Jul 19 01:02:50.655: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.311596ms
    Jul 19 01:02:50.655: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 01:02:52.658: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004517772s
    Jul 19 01:02:52.658: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jul 19 01:02:52.658: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 07/19/23 01:02:52.66
    Jul 19 01:02:52.662: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-7180" to be "running and ready"
    Jul 19 01:02:52.667: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231212ms
    Jul 19 01:02:52.667: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 01:02:54.669: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00690145s
    Jul 19 01:02:54.669: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jul 19 01:02:54.669: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 07/19/23 01:02:54.671
    STEP: delete the pod with lifecycle hook 07/19/23 01:02:54.675
    Jul 19 01:02:54.678: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jul 19 01:02:54.682: INFO: Pod pod-with-poststart-http-hook still exists
    Jul 19 01:02:56.683: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jul 19 01:02:56.685: INFO: Pod pod-with-poststart-http-hook still exists
    Jul 19 01:02:58.682: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jul 19 01:02:58.684: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:02:58.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7180" for this suite. 07/19/23 01:02:58.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:02:58.69
Jul 19 01:02:58.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename services 07/19/23 01:02:58.691
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:58.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:58.7
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-607 07/19/23 01:02:58.702
STEP: changing the ExternalName service to type=ClusterIP 07/19/23 01:02:58.703
STEP: creating replication controller externalname-service in namespace services-607 07/19/23 01:02:58.72
I0719 01:02:58.725973      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-607, replica count: 2
I0719 01:03:01.777521      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 19 01:03:01.777: INFO: Creating new exec pod
Jul 19 01:03:01.781: INFO: Waiting up to 5m0s for pod "execpod6zrwv" in namespace "services-607" to be "running"
Jul 19 01:03:01.782: INFO: Pod "execpod6zrwv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.399032ms
Jul 19 01:03:03.784: INFO: Pod "execpod6zrwv": Phase="Running", Reason="", readiness=true. Elapsed: 2.003601589s
Jul 19 01:03:03.784: INFO: Pod "execpod6zrwv" satisfied condition "running"
Jul 19 01:03:04.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-607 exec execpod6zrwv -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jul 19 01:03:04.892: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 19 01:03:04.892: INFO: stdout: ""
Jul 19 01:03:04.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-607 exec execpod6zrwv -- /bin/sh -x -c nc -v -z -w 2 10.100.31.46 80'
Jul 19 01:03:04.995: INFO: stderr: "+ nc -v -z -w 2 10.100.31.46 80\nConnection to 10.100.31.46 80 port [tcp/http] succeeded!\n"
Jul 19 01:03:04.995: INFO: stdout: ""
Jul 19 01:03:04.995: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jul 19 01:03:05.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-607" for this suite. 07/19/23 01:03:05.006
------------------------------
• [SLOW TEST] [6.319 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:02:58.69
    Jul 19 01:02:58.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename services 07/19/23 01:02:58.691
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:02:58.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:02:58.7
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-607 07/19/23 01:02:58.702
    STEP: changing the ExternalName service to type=ClusterIP 07/19/23 01:02:58.703
    STEP: creating replication controller externalname-service in namespace services-607 07/19/23 01:02:58.72
    I0719 01:02:58.725973      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-607, replica count: 2
    I0719 01:03:01.777521      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jul 19 01:03:01.777: INFO: Creating new exec pod
    Jul 19 01:03:01.781: INFO: Waiting up to 5m0s for pod "execpod6zrwv" in namespace "services-607" to be "running"
    Jul 19 01:03:01.782: INFO: Pod "execpod6zrwv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.399032ms
    Jul 19 01:03:03.784: INFO: Pod "execpod6zrwv": Phase="Running", Reason="", readiness=true. Elapsed: 2.003601589s
    Jul 19 01:03:03.784: INFO: Pod "execpod6zrwv" satisfied condition "running"
    Jul 19 01:03:04.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-607 exec execpod6zrwv -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jul 19 01:03:04.892: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jul 19 01:03:04.892: INFO: stdout: ""
    Jul 19 01:03:04.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=services-607 exec execpod6zrwv -- /bin/sh -x -c nc -v -z -w 2 10.100.31.46 80'
    Jul 19 01:03:04.995: INFO: stderr: "+ nc -v -z -w 2 10.100.31.46 80\nConnection to 10.100.31.46 80 port [tcp/http] succeeded!\n"
    Jul 19 01:03:04.995: INFO: stdout: ""
    Jul 19 01:03:04.995: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:03:05.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-607" for this suite. 07/19/23 01:03:05.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:03:05.009
Jul 19 01:03:05.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename statefulset 07/19/23 01:03:05.01
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:03:05.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:03:05.018
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5955 07/19/23 01:03:05.02
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-5955 07/19/23 01:03:05.027
Jul 19 01:03:05.031: INFO: Found 0 stateful pods, waiting for 1
Jul 19 01:03:15.033: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 07/19/23 01:03:15.037
STEP: Getting /status 07/19/23 01:03:15.044
Jul 19 01:03:15.047: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 07/19/23 01:03:15.047
Jul 19 01:03:15.052: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 07/19/23 01:03:15.052
Jul 19 01:03:15.053: INFO: Observed &StatefulSet event: ADDED
Jul 19 01:03:15.053: INFO: Found Statefulset ss in namespace statefulset-5955 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 19 01:03:15.053: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 07/19/23 01:03:15.053
Jul 19 01:03:15.053: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul 19 01:03:15.057: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 07/19/23 01:03:15.057
Jul 19 01:03:15.058: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jul 19 01:03:15.058: INFO: Deleting all statefulset in ns statefulset-5955
Jul 19 01:03:15.061: INFO: Scaling statefulset ss to 0
Jul 19 01:03:25.070: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 01:03:25.072: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jul 19 01:03:25.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5955" for this suite. 07/19/23 01:03:25.079
------------------------------
• [SLOW TEST] [20.072 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:03:05.009
    Jul 19 01:03:05.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename statefulset 07/19/23 01:03:05.01
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:03:05.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:03:05.018
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5955 07/19/23 01:03:05.02
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-5955 07/19/23 01:03:05.027
    Jul 19 01:03:05.031: INFO: Found 0 stateful pods, waiting for 1
    Jul 19 01:03:15.033: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 07/19/23 01:03:15.037
    STEP: Getting /status 07/19/23 01:03:15.044
    Jul 19 01:03:15.047: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 07/19/23 01:03:15.047
    Jul 19 01:03:15.052: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 07/19/23 01:03:15.052
    Jul 19 01:03:15.053: INFO: Observed &StatefulSet event: ADDED
    Jul 19 01:03:15.053: INFO: Found Statefulset ss in namespace statefulset-5955 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jul 19 01:03:15.053: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 07/19/23 01:03:15.053
    Jul 19 01:03:15.053: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jul 19 01:03:15.057: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 07/19/23 01:03:15.057
    Jul 19 01:03:15.058: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jul 19 01:03:15.058: INFO: Deleting all statefulset in ns statefulset-5955
    Jul 19 01:03:15.061: INFO: Scaling statefulset ss to 0
    Jul 19 01:03:25.070: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 19 01:03:25.072: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:03:25.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5955" for this suite. 07/19/23 01:03:25.079
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:03:25.082
Jul 19 01:03:25.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename subpath 07/19/23 01:03:25.083
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:03:25.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:03:25.095
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 07/19/23 01:03:25.097
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-5k76 07/19/23 01:03:25.101
STEP: Creating a pod to test atomic-volume-subpath 07/19/23 01:03:25.101
Jul 19 01:03:25.105: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-5k76" in namespace "subpath-3414" to be "Succeeded or Failed"
Jul 19 01:03:25.108: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.990629ms
Jul 19 01:03:27.111: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 2.005348765s
Jul 19 01:03:29.112: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 4.006257043s
Jul 19 01:03:31.111: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 6.005717978s
Jul 19 01:03:33.110: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 8.004931438s
Jul 19 01:03:35.112: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 10.006262055s
Jul 19 01:03:37.114: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 12.008735119s
Jul 19 01:03:39.112: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 14.00670252s
Jul 19 01:03:41.111: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 16.005870584s
Jul 19 01:03:43.112: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 18.006558018s
Jul 19 01:03:45.111: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 20.005682985s
Jul 19 01:03:47.112: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=false. Elapsed: 22.006993338s
Jul 19 01:03:49.111: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006223531s
STEP: Saw pod success 07/19/23 01:03:49.112
Jul 19 01:03:49.112: INFO: Pod "pod-subpath-test-downwardapi-5k76" satisfied condition "Succeeded or Failed"
Jul 19 01:03:49.113: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-downwardapi-5k76 container test-container-subpath-downwardapi-5k76: <nil>
STEP: delete the pod 07/19/23 01:03:49.117
Jul 19 01:03:49.124: INFO: Waiting for pod pod-subpath-test-downwardapi-5k76 to disappear
Jul 19 01:03:49.126: INFO: Pod pod-subpath-test-downwardapi-5k76 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-5k76 07/19/23 01:03:49.126
Jul 19 01:03:49.126: INFO: Deleting pod "pod-subpath-test-downwardapi-5k76" in namespace "subpath-3414"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jul 19 01:03:49.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3414" for this suite. 07/19/23 01:03:49.13
------------------------------
• [SLOW TEST] [24.051 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:03:25.082
    Jul 19 01:03:25.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename subpath 07/19/23 01:03:25.083
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:03:25.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:03:25.095
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 07/19/23 01:03:25.097
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-5k76 07/19/23 01:03:25.101
    STEP: Creating a pod to test atomic-volume-subpath 07/19/23 01:03:25.101
    Jul 19 01:03:25.105: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-5k76" in namespace "subpath-3414" to be "Succeeded or Failed"
    Jul 19 01:03:25.108: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.990629ms
    Jul 19 01:03:27.111: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 2.005348765s
    Jul 19 01:03:29.112: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 4.006257043s
    Jul 19 01:03:31.111: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 6.005717978s
    Jul 19 01:03:33.110: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 8.004931438s
    Jul 19 01:03:35.112: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 10.006262055s
    Jul 19 01:03:37.114: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 12.008735119s
    Jul 19 01:03:39.112: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 14.00670252s
    Jul 19 01:03:41.111: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 16.005870584s
    Jul 19 01:03:43.112: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 18.006558018s
    Jul 19 01:03:45.111: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=true. Elapsed: 20.005682985s
    Jul 19 01:03:47.112: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Running", Reason="", readiness=false. Elapsed: 22.006993338s
    Jul 19 01:03:49.111: INFO: Pod "pod-subpath-test-downwardapi-5k76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006223531s
    STEP: Saw pod success 07/19/23 01:03:49.112
    Jul 19 01:03:49.112: INFO: Pod "pod-subpath-test-downwardapi-5k76" satisfied condition "Succeeded or Failed"
    Jul 19 01:03:49.113: INFO: Trying to get logs from node controller-1 pod pod-subpath-test-downwardapi-5k76 container test-container-subpath-downwardapi-5k76: <nil>
    STEP: delete the pod 07/19/23 01:03:49.117
    Jul 19 01:03:49.124: INFO: Waiting for pod pod-subpath-test-downwardapi-5k76 to disappear
    Jul 19 01:03:49.126: INFO: Pod pod-subpath-test-downwardapi-5k76 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-5k76 07/19/23 01:03:49.126
    Jul 19 01:03:49.126: INFO: Deleting pod "pod-subpath-test-downwardapi-5k76" in namespace "subpath-3414"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:03:49.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3414" for this suite. 07/19/23 01:03:49.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:03:49.133
Jul 19 01:03:49.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename webhook 07/19/23 01:03:49.134
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:03:49.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:03:49.143
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 07/19/23 01:03:49.152
STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 01:03:49.577
STEP: Deploying the webhook pod 07/19/23 01:03:49.58
STEP: Wait for the deployment to be ready 07/19/23 01:03:49.586
Jul 19 01:03:49.591: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 07/19/23 01:03:51.597
STEP: Verifying the service has paired with the endpoint 07/19/23 01:03:51.607
Jul 19 01:03:52.607: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 07/19/23 01:03:52.65
STEP: Creating a configMap that should be mutated 07/19/23 01:03:52.659
STEP: Deleting the collection of validation webhooks 07/19/23 01:03:52.675
STEP: Creating a configMap that should not be mutated 07/19/23 01:03:52.699
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 01:03:52.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9232" for this suite. 07/19/23 01:03:52.724
STEP: Destroying namespace "webhook-9232-markers" for this suite. 07/19/23 01:03:52.726
------------------------------
• [3.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:03:49.133
    Jul 19 01:03:49.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename webhook 07/19/23 01:03:49.134
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:03:49.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:03:49.143
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 07/19/23 01:03:49.152
    STEP: Create role binding to let webhook read extension-apiserver-authentication 07/19/23 01:03:49.577
    STEP: Deploying the webhook pod 07/19/23 01:03:49.58
    STEP: Wait for the deployment to be ready 07/19/23 01:03:49.586
    Jul 19 01:03:49.591: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 07/19/23 01:03:51.597
    STEP: Verifying the service has paired with the endpoint 07/19/23 01:03:51.607
    Jul 19 01:03:52.607: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 07/19/23 01:03:52.65
    STEP: Creating a configMap that should be mutated 07/19/23 01:03:52.659
    STEP: Deleting the collection of validation webhooks 07/19/23 01:03:52.675
    STEP: Creating a configMap that should not be mutated 07/19/23 01:03:52.699
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:03:52.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9232" for this suite. 07/19/23 01:03:52.724
    STEP: Destroying namespace "webhook-9232-markers" for this suite. 07/19/23 01:03:52.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:03:52.732
Jul 19 01:03:52.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename configmap 07/19/23 01:03:52.733
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:03:52.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:03:52.742
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jul 19 01:03:52.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5502" for this suite. 07/19/23 01:03:52.763
------------------------------
• [0.034 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:03:52.732
    Jul 19 01:03:52.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename configmap 07/19/23 01:03:52.733
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:03:52.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:03:52.742
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:03:52.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5502" for this suite. 07/19/23 01:03:52.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:03:52.766
Jul 19 01:03:52.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename var-expansion 07/19/23 01:03:52.767
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:03:52.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:03:52.775
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 07/19/23 01:03:52.776
STEP: waiting for pod running 07/19/23 01:03:52.783
Jul 19 01:03:52.783: INFO: Waiting up to 2m0s for pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" in namespace "var-expansion-8826" to be "running"
Jul 19 01:03:52.785: INFO: Pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234": Phase="Pending", Reason="", readiness=false. Elapsed: 1.380816ms
Jul 19 01:03:54.787: INFO: Pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234": Phase="Running", Reason="", readiness=true. Elapsed: 2.003782379s
Jul 19 01:03:54.787: INFO: Pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" satisfied condition "running"
STEP: creating a file in subpath 07/19/23 01:03:54.787
Jul 19 01:03:54.789: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8826 PodName:var-expansion-0f8780b1-267f-4064-9d53-3328bce73234 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 01:03:54.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 01:03:54.790: INFO: ExecWithOptions: Clientset creation
Jul 19 01:03:54.790: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8826/pods/var-expansion-0f8780b1-267f-4064-9d53-3328bce73234/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 07/19/23 01:03:54.834
Jul 19 01:03:54.836: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8826 PodName:var-expansion-0f8780b1-267f-4064-9d53-3328bce73234 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 19 01:03:54.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
Jul 19 01:03:54.836: INFO: ExecWithOptions: Clientset creation
Jul 19 01:03:54.836: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8826/pods/var-expansion-0f8780b1-267f-4064-9d53-3328bce73234/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 07/19/23 01:03:54.884
Jul 19 01:03:55.391: INFO: Successfully updated pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234"
STEP: waiting for annotated pod running 07/19/23 01:03:55.391
Jul 19 01:03:55.391: INFO: Waiting up to 2m0s for pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" in namespace "var-expansion-8826" to be "running"
Jul 19 01:03:55.395: INFO: Pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234": Phase="Running", Reason="", readiness=true. Elapsed: 4.132206ms
Jul 19 01:03:55.395: INFO: Pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" satisfied condition "running"
STEP: deleting the pod gracefully 07/19/23 01:03:55.395
Jul 19 01:03:55.395: INFO: Deleting pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" in namespace "var-expansion-8826"
Jul 19 01:03:55.398: INFO: Wait up to 5m0s for pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jul 19 01:04:29.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8826" for this suite. 07/19/23 01:04:29.406
------------------------------
• [SLOW TEST] [36.643 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:03:52.766
    Jul 19 01:03:52.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename var-expansion 07/19/23 01:03:52.767
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:03:52.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:03:52.775
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 07/19/23 01:03:52.776
    STEP: waiting for pod running 07/19/23 01:03:52.783
    Jul 19 01:03:52.783: INFO: Waiting up to 2m0s for pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" in namespace "var-expansion-8826" to be "running"
    Jul 19 01:03:52.785: INFO: Pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234": Phase="Pending", Reason="", readiness=false. Elapsed: 1.380816ms
    Jul 19 01:03:54.787: INFO: Pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234": Phase="Running", Reason="", readiness=true. Elapsed: 2.003782379s
    Jul 19 01:03:54.787: INFO: Pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" satisfied condition "running"
    STEP: creating a file in subpath 07/19/23 01:03:54.787
    Jul 19 01:03:54.789: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8826 PodName:var-expansion-0f8780b1-267f-4064-9d53-3328bce73234 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 01:03:54.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 01:03:54.790: INFO: ExecWithOptions: Clientset creation
    Jul 19 01:03:54.790: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8826/pods/var-expansion-0f8780b1-267f-4064-9d53-3328bce73234/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 07/19/23 01:03:54.834
    Jul 19 01:03:54.836: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8826 PodName:var-expansion-0f8780b1-267f-4064-9d53-3328bce73234 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jul 19 01:03:54.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    Jul 19 01:03:54.836: INFO: ExecWithOptions: Clientset creation
    Jul 19 01:03:54.836: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8826/pods/var-expansion-0f8780b1-267f-4064-9d53-3328bce73234/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 07/19/23 01:03:54.884
    Jul 19 01:03:55.391: INFO: Successfully updated pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234"
    STEP: waiting for annotated pod running 07/19/23 01:03:55.391
    Jul 19 01:03:55.391: INFO: Waiting up to 2m0s for pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" in namespace "var-expansion-8826" to be "running"
    Jul 19 01:03:55.395: INFO: Pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234": Phase="Running", Reason="", readiness=true. Elapsed: 4.132206ms
    Jul 19 01:03:55.395: INFO: Pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" satisfied condition "running"
    STEP: deleting the pod gracefully 07/19/23 01:03:55.395
    Jul 19 01:03:55.395: INFO: Deleting pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" in namespace "var-expansion-8826"
    Jul 19 01:03:55.398: INFO: Wait up to 5m0s for pod "var-expansion-0f8780b1-267f-4064-9d53-3328bce73234" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:04:29.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8826" for this suite. 07/19/23 01:04:29.406
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:04:29.409
Jul 19 01:04:29.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename emptydir-wrapper 07/19/23 01:04:29.41
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:29.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:29.419
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jul 19 01:04:29.431: INFO: Waiting up to 5m0s for pod "pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db" in namespace "emptydir-wrapper-5440" to be "running and ready"
Jul 19 01:04:29.432: INFO: Pod "pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.282375ms
Jul 19 01:04:29.432: INFO: The phase of Pod pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db is Pending, waiting for it to be Running (with Ready = true)
Jul 19 01:04:31.436: INFO: Pod "pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db": Phase="Running", Reason="", readiness=true. Elapsed: 2.005406954s
Jul 19 01:04:31.436: INFO: The phase of Pod pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db is Running (Ready = true)
Jul 19 01:04:31.436: INFO: Pod "pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db" satisfied condition "running and ready"
STEP: Cleaning up the secret 07/19/23 01:04:31.438
STEP: Cleaning up the configmap 07/19/23 01:04:31.44
STEP: Cleaning up the pod 07/19/23 01:04:31.443
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jul 19 01:04:31.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5440" for this suite. 07/19/23 01:04:31.452
------------------------------
• [2.045 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:04:29.409
    Jul 19 01:04:29.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename emptydir-wrapper 07/19/23 01:04:29.41
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:29.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:29.419
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jul 19 01:04:29.431: INFO: Waiting up to 5m0s for pod "pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db" in namespace "emptydir-wrapper-5440" to be "running and ready"
    Jul 19 01:04:29.432: INFO: Pod "pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.282375ms
    Jul 19 01:04:29.432: INFO: The phase of Pod pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db is Pending, waiting for it to be Running (with Ready = true)
    Jul 19 01:04:31.436: INFO: Pod "pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db": Phase="Running", Reason="", readiness=true. Elapsed: 2.005406954s
    Jul 19 01:04:31.436: INFO: The phase of Pod pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db is Running (Ready = true)
    Jul 19 01:04:31.436: INFO: Pod "pod-secrets-7989e527-92b1-44b3-a0d7-f5487c62c0db" satisfied condition "running and ready"
    STEP: Cleaning up the secret 07/19/23 01:04:31.438
    STEP: Cleaning up the configmap 07/19/23 01:04:31.44
    STEP: Cleaning up the pod 07/19/23 01:04:31.443
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:04:31.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5440" for this suite. 07/19/23 01:04:31.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:04:31.454
Jul 19 01:04:31.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename kubectl 07/19/23 01:04:31.455
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:31.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:31.467
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 07/19/23 01:04:31.469
Jul 19 01:04:31.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-7198 create -f -'
Jul 19 01:04:31.680: INFO: stderr: ""
Jul 19 01:04:31.680: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 07/19/23 01:04:31.68
Jul 19 01:04:31.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-7198 diff -f -'
Jul 19 01:04:31.928: INFO: rc: 1
Jul 19 01:04:31.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-7198 delete -f -'
Jul 19 01:04:32.026: INFO: stderr: ""
Jul 19 01:04:32.026: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jul 19 01:04:32.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7198" for this suite. 07/19/23 01:04:32.029
------------------------------
• [0.581 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:04:31.454
    Jul 19 01:04:31.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename kubectl 07/19/23 01:04:31.455
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:31.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:31.467
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 07/19/23 01:04:31.469
    Jul 19 01:04:31.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-7198 create -f -'
    Jul 19 01:04:31.680: INFO: stderr: ""
    Jul 19 01:04:31.680: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 07/19/23 01:04:31.68
    Jul 19 01:04:31.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-7198 diff -f -'
    Jul 19 01:04:31.928: INFO: rc: 1
    Jul 19 01:04:31.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=kubectl-7198 delete -f -'
    Jul 19 01:04:32.026: INFO: stderr: ""
    Jul 19 01:04:32.026: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:04:32.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7198" for this suite. 07/19/23 01:04:32.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:04:32.037
Jul 19 01:04:32.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 01:04:32.038
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:32.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:32.049
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jul 19 01:04:32.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/19/23 01:04:34.053
Jul 19 01:04:34.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-6974 --namespace=crd-publish-openapi-6974 create -f -'
Jul 19 01:04:34.854: INFO: stderr: ""
Jul 19 01:04:34.854: INFO: stdout: "e2e-test-crd-publish-openapi-3463-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 19 01:04:34.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-6974 --namespace=crd-publish-openapi-6974 delete e2e-test-crd-publish-openapi-3463-crds test-cr'
Jul 19 01:04:34.939: INFO: stderr: ""
Jul 19 01:04:34.939: INFO: stdout: "e2e-test-crd-publish-openapi-3463-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 19 01:04:34.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-6974 --namespace=crd-publish-openapi-6974 apply -f -'
Jul 19 01:04:35.151: INFO: stderr: ""
Jul 19 01:04:35.151: INFO: stdout: "e2e-test-crd-publish-openapi-3463-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 19 01:04:35.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-6974 --namespace=crd-publish-openapi-6974 delete e2e-test-crd-publish-openapi-3463-crds test-cr'
Jul 19 01:04:35.212: INFO: stderr: ""
Jul 19 01:04:35.212: INFO: stdout: "e2e-test-crd-publish-openapi-3463-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 07/19/23 01:04:35.212
Jul 19 01:04:35.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-6974 explain e2e-test-crd-publish-openapi-3463-crds'
Jul 19 01:04:35.892: INFO: stderr: ""
Jul 19 01:04:35.893: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3463-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 01:04:38.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6974" for this suite. 07/19/23 01:04:38.414
------------------------------
• [SLOW TEST] [6.380 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:04:32.037
    Jul 19 01:04:32.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 01:04:32.038
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:32.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:32.049
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jul 19 01:04:32.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 07/19/23 01:04:34.053
    Jul 19 01:04:34.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-6974 --namespace=crd-publish-openapi-6974 create -f -'
    Jul 19 01:04:34.854: INFO: stderr: ""
    Jul 19 01:04:34.854: INFO: stdout: "e2e-test-crd-publish-openapi-3463-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jul 19 01:04:34.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-6974 --namespace=crd-publish-openapi-6974 delete e2e-test-crd-publish-openapi-3463-crds test-cr'
    Jul 19 01:04:34.939: INFO: stderr: ""
    Jul 19 01:04:34.939: INFO: stdout: "e2e-test-crd-publish-openapi-3463-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jul 19 01:04:34.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-6974 --namespace=crd-publish-openapi-6974 apply -f -'
    Jul 19 01:04:35.151: INFO: stderr: ""
    Jul 19 01:04:35.151: INFO: stdout: "e2e-test-crd-publish-openapi-3463-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jul 19 01:04:35.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-6974 --namespace=crd-publish-openapi-6974 delete e2e-test-crd-publish-openapi-3463-crds test-cr'
    Jul 19 01:04:35.212: INFO: stderr: ""
    Jul 19 01:04:35.212: INFO: stdout: "e2e-test-crd-publish-openapi-3463-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 07/19/23 01:04:35.212
    Jul 19 01:04:35.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-6974 explain e2e-test-crd-publish-openapi-3463-crds'
    Jul 19 01:04:35.892: INFO: stderr: ""
    Jul 19 01:04:35.893: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3463-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:04:38.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6974" for this suite. 07/19/23 01:04:38.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:04:38.418
Jul 19 01:04:38.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename podtemplate 07/19/23 01:04:38.418
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:38.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:38.429
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 07/19/23 01:04:38.431
Jul 19 01:04:38.434: INFO: created test-podtemplate-1
Jul 19 01:04:38.437: INFO: created test-podtemplate-2
Jul 19 01:04:38.443: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 07/19/23 01:04:38.443
STEP: delete collection of pod templates 07/19/23 01:04:38.445
Jul 19 01:04:38.445: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 07/19/23 01:04:38.452
Jul 19 01:04:38.452: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jul 19 01:04:38.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3380" for this suite. 07/19/23 01:04:38.456
------------------------------
• [0.041 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:04:38.418
    Jul 19 01:04:38.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename podtemplate 07/19/23 01:04:38.418
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:38.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:38.429
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 07/19/23 01:04:38.431
    Jul 19 01:04:38.434: INFO: created test-podtemplate-1
    Jul 19 01:04:38.437: INFO: created test-podtemplate-2
    Jul 19 01:04:38.443: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 07/19/23 01:04:38.443
    STEP: delete collection of pod templates 07/19/23 01:04:38.445
    Jul 19 01:04:38.445: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 07/19/23 01:04:38.452
    Jul 19 01:04:38.452: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:04:38.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3380" for this suite. 07/19/23 01:04:38.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:04:38.459
Jul 19 01:04:38.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 01:04:38.459
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:38.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:38.467
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-4004c270-7bfd-490b-9591-65ff9ee77826 07/19/23 01:04:38.469
STEP: Creating a pod to test consume secrets 07/19/23 01:04:38.471
Jul 19 01:04:38.476: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d" in namespace "projected-1123" to be "Succeeded or Failed"
Jul 19 01:04:38.478: INFO: Pod "pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.758801ms
Jul 19 01:04:40.481: INFO: Pod "pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00564385s
Jul 19 01:04:42.482: INFO: Pod "pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005810871s
STEP: Saw pod success 07/19/23 01:04:42.482
Jul 19 01:04:42.482: INFO: Pod "pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d" satisfied condition "Succeeded or Failed"
Jul 19 01:04:42.483: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d container projected-secret-volume-test: <nil>
STEP: delete the pod 07/19/23 01:04:42.494
Jul 19 01:04:42.522: INFO: Waiting for pod pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d to disappear
Jul 19 01:04:42.523: INFO: Pod pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jul 19 01:04:42.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1123" for this suite. 07/19/23 01:04:42.525
------------------------------
• [4.070 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:04:38.459
    Jul 19 01:04:38.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 01:04:38.459
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:38.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:38.467
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-4004c270-7bfd-490b-9591-65ff9ee77826 07/19/23 01:04:38.469
    STEP: Creating a pod to test consume secrets 07/19/23 01:04:38.471
    Jul 19 01:04:38.476: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d" in namespace "projected-1123" to be "Succeeded or Failed"
    Jul 19 01:04:38.478: INFO: Pod "pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.758801ms
    Jul 19 01:04:40.481: INFO: Pod "pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00564385s
    Jul 19 01:04:42.482: INFO: Pod "pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005810871s
    STEP: Saw pod success 07/19/23 01:04:42.482
    Jul 19 01:04:42.482: INFO: Pod "pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d" satisfied condition "Succeeded or Failed"
    Jul 19 01:04:42.483: INFO: Trying to get logs from node controller-1 pod pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d container projected-secret-volume-test: <nil>
    STEP: delete the pod 07/19/23 01:04:42.494
    Jul 19 01:04:42.522: INFO: Waiting for pod pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d to disappear
    Jul 19 01:04:42.523: INFO: Pod pod-projected-secrets-35842376-a45d-43aa-be90-f462798cd42d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:04:42.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1123" for this suite. 07/19/23 01:04:42.525
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:04:42.528
Jul 19 01:04:42.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 01:04:42.529
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:42.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:42.539
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jul 19 01:04:42.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 07/19/23 01:04:44.53
Jul 19 01:04:44.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 create -f -'
Jul 19 01:04:45.337: INFO: stderr: ""
Jul 19 01:04:45.337: INFO: stdout: "e2e-test-crd-publish-openapi-1442-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 19 01:04:45.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 delete e2e-test-crd-publish-openapi-1442-crds test-foo'
Jul 19 01:04:45.401: INFO: stderr: ""
Jul 19 01:04:45.401: INFO: stdout: "e2e-test-crd-publish-openapi-1442-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 19 01:04:45.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 apply -f -'
Jul 19 01:04:45.611: INFO: stderr: ""
Jul 19 01:04:45.611: INFO: stdout: "e2e-test-crd-publish-openapi-1442-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 19 01:04:45.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 delete e2e-test-crd-publish-openapi-1442-crds test-foo'
Jul 19 01:04:45.679: INFO: stderr: ""
Jul 19 01:04:45.679: INFO: stdout: "e2e-test-crd-publish-openapi-1442-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 07/19/23 01:04:45.679
Jul 19 01:04:45.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 create -f -'
Jul 19 01:04:45.881: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 07/19/23 01:04:45.881
Jul 19 01:04:45.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 create -f -'
Jul 19 01:04:46.553: INFO: rc: 1
Jul 19 01:04:46.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 apply -f -'
Jul 19 01:04:46.762: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 07/19/23 01:04:46.762
Jul 19 01:04:46.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 create -f -'
Jul 19 01:04:46.963: INFO: rc: 1
Jul 19 01:04:46.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 apply -f -'
Jul 19 01:04:47.173: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 07/19/23 01:04:47.173
Jul 19 01:04:47.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 explain e2e-test-crd-publish-openapi-1442-crds'
Jul 19 01:04:47.379: INFO: stderr: ""
Jul 19 01:04:47.379: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1442-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 07/19/23 01:04:47.379
Jul 19 01:04:47.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 explain e2e-test-crd-publish-openapi-1442-crds.metadata'
Jul 19 01:04:47.595: INFO: stderr: ""
Jul 19 01:04:47.595: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1442-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 19 01:04:47.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 explain e2e-test-crd-publish-openapi-1442-crds.spec'
Jul 19 01:04:47.798: INFO: stderr: ""
Jul 19 01:04:47.798: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1442-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 19 01:04:47.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 explain e2e-test-crd-publish-openapi-1442-crds.spec.bars'
Jul 19 01:04:48.002: INFO: stderr: ""
Jul 19 01:04:48.002: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1442-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 07/19/23 01:04:48.002
Jul 19 01:04:48.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 explain e2e-test-crd-publish-openapi-1442-crds.spec.bars2'
Jul 19 01:04:48.207: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jul 19 01:04:50.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8306" for this suite. 07/19/23 01:04:50.19
------------------------------
• [SLOW TEST] [7.665 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:04:42.528
    Jul 19 01:04:42.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename crd-publish-openapi 07/19/23 01:04:42.529
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:42.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:42.539
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jul 19 01:04:42.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 07/19/23 01:04:44.53
    Jul 19 01:04:44.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 create -f -'
    Jul 19 01:04:45.337: INFO: stderr: ""
    Jul 19 01:04:45.337: INFO: stdout: "e2e-test-crd-publish-openapi-1442-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jul 19 01:04:45.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 delete e2e-test-crd-publish-openapi-1442-crds test-foo'
    Jul 19 01:04:45.401: INFO: stderr: ""
    Jul 19 01:04:45.401: INFO: stdout: "e2e-test-crd-publish-openapi-1442-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jul 19 01:04:45.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 apply -f -'
    Jul 19 01:04:45.611: INFO: stderr: ""
    Jul 19 01:04:45.611: INFO: stdout: "e2e-test-crd-publish-openapi-1442-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jul 19 01:04:45.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 delete e2e-test-crd-publish-openapi-1442-crds test-foo'
    Jul 19 01:04:45.679: INFO: stderr: ""
    Jul 19 01:04:45.679: INFO: stdout: "e2e-test-crd-publish-openapi-1442-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 07/19/23 01:04:45.679
    Jul 19 01:04:45.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 create -f -'
    Jul 19 01:04:45.881: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 07/19/23 01:04:45.881
    Jul 19 01:04:45.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 create -f -'
    Jul 19 01:04:46.553: INFO: rc: 1
    Jul 19 01:04:46.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 apply -f -'
    Jul 19 01:04:46.762: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 07/19/23 01:04:46.762
    Jul 19 01:04:46.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 create -f -'
    Jul 19 01:04:46.963: INFO: rc: 1
    Jul 19 01:04:46.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 --namespace=crd-publish-openapi-8306 apply -f -'
    Jul 19 01:04:47.173: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 07/19/23 01:04:47.173
    Jul 19 01:04:47.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 explain e2e-test-crd-publish-openapi-1442-crds'
    Jul 19 01:04:47.379: INFO: stderr: ""
    Jul 19 01:04:47.379: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1442-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 07/19/23 01:04:47.379
    Jul 19 01:04:47.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 explain e2e-test-crd-publish-openapi-1442-crds.metadata'
    Jul 19 01:04:47.595: INFO: stderr: ""
    Jul 19 01:04:47.595: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1442-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jul 19 01:04:47.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 explain e2e-test-crd-publish-openapi-1442-crds.spec'
    Jul 19 01:04:47.798: INFO: stderr: ""
    Jul 19 01:04:47.798: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1442-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jul 19 01:04:47.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 explain e2e-test-crd-publish-openapi-1442-crds.spec.bars'
    Jul 19 01:04:48.002: INFO: stderr: ""
    Jul 19 01:04:48.002: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1442-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 07/19/23 01:04:48.002
    Jul 19 01:04:48.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=crd-publish-openapi-8306 explain e2e-test-crd-publish-openapi-1442-crds.spec.bars2'
    Jul 19 01:04:48.207: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:04:50.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8306" for this suite. 07/19/23 01:04:50.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:04:50.195
Jul 19 01:04:50.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename sched-pred 07/19/23 01:04:50.195
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:50.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:50.205
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jul 19 01:04:50.207: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 19 01:04:50.211: INFO: Waiting for terminating namespaces to be deleted...
Jul 19 01:04:50.213: INFO: 
Logging pods the apiserver thinks is on node controller-0 before test
Jul 19 01:04:50.222: INFO: cm-cert-manager-7fb65857f5-5mnw8 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container cert-manager-controller ready: true, restart count 2
Jul 19 01:04:50.222: INFO: cm-cert-manager-cainjector-86b69d7d69-6czpr from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container cert-manager-cainjector ready: true, restart count 1
Jul 19 01:04:50.222: INFO: cm-cert-manager-webhook-98ddcd5cb-2q6p9 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container cert-manager-webhook ready: true, restart count 1
Jul 19 01:04:50.222: INFO: helm-controller-5fb8ccb85d-nl9lf from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container manager ready: true, restart count 1
Jul 19 01:04:50.222: INFO: source-controller-69b5d8f7d8-66tt6 from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container manager ready: true, restart count 1
Jul 19 01:04:50.222: INFO: calico-kube-controllers-7f5cd5f684-5jblg from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container calico-kube-controllers ready: true, restart count 1
Jul 19 01:04:50.222: INFO: calico-node-hmrnj from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container calico-node ready: true, restart count 1
Jul 19 01:04:50.222: INFO: cephfs-nodeplugin-n2vfj from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 01:04:50.222: INFO: cephfs-provisioner-5f69fbf97-q8lg6 from kube-system started at 2023-07-18 22:35:12 +0000 UTC (4 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 01:04:50.222: INFO: coredns-66856967f4-whd5z from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container coredns ready: true, restart count 1
Jul 19 01:04:50.222: INFO: ic-nginx-ingress-ingress-nginx-controller-6hpm5 from kube-system started at 2023-07-18 21:49:16 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container controller ready: true, restart count 1
Jul 19 01:04:50.222: INFO: kube-apiserver-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container kube-apiserver ready: true, restart count 2
Jul 19 01:04:50.222: INFO: kube-controller-manager-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 19 01:04:50.222: INFO: kube-multus-ds-amd64-bcmw4 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container kube-multus ready: true, restart count 1
Jul 19 01:04:50.222: INFO: kube-proxy-kxjqb from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 19 01:04:50.222: INFO: kube-scheduler-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container kube-scheduler ready: true, restart count 1
Jul 19 01:04:50.222: INFO: kube-sriov-cni-ds-amd64-69r58 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container kube-sriov-cni ready: true, restart count 1
Jul 19 01:04:50.222: INFO: rbd-nodeplugin-j7slt from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 01:04:50.222: INFO: rbd-provisioner-54c6c894f7-sggw9 from kube-system started at 2023-07-18 22:34:50 +0000 UTC (6 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 01:04:50.222: INFO: dm-monitor-84b75cf89c-vnb6c from platform-deployment-manager started at 2023-07-18 21:53:06 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container dm-monitor ready: true, restart count 1
Jul 19 01:04:50.222: INFO: platform-deployment-manager-7ff76b89d-qg7g9 from platform-deployment-manager started at 2023-07-18 21:52:11 +0000 UTC (2 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jul 19 01:04:50.222: INFO: 	Container manager ready: true, restart count 1
Jul 19 01:04:50.222: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 01:04:50.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 19 01:04:50.222: INFO: 
Logging pods the apiserver thinks is on node controller-1 before test
Jul 19 01:04:50.230: INFO: cm-cert-manager-7fb65857f5-g94zp from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.230: INFO: 	Container cert-manager-controller ready: true, restart count 0
Jul 19 01:04:50.230: INFO: cm-cert-manager-cainjector-86b69d7d69-vl4v2 from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.230: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
Jul 19 01:04:50.231: INFO: cm-cert-manager-webhook-98ddcd5cb-tq28v from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container cert-manager-webhook ready: true, restart count 0
Jul 19 01:04:50.231: INFO: calico-node-hz2hv from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container calico-node ready: true, restart count 0
Jul 19 01:04:50.231: INFO: ceph-pools-audit-28162130-rr8lg from kube-system started at 2023-07-19 00:50:00 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jul 19 01:04:50.231: INFO: ceph-pools-audit-28162135-gtznm from kube-system started at 2023-07-19 00:55:00 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jul 19 01:04:50.231: INFO: ceph-pools-audit-28162140-sp28b from kube-system started at 2023-07-19 01:00:00 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
Jul 19 01:04:50.231: INFO: cephfs-nodeplugin-xn85z from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 01:04:50.231: INFO: cephfs-provisioner-5f69fbf97-vxdxv from kube-system started at 2023-07-19 00:41:43 +0000 UTC (4 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 01:04:50.231: INFO: coredns-66856967f4-vcphq from kube-system started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container coredns ready: true, restart count 0
Jul 19 01:04:50.231: INFO: ic-nginx-ingress-ingress-nginx-controller-vfpd9 from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container controller ready: true, restart count 0
Jul 19 01:04:50.231: INFO: kube-apiserver-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container kube-apiserver ready: true, restart count 1
Jul 19 01:04:50.231: INFO: kube-controller-manager-controller-1 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 19 01:04:50.231: INFO: kube-multus-ds-amd64-k6nb2 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container kube-multus ready: true, restart count 1
Jul 19 01:04:50.231: INFO: kube-proxy-66jhd from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 19 01:04:50.231: INFO: kube-scheduler-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container kube-scheduler ready: true, restart count 1
Jul 19 01:04:50.231: INFO: kube-sriov-cni-ds-amd64-rpb6b from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container kube-sriov-cni ready: true, restart count 0
Jul 19 01:04:50.231: INFO: rbd-nodeplugin-9mw7d from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container driver-registrar ready: true, restart count 0
Jul 19 01:04:50.231: INFO: rbd-provisioner-54c6c894f7-6k98h from kube-system started at 2023-07-19 00:41:43 +0000 UTC (6 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container csi-attacher ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container csi-provisioner ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container csi-resizer ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container csi-snapshotter ready: true, restart count 0
Jul 19 01:04:50.231: INFO: sonobuoy from sonobuoy started at 2023-07-18 23:44:15 +0000 UTC (1 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 19 01:04:50.231: INFO: sonobuoy-e2e-job-6b28f3e332364b76 from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container e2e ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 01:04:50.231: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-ckzmn from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
Jul 19 01:04:50.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 19 01:04:50.231: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 07/19/23 01:04:50.231
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17731f4a8d2766d1], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 07/19/23 01:04:50.251
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jul 19 01:04:51.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7883" for this suite. 07/19/23 01:04:51.252
------------------------------
• [1.060 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:04:50.195
    Jul 19 01:04:50.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename sched-pred 07/19/23 01:04:50.195
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:50.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:50.205
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jul 19 01:04:50.207: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jul 19 01:04:50.211: INFO: Waiting for terminating namespaces to be deleted...
    Jul 19 01:04:50.213: INFO: 
    Logging pods the apiserver thinks is on node controller-0 before test
    Jul 19 01:04:50.222: INFO: cm-cert-manager-7fb65857f5-5mnw8 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container cert-manager-controller ready: true, restart count 2
    Jul 19 01:04:50.222: INFO: cm-cert-manager-cainjector-86b69d7d69-6czpr from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container cert-manager-cainjector ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: cm-cert-manager-webhook-98ddcd5cb-2q6p9 from cert-manager started at 2023-07-18 21:50:26 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container cert-manager-webhook ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: helm-controller-5fb8ccb85d-nl9lf from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container manager ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: source-controller-69b5d8f7d8-66tt6 from flux-helm started at 2023-07-18 21:48:03 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container manager ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: calico-kube-controllers-7f5cd5f684-5jblg from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container calico-kube-controllers ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: calico-node-hmrnj from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container calico-node ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: cephfs-nodeplugin-n2vfj from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: cephfs-provisioner-5f69fbf97-q8lg6 from kube-system started at 2023-07-18 22:35:12 +0000 UTC (4 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: coredns-66856967f4-whd5z from kube-system started at 2023-07-18 21:43:49 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container coredns ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: ic-nginx-ingress-ingress-nginx-controller-6hpm5 from kube-system started at 2023-07-18 21:49:16 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container controller ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: kube-apiserver-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container kube-apiserver ready: true, restart count 2
    Jul 19 01:04:50.222: INFO: kube-controller-manager-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: kube-multus-ds-amd64-bcmw4 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container kube-multus ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: kube-proxy-kxjqb from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: kube-scheduler-controller-0 from kube-system started at 2023-07-18 22:03:42 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: kube-sriov-cni-ds-amd64-69r58 from kube-system started at 2023-07-18 21:43:47 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container kube-sriov-cni ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: rbd-nodeplugin-j7slt from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: rbd-provisioner-54c6c894f7-sggw9 from kube-system started at 2023-07-18 22:34:50 +0000 UTC (6 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container csi-attacher ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: dm-monitor-84b75cf89c-vnb6c from platform-deployment-manager started at 2023-07-18 21:53:06 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container dm-monitor ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: platform-deployment-manager-7ff76b89d-qg7g9 from platform-deployment-manager started at 2023-07-18 21:52:11 +0000 UTC (2 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: 	Container manager ready: true, restart count 1
    Jul 19 01:04:50.222: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-l464j from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 01:04:50.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 	Container systemd-logs ready: true, restart count 0
    Jul 19 01:04:50.222: INFO: 
    Logging pods the apiserver thinks is on node controller-1 before test
    Jul 19 01:04:50.230: INFO: cm-cert-manager-7fb65857f5-g94zp from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.230: INFO: 	Container cert-manager-controller ready: true, restart count 0
    Jul 19 01:04:50.230: INFO: cm-cert-manager-cainjector-86b69d7d69-vl4v2 from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.230: INFO: 	Container cert-manager-cainjector ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: cm-cert-manager-webhook-98ddcd5cb-tq28v from cert-manager started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container cert-manager-webhook ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: calico-node-hz2hv from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container calico-node ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: ceph-pools-audit-28162130-rr8lg from kube-system started at 2023-07-19 00:50:00 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
    Jul 19 01:04:50.231: INFO: ceph-pools-audit-28162135-gtznm from kube-system started at 2023-07-19 00:55:00 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
    Jul 19 01:04:50.231: INFO: ceph-pools-audit-28162140-sp28b from kube-system started at 2023-07-19 01:00:00 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container ceph-pools-audit-ceph-store ready: false, restart count 0
    Jul 19 01:04:50.231: INFO: cephfs-nodeplugin-xn85z from kube-system started at 2023-07-18 22:34:49 +0000 UTC (2 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: cephfs-provisioner-5f69fbf97-vxdxv from kube-system started at 2023-07-19 00:41:43 +0000 UTC (4 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: coredns-66856967f4-vcphq from kube-system started at 2023-07-19 00:41:43 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container coredns ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: ic-nginx-ingress-ingress-nginx-controller-vfpd9 from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container controller ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: kube-apiserver-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container kube-apiserver ready: true, restart count 1
    Jul 19 01:04:50.231: INFO: kube-controller-manager-controller-1 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container kube-controller-manager ready: true, restart count 1
    Jul 19 01:04:50.231: INFO: kube-multus-ds-amd64-k6nb2 from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container kube-multus ready: true, restart count 1
    Jul 19 01:04:50.231: INFO: kube-proxy-66jhd from kube-system started at 2023-07-18 22:26:30 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container kube-proxy ready: true, restart count 1
    Jul 19 01:04:50.231: INFO: kube-scheduler-controller-1 from kube-system started at 2023-07-18 22:26:28 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container kube-scheduler ready: true, restart count 1
    Jul 19 01:04:50.231: INFO: kube-sriov-cni-ds-amd64-rpb6b from kube-system started at 2023-07-19 00:41:42 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container kube-sriov-cni ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: rbd-nodeplugin-9mw7d from kube-system started at 2023-07-18 22:34:50 +0000 UTC (2 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container driver-registrar ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: rbd-provisioner-54c6c894f7-6k98h from kube-system started at 2023-07-19 00:41:43 +0000 UTC (6 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container csi-attacher ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container csi-provisioner ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container csi-rbdplugin ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container csi-resizer ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: sonobuoy from sonobuoy started at 2023-07-18 23:44:15 +0000 UTC (1 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: sonobuoy-e2e-job-6b28f3e332364b76 from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container e2e ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: sonobuoy-systemd-logs-daemon-set-9ddc6e7e7ea04641-ckzmn from sonobuoy started at 2023-07-18 23:44:18 +0000 UTC (2 container statuses recorded)
    Jul 19 01:04:50.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jul 19 01:04:50.231: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 07/19/23 01:04:50.231
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17731f4a8d2766d1], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 07/19/23 01:04:50.251
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:04:51.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7883" for this suite. 07/19/23 01:04:51.252
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:04:51.255
Jul 19 01:04:51.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename replication-controller 07/19/23 01:04:51.256
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:51.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:51.264
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-twm85" 07/19/23 01:04:51.266
Jul 19 01:04:51.269: INFO: Get Replication Controller "e2e-rc-twm85" to confirm replicas
Jul 19 01:04:52.271: INFO: Get Replication Controller "e2e-rc-twm85" to confirm replicas
Jul 19 01:04:52.274: INFO: Found 1 replicas for "e2e-rc-twm85" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-twm85" 07/19/23 01:04:52.274
STEP: Updating a scale subresource 07/19/23 01:04:52.275
STEP: Verifying replicas where modified for replication controller "e2e-rc-twm85" 07/19/23 01:04:52.278
Jul 19 01:04:52.278: INFO: Get Replication Controller "e2e-rc-twm85" to confirm replicas
Jul 19 01:04:53.282: INFO: Get Replication Controller "e2e-rc-twm85" to confirm replicas
Jul 19 01:04:53.284: INFO: Found 2 replicas for "e2e-rc-twm85" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jul 19 01:04:53.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9807" for this suite. 07/19/23 01:04:53.287
------------------------------
• [2.034 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:04:51.255
    Jul 19 01:04:51.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename replication-controller 07/19/23 01:04:51.256
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:51.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:51.264
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-twm85" 07/19/23 01:04:51.266
    Jul 19 01:04:51.269: INFO: Get Replication Controller "e2e-rc-twm85" to confirm replicas
    Jul 19 01:04:52.271: INFO: Get Replication Controller "e2e-rc-twm85" to confirm replicas
    Jul 19 01:04:52.274: INFO: Found 1 replicas for "e2e-rc-twm85" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-twm85" 07/19/23 01:04:52.274
    STEP: Updating a scale subresource 07/19/23 01:04:52.275
    STEP: Verifying replicas where modified for replication controller "e2e-rc-twm85" 07/19/23 01:04:52.278
    Jul 19 01:04:52.278: INFO: Get Replication Controller "e2e-rc-twm85" to confirm replicas
    Jul 19 01:04:53.282: INFO: Get Replication Controller "e2e-rc-twm85" to confirm replicas
    Jul 19 01:04:53.284: INFO: Found 2 replicas for "e2e-rc-twm85" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:04:53.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9807" for this suite. 07/19/23 01:04:53.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:04:53.29
Jul 19 01:04:53.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-probe 07/19/23 01:04:53.291
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:53.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:53.302
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-1b8ac35e-86f9-4820-97a0-036667cc4989 in namespace container-probe-3806 07/19/23 01:04:53.304
Jul 19 01:04:53.308: INFO: Waiting up to 5m0s for pod "liveness-1b8ac35e-86f9-4820-97a0-036667cc4989" in namespace "container-probe-3806" to be "not pending"
Jul 19 01:04:53.311: INFO: Pod "liveness-1b8ac35e-86f9-4820-97a0-036667cc4989": Phase="Pending", Reason="", readiness=false. Elapsed: 3.233058ms
Jul 19 01:04:55.313: INFO: Pod "liveness-1b8ac35e-86f9-4820-97a0-036667cc4989": Phase="Running", Reason="", readiness=true. Elapsed: 2.00527254s
Jul 19 01:04:55.313: INFO: Pod "liveness-1b8ac35e-86f9-4820-97a0-036667cc4989" satisfied condition "not pending"
Jul 19 01:04:55.313: INFO: Started pod liveness-1b8ac35e-86f9-4820-97a0-036667cc4989 in namespace container-probe-3806
STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 01:04:55.313
Jul 19 01:04:55.314: INFO: Initial restart count of pod liveness-1b8ac35e-86f9-4820-97a0-036667cc4989 is 0
STEP: deleting the pod 07/19/23 01:08:55.645
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jul 19 01:08:55.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3806" for this suite. 07/19/23 01:08:55.655
------------------------------
• [SLOW TEST] [242.368 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:04:53.29
    Jul 19 01:04:53.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-probe 07/19/23 01:04:53.291
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:04:53.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:04:53.302
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-1b8ac35e-86f9-4820-97a0-036667cc4989 in namespace container-probe-3806 07/19/23 01:04:53.304
    Jul 19 01:04:53.308: INFO: Waiting up to 5m0s for pod "liveness-1b8ac35e-86f9-4820-97a0-036667cc4989" in namespace "container-probe-3806" to be "not pending"
    Jul 19 01:04:53.311: INFO: Pod "liveness-1b8ac35e-86f9-4820-97a0-036667cc4989": Phase="Pending", Reason="", readiness=false. Elapsed: 3.233058ms
    Jul 19 01:04:55.313: INFO: Pod "liveness-1b8ac35e-86f9-4820-97a0-036667cc4989": Phase="Running", Reason="", readiness=true. Elapsed: 2.00527254s
    Jul 19 01:04:55.313: INFO: Pod "liveness-1b8ac35e-86f9-4820-97a0-036667cc4989" satisfied condition "not pending"
    Jul 19 01:04:55.313: INFO: Started pod liveness-1b8ac35e-86f9-4820-97a0-036667cc4989 in namespace container-probe-3806
    STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 01:04:55.313
    Jul 19 01:04:55.314: INFO: Initial restart count of pod liveness-1b8ac35e-86f9-4820-97a0-036667cc4989 is 0
    STEP: deleting the pod 07/19/23 01:08:55.645
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:08:55.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3806" for this suite. 07/19/23 01:08:55.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:08:55.659
Jul 19 01:08:55.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 01:08:55.66
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:08:55.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:08:55.667
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-7265/secret-test-067287ff-124e-4fa4-ba4e-2fceee43c6a7 07/19/23 01:08:55.672
STEP: Creating a pod to test consume secrets 07/19/23 01:08:55.675
Jul 19 01:08:55.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189" in namespace "secrets-7265" to be "Succeeded or Failed"
Jul 19 01:08:55.685: INFO: Pod "pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189": Phase="Pending", Reason="", readiness=false. Elapsed: 5.272743ms
Jul 19 01:08:57.688: INFO: Pod "pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008222059s
Jul 19 01:08:59.689: INFO: Pod "pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008795446s
STEP: Saw pod success 07/19/23 01:08:59.689
Jul 19 01:08:59.689: INFO: Pod "pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189" satisfied condition "Succeeded or Failed"
Jul 19 01:08:59.691: INFO: Trying to get logs from node controller-1 pod pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189 container env-test: <nil>
STEP: delete the pod 07/19/23 01:08:59.701
Jul 19 01:08:59.707: INFO: Waiting for pod pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189 to disappear
Jul 19 01:08:59.709: INFO: Pod pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 01:08:59.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7265" for this suite. 07/19/23 01:08:59.711
------------------------------
• [4.054 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:08:55.659
    Jul 19 01:08:55.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 01:08:55.66
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:08:55.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:08:55.667
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-7265/secret-test-067287ff-124e-4fa4-ba4e-2fceee43c6a7 07/19/23 01:08:55.672
    STEP: Creating a pod to test consume secrets 07/19/23 01:08:55.675
    Jul 19 01:08:55.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189" in namespace "secrets-7265" to be "Succeeded or Failed"
    Jul 19 01:08:55.685: INFO: Pod "pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189": Phase="Pending", Reason="", readiness=false. Elapsed: 5.272743ms
    Jul 19 01:08:57.688: INFO: Pod "pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008222059s
    Jul 19 01:08:59.689: INFO: Pod "pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008795446s
    STEP: Saw pod success 07/19/23 01:08:59.689
    Jul 19 01:08:59.689: INFO: Pod "pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189" satisfied condition "Succeeded or Failed"
    Jul 19 01:08:59.691: INFO: Trying to get logs from node controller-1 pod pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189 container env-test: <nil>
    STEP: delete the pod 07/19/23 01:08:59.701
    Jul 19 01:08:59.707: INFO: Waiting for pod pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189 to disappear
    Jul 19 01:08:59.709: INFO: Pod pod-configmaps-85e6eab1-af21-4332-83f0-15ce1b30d189 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:08:59.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7265" for this suite. 07/19/23 01:08:59.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:08:59.714
Jul 19 01:08:59.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename projected 07/19/23 01:08:59.715
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:08:59.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:08:59.721
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-f179f5f6-ad03-47c7-892b-be521854e986 07/19/23 01:08:59.724
STEP: Creating a pod to test consume configMaps 07/19/23 01:08:59.726
Jul 19 01:08:59.729: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110" in namespace "projected-8604" to be "Succeeded or Failed"
Jul 19 01:08:59.733: INFO: Pod "pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110": Phase="Pending", Reason="", readiness=false. Elapsed: 3.847771ms
Jul 19 01:09:01.736: INFO: Pod "pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007089325s
Jul 19 01:09:03.736: INFO: Pod "pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007061395s
STEP: Saw pod success 07/19/23 01:09:03.736
Jul 19 01:09:03.736: INFO: Pod "pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110" satisfied condition "Succeeded or Failed"
Jul 19 01:09:03.738: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110 container agnhost-container: <nil>
STEP: delete the pod 07/19/23 01:09:03.742
Jul 19 01:09:03.748: INFO: Waiting for pod pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110 to disappear
Jul 19 01:09:03.752: INFO: Pod pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jul 19 01:09:03.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8604" for this suite. 07/19/23 01:09:03.754
------------------------------
• [4.043 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:08:59.714
    Jul 19 01:08:59.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename projected 07/19/23 01:08:59.715
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:08:59.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:08:59.721
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-f179f5f6-ad03-47c7-892b-be521854e986 07/19/23 01:08:59.724
    STEP: Creating a pod to test consume configMaps 07/19/23 01:08:59.726
    Jul 19 01:08:59.729: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110" in namespace "projected-8604" to be "Succeeded or Failed"
    Jul 19 01:08:59.733: INFO: Pod "pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110": Phase="Pending", Reason="", readiness=false. Elapsed: 3.847771ms
    Jul 19 01:09:01.736: INFO: Pod "pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007089325s
    Jul 19 01:09:03.736: INFO: Pod "pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007061395s
    STEP: Saw pod success 07/19/23 01:09:03.736
    Jul 19 01:09:03.736: INFO: Pod "pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110" satisfied condition "Succeeded or Failed"
    Jul 19 01:09:03.738: INFO: Trying to get logs from node controller-1 pod pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110 container agnhost-container: <nil>
    STEP: delete the pod 07/19/23 01:09:03.742
    Jul 19 01:09:03.748: INFO: Waiting for pod pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110 to disappear
    Jul 19 01:09:03.752: INFO: Pod pod-projected-configmaps-fb074903-97a2-4e19-8631-c2e5ca481110 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:09:03.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8604" for this suite. 07/19/23 01:09:03.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:09:03.757
Jul 19 01:09:03.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename server-version 07/19/23 01:09:03.758
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:09:03.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:09:03.764
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 07/19/23 01:09:03.766
STEP: Confirm major version 07/19/23 01:09:03.767
Jul 19 01:09:03.767: INFO: Major version: 1
STEP: Confirm minor version 07/19/23 01:09:03.767
Jul 19 01:09:03.767: INFO: cleanMinorVersion: 26
Jul 19 01:09:03.767: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jul 19 01:09:03.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-8931" for this suite. 07/19/23 01:09:03.769
------------------------------
• [0.014 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:09:03.757
    Jul 19 01:09:03.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename server-version 07/19/23 01:09:03.758
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:09:03.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:09:03.764
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 07/19/23 01:09:03.766
    STEP: Confirm major version 07/19/23 01:09:03.767
    Jul 19 01:09:03.767: INFO: Major version: 1
    STEP: Confirm minor version 07/19/23 01:09:03.767
    Jul 19 01:09:03.767: INFO: cleanMinorVersion: 26
    Jul 19 01:09:03.767: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:09:03.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-8931" for this suite. 07/19/23 01:09:03.769
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:09:03.771
Jul 19 01:09:03.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename aggregator 07/19/23 01:09:03.772
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:09:03.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:09:03.779
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jul 19 01:09:03.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 07/19/23 01:09:03.782
Jul 19 01:09:04.286: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 19 01:09:06.311: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:08.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:10.316: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:12.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:14.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:16.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:18.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:20.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:22.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:24.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:26.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:09:28.432: INFO: Waited 114.440721ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 07/19/23 01:09:28.492
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 07/19/23 01:09:28.494
STEP: List APIServices 07/19/23 01:09:28.497
Jul 19 01:09:28.503: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jul 19 01:09:28.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-6822" for this suite. 07/19/23 01:09:28.972
------------------------------
• [SLOW TEST] [25.253 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:09:03.771
    Jul 19 01:09:03.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename aggregator 07/19/23 01:09:03.772
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:09:03.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:09:03.779
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jul 19 01:09:03.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 07/19/23 01:09:03.782
    Jul 19 01:09:04.286: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jul 19 01:09:06.311: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:08.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:10.316: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:12.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:14.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:16.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:18.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:20.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:22.315: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:24.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:26.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 9, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:09:28.432: INFO: Waited 114.440721ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 07/19/23 01:09:28.492
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 07/19/23 01:09:28.494
    STEP: List APIServices 07/19/23 01:09:28.497
    Jul 19 01:09:28.503: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:09:28.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-6822" for this suite. 07/19/23 01:09:28.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:09:29.024
Jul 19 01:09:29.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename container-probe 07/19/23 01:09:29.025
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:09:29.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:09:29.033
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-d3695080-ffc3-4feb-9dad-7a620a898772 in namespace container-probe-683 07/19/23 01:09:29.035
Jul 19 01:09:29.039: INFO: Waiting up to 5m0s for pod "liveness-d3695080-ffc3-4feb-9dad-7a620a898772" in namespace "container-probe-683" to be "not pending"
Jul 19 01:09:29.042: INFO: Pod "liveness-d3695080-ffc3-4feb-9dad-7a620a898772": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546958ms
Jul 19 01:09:31.044: INFO: Pod "liveness-d3695080-ffc3-4feb-9dad-7a620a898772": Phase="Running", Reason="", readiness=true. Elapsed: 2.005053424s
Jul 19 01:09:31.044: INFO: Pod "liveness-d3695080-ffc3-4feb-9dad-7a620a898772" satisfied condition "not pending"
Jul 19 01:09:31.044: INFO: Started pod liveness-d3695080-ffc3-4feb-9dad-7a620a898772 in namespace container-probe-683
STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 01:09:31.044
Jul 19 01:09:31.046: INFO: Initial restart count of pod liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is 0
Jul 19 01:09:51.073: INFO: Restart count of pod container-probe-683/liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is now 1 (20.026899372s elapsed)
Jul 19 01:10:11.101: INFO: Restart count of pod container-probe-683/liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is now 2 (40.05497992s elapsed)
Jul 19 01:10:31.128: INFO: Restart count of pod container-probe-683/liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is now 3 (1m0.082224171s elapsed)
Jul 19 01:10:51.158: INFO: Restart count of pod container-probe-683/liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is now 4 (1m20.111894977s elapsed)
Jul 19 01:11:59.254: INFO: Restart count of pod container-probe-683/liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is now 5 (2m28.208356608s elapsed)
STEP: deleting the pod 07/19/23 01:11:59.254
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jul 19 01:11:59.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-683" for this suite. 07/19/23 01:11:59.263
------------------------------
• [SLOW TEST] [150.241 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:09:29.024
    Jul 19 01:09:29.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename container-probe 07/19/23 01:09:29.025
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:09:29.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:09:29.033
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-d3695080-ffc3-4feb-9dad-7a620a898772 in namespace container-probe-683 07/19/23 01:09:29.035
    Jul 19 01:09:29.039: INFO: Waiting up to 5m0s for pod "liveness-d3695080-ffc3-4feb-9dad-7a620a898772" in namespace "container-probe-683" to be "not pending"
    Jul 19 01:09:29.042: INFO: Pod "liveness-d3695080-ffc3-4feb-9dad-7a620a898772": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546958ms
    Jul 19 01:09:31.044: INFO: Pod "liveness-d3695080-ffc3-4feb-9dad-7a620a898772": Phase="Running", Reason="", readiness=true. Elapsed: 2.005053424s
    Jul 19 01:09:31.044: INFO: Pod "liveness-d3695080-ffc3-4feb-9dad-7a620a898772" satisfied condition "not pending"
    Jul 19 01:09:31.044: INFO: Started pod liveness-d3695080-ffc3-4feb-9dad-7a620a898772 in namespace container-probe-683
    STEP: checking the pod's current state and verifying that restartCount is present 07/19/23 01:09:31.044
    Jul 19 01:09:31.046: INFO: Initial restart count of pod liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is 0
    Jul 19 01:09:51.073: INFO: Restart count of pod container-probe-683/liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is now 1 (20.026899372s elapsed)
    Jul 19 01:10:11.101: INFO: Restart count of pod container-probe-683/liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is now 2 (40.05497992s elapsed)
    Jul 19 01:10:31.128: INFO: Restart count of pod container-probe-683/liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is now 3 (1m0.082224171s elapsed)
    Jul 19 01:10:51.158: INFO: Restart count of pod container-probe-683/liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is now 4 (1m20.111894977s elapsed)
    Jul 19 01:11:59.254: INFO: Restart count of pod container-probe-683/liveness-d3695080-ffc3-4feb-9dad-7a620a898772 is now 5 (2m28.208356608s elapsed)
    STEP: deleting the pod 07/19/23 01:11:59.254
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:11:59.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-683" for this suite. 07/19/23 01:11:59.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:11:59.266
Jul 19 01:11:59.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename deployment 07/19/23 01:11:59.267
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:11:59.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:11:59.278
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jul 19 01:11:59.284: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 19 01:12:04.288: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 07/19/23 01:12:04.288
Jul 19 01:12:04.288: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 19 01:12:06.291: INFO: Creating deployment "test-rollover-deployment"
Jul 19 01:12:06.296: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 19 01:12:08.300: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 19 01:12:08.303: INFO: Ensure that both replica sets have 1 created replica
Jul 19 01:12:08.307: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 19 01:12:08.311: INFO: Updating deployment test-rollover-deployment
Jul 19 01:12:08.311: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 19 01:12:10.316: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 19 01:12:10.319: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 19 01:12:10.322: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 01:12:10.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:12:12.327: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 01:12:12.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:12:14.328: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 01:12:14.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:12:16.327: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 01:12:16.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:12:18.327: INFO: all replica sets need to contain the pod-template-hash label
Jul 19 01:12:18.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 19 01:12:20.326: INFO: 
Jul 19 01:12:20.326: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jul 19 01:12:20.330: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2852  d78dae65-7a9a-410a-a93a-0fc5935ede7a 93758 2 2023-07-19 01:12:06 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041cd8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-19 01:12:06 +0000 UTC,LastTransitionTime:2023-07-19 01:12:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-07-19 01:12:19 +0000 UTC,LastTransitionTime:2023-07-19 01:12:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 19 01:12:20.333: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2852  dde9a246-0187-482f-b47d-921b1f7f72cb 93748 2 2023-07-19 01:12:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d78dae65-7a9a-410a-a93a-0fc5935ede7a 0xc0041cdd97 0xc0041cdd98}] [] [{kube-controller-manager Update apps/v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d78dae65-7a9a-410a-a93a-0fc5935ede7a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:12:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041cde48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 19 01:12:20.333: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 19 01:12:20.333: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2852  7d16f35a-a65b-447f-8311-5e7f36ac1fa7 93757 2 2023-07-19 01:11:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d78dae65-7a9a-410a-a93a-0fc5935ede7a 0xc0041cdc57 0xc0041cdc58}] [] [{e2e.test Update apps/v1 2023-07-19 01:11:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d78dae65-7a9a-410a-a93a-0fc5935ede7a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:12:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041cdd28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 01:12:20.333: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2852  c4708e48-f504-4f90-92e9-00c200b55782 93665 2 2023-07-19 01:12:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d78dae65-7a9a-410a-a93a-0fc5935ede7a 0xc0041cdec7 0xc0041cdec8}] [] [{kube-controller-manager Update apps/v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d78dae65-7a9a-410a-a93a-0fc5935ede7a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041cdf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 19 01:12:20.335: INFO: Pod "test-rollover-deployment-6c6df9974f-nrh5m" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-nrh5m test-rollover-deployment-6c6df9974f- deployment-2852  693e6fa9-7871-471e-befa-f983bf137e60 93687 0 2023-07-19 01:12:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:dca53b2e064bc8f097df021e1c64fe71c0cd13c40da1c05e10c3d4db597c8201 cni.projectcalico.org/podIP:172.16.166.137/32 cni.projectcalico.org/podIPs:172.16.166.137/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.137"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "chain",
    "ips": [
        "172.16.166.137"
    ],
    "default": true,
    "dns": {}
}]] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f dde9a246-0187-482f-b47d-921b1f7f72cb 0xc007bc3667 0xc007bc3668}] [] [{calico Update v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dde9a246-0187-482f-b47d-921b1f7f72cb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 01:12:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8gbzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8gbzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:12:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.137,StartTime:2023-07-19 01:12:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 01:12:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1b44f8930009b719c2198edfd7ae85441b5ef8fb02341ac256ad4e71be3c38ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jul 19 01:12:20.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2852" for this suite. 07/19/23 01:12:20.337
------------------------------
• [SLOW TEST] [21.073 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:11:59.266
    Jul 19 01:11:59.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename deployment 07/19/23 01:11:59.267
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:11:59.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:11:59.278
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jul 19 01:11:59.284: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jul 19 01:12:04.288: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 07/19/23 01:12:04.288
    Jul 19 01:12:04.288: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jul 19 01:12:06.291: INFO: Creating deployment "test-rollover-deployment"
    Jul 19 01:12:06.296: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jul 19 01:12:08.300: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jul 19 01:12:08.303: INFO: Ensure that both replica sets have 1 created replica
    Jul 19 01:12:08.307: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jul 19 01:12:08.311: INFO: Updating deployment test-rollover-deployment
    Jul 19 01:12:08.311: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jul 19 01:12:10.316: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jul 19 01:12:10.319: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jul 19 01:12:10.322: INFO: all replica sets need to contain the pod-template-hash label
    Jul 19 01:12:10.322: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:12:12.327: INFO: all replica sets need to contain the pod-template-hash label
    Jul 19 01:12:12.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:12:14.328: INFO: all replica sets need to contain the pod-template-hash label
    Jul 19 01:12:14.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:12:16.327: INFO: all replica sets need to contain the pod-template-hash label
    Jul 19 01:12:16.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:12:18.327: INFO: all replica sets need to contain the pod-template-hash label
    Jul 19 01:12:18.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 19, 1, 12, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 19, 1, 12, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jul 19 01:12:20.326: INFO: 
    Jul 19 01:12:20.326: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jul 19 01:12:20.330: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2852  d78dae65-7a9a-410a-a93a-0fc5935ede7a 93758 2 2023-07-19 01:12:06 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041cd8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-19 01:12:06 +0000 UTC,LastTransitionTime:2023-07-19 01:12:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-07-19 01:12:19 +0000 UTC,LastTransitionTime:2023-07-19 01:12:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jul 19 01:12:20.333: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-2852  dde9a246-0187-482f-b47d-921b1f7f72cb 93748 2 2023-07-19 01:12:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d78dae65-7a9a-410a-a93a-0fc5935ede7a 0xc0041cdd97 0xc0041cdd98}] [] [{kube-controller-manager Update apps/v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d78dae65-7a9a-410a-a93a-0fc5935ede7a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:12:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041cde48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 01:12:20.333: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jul 19 01:12:20.333: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2852  7d16f35a-a65b-447f-8311-5e7f36ac1fa7 93757 2 2023-07-19 01:11:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d78dae65-7a9a-410a-a93a-0fc5935ede7a 0xc0041cdc57 0xc0041cdc58}] [] [{e2e.test Update apps/v1 2023-07-19 01:11:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d78dae65-7a9a-410a-a93a-0fc5935ede7a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:12:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041cdd28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 01:12:20.333: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-2852  c4708e48-f504-4f90-92e9-00c200b55782 93665 2 2023-07-19 01:12:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d78dae65-7a9a-410a-a93a-0fc5935ede7a 0xc0041cdec7 0xc0041cdec8}] [] [{kube-controller-manager Update apps/v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d78dae65-7a9a-410a-a93a-0fc5935ede7a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041cdf78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jul 19 01:12:20.335: INFO: Pod "test-rollover-deployment-6c6df9974f-nrh5m" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-nrh5m test-rollover-deployment-6c6df9974f- deployment-2852  693e6fa9-7871-471e-befa-f983bf137e60 93687 0 2023-07-19 01:12:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:dca53b2e064bc8f097df021e1c64fe71c0cd13c40da1c05e10c3d4db597c8201 cni.projectcalico.org/podIP:172.16.166.137/32 cni.projectcalico.org/podIPs:172.16.166.137/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.137"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "chain",
        "ips": [
            "172.16.166.137"
        ],
        "default": true,
        "dns": {}
    }]] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f dde9a246-0187-482f-b47d-921b1f7f72cb 0xc007bc3667 0xc007bc3668}] [] [{calico Update v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dde9a246-0187-482f-b47d-921b1f7f72cb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-07-19 01:12:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-07-19 01:12:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.166.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8gbzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8gbzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:controller-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*30,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-19 01:12:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.206.3,PodIP:172.16.166.137,StartTime:2023-07-19 01:12:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-19 01:12:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1b44f8930009b719c2198edfd7ae85441b5ef8fb02341ac256ad4e71be3c38ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.166.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:12:20.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2852" for this suite. 07/19/23 01:12:20.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:12:20.346
Jul 19 01:12:20.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 01:12:20.347
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:12:20.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:12:20.355
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 07/19/23 01:12:20.357
STEP: listing secrets in all namespaces to ensure that there are more than zero 07/19/23 01:12:20.359
STEP: patching the secret 07/19/23 01:12:20.366
STEP: deleting the secret using a LabelSelector 07/19/23 01:12:20.37
STEP: listing secrets in all namespaces, searching for label name and value in patch 07/19/23 01:12:20.373
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 01:12:20.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8885" for this suite. 07/19/23 01:12:20.382
------------------------------
• [0.038 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:12:20.346
    Jul 19 01:12:20.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 01:12:20.347
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:12:20.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:12:20.355
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 07/19/23 01:12:20.357
    STEP: listing secrets in all namespaces to ensure that there are more than zero 07/19/23 01:12:20.359
    STEP: patching the secret 07/19/23 01:12:20.366
    STEP: deleting the secret using a LabelSelector 07/19/23 01:12:20.37
    STEP: listing secrets in all namespaces, searching for label name and value in patch 07/19/23 01:12:20.373
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:12:20.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8885" for this suite. 07/19/23 01:12:20.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:12:20.385
Jul 19 01:12:20.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename secrets 07/19/23 01:12:20.386
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:12:20.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:12:20.393
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-bd56dcb4-019f-468d-bdf2-0b5903095e5d 07/19/23 01:12:20.395
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jul 19 01:12:20.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1091" for this suite. 07/19/23 01:12:20.399
------------------------------
• [0.017 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:12:20.385
    Jul 19 01:12:20.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename secrets 07/19/23 01:12:20.386
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:12:20.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:12:20.393
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-bd56dcb4-019f-468d-bdf2-0b5903095e5d 07/19/23 01:12:20.395
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:12:20.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1091" for this suite. 07/19/23 01:12:20.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 07/19/23 01:12:20.403
Jul 19 01:12:20.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
STEP: Building a namespace api object, basename statefulset 07/19/23 01:12:20.404
STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:12:20.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:12:20.411
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6370 07/19/23 01:12:20.413
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 07/19/23 01:12:20.415
STEP: Creating stateful set ss in namespace statefulset-6370 07/19/23 01:12:20.417
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6370 07/19/23 01:12:20.42
Jul 19 01:12:20.424: INFO: Found 0 stateful pods, waiting for 1
Jul 19 01:12:30.427: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 07/19/23 01:12:30.427
Jul 19 01:12:30.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 01:12:30.537: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 01:12:30.537: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 01:12:30.537: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 01:12:30.539: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 19 01:12:40.542: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 01:12:40.542: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 01:12:40.551: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999794s
Jul 19 01:12:41.553: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998198297s
Jul 19 01:12:42.555: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.996094442s
Jul 19 01:12:43.558: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.99308951s
Jul 19 01:12:44.562: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989958928s
Jul 19 01:12:45.564: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987137049s
Jul 19 01:12:46.567: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.983934628s
Jul 19 01:12:47.570: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.980812926s
Jul 19 01:12:48.573: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978592645s
Jul 19 01:12:49.575: INFO: Verifying statefulset ss doesn't scale past 1 for another 976.033334ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6370 07/19/23 01:12:50.576
Jul 19 01:12:50.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 01:12:50.682: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 01:12:50.682: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 01:12:50.682: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 01:12:50.683: INFO: Found 1 stateful pods, waiting for 3
Jul 19 01:13:00.687: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 01:13:00.687: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 19 01:13:00.687: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 07/19/23 01:13:00.687
STEP: Scale down will halt with unhealthy stateful pod 07/19/23 01:13:00.687
Jul 19 01:13:00.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 01:13:00.789: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 01:13:00.789: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 01:13:00.789: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 01:13:00.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 01:13:00.891: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 01:13:00.891: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 01:13:00.891: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 01:13:00.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 19 01:13:00.997: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 19 01:13:00.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 19 01:13:00.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 19 01:13:00.997: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 01:13:00.999: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 19 01:13:11.004: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 01:13:11.004: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 01:13:11.004: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 19 01:13:11.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999762s
Jul 19 01:13:12.014: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997484282s
Jul 19 01:13:13.017: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994615366s
Jul 19 01:13:14.019: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991629002s
Jul 19 01:13:15.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988812815s
Jul 19 01:13:16.026: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986028088s
Jul 19 01:13:17.028: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982478186s
Jul 19 01:13:18.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97938321s
Jul 19 01:13:19.034: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.976683779s
Jul 19 01:13:20.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.797923ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6370 07/19/23 01:13:21.038
Jul 19 01:13:21.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 01:13:21.147: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 01:13:21.147: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 01:13:21.147: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 01:13:21.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 01:13:21.258: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 01:13:21.258: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 01:13:21.258: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 01:13:21.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 19 01:13:21.355: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 19 01:13:21.355: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 19 01:13:21.355: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 19 01:13:21.355: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 07/19/23 01:13:31.364
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jul 19 01:13:31.364: INFO: Deleting all statefulset in ns statefulset-6370
Jul 19 01:13:31.366: INFO: Scaling statefulset ss to 0
Jul 19 01:13:31.372: INFO: Waiting for statefulset status.replicas updated to 0
Jul 19 01:13:31.374: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jul 19 01:13:31.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6370" for this suite. 07/19/23 01:13:31.381
------------------------------
• [SLOW TEST] [70.981 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 07/19/23 01:12:20.403
    Jul 19 01:12:20.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3519786796
    STEP: Building a namespace api object, basename statefulset 07/19/23 01:12:20.404
    STEP: Waiting for a default service account to be provisioned in namespace 07/19/23 01:12:20.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 07/19/23 01:12:20.411
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6370 07/19/23 01:12:20.413
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 07/19/23 01:12:20.415
    STEP: Creating stateful set ss in namespace statefulset-6370 07/19/23 01:12:20.417
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6370 07/19/23 01:12:20.42
    Jul 19 01:12:20.424: INFO: Found 0 stateful pods, waiting for 1
    Jul 19 01:12:30.427: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 07/19/23 01:12:30.427
    Jul 19 01:12:30.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 19 01:12:30.537: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 19 01:12:30.537: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 19 01:12:30.537: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 19 01:12:30.539: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jul 19 01:12:40.542: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jul 19 01:12:40.542: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 19 01:12:40.551: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999794s
    Jul 19 01:12:41.553: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998198297s
    Jul 19 01:12:42.555: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.996094442s
    Jul 19 01:12:43.558: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.99308951s
    Jul 19 01:12:44.562: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989958928s
    Jul 19 01:12:45.564: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987137049s
    Jul 19 01:12:46.567: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.983934628s
    Jul 19 01:12:47.570: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.980812926s
    Jul 19 01:12:48.573: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978592645s
    Jul 19 01:12:49.575: INFO: Verifying statefulset ss doesn't scale past 1 for another 976.033334ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6370 07/19/23 01:12:50.576
    Jul 19 01:12:50.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 19 01:12:50.682: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 19 01:12:50.682: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 19 01:12:50.682: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 19 01:12:50.683: INFO: Found 1 stateful pods, waiting for 3
    Jul 19 01:13:00.687: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 01:13:00.687: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jul 19 01:13:00.687: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 07/19/23 01:13:00.687
    STEP: Scale down will halt with unhealthy stateful pod 07/19/23 01:13:00.687
    Jul 19 01:13:00.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 19 01:13:00.789: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 19 01:13:00.789: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 19 01:13:00.789: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 19 01:13:00.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 19 01:13:00.891: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 19 01:13:00.891: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 19 01:13:00.891: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 19 01:13:00.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jul 19 01:13:00.997: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jul 19 01:13:00.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jul 19 01:13:00.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jul 19 01:13:00.997: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 19 01:13:00.999: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jul 19 01:13:11.004: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jul 19 01:13:11.004: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jul 19 01:13:11.004: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jul 19 01:13:11.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999762s
    Jul 19 01:13:12.014: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997484282s
    Jul 19 01:13:13.017: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994615366s
    Jul 19 01:13:14.019: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991629002s
    Jul 19 01:13:15.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988812815s
    Jul 19 01:13:16.026: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986028088s
    Jul 19 01:13:17.028: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982478186s
    Jul 19 01:13:18.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97938321s
    Jul 19 01:13:19.034: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.976683779s
    Jul 19 01:13:20.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.797923ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6370 07/19/23 01:13:21.038
    Jul 19 01:13:21.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 19 01:13:21.147: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 19 01:13:21.147: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 19 01:13:21.147: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 19 01:13:21.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 19 01:13:21.258: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 19 01:13:21.258: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 19 01:13:21.258: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 19 01:13:21.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3519786796 --namespace=statefulset-6370 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jul 19 01:13:21.355: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jul 19 01:13:21.355: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jul 19 01:13:21.355: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jul 19 01:13:21.355: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 07/19/23 01:13:31.364
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jul 19 01:13:31.364: INFO: Deleting all statefulset in ns statefulset-6370
    Jul 19 01:13:31.366: INFO: Scaling statefulset ss to 0
    Jul 19 01:13:31.372: INFO: Waiting for statefulset status.replicas updated to 0
    Jul 19 01:13:31.374: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jul 19 01:13:31.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6370" for this suite. 07/19/23 01:13:31.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jul 19 01:13:31.385: INFO: Running AfterSuite actions on node 1
Jul 19 01:13:31.385: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jul 19 01:13:31.385: INFO: Running AfterSuite actions on node 1
    Jul 19 01:13:31.385: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.077 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5336.674 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h28m56.944823976s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

