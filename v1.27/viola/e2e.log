  I0522 05:20:19.221159      25 e2e.go:117] Starting e2e run "58979e28-e9ea-4c59-9bf7-7b6c51b0e65a" on Ginkgo node 1
  May 22 05:20:19.253: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1684732819 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  May 22 05:20:19.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:20:19.417: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  May 22 05:20:19.441: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  May 22 05:20:19.443: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  May 22 05:20:19.443: INFO: e2e test version: v1.27.1
  May 22 05:20:19.443: INFO: kube-apiserver version: v1.27.1
  May 22 05:20:19.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:20:19.446: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 05/22/23 05:20:19.682
  May 22 05:20:19.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 05:20:19.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:20:19.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:20:19.695
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/22/23 05:20:19.697
  STEP: Saw pod success @ 05/22/23 05:20:23.71
  May 22 05:20:23.712: INFO: Trying to get logs from node node2 pod pod-2a1949ac-f7e1-42e8-af88-f12e290c7fda container test-container: <nil>
  STEP: delete the pod @ 05/22/23 05:20:23.724
  May 22 05:20:23.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3015" for this suite. @ 05/22/23 05:20:23.734
• [4.054 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 05/22/23 05:20:23.737
  May 22 05:20:23.737: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:20:23.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:20:23.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:20:23.748
  STEP: Creating the pod @ 05/22/23 05:20:23.75
  May 22 05:20:26.275: INFO: Successfully updated pod "annotationupdate8464b0cb-8d4a-4252-bb8d-07188d7e0cdf"
  May 22 05:20:30.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2944" for this suite. @ 05/22/23 05:20:30.294
• [6.560 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 05/22/23 05:20:30.298
  May 22 05:20:30.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 05:20:30.299
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:20:30.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:20:30.308
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-2830 @ 05/22/23 05:20:30.31
  STEP: changing the ExternalName service to type=ClusterIP @ 05/22/23 05:20:30.312
  STEP: creating replication controller externalname-service in namespace services-2830 @ 05/22/23 05:20:30.322
  I0522 05:20:30.325921      25 runners.go:194] Created replication controller with name: externalname-service, namespace: services-2830, replica count: 2
  I0522 05:20:33.376855      25 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 22 05:20:33.376: INFO: Creating new exec pod
  May 22 05:20:36.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-2830 exec execpodpcwld -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 22 05:20:36.514: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 22 05:20:36.514: INFO: stdout: "externalname-service-xd4w4"
  May 22 05:20:36.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-2830 exec execpodpcwld -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.178.30 80'
  May 22 05:20:36.637: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.178.30 80\nConnection to 10.108.178.30 80 port [tcp/http] succeeded!\n"
  May 22 05:20:36.637: INFO: stdout: "externalname-service-4pfb8"
  May 22 05:20:36.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 05:20:36.640: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-2830" for this suite. @ 05/22/23 05:20:36.651
• [6.357 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 05/22/23 05:20:36.655
  May 22 05:20:36.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:20:36.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:20:36.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:20:36.665
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 05:20:36.667
  STEP: Saw pod success @ 05/22/23 05:20:40.681
  May 22 05:20:40.683: INFO: Trying to get logs from node node2 pod downwardapi-volume-932ab172-f64d-43f5-88d7-3f6e96a25d55 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 05:20:40.687
  May 22 05:20:40.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4681" for this suite. @ 05/22/23 05:20:40.696
• [4.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 05/22/23 05:20:40.699
  May 22 05:20:40.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename podtemplate @ 05/22/23 05:20:40.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:20:40.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:20:40.708
  May 22 05:20:40.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6071" for this suite. @ 05/22/23 05:20:40.727
• [0.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/22/23 05:20:40.73
  May 22 05:20:40.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename cronjob @ 05/22/23 05:20:40.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:20:40.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:20:40.739
  STEP: Creating a suspended cronjob @ 05/22/23 05:20:40.741
  STEP: Ensuring no jobs are scheduled @ 05/22/23 05:20:40.744
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/22/23 05:25:40.75
  STEP: Removing cronjob @ 05/22/23 05:25:40.751
  May 22 05:25:40.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9332" for this suite. @ 05/22/23 05:25:40.757
• [300.030 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 05/22/23 05:25:40.76
  May 22 05:25:40.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-probe @ 05/22/23 05:25:40.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:25:40.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:25:40.768
  May 22 05:26:40.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1843" for this suite. @ 05/22/23 05:26:40.78
• [60.024 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 05/22/23 05:26:40.784
  May 22 05:26:40.784: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 05:26:40.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:26:40.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:26:40.796
  STEP: creating a replication controller @ 05/22/23 05:26:40.799
  May 22 05:26:40.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 create -f -'
  May 22 05:26:41.830: INFO: stderr: ""
  May 22 05:26:41.830: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/22/23 05:26:41.83
  May 22 05:26:41.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 22 05:26:41.893: INFO: stderr: ""
  May 22 05:26:41.893: INFO: stdout: "update-demo-nautilus-fkpv7 update-demo-nautilus-qv92f "
  May 22 05:26:41.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 get pods update-demo-nautilus-fkpv7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 22 05:26:41.952: INFO: stderr: ""
  May 22 05:26:41.952: INFO: stdout: ""
  May 22 05:26:41.952: INFO: update-demo-nautilus-fkpv7 is created but not running
  May 22 05:26:46.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 22 05:26:47.014: INFO: stderr: ""
  May 22 05:26:47.014: INFO: stdout: "update-demo-nautilus-fkpv7 update-demo-nautilus-qv92f "
  May 22 05:26:47.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 get pods update-demo-nautilus-fkpv7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 22 05:26:47.071: INFO: stderr: ""
  May 22 05:26:47.071: INFO: stdout: "true"
  May 22 05:26:47.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 get pods update-demo-nautilus-fkpv7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 22 05:26:47.140: INFO: stderr: ""
  May 22 05:26:47.140: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 22 05:26:47.140: INFO: validating pod update-demo-nautilus-fkpv7
  May 22 05:26:47.143: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 22 05:26:47.143: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 22 05:26:47.143: INFO: update-demo-nautilus-fkpv7 is verified up and running
  May 22 05:26:47.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 get pods update-demo-nautilus-qv92f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 22 05:26:47.199: INFO: stderr: ""
  May 22 05:26:47.199: INFO: stdout: "true"
  May 22 05:26:47.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 get pods update-demo-nautilus-qv92f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 22 05:26:47.257: INFO: stderr: ""
  May 22 05:26:47.257: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 22 05:26:47.257: INFO: validating pod update-demo-nautilus-qv92f
  May 22 05:26:47.260: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 22 05:26:47.260: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 22 05:26:47.260: INFO: update-demo-nautilus-qv92f is verified up and running
  STEP: using delete to clean up resources @ 05/22/23 05:26:47.26
  May 22 05:26:47.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 delete --grace-period=0 --force -f -'
  May 22 05:26:47.319: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 22 05:26:47.319: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 22 05:26:47.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 get rc,svc -l name=update-demo --no-headers'
  May 22 05:26:47.390: INFO: stderr: "No resources found in kubectl-6274 namespace.\n"
  May 22 05:26:47.390: INFO: stdout: ""
  May 22 05:26:47.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-6274 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 22 05:26:47.462: INFO: stderr: ""
  May 22 05:26:47.462: INFO: stdout: ""
  May 22 05:26:47.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6274" for this suite. @ 05/22/23 05:26:47.465
• [6.684 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 05/22/23 05:26:47.471
  May 22 05:26:47.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 05:26:47.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:26:47.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:26:47.48
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2665 @ 05/22/23 05:26:47.482
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/22/23 05:26:47.489
  STEP: creating service externalsvc in namespace services-2665 @ 05/22/23 05:26:47.489
  STEP: creating replication controller externalsvc in namespace services-2665 @ 05/22/23 05:26:47.499
  I0522 05:26:47.503146      25 runners.go:194] Created replication controller with name: externalsvc, namespace: services-2665, replica count: 2
  I0522 05:26:50.556080      25 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/22/23 05:26:50.558
  May 22 05:26:50.568: INFO: Creating new exec pod
  May 22 05:26:52.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-2665 exec execpodw24xp -- /bin/sh -x -c nslookup clusterip-service.services-2665.svc.cluster.local'
  May 22 05:26:52.736: INFO: stderr: "+ nslookup clusterip-service.services-2665.svc.cluster.local\n"
  May 22 05:26:52.736: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2665.svc.cluster.local\tcanonical name = externalsvc.services-2665.svc.cluster.local.\nName:\texternalsvc.services-2665.svc.cluster.local\nAddress: 10.96.72.163\n\n"
  May 22 05:26:52.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-2665, will wait for the garbage collector to delete the pods @ 05/22/23 05:26:52.739
  May 22 05:26:52.795: INFO: Deleting ReplicationController externalsvc took: 4.042796ms
  May 22 05:26:52.896: INFO: Terminating ReplicationController externalsvc pods took: 101.02241ms
  May 22 05:26:55.108: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-2665" for this suite. @ 05/22/23 05:26:55.113
• [7.646 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 05/22/23 05:26:55.116
  May 22 05:26:55.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/22/23 05:26:55.117
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:26:55.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:26:55.127
  STEP: set up a multi version CRD @ 05/22/23 05:26:55.128
  May 22 05:26:55.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: rename a version @ 05/22/23 05:26:58.864
  STEP: check the new version name is served @ 05/22/23 05:26:58.874
  STEP: check the old version name is removed @ 05/22/23 05:27:00.278
  STEP: check the other version is not changed @ 05/22/23 05:27:01.087
  May 22 05:27:04.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2586" for this suite. @ 05/22/23 05:27:04.092
• [8.979 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 05/22/23 05:27:04.096
  May 22 05:27:04.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 05:27:04.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:27:04.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:27:04.107
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/22/23 05:27:04.109
  May 22 05:27:04.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-1893 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 22 05:27:04.173: INFO: stderr: ""
  May 22 05:27:04.173: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/22/23 05:27:04.173
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/22/23 05:27:09.224
  May 22 05:27:09.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-1893 get pod e2e-test-httpd-pod -o json'
  May 22 05:27:09.298: INFO: stderr: ""
  May 22 05:27:09.298: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"2bbe949652a5deff23e344c3c5940b9f2beaebaa0335cbbcc6f72e442d86d6f1\",\n            \"cni.projectcalico.org/podIP\": \"192.168.104.32/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.104.32/32\"\n        },\n        \"creationTimestamp\": \"2023-05-22T05:27:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1893\",\n        \"resourceVersion\": \"871771\",\n        \"uid\": \"90713b39-5cdb-4210-862b-26608f76a9f6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-nqgvx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-nqgvx\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-22T05:27:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-22T05:27:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-22T05:27:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-22T05:27:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a19b6b3aafed576d486541e738cf872a9d6b653d068640b5ecfa808db93805ca\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-22T05:27:04Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.33.122\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.104.32\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.104.32\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-22T05:27:04Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/22/23 05:27:09.299
  May 22 05:27:09.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-1893 replace -f -'
  May 22 05:27:09.613: INFO: stderr: ""
  May 22 05:27:09.613: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 05/22/23 05:27:09.613
  May 22 05:27:09.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-1893 delete pods e2e-test-httpd-pod'
  May 22 05:27:11.762: INFO: stderr: ""
  May 22 05:27:11.762: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 22 05:27:11.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1893" for this suite. @ 05/22/23 05:27:11.765
• [7.672 seconds]
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 05/22/23 05:27:11.768
  May 22 05:27:11.768: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename events @ 05/22/23 05:27:11.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:27:11.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:27:11.782
  STEP: Create set of events @ 05/22/23 05:27:11.784
  May 22 05:27:11.786: INFO: created test-event-1
  May 22 05:27:11.788: INFO: created test-event-2
  May 22 05:27:11.790: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/22/23 05:27:11.79
  STEP: delete collection of events @ 05/22/23 05:27:11.792
  May 22 05:27:11.792: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/22/23 05:27:11.799
  May 22 05:27:11.799: INFO: requesting list of events to confirm quantity
  May 22 05:27:11.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5631" for this suite. @ 05/22/23 05:27:11.802
• [0.038 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 05/22/23 05:27:11.806
  May 22 05:27:11.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/22/23 05:27:11.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:27:11.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:27:11.815
  STEP: fetching the /apis discovery document @ 05/22/23 05:27:11.816
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/22/23 05:27:11.817
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/22/23 05:27:11.817
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/22/23 05:27:11.817
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/22/23 05:27:11.817
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/22/23 05:27:11.817
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/22/23 05:27:11.818
  May 22 05:27:11.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5681" for this suite. @ 05/22/23 05:27:11.821
• [0.018 seconds]
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 05/22/23 05:27:11.824
  May 22 05:27:11.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename podtemplate @ 05/22/23 05:27:11.825
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:27:11.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:27:11.834
  STEP: Create set of pod templates @ 05/22/23 05:27:11.835
  May 22 05:27:11.839: INFO: created test-podtemplate-1
  May 22 05:27:11.841: INFO: created test-podtemplate-2
  May 22 05:27:11.843: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/22/23 05:27:11.843
  STEP: delete collection of pod templates @ 05/22/23 05:27:11.845
  May 22 05:27:11.845: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 05/22/23 05:27:11.85
  May 22 05:27:11.850: INFO: requesting list of pod templates to confirm quantity
  May 22 05:27:11.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4077" for this suite. @ 05/22/23 05:27:11.854
• [0.033 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 05/22/23 05:27:11.857
  May 22 05:27:11.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-probe @ 05/22/23 05:27:11.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:27:11.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:27:11.866
  STEP: Creating pod test-grpc-384ce4c3-fad1-4a7f-b6ff-94abbe6cda2e in namespace container-probe-8415 @ 05/22/23 05:27:11.868
  May 22 05:27:13.878: INFO: Started pod test-grpc-384ce4c3-fad1-4a7f-b6ff-94abbe6cda2e in namespace container-probe-8415
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/22/23 05:27:13.878
  May 22 05:27:13.880: INFO: Initial restart count of pod test-grpc-384ce4c3-fad1-4a7f-b6ff-94abbe6cda2e is 0
  May 22 05:31:14.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 05:31:14.281
  STEP: Destroying namespace "container-probe-8415" for this suite. @ 05/22/23 05:31:14.288
• [242.433 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 05/22/23 05:31:14.291
  May 22 05:31:14.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:31:14.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:31:14.299
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:31:14.301
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 05:31:14.303
  STEP: Saw pod success @ 05/22/23 05:31:18.317
  May 22 05:31:18.319: INFO: Trying to get logs from node node2 pod downwardapi-volume-5fbd1f5e-4b88-44e5-9c6a-61c5b936f015 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 05:31:18.333
  May 22 05:31:18.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-211" for this suite. @ 05/22/23 05:31:18.343
• [4.056 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 05/22/23 05:31:18.348
  May 22 05:31:18.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename field-validation @ 05/22/23 05:31:18.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:31:18.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:31:18.357
  STEP: apply creating a deployment @ 05/22/23 05:31:18.359
  May 22 05:31:18.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8493" for this suite. @ 05/22/23 05:31:18.368
• [0.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 05/22/23 05:31:18.373
  May 22 05:31:18.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename svcaccounts @ 05/22/23 05:31:18.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:31:18.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:31:18.383
  STEP: creating a ServiceAccount @ 05/22/23 05:31:18.385
  STEP: watching for the ServiceAccount to be added @ 05/22/23 05:31:18.389
  STEP: patching the ServiceAccount @ 05/22/23 05:31:18.391
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/22/23 05:31:18.394
  STEP: deleting the ServiceAccount @ 05/22/23 05:31:18.396
  May 22 05:31:18.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7952" for this suite. @ 05/22/23 05:31:18.403
• [0.032 seconds]
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/22/23 05:31:18.406
  May 22 05:31:18.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 05:31:18.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:31:18.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:31:18.414
  May 22 05:31:18.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: creating the pod @ 05/22/23 05:31:18.417
  STEP: submitting the pod to kubernetes @ 05/22/23 05:31:18.417
  May 22 05:31:20.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7019" for this suite. @ 05/22/23 05:31:20.506
• [2.103 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 05/22/23 05:31:20.509
  May 22 05:31:20.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename endpointslice @ 05/22/23 05:31:20.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:31:20.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:31:20.519
  May 22 05:31:20.524: INFO: Endpoints addresses: [192.168.34.171] , ports: [6443]
  May 22 05:31:20.524: INFO: EndpointSlices addresses: [192.168.34.171] , ports: [6443]
  May 22 05:31:20.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3279" for this suite. @ 05/22/23 05:31:20.527
• [0.020 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 05/22/23 05:31:20.531
  May 22 05:31:20.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/22/23 05:31:20.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:31:20.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:31:20.54
  May 22 05:31:20.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:31:23.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8772" for this suite. @ 05/22/23 05:31:23.709
• [3.182 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 05/22/23 05:31:23.713
  May 22 05:31:23.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename events @ 05/22/23 05:31:23.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:31:23.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:31:23.725
  STEP: creating a test event @ 05/22/23 05:31:23.727
  STEP: listing all events in all namespaces @ 05/22/23 05:31:23.73
  STEP: patching the test event @ 05/22/23 05:31:23.732
  STEP: fetching the test event @ 05/22/23 05:31:23.736
  STEP: updating the test event @ 05/22/23 05:31:23.737
  STEP: getting the test event @ 05/22/23 05:31:23.742
  STEP: deleting the test event @ 05/22/23 05:31:23.744
  STEP: listing all events in all namespaces @ 05/22/23 05:31:23.746
  May 22 05:31:23.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4655" for this suite. @ 05/22/23 05:31:23.751
• [0.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/22/23 05:31:23.756
  May 22 05:31:23.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename cronjob @ 05/22/23 05:31:23.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:31:23.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:31:23.765
  STEP: Creating a ForbidConcurrent cronjob @ 05/22/23 05:31:23.767
  STEP: Ensuring a job is scheduled @ 05/22/23 05:31:23.77
  STEP: Ensuring exactly one is scheduled @ 05/22/23 05:32:01.774
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/22/23 05:32:01.776
  STEP: Ensuring no more jobs are scheduled @ 05/22/23 05:32:01.778
  STEP: Removing cronjob @ 05/22/23 05:37:01.784
  May 22 05:37:01.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7454" for this suite. @ 05/22/23 05:37:01.791
• [338.039 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 05/22/23 05:37:01.796
  May 22 05:37:01.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename dns @ 05/22/23 05:37:01.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:37:01.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:37:01.808
  STEP: Creating a test headless service @ 05/22/23 05:37:01.81
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1495 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1495;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1495 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1495;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1495.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1495.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1495.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1495.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1495.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1495.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1495.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1495.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1495.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1495.svc;check="$$(dig +notcp +noall +answer +search 116.168.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.168.116_udp@PTR;check="$$(dig +tcp +noall +answer +search 116.168.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.168.116_tcp@PTR;sleep 1; done
   @ 05/22/23 05:37:01.82
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1495 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1495;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1495 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1495;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1495.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1495.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1495.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1495.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1495.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1495.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1495.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1495.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1495.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1495.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1495.svc;check="$$(dig +notcp +noall +answer +search 116.168.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.168.116_udp@PTR;check="$$(dig +tcp +noall +answer +search 116.168.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.168.116_tcp@PTR;sleep 1; done
   @ 05/22/23 05:37:01.82
  STEP: creating a pod to probe DNS @ 05/22/23 05:37:01.82
  STEP: submitting the pod to kubernetes @ 05/22/23 05:37:01.82
  STEP: retrieving the pod @ 05/22/23 05:37:03.831
  STEP: looking for the results for each expected name from probers @ 05/22/23 05:37:03.833
  May 22 05:37:03.835: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.837: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.839: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.841: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.843: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.845: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.847: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.849: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.858: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.860: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.862: INFO: Unable to read jessie_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.864: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.865: INFO: Unable to read jessie_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.867: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.869: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.871: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:03.877: INFO: Lookups using dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1495 wheezy_tcp@dns-test-service.dns-1495 wheezy_udp@dns-test-service.dns-1495.svc wheezy_tcp@dns-test-service.dns-1495.svc wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1495 jessie_tcp@dns-test-service.dns-1495 jessie_udp@dns-test-service.dns-1495.svc jessie_tcp@dns-test-service.dns-1495.svc jessie_udp@_http._tcp.dns-test-service.dns-1495.svc jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc]

  May 22 05:37:08.881: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.883: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.886: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.887: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.890: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.892: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.894: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.895: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.904: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.906: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.908: INFO: Unable to read jessie_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.909: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.911: INFO: Unable to read jessie_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.914: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.916: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.918: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:08.924: INFO: Lookups using dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1495 wheezy_tcp@dns-test-service.dns-1495 wheezy_udp@dns-test-service.dns-1495.svc wheezy_tcp@dns-test-service.dns-1495.svc wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1495 jessie_tcp@dns-test-service.dns-1495 jessie_udp@dns-test-service.dns-1495.svc jessie_tcp@dns-test-service.dns-1495.svc jessie_udp@_http._tcp.dns-test-service.dns-1495.svc jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc]

  May 22 05:37:13.885: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.887: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.889: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.891: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.893: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.895: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.897: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.899: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.909: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.911: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.912: INFO: Unable to read jessie_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.914: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.916: INFO: Unable to read jessie_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.918: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.921: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.923: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:13.931: INFO: Lookups using dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1495 wheezy_tcp@dns-test-service.dns-1495 wheezy_udp@dns-test-service.dns-1495.svc wheezy_tcp@dns-test-service.dns-1495.svc wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1495 jessie_tcp@dns-test-service.dns-1495 jessie_udp@dns-test-service.dns-1495.svc jessie_tcp@dns-test-service.dns-1495.svc jessie_udp@_http._tcp.dns-test-service.dns-1495.svc jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc]

  May 22 05:37:18.885: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.886: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.888: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.890: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.891: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.893: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.895: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.897: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.906: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.908: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.909: INFO: Unable to read jessie_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.911: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.913: INFO: Unable to read jessie_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.914: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.916: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.917: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:18.924: INFO: Lookups using dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1495 wheezy_tcp@dns-test-service.dns-1495 wheezy_udp@dns-test-service.dns-1495.svc wheezy_tcp@dns-test-service.dns-1495.svc wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1495 jessie_tcp@dns-test-service.dns-1495 jessie_udp@dns-test-service.dns-1495.svc jessie_tcp@dns-test-service.dns-1495.svc jessie_udp@_http._tcp.dns-test-service.dns-1495.svc jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc]

  May 22 05:37:23.882: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.885: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.888: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.890: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.892: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.894: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.896: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.898: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.907: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.908: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.910: INFO: Unable to read jessie_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.911: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.913: INFO: Unable to read jessie_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.915: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.917: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.918: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:23.926: INFO: Lookups using dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1495 wheezy_tcp@dns-test-service.dns-1495 wheezy_udp@dns-test-service.dns-1495.svc wheezy_tcp@dns-test-service.dns-1495.svc wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1495 jessie_tcp@dns-test-service.dns-1495 jessie_udp@dns-test-service.dns-1495.svc jessie_tcp@dns-test-service.dns-1495.svc jessie_udp@_http._tcp.dns-test-service.dns-1495.svc jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc]

  May 22 05:37:28.884: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.887: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.890: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.892: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.895: INFO: Unable to read wheezy_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.897: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.899: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.901: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.916: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.918: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.920: INFO: Unable to read jessie_udp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.922: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495 from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.925: INFO: Unable to read jessie_udp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.927: INFO: Unable to read jessie_tcp@dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.929: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.931: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc from pod dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262: the server could not find the requested resource (get pods dns-test-b80d5100-c779-4911-b354-ce6617841262)
  May 22 05:37:28.940: INFO: Lookups using dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1495 wheezy_tcp@dns-test-service.dns-1495 wheezy_udp@dns-test-service.dns-1495.svc wheezy_tcp@dns-test-service.dns-1495.svc wheezy_udp@_http._tcp.dns-test-service.dns-1495.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1495.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1495 jessie_tcp@dns-test-service.dns-1495 jessie_udp@dns-test-service.dns-1495.svc jessie_tcp@dns-test-service.dns-1495.svc jessie_udp@_http._tcp.dns-test-service.dns-1495.svc jessie_tcp@_http._tcp.dns-test-service.dns-1495.svc]

  May 22 05:37:33.920: INFO: DNS probes using dns-1495/dns-test-b80d5100-c779-4911-b354-ce6617841262 succeeded

  May 22 05:37:33.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 05:37:33.923
  STEP: deleting the test service @ 05/22/23 05:37:33.933
  STEP: deleting the test headless service @ 05/22/23 05:37:33.987
  STEP: Destroying namespace "dns-1495" for this suite. @ 05/22/23 05:37:33.993
• [32.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 05/22/23 05:37:33.997
  May 22 05:37:33.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/22/23 05:37:33.998
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:37:34.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:37:34.009
  May 22 05:37:34.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:37:34.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-3265" for this suite. @ 05/22/23 05:37:34.545
• [0.553 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 05/22/23 05:37:34.551
  May 22 05:37:34.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename security-context @ 05/22/23 05:37:34.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:37:34.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:37:34.56
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/22/23 05:37:34.562
  STEP: Saw pod success @ 05/22/23 05:37:38.574
  May 22 05:37:38.576: INFO: Trying to get logs from node node2 pod security-context-3a38ad5d-5a37-43b7-b7ad-c08fd1004600 container test-container: <nil>
  STEP: delete the pod @ 05/22/23 05:37:38.59
  May 22 05:37:38.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9960" for this suite. @ 05/22/23 05:37:38.601
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 05/22/23 05:37:38.605
  May 22 05:37:38.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename runtimeclass @ 05/22/23 05:37:38.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:37:38.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:37:38.615
  May 22 05:37:38.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3304" for this suite. @ 05/22/23 05:37:38.624
• [0.022 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/22/23 05:37:38.627
  May 22 05:37:38.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 05:37:38.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:37:38.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:37:38.636
  STEP: Creating secret with name s-test-opt-del-5f22a14e-5738-4110-a516-f16e1701ce1a @ 05/22/23 05:37:38.64
  STEP: Creating secret with name s-test-opt-upd-eff425b5-7a1c-44d7-8e6a-44ea63f980ac @ 05/22/23 05:37:38.642
  STEP: Creating the pod @ 05/22/23 05:37:38.644
  STEP: Deleting secret s-test-opt-del-5f22a14e-5738-4110-a516-f16e1701ce1a @ 05/22/23 05:37:40.672
  STEP: Updating secret s-test-opt-upd-eff425b5-7a1c-44d7-8e6a-44ea63f980ac @ 05/22/23 05:37:40.676
  STEP: Creating secret with name s-test-opt-create-7afe502f-6d12-4a0e-b4d6-16c27559e349 @ 05/22/23 05:37:40.681
  STEP: waiting to observe update in volume @ 05/22/23 05:37:40.685
  May 22 05:37:42.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-211" for this suite. @ 05/22/23 05:37:42.704
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 05/22/23 05:37:42.707
  May 22 05:37:42.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename gc @ 05/22/23 05:37:42.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:37:42.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:37:42.719
  STEP: create the rc1 @ 05/22/23 05:37:42.723
  STEP: create the rc2 @ 05/22/23 05:37:42.726
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/22/23 05:37:48.732
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/22/23 05:37:48.988
  STEP: wait for the rc to be deleted @ 05/22/23 05:37:48.99
  May 22 05:37:54.004: INFO: 74 pods remaining
  May 22 05:37:54.004: INFO: 74 pods has nil DeletionTimestamp
  May 22 05:37:54.004: INFO: 
  STEP: Gathering metrics @ 05/22/23 05:37:59.005
  May 22 05:37:59.093: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 22 05:37:59.093: INFO: Deleting pod "simpletest-rc-to-be-deleted-25tpw" in namespace "gc-2978"
  May 22 05:37:59.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-2c4bw" in namespace "gc-2978"
  May 22 05:37:59.108: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fkmf" in namespace "gc-2978"
  May 22 05:37:59.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fpns" in namespace "gc-2978"
  May 22 05:37:59.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xqnv" in namespace "gc-2978"
  May 22 05:37:59.135: INFO: Deleting pod "simpletest-rc-to-be-deleted-45bg4" in namespace "gc-2978"
  May 22 05:37:59.141: INFO: Deleting pod "simpletest-rc-to-be-deleted-48lmd" in namespace "gc-2978"
  May 22 05:37:59.152: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lgsx" in namespace "gc-2978"
  May 22 05:37:59.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mgf4" in namespace "gc-2978"
  May 22 05:37:59.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pfjv" in namespace "gc-2978"
  May 22 05:37:59.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-4t8mt" in namespace "gc-2978"
  May 22 05:37:59.199: INFO: Deleting pod "simpletest-rc-to-be-deleted-549zg" in namespace "gc-2978"
  May 22 05:37:59.208: INFO: Deleting pod "simpletest-rc-to-be-deleted-57w8w" in namespace "gc-2978"
  May 22 05:37:59.228: INFO: Deleting pod "simpletest-rc-to-be-deleted-58kwb" in namespace "gc-2978"
  May 22 05:37:59.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dwtc" in namespace "gc-2978"
  May 22 05:37:59.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fjfz" in namespace "gc-2978"
  May 22 05:37:59.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ncqv" in namespace "gc-2978"
  May 22 05:37:59.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nzz6" in namespace "gc-2978"
  May 22 05:37:59.346: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pjkt" in namespace "gc-2978"
  May 22 05:37:59.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-5q54q" in namespace "gc-2978"
  May 22 05:37:59.409: INFO: Deleting pod "simpletest-rc-to-be-deleted-67pgf" in namespace "gc-2978"
  May 22 05:37:59.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-6d5vq" in namespace "gc-2978"
  May 22 05:37:59.450: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hfg4" in namespace "gc-2978"
  May 22 05:37:59.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mhtb" in namespace "gc-2978"
  May 22 05:37:59.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-72lwm" in namespace "gc-2978"
  May 22 05:37:59.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-77n67" in namespace "gc-2978"
  May 22 05:37:59.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dmk8" in namespace "gc-2978"
  May 22 05:37:59.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-7f9w5" in namespace "gc-2978"
  May 22 05:37:59.639: INFO: Deleting pod "simpletest-rc-to-be-deleted-7j6lr" in namespace "gc-2978"
  May 22 05:37:59.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vnp4" in namespace "gc-2978"
  May 22 05:37:59.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-8djqm" in namespace "gc-2978"
  May 22 05:37:59.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-8t5hw" in namespace "gc-2978"
  May 22 05:37:59.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-8v88k" in namespace "gc-2978"
  May 22 05:37:59.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kdx5" in namespace "gc-2978"
  May 22 05:37:59.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kzcp" in namespace "gc-2978"
  May 22 05:37:59.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjn2m" in namespace "gc-2978"
  May 22 05:37:59.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-clw2g" in namespace "gc-2978"
  May 22 05:37:59.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-cntxp" in namespace "gc-2978"
  May 22 05:37:59.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2llw" in namespace "gc-2978"
  May 22 05:37:59.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-d49s2" in namespace "gc-2978"
  May 22 05:37:59.944: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6ssn" in namespace "gc-2978"
  May 22 05:37:59.981: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9zz4" in namespace "gc-2978"
  May 22 05:38:00.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-df5jh" in namespace "gc-2978"
  May 22 05:38:00.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnlj5" in namespace "gc-2978"
  May 22 05:38:00.144: INFO: Deleting pod "simpletest-rc-to-be-deleted-dp5tz" in namespace "gc-2978"
  May 22 05:38:00.195: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwjw5" in namespace "gc-2978"
  May 22 05:38:00.227: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2fsn" in namespace "gc-2978"
  May 22 05:38:00.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6v64" in namespace "gc-2978"
  May 22 05:38:00.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-ff86x" in namespace "gc-2978"
  May 22 05:38:00.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv2t7" in namespace "gc-2978"
  May 22 05:38:00.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2978" for this suite. @ 05/22/23 05:38:00.333
• [17.631 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 05/22/23 05:38:00.339
  May 22 05:38:00.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 05:38:00.34
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:38:00.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:38:00.366
  STEP: creating Agnhost RC @ 05/22/23 05:38:00.37
  May 22 05:38:00.370: INFO: namespace kubectl-4821
  May 22 05:38:00.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4821 create -f -'
  May 22 05:38:00.941: INFO: stderr: ""
  May 22 05:38:00.941: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/22/23 05:38:00.941
  May 22 05:38:01.945: INFO: Selector matched 1 pods for map[app:agnhost]
  May 22 05:38:01.945: INFO: Found 0 / 1
  May 22 05:38:02.944: INFO: Selector matched 1 pods for map[app:agnhost]
  May 22 05:38:02.944: INFO: Found 1 / 1
  May 22 05:38:02.944: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 22 05:38:02.946: INFO: Selector matched 1 pods for map[app:agnhost]
  May 22 05:38:02.946: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 22 05:38:02.946: INFO: wait on agnhost-primary startup in kubectl-4821 
  May 22 05:38:02.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4821 logs agnhost-primary-tjjwd agnhost-primary'
  May 22 05:38:03.006: INFO: stderr: ""
  May 22 05:38:03.006: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 05/22/23 05:38:03.006
  May 22 05:38:03.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4821 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  May 22 05:38:03.074: INFO: stderr: ""
  May 22 05:38:03.074: INFO: stdout: "service/rm2 exposed\n"
  May 22 05:38:03.076: INFO: Service rm2 in namespace kubectl-4821 found.
  STEP: exposing service @ 05/22/23 05:38:05.081
  May 22 05:38:05.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4821 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  May 22 05:38:05.149: INFO: stderr: ""
  May 22 05:38:05.149: INFO: stdout: "service/rm3 exposed\n"
  May 22 05:38:05.151: INFO: Service rm3 in namespace kubectl-4821 found.
  May 22 05:38:07.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4821" for this suite. @ 05/22/23 05:38:07.16
• [6.824 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 05/22/23 05:38:07.164
  May 22 05:38:07.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 05:38:07.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:38:07.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:38:07.173
  STEP: Setting up server cert @ 05/22/23 05:38:07.183
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 05:38:07.678
  STEP: Deploying the webhook pod @ 05/22/23 05:38:07.683
  STEP: Wait for the deployment to be ready @ 05/22/23 05:38:07.689
  May 22 05:38:07.692: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/22/23 05:38:09.698
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 05:38:09.705
  May 22 05:38:10.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 22 05:38:10.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/22/23 05:38:11.214
  STEP: Creating a custom resource that should be denied by the webhook @ 05/22/23 05:38:11.225
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/22/23 05:38:13.246
  STEP: Updating the custom resource with disallowed data should be denied @ 05/22/23 05:38:13.253
  STEP: Deleting the custom resource should be denied @ 05/22/23 05:38:13.258
  STEP: Remove the offending key and value from the custom resource data @ 05/22/23 05:38:13.262
  STEP: Deleting the updated custom resource should be successful @ 05/22/23 05:38:13.268
  May 22 05:38:13.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3327" for this suite. @ 05/22/23 05:38:13.808
  STEP: Destroying namespace "webhook-markers-4073" for this suite. @ 05/22/23 05:38:13.811
• [6.651 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 05/22/23 05:38:13.815
  May 22 05:38:13.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename disruption @ 05/22/23 05:38:13.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:38:13.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:38:13.825
  STEP: creating the pdb @ 05/22/23 05:38:13.828
  STEP: Waiting for the pdb to be processed @ 05/22/23 05:38:13.83
  STEP: updating the pdb @ 05/22/23 05:38:15.837
  STEP: Waiting for the pdb to be processed @ 05/22/23 05:38:15.841
  STEP: patching the pdb @ 05/22/23 05:38:17.846
  STEP: Waiting for the pdb to be processed @ 05/22/23 05:38:17.852
  STEP: Waiting for the pdb to be deleted @ 05/22/23 05:38:19.859
  May 22 05:38:19.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1159" for this suite. @ 05/22/23 05:38:19.863
• [6.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/22/23 05:38:19.869
  May 22 05:38:19.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename prestop @ 05/22/23 05:38:19.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:38:19.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:38:19.88
  STEP: Creating server pod server in namespace prestop-8715 @ 05/22/23 05:38:19.882
  STEP: Waiting for pods to come up. @ 05/22/23 05:38:19.923
  STEP: Creating tester pod tester in namespace prestop-8715 @ 05/22/23 05:38:21.931
  STEP: Deleting pre-stop pod @ 05/22/23 05:38:25.944
  May 22 05:38:30.953: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  May 22 05:38:30.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 05/22/23 05:38:30.956
  STEP: Destroying namespace "prestop-8715" for this suite. @ 05/22/23 05:38:30.962
• [11.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 05/22/23 05:38:30.969
  May 22 05:38:30.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename disruption @ 05/22/23 05:38:30.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:38:30.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:38:30.98
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/22/23 05:38:30.982
  STEP: Waiting for the pdb to be processed @ 05/22/23 05:38:30.984
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/22/23 05:38:32.993
  STEP: Waiting for all pods to be running @ 05/22/23 05:38:32.993
  May 22 05:38:32.994: INFO: pods: 0 < 3
  STEP: locating a running pod @ 05/22/23 05:38:34.998
  STEP: Updating the pdb to allow a pod to be evicted @ 05/22/23 05:38:35.004
  STEP: Waiting for the pdb to be processed @ 05/22/23 05:38:35.007
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/22/23 05:38:37.011
  STEP: Waiting for all pods to be running @ 05/22/23 05:38:37.011
  STEP: Waiting for the pdb to observed all healthy pods @ 05/22/23 05:38:37.013
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/22/23 05:38:37.025
  STEP: Waiting for the pdb to be processed @ 05/22/23 05:38:37.04
  STEP: Waiting for all pods to be running @ 05/22/23 05:38:37.047
  May 22 05:38:37.049: INFO: running pods: 2 < 3
  STEP: locating a running pod @ 05/22/23 05:38:39.052
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/22/23 05:38:39.059
  STEP: Waiting for the pdb to be deleted @ 05/22/23 05:38:39.061
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/22/23 05:38:39.064
  STEP: Waiting for all pods to be running @ 05/22/23 05:38:39.064
  May 22 05:38:39.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-391" for this suite. @ 05/22/23 05:38:39.077
• [8.112 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 05/22/23 05:38:39.083
  May 22 05:38:39.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 05:38:39.084
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:38:39.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:38:39.094
  STEP: Creating configMap configmap-4511/configmap-test-57c06274-4b01-434e-95b8-291c1dc83821 @ 05/22/23 05:38:39.142
  STEP: Creating a pod to test consume configMaps @ 05/22/23 05:38:39.145
  STEP: Saw pod success @ 05/22/23 05:38:43.157
  May 22 05:38:43.159: INFO: Trying to get logs from node node2 pod pod-configmaps-fcf6a17a-35db-4f34-8c25-ada253b20436 container env-test: <nil>
  STEP: delete the pod @ 05/22/23 05:38:43.163
  May 22 05:38:43.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4511" for this suite. @ 05/22/23 05:38:43.173
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 05/22/23 05:38:43.178
  May 22 05:38:43.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename statefulset @ 05/22/23 05:38:43.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:38:43.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:38:43.187
  STEP: Creating service test in namespace statefulset-5110 @ 05/22/23 05:38:43.189
  STEP: Creating stateful set ss in namespace statefulset-5110 @ 05/22/23 05:38:43.192
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5110 @ 05/22/23 05:38:43.195
  May 22 05:38:43.197: INFO: Found 0 stateful pods, waiting for 1
  May 22 05:38:53.204: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/22/23 05:38:53.204
  May 22 05:38:53.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5110 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 22 05:38:53.324: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 22 05:38:53.324: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 22 05:38:53.324: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 22 05:38:53.326: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May 22 05:39:03.330: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 22 05:39:03.330: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 05:39:03.339: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
  May 22 05:39:03.339: INFO: ss-0  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:38:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:38:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:38:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:38:43 +0000 UTC  }]
  May 22 05:39:03.339: INFO: 
  May 22 05:39:03.339: INFO: StatefulSet ss has not reached scale 3, at 1
  May 22 05:39:04.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998041085s
  May 22 05:39:05.344: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995261748s
  May 22 05:39:06.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992416617s
  May 22 05:39:07.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989631883s
  May 22 05:39:08.354: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986193289s
  May 22 05:39:09.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982479253s
  May 22 05:39:10.361: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.979544119s
  May 22 05:39:11.363: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975976543s
  May 22 05:39:12.367: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.098403ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5110 @ 05/22/23 05:39:13.367
  May 22 05:39:13.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5110 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 22 05:39:13.503: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 22 05:39:13.503: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 22 05:39:13.503: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 22 05:39:13.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5110 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 22 05:39:13.641: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 22 05:39:13.641: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 22 05:39:13.641: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 22 05:39:13.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5110 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 22 05:39:13.778: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 22 05:39:13.778: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 22 05:39:13.778: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 22 05:39:13.781: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 22 05:39:13.781: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 22 05:39:13.781: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/22/23 05:39:13.782
  May 22 05:39:13.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5110 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 22 05:39:13.919: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 22 05:39:13.919: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 22 05:39:13.919: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 22 05:39:13.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5110 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 22 05:39:14.059: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 22 05:39:14.059: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 22 05:39:14.059: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 22 05:39:14.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5110 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 22 05:39:14.188: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 22 05:39:14.189: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 22 05:39:14.189: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 22 05:39:14.189: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 05:39:14.191: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  May 22 05:39:24.197: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 22 05:39:24.197: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 22 05:39:24.197: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 22 05:39:24.203: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
  May 22 05:39:24.203: INFO: ss-0  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:38:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:38:43 +0000 UTC  }]
  May 22 05:39:24.203: INFO: ss-1  node3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:03 +0000 UTC  }]
  May 22 05:39:24.203: INFO: ss-2  node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:03 +0000 UTC  }]
  May 22 05:39:24.203: INFO: 
  May 22 05:39:24.203: INFO: StatefulSet ss has not reached scale 0, at 3
  May 22 05:39:25.207: INFO: POD   NODE   PHASE      GRACE  CONDITIONS
  May 22 05:39:25.207: INFO: ss-2  node1  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:03 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:14 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:14 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 05:39:03 +0000 UTC  }]
  May 22 05:39:25.207: INFO: 
  May 22 05:39:25.207: INFO: StatefulSet ss has not reached scale 0, at 1
  May 22 05:39:26.209: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993880984s
  May 22 05:39:27.211: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.991686665s
  May 22 05:39:28.214: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.989581195s
  May 22 05:39:29.217: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.986652507s
  May 22 05:39:30.219: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.984261237s
  May 22 05:39:31.222: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.981743274s
  May 22 05:39:32.225: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.979254976s
  May 22 05:39:33.227: INFO: Verifying statefulset ss doesn't scale past 0 for another 975.902775ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5110 @ 05/22/23 05:39:34.228
  May 22 05:39:34.230: INFO: Scaling statefulset ss to 0
  May 22 05:39:34.237: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 05:39:34.239: INFO: Deleting all statefulset in ns statefulset-5110
  May 22 05:39:34.240: INFO: Scaling statefulset ss to 0
  May 22 05:39:34.246: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 05:39:34.248: INFO: Deleting statefulset ss
  May 22 05:39:34.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5110" for this suite. @ 05/22/23 05:39:34.258
• [51.084 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 05/22/23 05:39:34.262
  May 22 05:39:34.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename namespaces @ 05/22/23 05:39:34.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:39:34.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:39:34.272
  STEP: Read namespace status @ 05/22/23 05:39:34.273
  May 22 05:39:34.275: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/22/23 05:39:34.275
  May 22 05:39:34.278: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/22/23 05:39:34.278
  May 22 05:39:34.284: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  May 22 05:39:34.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5405" for this suite. @ 05/22/23 05:39:34.287
• [0.030 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 05/22/23 05:39:34.292
  May 22 05:39:34.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 05:39:34.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:39:34.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:39:34.302
  STEP: Setting up server cert @ 05/22/23 05:39:34.313
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 05:39:34.707
  STEP: Deploying the webhook pod @ 05/22/23 05:39:34.712
  STEP: Wait for the deployment to be ready @ 05/22/23 05:39:34.718
  May 22 05:39:34.723: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/22/23 05:39:36.73
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 05:39:36.738
  May 22 05:39:37.739: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/22/23 05:39:37.742
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/22/23 05:39:37.754
  May 22 05:39:37.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:39:37.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5568" for this suite. @ 05/22/23 05:39:37.79
  STEP: Destroying namespace "webhook-markers-2610" for this suite. @ 05/22/23 05:39:37.793
• [3.504 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 05/22/23 05:39:37.797
  May 22 05:39:37.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 05:39:37.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:39:37.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:39:37.806
  STEP: Counting existing ResourceQuota @ 05/22/23 05:39:37.809
  STEP: Creating a ResourceQuota @ 05/22/23 05:39:42.811
  STEP: Ensuring resource quota status is calculated @ 05/22/23 05:39:42.813
  STEP: Creating a Pod that fits quota @ 05/22/23 05:39:44.816
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/22/23 05:39:44.825
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/22/23 05:39:46.828
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/22/23 05:39:46.831
  STEP: Ensuring a pod cannot update its resource requirements @ 05/22/23 05:39:46.832
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/22/23 05:39:46.837
  STEP: Deleting the pod @ 05/22/23 05:39:48.841
  STEP: Ensuring resource quota status released the pod usage @ 05/22/23 05:39:48.85
  May 22 05:39:50.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-956" for this suite. @ 05/22/23 05:39:50.857
• [13.064 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 05/22/23 05:39:50.861
  May 22 05:39:50.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 05:39:50.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:39:50.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:39:50.872
  STEP: fetching services @ 05/22/23 05:39:50.873
  May 22 05:39:50.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1026" for this suite. @ 05/22/23 05:39:50.878
• [0.020 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 05/22/23 05:39:50.88
  May 22 05:39:50.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename deployment @ 05/22/23 05:39:50.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:39:50.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:39:50.89
  STEP: creating a Deployment @ 05/22/23 05:39:50.894
  May 22 05:39:50.894: INFO: Creating simple deployment test-deployment-96pq4
  May 22 05:39:50.900: INFO: deployment "test-deployment-96pq4" doesn't have the required revision set
  STEP: Getting /status @ 05/22/23 05:39:52.907
  May 22 05:39:52.911: INFO: Deployment test-deployment-96pq4 has Conditions: [{Available True 2023-05-22 05:39:51 +0000 UTC 2023-05-22 05:39:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-22 05:39:51 +0000 UTC 2023-05-22 05:39:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-96pq4-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/22/23 05:39:52.911
  May 22 05:39:52.916: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 39, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 39, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 39, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 39, 50, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-96pq4-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/22/23 05:39:52.916
  May 22 05:39:52.918: INFO: Observed &Deployment event: ADDED
  May 22 05:39:52.918: INFO: Observed Deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-22 05:39:50 +0000 UTC 2023-05-22 05:39:50 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-96pq4-5994cf9475"}
  May 22 05:39:52.918: INFO: Observed &Deployment event: MODIFIED
  May 22 05:39:52.918: INFO: Observed Deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-22 05:39:50 +0000 UTC 2023-05-22 05:39:50 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-96pq4-5994cf9475"}
  May 22 05:39:52.918: INFO: Observed Deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-22 05:39:50 +0000 UTC 2023-05-22 05:39:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 22 05:39:52.918: INFO: Observed &Deployment event: MODIFIED
  May 22 05:39:52.918: INFO: Observed Deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-22 05:39:50 +0000 UTC 2023-05-22 05:39:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 22 05:39:52.918: INFO: Observed Deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-22 05:39:50 +0000 UTC 2023-05-22 05:39:50 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-96pq4-5994cf9475" is progressing.}
  May 22 05:39:52.918: INFO: Observed &Deployment event: MODIFIED
  May 22 05:39:52.918: INFO: Observed Deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-22 05:39:51 +0000 UTC 2023-05-22 05:39:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 22 05:39:52.918: INFO: Observed Deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-22 05:39:51 +0000 UTC 2023-05-22 05:39:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-96pq4-5994cf9475" has successfully progressed.}
  May 22 05:39:52.918: INFO: Observed &Deployment event: MODIFIED
  May 22 05:39:52.918: INFO: Observed Deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-22 05:39:51 +0000 UTC 2023-05-22 05:39:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 22 05:39:52.918: INFO: Observed Deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-22 05:39:51 +0000 UTC 2023-05-22 05:39:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-96pq4-5994cf9475" has successfully progressed.}
  May 22 05:39:52.918: INFO: Found Deployment test-deployment-96pq4 in namespace deployment-5282 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 22 05:39:52.918: INFO: Deployment test-deployment-96pq4 has an updated status
  STEP: patching the Statefulset Status @ 05/22/23 05:39:52.918
  May 22 05:39:52.918: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 22 05:39:52.922: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/22/23 05:39:52.922
  May 22 05:39:52.924: INFO: Observed &Deployment event: ADDED
  May 22 05:39:52.924: INFO: Observed deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-22 05:39:50 +0000 UTC 2023-05-22 05:39:50 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-96pq4-5994cf9475"}
  May 22 05:39:52.924: INFO: Observed &Deployment event: MODIFIED
  May 22 05:39:52.924: INFO: Observed deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-22 05:39:50 +0000 UTC 2023-05-22 05:39:50 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-96pq4-5994cf9475"}
  May 22 05:39:52.924: INFO: Observed deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-22 05:39:50 +0000 UTC 2023-05-22 05:39:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 22 05:39:52.924: INFO: Observed &Deployment event: MODIFIED
  May 22 05:39:52.924: INFO: Observed deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-22 05:39:50 +0000 UTC 2023-05-22 05:39:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 22 05:39:52.924: INFO: Observed deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-22 05:39:50 +0000 UTC 2023-05-22 05:39:50 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-96pq4-5994cf9475" is progressing.}
  May 22 05:39:52.924: INFO: Observed &Deployment event: MODIFIED
  May 22 05:39:52.924: INFO: Observed deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-22 05:39:51 +0000 UTC 2023-05-22 05:39:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 22 05:39:52.924: INFO: Observed deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-22 05:39:51 +0000 UTC 2023-05-22 05:39:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-96pq4-5994cf9475" has successfully progressed.}
  May 22 05:39:52.924: INFO: Observed &Deployment event: MODIFIED
  May 22 05:39:52.924: INFO: Observed deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-22 05:39:51 +0000 UTC 2023-05-22 05:39:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 22 05:39:52.924: INFO: Observed deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-22 05:39:51 +0000 UTC 2023-05-22 05:39:50 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-96pq4-5994cf9475" has successfully progressed.}
  May 22 05:39:52.925: INFO: Observed deployment test-deployment-96pq4 in namespace deployment-5282 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 22 05:39:52.925: INFO: Observed &Deployment event: MODIFIED
  May 22 05:39:52.925: INFO: Found deployment test-deployment-96pq4 in namespace deployment-5282 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  May 22 05:39:52.925: INFO: Deployment test-deployment-96pq4 has a patched status
  May 22 05:39:52.927: INFO: Deployment "test-deployment-96pq4":
  &Deployment{ObjectMeta:{test-deployment-96pq4  deployment-5282  44154c3d-9b97-413c-a02a-3b3d4b301c0f 877973 1 2023-05-22 05:39:50 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-22 05:39:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 05:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-22 05:39:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043a05c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 22 05:39:52.928: INFO: New ReplicaSet "test-deployment-96pq4-5994cf9475" of Deployment "test-deployment-96pq4":
  &ReplicaSet{ObjectMeta:{test-deployment-96pq4-5994cf9475  deployment-5282  931ff001-a0a3-4980-b74b-e4be2cf14103 877967 1 2023-05-22 05:39:50 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-96pq4 44154c3d-9b97-413c-a02a-3b3d4b301c0f 0xc001d580a7 0xc001d580a8}] [] [{kube-controller-manager Update apps/v1 2023-05-22 05:39:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"44154c3d-9b97-413c-a02a-3b3d4b301c0f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 05:39:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d58158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 22 05:39:52.930: INFO: Pod "test-deployment-96pq4-5994cf9475-sqvpc" is available:
  &Pod{ObjectMeta:{test-deployment-96pq4-5994cf9475-sqvpc test-deployment-96pq4-5994cf9475- deployment-5282  903e57a6-1c57-4c39-84fb-d59af577b7a1 877966 0 2023-05-22 05:39:50 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[cni.projectcalico.org/containerID:914e8791e30332d7c3f97742601cce8623126fe0337a36798fe9782130bc3441 cni.projectcalico.org/podIP:192.168.104.50/32 cni.projectcalico.org/podIPs:192.168.104.50/32] [{apps/v1 ReplicaSet test-deployment-96pq4-5994cf9475 931ff001-a0a3-4980-b74b-e4be2cf14103 0xc0041d5900 0xc0041d5901}] [] [{kube-controller-manager Update v1 2023-05-22 05:39:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"931ff001-a0a3-4980-b74b-e4be2cf14103\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-22 05:39:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-22 05:39:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ss6hd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ss6hd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:39:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:39:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:39:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:39:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.50,StartTime:2023-05-22 05:39:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 05:39:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6dcbcefaa237ea2244204ee7de25bd6124152cc9687e1ea31b5a6a3ff0f8c134,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.50,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 05:39:52.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5282" for this suite. @ 05/22/23 05:39:52.933
• [2.055 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 05/22/23 05:39:52.936
  May 22 05:39:52.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sched-preemption @ 05/22/23 05:39:52.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:39:52.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:39:52.949
  May 22 05:39:52.958: INFO: Waiting up to 1m0s for all nodes to be ready
  May 22 05:40:53.029: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/22/23 05:40:53.031
  May 22 05:40:53.046: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 22 05:40:53.049: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 22 05:40:53.100: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 22 05:40:53.105: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  May 22 05:40:53.120: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  May 22 05:40:53.124: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/22/23 05:40:53.124
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/22/23 05:40:55.137
  May 22 05:41:01.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-2485" for this suite. @ 05/22/23 05:41:01.208
• [68.276 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 05/22/23 05:41:01.212
  May 22 05:41:01.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:41:01.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:41:01.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:41:01.224
  STEP: Creating configMap with name projected-configmap-test-volume-5fb25c70-0312-436a-88f0-2dff79770967 @ 05/22/23 05:41:01.226
  STEP: Creating a pod to test consume configMaps @ 05/22/23 05:41:01.229
  STEP: Saw pod success @ 05/22/23 05:41:05.242
  May 22 05:41:05.244: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-7ba5277b-b32d-4505-9283-e7c812bbb7a8 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 05:41:05.256
  May 22 05:41:05.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1796" for this suite. @ 05/22/23 05:41:05.27
• [4.061 seconds]
------------------------------
SS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 05/22/23 05:41:05.274
  May 22 05:41:05.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename disruption @ 05/22/23 05:41:05.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:41:05.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:41:05.282
  STEP: Waiting for the pdb to be processed @ 05/22/23 05:41:05.286
  STEP: Updating PodDisruptionBudget status @ 05/22/23 05:41:07.291
  STEP: Waiting for all pods to be running @ 05/22/23 05:41:07.296
  May 22 05:41:07.298: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 05/22/23 05:41:09.301
  STEP: Waiting for the pdb to be processed @ 05/22/23 05:41:09.308
  STEP: Patching PodDisruptionBudget status @ 05/22/23 05:41:09.311
  STEP: Waiting for the pdb to be processed @ 05/22/23 05:41:09.317
  May 22 05:41:09.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1990" for this suite. @ 05/22/23 05:41:09.321
• [4.051 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 05/22/23 05:41:09.325
  May 22 05:41:09.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 05:41:09.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:41:09.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:41:09.334
  STEP: Creating a ResourceQuota @ 05/22/23 05:41:09.336
  STEP: Getting a ResourceQuota @ 05/22/23 05:41:09.339
  STEP: Updating a ResourceQuota @ 05/22/23 05:41:09.34
  STEP: Verifying a ResourceQuota was modified @ 05/22/23 05:41:09.343
  STEP: Deleting a ResourceQuota @ 05/22/23 05:41:09.345
  STEP: Verifying the deleted ResourceQuota @ 05/22/23 05:41:09.347
  May 22 05:41:09.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6211" for this suite. @ 05/22/23 05:41:09.351
• [0.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/22/23 05:41:09.355
  May 22 05:41:09.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:41:09.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:41:09.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:41:09.364
  STEP: Creating secret with name projected-secret-test-9925ee7a-00d7-4089-93d7-d4c24c4b82dd @ 05/22/23 05:41:09.366
  STEP: Creating a pod to test consume secrets @ 05/22/23 05:41:09.368
  STEP: Saw pod success @ 05/22/23 05:41:13.381
  May 22 05:41:13.382: INFO: Trying to get logs from node node2 pod pod-projected-secrets-5536b0a0-d6ba-43c5-ac3b-b1ac8ca6f2db container secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 05:41:13.387
  May 22 05:41:13.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8675" for this suite. @ 05/22/23 05:41:13.397
• [4.045 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 05/22/23 05:41:13.4
  May 22 05:41:13.400: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 05:41:13.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:41:13.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:41:13.409
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/22/23 05:41:13.411
  STEP: Saw pod success @ 05/22/23 05:41:17.424
  May 22 05:41:17.425: INFO: Trying to get logs from node node2 pod pod-64cadd5c-e334-49b4-bbd0-f4b5f0234541 container test-container: <nil>
  STEP: delete the pod @ 05/22/23 05:41:17.429
  May 22 05:41:17.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8242" for this suite. @ 05/22/23 05:41:17.439
• [4.096 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 05/22/23 05:41:17.496
  May 22 05:41:17.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-probe @ 05/22/23 05:41:17.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:41:17.503
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:41:17.505
  May 22 05:41:39.551: INFO: Container started at 2023-05-22 05:41:18 +0000 UTC, pod became ready at 2023-05-22 05:41:37 +0000 UTC
  May 22 05:41:39.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6976" for this suite. @ 05/22/23 05:41:39.554
• [22.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 05/22/23 05:41:39.559
  May 22 05:41:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename var-expansion @ 05/22/23 05:41:39.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:41:39.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:41:39.569
  STEP: creating the pod @ 05/22/23 05:41:39.57
  STEP: waiting for pod running @ 05/22/23 05:41:39.575
  STEP: creating a file in subpath @ 05/22/23 05:41:41.584
  May 22 05:41:41.586: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-807 PodName:var-expansion-de6ed948-32b0-45d5-9566-0af5c8fec6a5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:41:41.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:41:41.587: INFO: ExecWithOptions: Clientset creation
  May 22 05:41:41.587: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-807/pods/var-expansion-de6ed948-32b0-45d5-9566-0af5c8fec6a5/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 05/22/23 05:41:41.65
  May 22 05:41:41.652: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-807 PodName:var-expansion-de6ed948-32b0-45d5-9566-0af5c8fec6a5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:41:41.653: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:41:41.653: INFO: ExecWithOptions: Clientset creation
  May 22 05:41:41.654: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-807/pods/var-expansion-de6ed948-32b0-45d5-9566-0af5c8fec6a5/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/22/23 05:41:41.715
  May 22 05:41:42.223: INFO: Successfully updated pod "var-expansion-de6ed948-32b0-45d5-9566-0af5c8fec6a5"
  STEP: waiting for annotated pod running @ 05/22/23 05:41:42.223
  STEP: deleting the pod gracefully @ 05/22/23 05:41:42.225
  May 22 05:41:42.225: INFO: Deleting pod "var-expansion-de6ed948-32b0-45d5-9566-0af5c8fec6a5" in namespace "var-expansion-807"
  May 22 05:41:42.229: INFO: Wait up to 5m0s for pod "var-expansion-de6ed948-32b0-45d5-9566-0af5c8fec6a5" to be fully deleted
  May 22 05:42:16.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-807" for this suite. @ 05/22/23 05:42:16.3
• [36.746 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 05/22/23 05:42:16.305
  May 22 05:42:16.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename cronjob @ 05/22/23 05:42:16.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:42:16.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:42:16.316
  STEP: Creating a ReplaceConcurrent cronjob @ 05/22/23 05:42:16.319
  STEP: Ensuring a job is scheduled @ 05/22/23 05:42:16.322
  STEP: Ensuring exactly one is scheduled @ 05/22/23 05:43:00.324
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/22/23 05:43:00.326
  STEP: Ensuring the job is replaced with a new one @ 05/22/23 05:43:00.328
  STEP: Removing cronjob @ 05/22/23 05:44:00.332
  May 22 05:44:00.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3946" for this suite. @ 05/22/23 05:44:00.338
• [104.036 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/22/23 05:44:00.342
  May 22 05:44:00.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:44:00.343
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:00.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:00.352
  STEP: Creating projection with secret that has name projected-secret-test-9f6ef627-fff8-415b-bc69-af7dde12d8c4 @ 05/22/23 05:44:00.354
  STEP: Creating a pod to test consume secrets @ 05/22/23 05:44:00.356
  STEP: Saw pod success @ 05/22/23 05:44:04.367
  May 22 05:44:04.368: INFO: Trying to get logs from node node2 pod pod-projected-secrets-4bb83cbf-48b9-48b8-bd7a-acfe3822ef96 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 05:44:04.387
  May 22 05:44:04.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1106" for this suite. @ 05/22/23 05:44:04.399
• [4.060 seconds]
------------------------------
S
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/22/23 05:44:04.402
  May 22 05:44:04.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename events @ 05/22/23 05:44:04.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:04.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:04.411
  STEP: creating a test event @ 05/22/23 05:44:04.413
  STEP: listing events in all namespaces @ 05/22/23 05:44:04.415
  STEP: listing events in test namespace @ 05/22/23 05:44:04.418
  STEP: listing events with field selection filtering on source @ 05/22/23 05:44:04.419
  STEP: listing events with field selection filtering on reportingController @ 05/22/23 05:44:04.42
  STEP: getting the test event @ 05/22/23 05:44:04.422
  STEP: patching the test event @ 05/22/23 05:44:04.424
  STEP: getting the test event @ 05/22/23 05:44:04.428
  STEP: updating the test event @ 05/22/23 05:44:04.429
  STEP: getting the test event @ 05/22/23 05:44:04.432
  STEP: deleting the test event @ 05/22/23 05:44:04.434
  STEP: listing events in all namespaces @ 05/22/23 05:44:04.437
  STEP: listing events in test namespace @ 05/22/23 05:44:04.439
  May 22 05:44:04.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2182" for this suite. @ 05/22/23 05:44:04.444
• [0.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 05/22/23 05:44:04.452
  May 22 05:44:04.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename var-expansion @ 05/22/23 05:44:04.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:04.465
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:04.466
  STEP: Creating a pod to test substitution in container's command @ 05/22/23 05:44:04.468
  STEP: Saw pod success @ 05/22/23 05:44:08.481
  May 22 05:44:08.483: INFO: Trying to get logs from node node3 pod var-expansion-5c494ebc-3c41-43f8-b2b0-e57db4d1fce0 container dapi-container: <nil>
  STEP: delete the pod @ 05/22/23 05:44:08.5
  May 22 05:44:08.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7945" for this suite. @ 05/22/23 05:44:08.512
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 05/22/23 05:44:08.516
  May 22 05:44:08.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 05:44:08.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:08.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:08.526
  STEP: Creating a pod to test downward api env vars @ 05/22/23 05:44:08.528
  STEP: Saw pod success @ 05/22/23 05:44:12.542
  May 22 05:44:12.544: INFO: Trying to get logs from node node3 pod downward-api-0b24aad9-123e-4f23-8962-739f80a9ca30 container dapi-container: <nil>
  STEP: delete the pod @ 05/22/23 05:44:12.549
  May 22 05:44:12.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2641" for this suite. @ 05/22/23 05:44:12.619
• [4.105 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 05/22/23 05:44:12.622
  May 22 05:44:12.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 05:44:12.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:12.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:12.632
  STEP: Creating resourceQuota "e2e-rq-status-bnl9f" @ 05/22/23 05:44:12.635
  May 22 05:44:12.638: INFO: Resource quota "e2e-rq-status-bnl9f" reports spec: hard cpu limit of 500m
  May 22 05:44:12.638: INFO: Resource quota "e2e-rq-status-bnl9f" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-bnl9f" /status @ 05/22/23 05:44:12.638
  STEP: Confirm /status for "e2e-rq-status-bnl9f" resourceQuota via watch @ 05/22/23 05:44:12.642
  May 22 05:44:12.643: INFO: observed resourceQuota "e2e-rq-status-bnl9f" in namespace "resourcequota-7200" with hard status: v1.ResourceList(nil)
  May 22 05:44:12.643: INFO: Found resourceQuota "e2e-rq-status-bnl9f" in namespace "resourcequota-7200" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 22 05:44:12.643: INFO: ResourceQuota "e2e-rq-status-bnl9f" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/22/23 05:44:12.645
  May 22 05:44:12.647: INFO: Resource quota "e2e-rq-status-bnl9f" reports spec: hard cpu limit of 1
  May 22 05:44:12.647: INFO: Resource quota "e2e-rq-status-bnl9f" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-bnl9f" /status @ 05/22/23 05:44:12.647
  STEP: Confirm /status for "e2e-rq-status-bnl9f" resourceQuota via watch @ 05/22/23 05:44:12.649
  May 22 05:44:12.650: INFO: observed resourceQuota "e2e-rq-status-bnl9f" in namespace "resourcequota-7200" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 22 05:44:12.650: INFO: Found resourceQuota "e2e-rq-status-bnl9f" in namespace "resourcequota-7200" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  May 22 05:44:12.651: INFO: ResourceQuota "e2e-rq-status-bnl9f" /status was patched
  STEP: Get "e2e-rq-status-bnl9f" /status @ 05/22/23 05:44:12.651
  May 22 05:44:12.652: INFO: Resourcequota "e2e-rq-status-bnl9f" reports status: hard cpu of 1
  May 22 05:44:12.652: INFO: Resourcequota "e2e-rq-status-bnl9f" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-bnl9f" /status before checking Spec is unchanged @ 05/22/23 05:44:12.654
  May 22 05:44:12.657: INFO: Resourcequota "e2e-rq-status-bnl9f" reports status: hard cpu of 2
  May 22 05:44:12.657: INFO: Resourcequota "e2e-rq-status-bnl9f" reports status: hard memory of 2Gi
  May 22 05:44:12.658: INFO: Found resourceQuota "e2e-rq-status-bnl9f" in namespace "resourcequota-7200" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  May 22 05:44:17.661: INFO: ResourceQuota "e2e-rq-status-bnl9f" Spec was unchanged and /status reset
  May 22 05:44:17.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7200" for this suite. @ 05/22/23 05:44:17.665
• [5.046 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/22/23 05:44:17.669
  May 22 05:44:17.669: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 05:44:17.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:17.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:17.678
  STEP: creating the pod @ 05/22/23 05:44:17.68
  STEP: submitting the pod to kubernetes @ 05/22/23 05:44:17.68
  W0522 05:44:17.685418      25 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 05/22/23 05:44:19.691
  STEP: updating the pod @ 05/22/23 05:44:19.694
  May 22 05:44:20.202: INFO: Successfully updated pod "pod-update-activedeadlineseconds-749625c2-5654-483a-be8d-c23581a36c17"
  May 22 05:44:24.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8717" for this suite. @ 05/22/23 05:44:24.213
• [6.547 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 05/22/23 05:44:24.217
  May 22 05:44:24.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 05:44:24.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:24.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:24.226
  May 22 05:44:24.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1294" for this suite. @ 05/22/23 05:44:24.247
• [0.034 seconds]
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 05/22/23 05:44:24.25
  May 22 05:44:24.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pod-network-test @ 05/22/23 05:44:24.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:24.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:24.259
  STEP: Performing setup for networking test in namespace pod-network-test-7790 @ 05/22/23 05:44:24.261
  STEP: creating a selector @ 05/22/23 05:44:24.261
  STEP: Creating the service pods in kubernetes @ 05/22/23 05:44:24.261
  May 22 05:44:24.261: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/22/23 05:44:36.311
  May 22 05:44:38.321: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 22 05:44:38.321: INFO: Breadth first check of 192.168.166.140 on host 192.168.33.121...
  May 22 05:44:38.323: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.104.27:9080/dial?request=hostname&protocol=udp&host=192.168.166.140&port=8081&tries=1'] Namespace:pod-network-test-7790 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:44:38.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:44:38.324: INFO: ExecWithOptions: Clientset creation
  May 22 05:44:38.324: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7790/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.104.27%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.166.140%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 22 05:44:38.394: INFO: Waiting for responses: map[]
  May 22 05:44:38.394: INFO: reached 192.168.166.140 after 0/1 tries
  May 22 05:44:38.394: INFO: Breadth first check of 192.168.104.59 on host 192.168.33.122...
  May 22 05:44:38.396: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.104.27:9080/dial?request=hostname&protocol=udp&host=192.168.104.59&port=8081&tries=1'] Namespace:pod-network-test-7790 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:44:38.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:44:38.397: INFO: ExecWithOptions: Clientset creation
  May 22 05:44:38.397: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7790/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.104.27%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.104.59%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 22 05:44:38.457: INFO: Waiting for responses: map[]
  May 22 05:44:38.457: INFO: reached 192.168.104.59 after 0/1 tries
  May 22 05:44:38.457: INFO: Breadth first check of 192.168.135.24 on host 192.168.33.123...
  May 22 05:44:38.459: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.104.27:9080/dial?request=hostname&protocol=udp&host=192.168.135.24&port=8081&tries=1'] Namespace:pod-network-test-7790 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:44:38.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:44:38.460: INFO: ExecWithOptions: Clientset creation
  May 22 05:44:38.460: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7790/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.104.27%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.135.24%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 22 05:44:38.524: INFO: Waiting for responses: map[]
  May 22 05:44:38.524: INFO: reached 192.168.135.24 after 0/1 tries
  May 22 05:44:38.524: INFO: Going to retry 0 out of 3 pods....
  May 22 05:44:38.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7790" for this suite. @ 05/22/23 05:44:38.529
• [14.283 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 05/22/23 05:44:38.533
  May 22 05:44:38.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename init-container @ 05/22/23 05:44:38.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:38.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:38.545
  STEP: creating the pod @ 05/22/23 05:44:38.546
  May 22 05:44:38.546: INFO: PodSpec: initContainers in spec.initContainers
  May 22 05:44:42.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1383" for this suite. @ 05/22/23 05:44:42.855
• [4.326 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 05/22/23 05:44:42.86
  May 22 05:44:42.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/22/23 05:44:42.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:42.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:42.869
  STEP: create the container to handle the HTTPGet hook request. @ 05/22/23 05:44:42.873
  STEP: create the pod with lifecycle hook @ 05/22/23 05:44:44.883
  STEP: delete the pod with lifecycle hook @ 05/22/23 05:44:46.893
  STEP: check prestop hook @ 05/22/23 05:44:50.905
  May 22 05:44:50.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1175" for this suite. @ 05/22/23 05:44:50.919
• [8.062 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/22/23 05:44:50.922
  May 22 05:44:50.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename watch @ 05/22/23 05:44:50.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:50.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:50.931
  STEP: creating a watch on configmaps @ 05/22/23 05:44:50.933
  STEP: creating a new configmap @ 05/22/23 05:44:50.934
  STEP: modifying the configmap once @ 05/22/23 05:44:50.936
  STEP: closing the watch once it receives two notifications @ 05/22/23 05:44:50.939
  May 22 05:44:50.940: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7126  86086907-6f91-4cb2-b0a7-713695b3bc6e 879840 0 2023-05-22 05:44:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-22 05:44:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 05:44:50.940: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7126  86086907-6f91-4cb2-b0a7-713695b3bc6e 879841 0 2023-05-22 05:44:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-22 05:44:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/22/23 05:44:50.94
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/22/23 05:44:50.943
  STEP: deleting the configmap @ 05/22/23 05:44:50.944
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/22/23 05:44:50.984
  May 22 05:44:50.984: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7126  86086907-6f91-4cb2-b0a7-713695b3bc6e 879842 0 2023-05-22 05:44:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-22 05:44:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 05:44:50.984: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7126  86086907-6f91-4cb2-b0a7-713695b3bc6e 879843 0 2023-05-22 05:44:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-22 05:44:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 05:44:50.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7126" for this suite. @ 05/22/23 05:44:50.987
• [0.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 05/22/23 05:44:50.991
  May 22 05:44:50.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 05:44:50.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:50.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:51.001
  STEP: Setting up server cert @ 05/22/23 05:44:51.011
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 05:44:51.381
  STEP: Deploying the webhook pod @ 05/22/23 05:44:51.386
  STEP: Wait for the deployment to be ready @ 05/22/23 05:44:51.392
  May 22 05:44:51.396: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/22/23 05:44:53.403
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 05:44:53.41
  May 22 05:44:54.411: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/22/23 05:44:54.413
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/22/23 05:44:54.425
  STEP: Creating a configMap that should not be mutated @ 05/22/23 05:44:54.429
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/22/23 05:44:54.433
  STEP: Creating a configMap that should be mutated @ 05/22/23 05:44:54.437
  May 22 05:44:54.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5724" for this suite. @ 05/22/23 05:44:54.471
  STEP: Destroying namespace "webhook-markers-4215" for this suite. @ 05/22/23 05:44:54.474
• [3.486 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 05/22/23 05:44:54.477
  May 22 05:44:54.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replicaset @ 05/22/23 05:44:54.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:54.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:54.488
  STEP: Create a ReplicaSet @ 05/22/23 05:44:54.49
  STEP: Verify that the required pods have come up @ 05/22/23 05:44:54.495
  May 22 05:44:54.497: INFO: Pod name sample-pod: Found 0 pods out of 3
  May 22 05:44:59.499: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/22/23 05:44:59.499
  May 22 05:44:59.501: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/22/23 05:44:59.501
  STEP: DeleteCollection of the ReplicaSets @ 05/22/23 05:44:59.503
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/22/23 05:44:59.506
  May 22 05:44:59.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5508" for this suite. @ 05/22/23 05:44:59.516
• [5.044 seconds]
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 05/22/23 05:44:59.521
  May 22 05:44:59.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename security-context-test @ 05/22/23 05:44:59.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:44:59.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:44:59.532
  May 22 05:45:01.546: INFO: Got logs for pod "busybox-privileged-false-ef282c52-b9f9-4f24-be64-35a06fc3aa3d": "ip: RTNETLINK answers: Operation not permitted\n"
  May 22 05:45:01.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4106" for this suite. @ 05/22/23 05:45:01.553
• [2.034 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 05/22/23 05:45:01.556
  May 22 05:45:01.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 05:45:01.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:45:01.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:45:01.565
  STEP: Discovering how many secrets are in namespace by default @ 05/22/23 05:45:01.566
  STEP: Counting existing ResourceQuota @ 05/22/23 05:45:06.569
  STEP: Creating a ResourceQuota @ 05/22/23 05:45:11.571
  STEP: Ensuring resource quota status is calculated @ 05/22/23 05:45:11.574
  STEP: Creating a Secret @ 05/22/23 05:45:13.578
  STEP: Ensuring resource quota status captures secret creation @ 05/22/23 05:45:13.584
  STEP: Deleting a secret @ 05/22/23 05:45:15.587
  STEP: Ensuring resource quota status released usage @ 05/22/23 05:45:15.59
  May 22 05:45:17.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6463" for this suite. @ 05/22/23 05:45:17.597
• [16.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 05/22/23 05:45:17.602
  May 22 05:45:17.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 05:45:17.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:45:17.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:45:17.611
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/22/23 05:45:17.614
  STEP: Saw pod success @ 05/22/23 05:45:21.626
  May 22 05:45:21.628: INFO: Trying to get logs from node node2 pod pod-f97f2cbc-5830-4fff-bf04-3b3926ecb77a container test-container: <nil>
  STEP: delete the pod @ 05/22/23 05:45:21.633
  May 22 05:45:21.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6601" for this suite. @ 05/22/23 05:45:21.642
• [4.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 05/22/23 05:45:21.646
  May 22 05:45:21.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 05:45:21.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:45:21.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:45:21.656
  STEP: Setting up server cert @ 05/22/23 05:45:21.668
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 05:45:22.03
  STEP: Deploying the webhook pod @ 05/22/23 05:45:22.036
  STEP: Wait for the deployment to be ready @ 05/22/23 05:45:22.044
  May 22 05:45:22.050: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/22/23 05:45:24.057
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 05:45:24.064
  May 22 05:45:25.064: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/22/23 05:45:25.066
  STEP: create a pod that should be updated by the webhook @ 05/22/23 05:45:25.076
  May 22 05:45:25.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-54" for this suite. @ 05/22/23 05:45:25.114
  STEP: Destroying namespace "webhook-markers-969" for this suite. @ 05/22/23 05:45:25.117
• [3.475 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 05/22/23 05:45:25.121
  May 22 05:45:25.121: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 05:45:25.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:45:25.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:45:25.131
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/22/23 05:45:25.133
  STEP: Saw pod success @ 05/22/23 05:45:29.144
  May 22 05:45:29.146: INFO: Trying to get logs from node node2 pod pod-63978968-0e41-4dc1-9fab-09630d0a920a container test-container: <nil>
  STEP: delete the pod @ 05/22/23 05:45:29.15
  May 22 05:45:29.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3184" for this suite. @ 05/22/23 05:45:29.161
• [4.042 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 05/22/23 05:45:29.164
  May 22 05:45:29.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 05:45:29.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:45:29.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:45:29.173
  STEP: creating service nodeport-test with type=NodePort in namespace services-3173 @ 05/22/23 05:45:29.175
  STEP: creating replication controller nodeport-test in namespace services-3173 @ 05/22/23 05:45:29.184
  I0522 05:45:29.187453      25 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-3173, replica count: 2
  I0522 05:45:32.239018      25 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 22 05:45:32.239: INFO: Creating new exec pod
  May 22 05:45:35.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3173 exec execpodltsh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May 22 05:45:35.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May 22 05:45:35.361: INFO: stdout: "nodeport-test-cczkq"
  May 22 05:45:35.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3173 exec execpodltsh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.55.123 80'
  May 22 05:45:35.490: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.55.123 80\nConnection to 10.110.55.123 80 port [tcp/http] succeeded!\n"
  May 22 05:45:35.490: INFO: stdout: "nodeport-test-lzvcn"
  May 22 05:45:35.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3173 exec execpodltsh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.121 31607'
  May 22 05:45:35.617: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.121 31607\nConnection to 192.168.33.121 31607 port [tcp/*] succeeded!\n"
  May 22 05:45:35.617: INFO: stdout: ""
  May 22 05:45:36.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3173 exec execpodltsh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.121 31607'
  May 22 05:45:36.748: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.121 31607\nConnection to 192.168.33.121 31607 port [tcp/*] succeeded!\n"
  May 22 05:45:36.748: INFO: stdout: ""
  May 22 05:45:37.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3173 exec execpodltsh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.121 31607'
  May 22 05:45:37.743: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.121 31607\nConnection to 192.168.33.121 31607 port [tcp/*] succeeded!\n"
  May 22 05:45:37.743: INFO: stdout: "nodeport-test-cczkq"
  May 22 05:45:37.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3173 exec execpodltsh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.123 31607'
  May 22 05:45:37.857: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.123 31607\nConnection to 192.168.33.123 31607 port [tcp/*] succeeded!\n"
  May 22 05:45:37.857: INFO: stdout: ""
  May 22 05:45:38.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3173 exec execpodltsh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.123 31607'
  May 22 05:45:38.977: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.123 31607\nConnection to 192.168.33.123 31607 port [tcp/*] succeeded!\n"
  May 22 05:45:38.977: INFO: stdout: ""
  May 22 05:45:39.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3173 exec execpodltsh5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.123 31607'
  May 22 05:45:39.973: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.123 31607\nConnection to 192.168.33.123 31607 port [tcp/*] succeeded!\n"
  May 22 05:45:39.973: INFO: stdout: "nodeport-test-lzvcn"
  May 22 05:45:39.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3173" for this suite. @ 05/22/23 05:45:39.976
• [10.816 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 05/22/23 05:45:39.981
  May 22 05:45:39.981: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename var-expansion @ 05/22/23 05:45:39.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:45:39.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:45:39.989
  STEP: Creating a pod to test env composition @ 05/22/23 05:45:39.991
  STEP: Saw pod success @ 05/22/23 05:45:44.003
  May 22 05:45:44.005: INFO: Trying to get logs from node node2 pod var-expansion-d56bfe8b-bb6d-4649-a0b2-1cf4492c179b container dapi-container: <nil>
  STEP: delete the pod @ 05/22/23 05:45:44.01
  May 22 05:45:44.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3477" for this suite. @ 05/22/23 05:45:44.021
• [4.043 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 05/22/23 05:45:44.024
  May 22 05:45:44.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 05:45:44.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:45:44.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:45:44.034
  STEP: Setting up server cert @ 05/22/23 05:45:44.045
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 05:45:44.38
  STEP: Deploying the webhook pod @ 05/22/23 05:45:44.385
  STEP: Wait for the deployment to be ready @ 05/22/23 05:45:44.391
  May 22 05:45:44.395: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/22/23 05:45:46.4
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 05:45:46.407
  May 22 05:45:47.407: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/22/23 05:45:47.409
  STEP: create a pod that should be denied by the webhook @ 05/22/23 05:45:47.421
  STEP: create a pod that causes the webhook to hang @ 05/22/23 05:45:47.429
  STEP: create a configmap that should be denied by the webhook @ 05/22/23 05:45:57.435
  STEP: create a configmap that should be admitted by the webhook @ 05/22/23 05:45:57.442
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/22/23 05:45:57.449
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/22/23 05:45:57.453
  STEP: create a namespace that bypass the webhook @ 05/22/23 05:45:57.456
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/22/23 05:45:57.463
  May 22 05:45:57.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1579" for this suite. @ 05/22/23 05:45:57.495
  STEP: Destroying namespace "webhook-markers-8937" for this suite. @ 05/22/23 05:45:57.5
  STEP: Destroying namespace "exempted-namespace-7718" for this suite. @ 05/22/23 05:45:57.503
• [13.482 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 05/22/23 05:45:57.507
  May 22 05:45:57.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename dns @ 05/22/23 05:45:57.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:45:57.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:45:57.516
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4781.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4781.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/22/23 05:45:57.518
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4781.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4781.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/22/23 05:45:57.518
  STEP: creating a pod to probe /etc/hosts @ 05/22/23 05:45:57.518
  STEP: submitting the pod to kubernetes @ 05/22/23 05:45:57.518
  STEP: retrieving the pod @ 05/22/23 05:45:59.528
  STEP: looking for the results for each expected name from probers @ 05/22/23 05:45:59.529
  May 22 05:45:59.537: INFO: DNS probes using dns-4781/dns-test-f2d5444f-7d15-4304-a45a-66fc39cc9a95 succeeded

  May 22 05:45:59.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 05:45:59.539
  STEP: Destroying namespace "dns-4781" for this suite. @ 05/22/23 05:45:59.545
• [2.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 05/22/23 05:45:59.551
  May 22 05:45:59.551: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replicaset @ 05/22/23 05:45:59.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:45:59.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:45:59.56
  May 22 05:45:59.567: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 22 05:46:04.570: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/22/23 05:46:04.57
  STEP: Scaling up "test-rs" replicaset  @ 05/22/23 05:46:04.57
  May 22 05:46:04.576: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/22/23 05:46:04.576
  W0522 05:46:04.580860      25 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 22 05:46:04.582: INFO: observed ReplicaSet test-rs in namespace replicaset-4944 with ReadyReplicas 1, AvailableReplicas 1
  May 22 05:46:04.587: INFO: observed ReplicaSet test-rs in namespace replicaset-4944 with ReadyReplicas 1, AvailableReplicas 1
  May 22 05:46:04.594: INFO: observed ReplicaSet test-rs in namespace replicaset-4944 with ReadyReplicas 1, AvailableReplicas 1
  May 22 05:46:04.601: INFO: observed ReplicaSet test-rs in namespace replicaset-4944 with ReadyReplicas 1, AvailableReplicas 1
  May 22 05:46:05.786: INFO: observed ReplicaSet test-rs in namespace replicaset-4944 with ReadyReplicas 2, AvailableReplicas 2
  May 22 05:46:06.127: INFO: observed Replicaset test-rs in namespace replicaset-4944 with ReadyReplicas 3 found true
  May 22 05:46:06.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4944" for this suite. @ 05/22/23 05:46:06.13
• [6.581 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 05/22/23 05:46:06.133
  May 22 05:46:06.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 05:46:06.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:46:06.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:46:06.147
  STEP: Creating a pod to test downward api env vars @ 05/22/23 05:46:06.149
  STEP: Saw pod success @ 05/22/23 05:46:10.16
  May 22 05:46:10.162: INFO: Trying to get logs from node node2 pod downward-api-4027b182-4b85-4df7-9027-3886c1c09a63 container dapi-container: <nil>
  STEP: delete the pod @ 05/22/23 05:46:10.166
  May 22 05:46:10.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3407" for this suite. @ 05/22/23 05:46:10.178
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 05/22/23 05:46:10.182
  May 22 05:46:10.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 05:46:10.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:46:10.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:46:10.192
  May 22 05:46:10.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-664 version'
  May 22 05:46:10.247: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  May 22 05:46:10.247: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:14:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  May 22 05:46:10.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-664" for this suite. @ 05/22/23 05:46:10.25
• [0.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 05/22/23 05:46:10.256
  May 22 05:46:10.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename field-validation @ 05/22/23 05:46:10.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:46:10.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:46:10.265
  May 22 05:46:10.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  W0522 05:46:12.801874      25 warnings.go:70] unknown field "alpha"
  W0522 05:46:12.801901      25 warnings.go:70] unknown field "beta"
  W0522 05:46:12.801905      25 warnings.go:70] unknown field "delta"
  W0522 05:46:12.801909      25 warnings.go:70] unknown field "epsilon"
  W0522 05:46:12.801912      25 warnings.go:70] unknown field "gamma"
  May 22 05:46:12.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2163" for this suite. @ 05/22/23 05:46:12.816
• [2.563 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/22/23 05:46:12.82
  May 22 05:46:12.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:46:12.82
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:46:12.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:46:12.828
  STEP: Creating projection with secret that has name projected-secret-test-map-3f08dc6a-411f-42d0-9f13-0fba6c24c58f @ 05/22/23 05:46:12.829
  STEP: Creating a pod to test consume secrets @ 05/22/23 05:46:12.831
  STEP: Saw pod success @ 05/22/23 05:46:16.843
  May 22 05:46:16.845: INFO: Trying to get logs from node node2 pod pod-projected-secrets-9dec38eb-4df6-496c-9540-32ef721b411b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 05:46:16.849
  May 22 05:46:16.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-132" for this suite. @ 05/22/23 05:46:16.859
• [4.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/22/23 05:46:16.864
  May 22 05:46:16.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubelet-test @ 05/22/23 05:46:16.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:46:16.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:46:16.874
  May 22 05:46:16.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1515" for this suite. @ 05/22/23 05:46:16.891
• [0.030 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 05/22/23 05:46:16.895
  May 22 05:46:16.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 05:46:16.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:46:16.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:46:16.903
  STEP: Creating configMap with name configmap-test-volume-357d6098-b15e-4082-bd70-8a9c2ed93c83 @ 05/22/23 05:46:16.905
  STEP: Creating a pod to test consume configMaps @ 05/22/23 05:46:16.907
  STEP: Saw pod success @ 05/22/23 05:46:22.921
  May 22 05:46:22.923: INFO: Trying to get logs from node node2 pod pod-configmaps-79a58377-3ed2-464c-9c29-45df84206d0d container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 05:46:22.927
  May 22 05:46:22.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4317" for this suite. @ 05/22/23 05:46:22.937
• [6.045 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 05/22/23 05:46:22.94
  May 22 05:46:22.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename taint-single-pod @ 05/22/23 05:46:22.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:46:22.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:46:22.95
  May 22 05:46:22.952: INFO: Waiting up to 1m0s for all nodes to be ready
  May 22 05:47:22.975: INFO: Waiting for terminating namespaces to be deleted...
  May 22 05:47:22.977: INFO: Starting informer...
  STEP: Starting pod... @ 05/22/23 05:47:22.977
  May 22 05:47:23.185: INFO: Pod is running on node2. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/22/23 05:47:23.185
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/22/23 05:47:23.197
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/22/23 05:47:23.198
  May 22 05:47:23.198: INFO: Pod wasn't evicted. Proceeding
  May 22 05:47:23.198: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/22/23 05:47:23.211
  STEP: Waiting some time to make sure that toleration time passed. @ 05/22/23 05:47:23.214
  May 22 05:48:38.218: INFO: Pod wasn't evicted. Test successful
  May 22 05:48:38.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-857" for this suite. @ 05/22/23 05:48:38.221
• [135.285 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/22/23 05:48:38.226
  May 22 05:48:38.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename deployment @ 05/22/23 05:48:38.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:48:38.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:48:38.237
  May 22 05:48:38.240: INFO: Creating simple deployment test-new-deployment
  May 22 05:48:38.245: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
  STEP: getting scale subresource @ 05/22/23 05:48:40.255
  STEP: updating a scale subresource @ 05/22/23 05:48:40.256
  STEP: verifying the deployment Spec.Replicas was modified @ 05/22/23 05:48:40.259
  STEP: Patch a scale subresource @ 05/22/23 05:48:40.261
  May 22 05:48:40.269: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-9169  b3481b9e-8f8e-4491-8b5b-430440df2180 881610 3 2023-05-22 05:48:38 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-22 05:48:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 05:48:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003fe9e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-22 05:48:39 +0000 UTC,LastTransitionTime:2023-05-22 05:48:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-05-22 05:48:39 +0000 UTC,LastTransitionTime:2023-05-22 05:48:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 22 05:48:40.271: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-9169  0a09f861-f25c-4cce-8330-ce3b57223baf 881609 2 2023-05-22 05:48:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b3481b9e-8f8e-4491-8b5b-430440df2180 0xc0044c94f7 0xc0044c94f8}] [] [{kube-controller-manager Update apps/v1 2023-05-22 05:48:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-22 05:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3481b9e-8f8e-4491-8b5b-430440df2180\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044c9588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 22 05:48:40.275: INFO: Pod "test-new-deployment-67bd4bf6dc-6ngmx" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-6ngmx test-new-deployment-67bd4bf6dc- deployment-9169  87f8c420-f503-4e50-ad68-7fe1fef275ed 881615 0 2023-05-22 05:48:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 0a09f861-f25c-4cce-8330-ce3b57223baf 0xc0044c9977 0xc0044c9978}] [] [{kube-controller-manager Update v1 2023-05-22 05:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a09f861-f25c-4cce-8330-ce3b57223baf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9mcvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9mcvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:48:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 05:48:40.275: INFO: Pod "test-new-deployment-67bd4bf6dc-rd6bj" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-rd6bj test-new-deployment-67bd4bf6dc- deployment-9169  a3a33e4e-3b27-4229-99be-419eb44b775b 881604 0 2023-05-22 05:48:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:f91e07446343b54a4bee35ca8a24fb08f20db79cfb2bd953978dd47b0d3d29f0 cni.projectcalico.org/podIP:192.168.104.8/32 cni.projectcalico.org/podIPs:192.168.104.8/32] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 0a09f861-f25c-4cce-8330-ce3b57223baf 0xc0044c9b00 0xc0044c9b01}] [] [{calico Update v1 2023-05-22 05:48:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 05:48:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a09f861-f25c-4cce-8330-ce3b57223baf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 05:48:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzhpm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzhpm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:48:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:48:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.8,StartTime:2023-05-22 05:48:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 05:48:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ceeb0da023493fd159e5a6c60526d5b32e9a6bf24bdb71131efa55879cf9192e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.8,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 05:48:40.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9169" for this suite. @ 05/22/23 05:48:40.278
• [2.055 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/22/23 05:48:40.281
  May 22 05:48:40.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sched-pred @ 05/22/23 05:48:40.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:48:40.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:48:40.291
  May 22 05:48:40.293: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 22 05:48:40.297: INFO: Waiting for terminating namespaces to be deleted...
  May 22 05:48:40.299: INFO: 
  Logging pods the apiserver thinks is on node node1 before test
  May 22 05:48:40.305: INFO: calico-apiserver-666fc8f69-6l8kk from calico-apiserver started at 2023-05-19 09:15:01 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 22 05:48:40.305: INFO: calico-node-k76wg from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container calico-node ready: true, restart count 0
  May 22 05:48:40.305: INFO: csi-node-driver-vg8gt from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 05:48:40.305: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 05:48:40.305: INFO: csi-rbdplugin-hjnwl from default started at 2023-05-19 12:08:20 +0000 UTC (3 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:48:40.305: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 05:48:40.305: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:48:40.305: INFO: csi-rbdplugin-provisioner-6d5864d5f5-2scr8 from default started at 2023-05-19 12:08:07 +0000 UTC (7 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container csi-attacher ready: true, restart count 0
  May 22 05:48:40.305: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 22 05:48:40.305: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:48:40.305: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 22 05:48:40.305: INFO: 	Container csi-resizer ready: true, restart count 0
  May 22 05:48:40.305: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 05:48:40.305: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:48:40.305: INFO: coredns-5d78c9869d-nckgt from kube-system started at 2023-05-17 07:47:08 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container coredns ready: true, restart count 0
  May 22 05:48:40.305: INFO: coredns-5d78c9869d-wrhg2 from kube-system started at 2023-05-17 07:47:08 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container coredns ready: true, restart count 0
  May 22 05:48:40.305: INFO: etcd-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container etcd ready: true, restart count 0
  May 22 05:48:40.305: INFO: kube-apiserver-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container kube-apiserver ready: true, restart count 0
  May 22 05:48:40.305: INFO: kube-controller-manager-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container kube-controller-manager ready: true, restart count 1
  May 22 05:48:40.305: INFO: kube-proxy-qx9ss from kube-system started at 2023-05-17 07:19:40 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 05:48:40.305: INFO: kube-scheduler-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container kube-scheduler ready: true, restart count 1
  May 22 05:48:40.305: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-bdnbl from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 05:48:40.305: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 05:48:40.305: INFO: 	Container systemd-logs ready: true, restart count 0
  May 22 05:48:40.305: INFO: 
  Logging pods the apiserver thinks is on node node2 before test
  May 22 05:48:40.311: INFO: calico-node-s8lxt from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container calico-node ready: true, restart count 0
  May 22 05:48:40.311: INFO: calico-typha-cf4d768f9-99xbv from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container calico-typha ready: true, restart count 0
  May 22 05:48:40.311: INFO: csi-node-driver-ffpxl from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 05:48:40.311: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 05:48:40.311: INFO: csi-rbdplugin-provisioner-6d5864d5f5-8kmgv from default started at 2023-05-22 05:47:24 +0000 UTC (7 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container csi-attacher ready: true, restart count 0
  May 22 05:48:40.311: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 22 05:48:40.311: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:48:40.311: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 22 05:48:40.311: INFO: 	Container csi-resizer ready: true, restart count 0
  May 22 05:48:40.311: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 05:48:40.311: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:48:40.311: INFO: csi-rbdplugin-sv5xd from default started at 2023-05-22 05:47:23 +0000 UTC (3 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:48:40.311: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 05:48:40.311: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:48:40.311: INFO: test-new-deployment-67bd4bf6dc-rd6bj from deployment-9169 started at 2023-05-22 05:48:38 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container httpd ready: true, restart count 0
  May 22 05:48:40.311: INFO: kube-proxy-k5zhn from kube-system started at 2023-05-17 07:50:57 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 05:48:40.311: INFO: sonobuoy from sonobuoy started at 2023-05-22 05:20:17 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 22 05:48:40.311: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-hm7kk from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 05:48:40.311: INFO: 	Container systemd-logs ready: true, restart count 0
  May 22 05:48:40.311: INFO: taint-eviction-4 from taint-single-pod-857 started at 2023-05-22 05:47:22 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container pause ready: true, restart count 0
  May 22 05:48:40.311: INFO: tigera-operator-549d4f9bdb-g29xs from tigera-operator started at 2023-05-19 08:43:58 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.311: INFO: 	Container tigera-operator ready: true, restart count 2
  May 22 05:48:40.311: INFO: 
  Logging pods the apiserver thinks is on node node3 before test
  May 22 05:48:40.317: INFO: calico-apiserver-666fc8f69-48crp from calico-apiserver started at 2023-05-19 12:51:48 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.317: INFO: 	Container calico-apiserver ready: true, restart count 1
  May 22 05:48:40.317: INFO: calico-kube-controllers-789dc4c76b-qqbbj from calico-system started at 2023-05-19 12:51:48 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.317: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 22 05:48:40.317: INFO: calico-node-g6cnn from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.317: INFO: 	Container calico-node ready: true, restart count 0
  May 22 05:48:40.317: INFO: calico-typha-cf4d768f9-fgk49 from calico-system started at 2023-05-19 08:44:10 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.317: INFO: 	Container calico-typha ready: true, restart count 0
  May 22 05:48:40.317: INFO: csi-node-driver-jjbvg from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 05:48:40.317: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 05:48:40.317: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 05:48:40.317: INFO: csi-rbdplugin-nkgs7 from default started at 2023-05-19 12:08:20 +0000 UTC (3 container statuses recorded)
  May 22 05:48:40.317: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:48:40.317: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 05:48:40.317: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:48:40.317: INFO: csi-rbdplugin-provisioner-6d5864d5f5-cn9zx from default started at 2023-05-19 12:08:07 +0000 UTC (7 container statuses recorded)
  May 22 05:48:40.317: INFO: 	Container csi-attacher ready: true, restart count 2
  May 22 05:48:40.318: INFO: 	Container csi-provisioner ready: true, restart count 1
  May 22 05:48:40.318: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:48:40.318: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 1
  May 22 05:48:40.318: INFO: 	Container csi-resizer ready: true, restart count 2
  May 22 05:48:40.318: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 05:48:40.318: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:48:40.318: INFO: test-new-deployment-67bd4bf6dc-6ngmx from deployment-9169 started at 2023-05-22 05:48:40 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.318: INFO: 	Container httpd ready: false, restart count 0
  May 22 05:48:40.318: INFO: kube-proxy-kgvhd from kube-system started at 2023-05-17 07:51:09 +0000 UTC (1 container statuses recorded)
  May 22 05:48:40.318: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 05:48:40.318: INFO: sonobuoy-e2e-job-3f7387b6fe3c48b5 from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 05:48:40.318: INFO: 	Container e2e ready: true, restart count 0
  May 22 05:48:40.318: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 05:48:40.318: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-d6c6x from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 05:48:40.318: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 05:48:40.318: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node node1 @ 05/22/23 05:48:40.334
  STEP: verifying the node has the label node node2 @ 05/22/23 05:48:40.349
  STEP: verifying the node has the label node node3 @ 05/22/23 05:48:40.361
  May 22 05:48:40.372: INFO: Pod calico-apiserver-666fc8f69-48crp requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod calico-apiserver-666fc8f69-6l8kk requesting resource cpu=0m on Node node1
  May 22 05:48:40.372: INFO: Pod calico-kube-controllers-789dc4c76b-qqbbj requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod calico-node-g6cnn requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod calico-node-k76wg requesting resource cpu=0m on Node node1
  May 22 05:48:40.372: INFO: Pod calico-node-s8lxt requesting resource cpu=0m on Node node2
  May 22 05:48:40.372: INFO: Pod calico-typha-cf4d768f9-99xbv requesting resource cpu=0m on Node node2
  May 22 05:48:40.372: INFO: Pod calico-typha-cf4d768f9-fgk49 requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod csi-node-driver-ffpxl requesting resource cpu=0m on Node node2
  May 22 05:48:40.372: INFO: Pod csi-node-driver-jjbvg requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod csi-node-driver-vg8gt requesting resource cpu=0m on Node node1
  May 22 05:48:40.372: INFO: Pod csi-rbdplugin-hjnwl requesting resource cpu=0m on Node node1
  May 22 05:48:40.372: INFO: Pod csi-rbdplugin-nkgs7 requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod csi-rbdplugin-provisioner-6d5864d5f5-2scr8 requesting resource cpu=0m on Node node1
  May 22 05:48:40.372: INFO: Pod csi-rbdplugin-provisioner-6d5864d5f5-8kmgv requesting resource cpu=0m on Node node2
  May 22 05:48:40.372: INFO: Pod csi-rbdplugin-provisioner-6d5864d5f5-cn9zx requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod csi-rbdplugin-sv5xd requesting resource cpu=0m on Node node2
  May 22 05:48:40.372: INFO: Pod test-new-deployment-67bd4bf6dc-6ngmx requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod test-new-deployment-67bd4bf6dc-rd6bj requesting resource cpu=0m on Node node2
  May 22 05:48:40.372: INFO: Pod coredns-5d78c9869d-nckgt requesting resource cpu=100m on Node node1
  May 22 05:48:40.372: INFO: Pod coredns-5d78c9869d-wrhg2 requesting resource cpu=100m on Node node1
  May 22 05:48:40.372: INFO: Pod etcd-node1 requesting resource cpu=100m on Node node1
  May 22 05:48:40.372: INFO: Pod kube-apiserver-node1 requesting resource cpu=250m on Node node1
  May 22 05:48:40.372: INFO: Pod kube-controller-manager-node1 requesting resource cpu=200m on Node node1
  May 22 05:48:40.372: INFO: Pod kube-proxy-k5zhn requesting resource cpu=0m on Node node2
  May 22 05:48:40.372: INFO: Pod kube-proxy-kgvhd requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod kube-proxy-qx9ss requesting resource cpu=0m on Node node1
  May 22 05:48:40.372: INFO: Pod kube-scheduler-node1 requesting resource cpu=100m on Node node1
  May 22 05:48:40.372: INFO: Pod sonobuoy requesting resource cpu=0m on Node node2
  May 22 05:48:40.372: INFO: Pod sonobuoy-e2e-job-3f7387b6fe3c48b5 requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-bdnbl requesting resource cpu=0m on Node node1
  May 22 05:48:40.372: INFO: Pod sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-d6c6x requesting resource cpu=0m on Node node3
  May 22 05:48:40.372: INFO: Pod sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-hm7kk requesting resource cpu=0m on Node node2
  May 22 05:48:40.372: INFO: Pod taint-eviction-4 requesting resource cpu=0m on Node node2
  May 22 05:48:40.372: INFO: Pod tigera-operator-549d4f9bdb-g29xs requesting resource cpu=0m on Node node2
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/22/23 05:48:40.372
  May 22 05:48:40.372: INFO: Creating a pod which consumes cpu=5005m on Node node1
  May 22 05:48:40.378: INFO: Creating a pod which consumes cpu=5600m on Node node2
  May 22 05:48:40.382: INFO: Creating a pod which consumes cpu=5600m on Node node3
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/22/23 05:48:42.397
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-73c9acb8-b624-4212-b723-e61c944c7c42.1761611ec481553e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4394/filler-pod-73c9acb8-b624-4212-b723-e61c944c7c42 to node3] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-73c9acb8-b624-4212-b723-e61c944c7c42.1761611ee812b0d8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-73c9acb8-b624-4212-b723-e61c944c7c42.1761611ee8a35810], Reason = [Created], Message = [Created container filler-pod-73c9acb8-b624-4212-b723-e61c944c7c42] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-73c9acb8-b624-4212-b723-e61c944c7c42.1761611eecfe01fe], Reason = [Started], Message = [Started container filler-pod-73c9acb8-b624-4212-b723-e61c944c7c42] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c13f1bda-df57-4472-9952-10a955753e87.1761611ec469f060], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4394/filler-pod-c13f1bda-df57-4472-9952-10a955753e87 to node2] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c13f1bda-df57-4472-9952-10a955753e87.1761611ee9d50cbc], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c13f1bda-df57-4472-9952-10a955753e87.1761611eea611c90], Reason = [Created], Message = [Created container filler-pod-c13f1bda-df57-4472-9952-10a955753e87] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-c13f1bda-df57-4472-9952-10a955753e87.1761611eef594605], Reason = [Started], Message = [Started container filler-pod-c13f1bda-df57-4472-9952-10a955753e87] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d8bebeb5-80a3-4e4c-b536-d565f98aaf6a.1761611ec3eb0036], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4394/filler-pod-d8bebeb5-80a3-4e4c-b536-d565f98aaf6a to node1] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d8bebeb5-80a3-4e4c-b536-d565f98aaf6a.1761611eeaa7bc94], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d8bebeb5-80a3-4e4c-b536-d565f98aaf6a.1761611eeb349d4d], Reason = [Created], Message = [Created container filler-pod-d8bebeb5-80a3-4e4c-b536-d565f98aaf6a] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-d8bebeb5-80a3-4e4c-b536-d565f98aaf6a.1761611ef099b7a8], Reason = [Started], Message = [Started container filler-pod-d8bebeb5-80a3-4e4c-b536-d565f98aaf6a] @ 05/22/23 05:48:42.399
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.1761611f3c6feba0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 05/22/23 05:48:42.406
  STEP: removing the label node off the node node1 @ 05/22/23 05:48:43.407
  STEP: verifying the node doesn't have the label node @ 05/22/23 05:48:43.416
  STEP: removing the label node off the node node2 @ 05/22/23 05:48:43.419
  STEP: verifying the node doesn't have the label node @ 05/22/23 05:48:43.428
  STEP: removing the label node off the node node3 @ 05/22/23 05:48:43.433
  STEP: verifying the node doesn't have the label node @ 05/22/23 05:48:43.445
  May 22 05:48:43.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4394" for this suite. @ 05/22/23 05:48:43.45
• [3.172 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 05/22/23 05:48:43.454
  May 22 05:48:43.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 05:48:43.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:48:43.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:48:43.465
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/22/23 05:48:43.469
  STEP: Saw pod success @ 05/22/23 05:48:47.482
  May 22 05:48:47.484: INFO: Trying to get logs from node node1 pod pod-63655f1b-688f-4efd-8647-8de095981351 container test-container: <nil>
  STEP: delete the pod @ 05/22/23 05:48:47.494
  May 22 05:48:47.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-388" for this suite. @ 05/22/23 05:48:47.505
• [4.055 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 05/22/23 05:48:47.509
  May 22 05:48:47.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:48:47.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:48:47.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:48:47.519
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-6141d797-f3ba-4757-b0b0-d37f84ee7659 @ 05/22/23 05:48:47.523
  STEP: Creating the pod @ 05/22/23 05:48:47.526
  STEP: Updating configmap projected-configmap-test-upd-6141d797-f3ba-4757-b0b0-d37f84ee7659 @ 05/22/23 05:48:49.549
  STEP: waiting to observe update in volume @ 05/22/23 05:48:49.555
  May 22 05:48:51.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-742" for this suite. @ 05/22/23 05:48:51.568
• [4.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 05/22/23 05:48:51.571
  May 22 05:48:51.571: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/22/23 05:48:51.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:48:51.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:48:51.581
  STEP: create the container to handle the HTTPGet hook request. @ 05/22/23 05:48:51.586
  STEP: create the pod with lifecycle hook @ 05/22/23 05:48:53.597
  STEP: delete the pod with lifecycle hook @ 05/22/23 05:48:55.607
  STEP: check prestop hook @ 05/22/23 05:48:59.619
  May 22 05:48:59.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5514" for this suite. @ 05/22/23 05:48:59.625
• [8.057 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/22/23 05:48:59.629
  May 22 05:48:59.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 05:48:59.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:48:59.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:48:59.637
  STEP: Create a pod @ 05/22/23 05:48:59.639
  STEP: patching /status @ 05/22/23 05:49:01.648
  May 22 05:49:01.658: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  May 22 05:49:01.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3493" for this suite. @ 05/22/23 05:49:01.663
• [2.037 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 05/22/23 05:49:01.666
  May 22 05:49:01.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename dns @ 05/22/23 05:49:01.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:49:01.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:49:01.674
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/22/23 05:49:01.676
  May 22 05:49:01.680: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2788  64dc7da3-1e75-4443-b1dc-044f697b8d57 881958 0 2023-05-22 05:49:01 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-22 05:49:01 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ml7bg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ml7bg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/22/23 05:49:03.685
  May 22 05:49:03.685: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2788 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:49:03.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:49:03.686: INFO: ExecWithOptions: Clientset creation
  May 22 05:49:03.687: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2788/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/22/23 05:49:03.757
  May 22 05:49:03.757: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2788 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:49:03.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:49:03.758: INFO: ExecWithOptions: Clientset creation
  May 22 05:49:03.758: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2788/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 22 05:49:03.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 05:49:03.835: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-2788" for this suite. @ 05/22/23 05:49:03.842
• [2.180 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 05/22/23 05:49:03.845
  May 22 05:49:03.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-probe @ 05/22/23 05:49:03.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:49:03.853
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:49:03.855
  STEP: Creating pod liveness-466cfa33-b6a5-487d-b35e-e0a34737a37c in namespace container-probe-7296 @ 05/22/23 05:49:03.857
  May 22 05:49:05.867: INFO: Started pod liveness-466cfa33-b6a5-487d-b35e-e0a34737a37c in namespace container-probe-7296
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/22/23 05:49:05.867
  May 22 05:49:05.869: INFO: Initial restart count of pod liveness-466cfa33-b6a5-487d-b35e-e0a34737a37c is 0
  May 22 05:49:25.900: INFO: Restart count of pod container-probe-7296/liveness-466cfa33-b6a5-487d-b35e-e0a34737a37c is now 1 (20.030924979s elapsed)
  May 22 05:49:25.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 05:49:25.903
  STEP: Destroying namespace "container-probe-7296" for this suite. @ 05/22/23 05:49:25.908
• [22.066 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 05/22/23 05:49:25.912
  May 22 05:49:25.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 05:49:25.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:49:25.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:49:25.921
  STEP: Creating a ResourceQuota with terminating scope @ 05/22/23 05:49:25.923
  STEP: Ensuring ResourceQuota status is calculated @ 05/22/23 05:49:25.925
  STEP: Creating a ResourceQuota with not terminating scope @ 05/22/23 05:49:27.929
  STEP: Ensuring ResourceQuota status is calculated @ 05/22/23 05:49:27.931
  STEP: Creating a long running pod @ 05/22/23 05:49:29.934
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/22/23 05:49:29.941
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/22/23 05:49:31.944
  STEP: Deleting the pod @ 05/22/23 05:49:33.947
  STEP: Ensuring resource quota status released the pod usage @ 05/22/23 05:49:33.953
  STEP: Creating a terminating pod @ 05/22/23 05:49:35.955
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/22/23 05:49:35.962
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/22/23 05:49:37.966
  STEP: Deleting the pod @ 05/22/23 05:49:39.969
  STEP: Ensuring resource quota status released the pod usage @ 05/22/23 05:49:39.98
  May 22 05:49:41.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8977" for this suite. @ 05/22/23 05:49:41.986
• [16.078 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/22/23 05:49:41.99
  May 22 05:49:41.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:49:41.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:49:41.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:49:41.999
  STEP: Creating secret with name s-test-opt-del-d6cf4add-e667-4a02-a946-a90b8b638520 @ 05/22/23 05:49:42.051
  STEP: Creating secret with name s-test-opt-upd-ef0d0605-bfa0-44ba-b895-d8ffab636884 @ 05/22/23 05:49:42.055
  STEP: Creating the pod @ 05/22/23 05:49:42.057
  STEP: Deleting secret s-test-opt-del-d6cf4add-e667-4a02-a946-a90b8b638520 @ 05/22/23 05:49:44.086
  STEP: Updating secret s-test-opt-upd-ef0d0605-bfa0-44ba-b895-d8ffab636884 @ 05/22/23 05:49:44.089
  STEP: Creating secret with name s-test-opt-create-664e3178-4bac-449e-b0a1-d59912285689 @ 05/22/23 05:49:44.092
  STEP: waiting to observe update in volume @ 05/22/23 05:49:44.096
  May 22 05:49:46.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3520" for this suite. @ 05/22/23 05:49:46.117
• [4.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 05/22/23 05:49:46.123
  May 22 05:49:46.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename statefulset @ 05/22/23 05:49:46.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:49:46.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:49:46.131
  STEP: Creating service test in namespace statefulset-532 @ 05/22/23 05:49:46.133
  May 22 05:49:46.141: INFO: Found 0 stateful pods, waiting for 1
  May 22 05:49:56.145: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/22/23 05:49:56.148
  W0522 05:49:56.152606      25 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 22 05:49:56.155: INFO: Found 1 stateful pods, waiting for 2
  May 22 05:50:06.159: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 22 05:50:06.159: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/22/23 05:50:06.163
  STEP: Delete all of the StatefulSets @ 05/22/23 05:50:06.164
  STEP: Verify that StatefulSets have been deleted @ 05/22/23 05:50:06.167
  May 22 05:50:06.169: INFO: Deleting all statefulset in ns statefulset-532
  May 22 05:50:06.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-532" for this suite. @ 05/22/23 05:50:06.176
• [20.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 05/22/23 05:50:06.18
  May 22 05:50:06.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 05:50:06.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:50:06.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:50:06.193
  STEP: Setting up server cert @ 05/22/23 05:50:06.203
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 05:50:06.411
  STEP: Deploying the webhook pod @ 05/22/23 05:50:06.421
  STEP: Wait for the deployment to be ready @ 05/22/23 05:50:06.43
  May 22 05:50:06.435: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/22/23 05:50:08.441
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 05:50:08.449
  May 22 05:50:09.449: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 22 05:50:09.452: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7221-crds.webhook.example.com via the AdmissionRegistration API @ 05/22/23 05:50:09.96
  STEP: Creating a custom resource while v1 is storage version @ 05/22/23 05:50:09.972
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/22/23 05:50:12.006
  STEP: Patching the custom resource while v2 is storage version @ 05/22/23 05:50:12.01
  May 22 05:50:12.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3695" for this suite. @ 05/22/23 05:50:12.566
  STEP: Destroying namespace "webhook-markers-7885" for this suite. @ 05/22/23 05:50:12.569
• [6.391 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 05/22/23 05:50:12.573
  May 22 05:50:12.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/22/23 05:50:12.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:50:12.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:50:12.583
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/22/23 05:50:12.584
  May 22 05:50:12.585: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:50:14.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:50:20.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1565" for this suite. @ 05/22/23 05:50:20.794
• [8.224 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 05/22/23 05:50:20.797
  May 22 05:50:20.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sysctl @ 05/22/23 05:50:20.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:50:20.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:50:20.807
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/22/23 05:50:20.809
  STEP: Watching for error events or started pod @ 05/22/23 05:50:20.813
  STEP: Waiting for pod completion @ 05/22/23 05:50:22.816
  STEP: Checking that the pod succeeded @ 05/22/23 05:50:24.824
  STEP: Getting logs from the pod @ 05/22/23 05:50:24.824
  STEP: Checking that the sysctl is actually updated @ 05/22/23 05:50:24.827
  May 22 05:50:24.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-9672" for this suite. @ 05/22/23 05:50:24.83
• [4.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/22/23 05:50:24.834
  May 22 05:50:24.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 05:50:24.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:50:24.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:50:24.844
  STEP: Creating secret with name secret-test-0d8f7521-27cc-4862-901d-71ebabd9a090 @ 05/22/23 05:50:24.846
  STEP: Creating a pod to test consume secrets @ 05/22/23 05:50:24.849
  STEP: Saw pod success @ 05/22/23 05:50:28.861
  May 22 05:50:28.863: INFO: Trying to get logs from node node2 pod pod-secrets-97894e3e-2f6b-4972-8399-2917e9f99368 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 05:50:28.867
  May 22 05:50:28.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1280" for this suite. @ 05/22/23 05:50:28.878
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 05/22/23 05:50:28.883
  May 22 05:50:28.883: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename security-context-test @ 05/22/23 05:50:28.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:50:28.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:50:28.892
  May 22 05:50:32.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5667" for this suite. @ 05/22/23 05:50:32.91
• [4.030 seconds]
------------------------------
S
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 05/22/23 05:50:32.913
  May 22 05:50:32.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 05:50:32.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:50:32.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:50:32.923
  STEP: Creating a pod to test downward api env vars @ 05/22/23 05:50:32.925
  STEP: Saw pod success @ 05/22/23 05:50:36.939
  May 22 05:50:36.941: INFO: Trying to get logs from node node2 pod downward-api-66eff32a-8943-4a7d-8e1b-2ddc3a16cc65 container dapi-container: <nil>
  STEP: delete the pod @ 05/22/23 05:50:36.945
  May 22 05:50:36.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1774" for this suite. @ 05/22/23 05:50:36.956
• [4.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/22/23 05:50:36.961
  May 22 05:50:36.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sched-pred @ 05/22/23 05:50:36.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:50:36.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:50:36.969
  May 22 05:50:36.971: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 22 05:50:36.976: INFO: Waiting for terminating namespaces to be deleted...
  May 22 05:50:36.977: INFO: 
  Logging pods the apiserver thinks is on node node1 before test
  May 22 05:50:36.982: INFO: calico-apiserver-666fc8f69-6l8kk from calico-apiserver started at 2023-05-19 09:15:01 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.982: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 22 05:50:36.982: INFO: calico-node-k76wg from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.982: INFO: 	Container calico-node ready: true, restart count 0
  May 22 05:50:36.982: INFO: csi-node-driver-vg8gt from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 05:50:36.982: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 05:50:36.982: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 05:50:36.982: INFO: csi-rbdplugin-hjnwl from default started at 2023-05-19 12:08:20 +0000 UTC (3 container statuses recorded)
  May 22 05:50:36.982: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:50:36.982: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 05:50:36.982: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:50:36.983: INFO: csi-rbdplugin-provisioner-6d5864d5f5-2scr8 from default started at 2023-05-19 12:08:07 +0000 UTC (7 container statuses recorded)
  May 22 05:50:36.983: INFO: 	Container csi-attacher ready: true, restart count 0
  May 22 05:50:36.983: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 22 05:50:36.983: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:50:36.983: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 22 05:50:36.983: INFO: 	Container csi-resizer ready: true, restart count 0
  May 22 05:50:36.983: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 05:50:36.983: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:50:36.983: INFO: coredns-5d78c9869d-nckgt from kube-system started at 2023-05-17 07:47:08 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.983: INFO: 	Container coredns ready: true, restart count 0
  May 22 05:50:36.983: INFO: coredns-5d78c9869d-wrhg2 from kube-system started at 2023-05-17 07:47:08 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.983: INFO: 	Container coredns ready: true, restart count 0
  May 22 05:50:36.983: INFO: etcd-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.983: INFO: 	Container etcd ready: true, restart count 0
  May 22 05:50:36.983: INFO: kube-apiserver-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.983: INFO: 	Container kube-apiserver ready: true, restart count 0
  May 22 05:50:36.983: INFO: kube-controller-manager-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.983: INFO: 	Container kube-controller-manager ready: true, restart count 1
  May 22 05:50:36.983: INFO: kube-proxy-qx9ss from kube-system started at 2023-05-17 07:19:40 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.983: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 05:50:36.983: INFO: kube-scheduler-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.983: INFO: 	Container kube-scheduler ready: true, restart count 1
  May 22 05:50:36.983: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-bdnbl from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 05:50:36.983: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 05:50:36.983: INFO: 	Container systemd-logs ready: true, restart count 0
  May 22 05:50:36.983: INFO: 
  Logging pods the apiserver thinks is on node node2 before test
  May 22 05:50:36.988: INFO: calico-node-s8lxt from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.988: INFO: 	Container calico-node ready: true, restart count 0
  May 22 05:50:36.988: INFO: calico-typha-cf4d768f9-99xbv from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.988: INFO: 	Container calico-typha ready: true, restart count 0
  May 22 05:50:36.988: INFO: csi-node-driver-ffpxl from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 05:50:36.988: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 05:50:36.988: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 05:50:36.988: INFO: csi-rbdplugin-provisioner-6d5864d5f5-8kmgv from default started at 2023-05-22 05:47:24 +0000 UTC (7 container statuses recorded)
  May 22 05:50:36.988: INFO: 	Container csi-attacher ready: true, restart count 0
  May 22 05:50:36.988: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 22 05:50:36.988: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:50:36.988: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 22 05:50:36.988: INFO: 	Container csi-resizer ready: true, restart count 0
  May 22 05:50:36.988: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 05:50:36.988: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:50:36.988: INFO: csi-rbdplugin-sv5xd from default started at 2023-05-22 05:47:23 +0000 UTC (3 container statuses recorded)
  May 22 05:50:36.988: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:50:36.988: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 05:50:36.988: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:50:36.988: INFO: kube-proxy-k5zhn from kube-system started at 2023-05-17 07:50:57 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.988: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 05:50:36.988: INFO: busybox-user-65534-224a0ded-a631-49a3-8b34-a78625375e09 from security-context-test-5667 started at 2023-05-22 05:50:28 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.988: INFO: 	Container busybox-user-65534-224a0ded-a631-49a3-8b34-a78625375e09 ready: false, restart count 0
  May 22 05:50:36.988: INFO: sonobuoy from sonobuoy started at 2023-05-22 05:20:17 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.988: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 22 05:50:36.988: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-hm7kk from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 05:50:36.988: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 05:50:36.988: INFO: 	Container systemd-logs ready: true, restart count 0
  May 22 05:50:36.988: INFO: tigera-operator-549d4f9bdb-g29xs from tigera-operator started at 2023-05-19 08:43:58 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.988: INFO: 	Container tigera-operator ready: true, restart count 2
  May 22 05:50:36.988: INFO: 
  Logging pods the apiserver thinks is on node node3 before test
  May 22 05:50:36.993: INFO: calico-apiserver-666fc8f69-48crp from calico-apiserver started at 2023-05-19 12:51:48 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.993: INFO: 	Container calico-apiserver ready: true, restart count 1
  May 22 05:50:36.993: INFO: calico-kube-controllers-789dc4c76b-qqbbj from calico-system started at 2023-05-19 12:51:48 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.993: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 22 05:50:36.993: INFO: calico-node-g6cnn from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.993: INFO: 	Container calico-node ready: true, restart count 0
  May 22 05:50:36.993: INFO: calico-typha-cf4d768f9-fgk49 from calico-system started at 2023-05-19 08:44:10 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.993: INFO: 	Container calico-typha ready: true, restart count 0
  May 22 05:50:36.993: INFO: csi-node-driver-jjbvg from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 05:50:36.993: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 05:50:36.993: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 05:50:36.993: INFO: csi-rbdplugin-nkgs7 from default started at 2023-05-19 12:08:20 +0000 UTC (3 container statuses recorded)
  May 22 05:50:36.993: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:50:36.993: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 05:50:36.993: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:50:36.993: INFO: csi-rbdplugin-provisioner-6d5864d5f5-cn9zx from default started at 2023-05-19 12:08:07 +0000 UTC (7 container statuses recorded)
  May 22 05:50:36.993: INFO: 	Container csi-attacher ready: true, restart count 2
  May 22 05:50:36.993: INFO: 	Container csi-provisioner ready: true, restart count 1
  May 22 05:50:36.993: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 05:50:36.993: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 1
  May 22 05:50:36.993: INFO: 	Container csi-resizer ready: true, restart count 2
  May 22 05:50:36.993: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 05:50:36.993: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 05:50:36.993: INFO: kube-proxy-kgvhd from kube-system started at 2023-05-17 07:51:09 +0000 UTC (1 container statuses recorded)
  May 22 05:50:36.993: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 05:50:36.993: INFO: sonobuoy-e2e-job-3f7387b6fe3c48b5 from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 05:50:36.993: INFO: 	Container e2e ready: true, restart count 0
  May 22 05:50:36.993: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 05:50:36.993: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-d6c6x from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 05:50:36.993: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 05:50:36.993: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/22/23 05:50:36.993
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17616139eb5c3362], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 05/22/23 05:50:37.011
  May 22 05:50:38.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1027" for this suite. @ 05/22/23 05:50:38.014
• [1.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 05/22/23 05:50:38.017
  May 22 05:50:38.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename daemonsets @ 05/22/23 05:50:38.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:50:38.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:50:38.025
  STEP: Creating simple DaemonSet "daemon-set" @ 05/22/23 05:50:38.039
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/22/23 05:50:38.042
  May 22 05:50:38.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 05:50:38.046: INFO: Node node1 is running 0 daemon pod, expected 1
  May 22 05:50:39.051: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 22 05:50:39.051: INFO: Node node2 is running 0 daemon pod, expected 1
  May 22 05:50:40.052: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 22 05:50:40.052: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/22/23 05:50:40.054
  May 22 05:50:40.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 22 05:50:40.064: INFO: Node node3 is running 0 daemon pod, expected 1
  May 22 05:50:41.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 22 05:50:41.071: INFO: Node node3 is running 0 daemon pod, expected 1
  May 22 05:50:42.069: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 22 05:50:42.070: INFO: Node node3 is running 0 daemon pod, expected 1
  May 22 05:50:43.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 22 05:50:43.071: INFO: Node node3 is running 0 daemon pod, expected 1
  May 22 05:50:44.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 22 05:50:44.070: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/22/23 05:50:44.072
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9238, will wait for the garbage collector to delete the pods @ 05/22/23 05:50:44.072
  May 22 05:50:44.127: INFO: Deleting DaemonSet.extensions daemon-set took: 3.472475ms
  May 22 05:50:44.228: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.819603ms
  May 22 05:50:46.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 05:50:46.531: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 22 05:50:46.533: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"882960"},"items":null}

  May 22 05:50:46.535: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"882960"},"items":null}

  May 22 05:50:46.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9238" for this suite. @ 05/22/23 05:50:46.545
• [8.532 seconds]
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 05/22/23 05:50:46.549
  May 22 05:50:46.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/22/23 05:50:46.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:50:46.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:50:46.562
  STEP: create the container to handle the HTTPGet hook request. @ 05/22/23 05:50:46.567
  STEP: create the pod with lifecycle hook @ 05/22/23 05:50:48.579
  STEP: check poststart hook @ 05/22/23 05:50:50.589
  STEP: delete the pod with lifecycle hook @ 05/22/23 05:50:50.603
  May 22 05:50:54.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7755" for this suite. @ 05/22/23 05:50:54.618
• [8.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 05/22/23 05:50:54.631
  May 22 05:50:54.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename subpath @ 05/22/23 05:50:54.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:50:54.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:50:54.641
  STEP: Setting up data @ 05/22/23 05:50:54.643
  STEP: Creating pod pod-subpath-test-configmap-vcrx @ 05/22/23 05:50:54.647
  STEP: Creating a pod to test atomic-volume-subpath @ 05/22/23 05:50:54.647
  STEP: Saw pod success @ 05/22/23 05:51:18.695
  May 22 05:51:18.697: INFO: Trying to get logs from node node2 pod pod-subpath-test-configmap-vcrx container test-container-subpath-configmap-vcrx: <nil>
  STEP: delete the pod @ 05/22/23 05:51:18.702
  STEP: Deleting pod pod-subpath-test-configmap-vcrx @ 05/22/23 05:51:18.71
  May 22 05:51:18.710: INFO: Deleting pod "pod-subpath-test-configmap-vcrx" in namespace "subpath-2555"
  May 22 05:51:18.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2555" for this suite. @ 05/22/23 05:51:18.714
• [24.086 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 05/22/23 05:51:18.718
  May 22 05:51:18.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename aggregator @ 05/22/23 05:51:18.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:51:18.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:51:18.728
  May 22 05:51:18.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Registering the sample API server. @ 05/22/23 05:51:18.731
  May 22 05:51:19.200: INFO: Found ClusterRoles; assuming RBAC is enabled.
  May 22 05:51:19.212: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  May 22 05:51:21.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:23.241: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:25.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:27.242: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:29.241: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:31.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:33.241: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:35.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:37.242: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:39.241: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:41.242: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 05:51:43.354: INFO: Waited 109.075216ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/22/23 05:51:43.386
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/22/23 05:51:43.388
  STEP: List APIServices @ 05/22/23 05:51:43.392
  May 22 05:51:43.396: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/22/23 05:51:43.396
  May 22 05:51:43.404: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/22/23 05:51:43.404
  May 22 05:51:43.413: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.May, 22, 5, 51, 43, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/22/23 05:51:43.413
  May 22 05:51:43.415: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-05-22 05:51:43 +0000 UTC Passed all checks passed}
  May 22 05:51:43.415: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 22 05:51:43.415: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/22/23 05:51:43.415
  May 22 05:51:43.424: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1376843347" @ 05/22/23 05:51:43.425
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/22/23 05:51:43.441
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/22/23 05:51:43.445
  STEP: Patch APIService Status @ 05/22/23 05:51:43.448
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/22/23 05:51:43.452
  May 22 05:51:43.454: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-05-22 05:51:43 +0000 UTC Passed all checks passed}
  May 22 05:51:43.454: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 22 05:51:43.454: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  May 22 05:51:43.454: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 05/22/23 05:51:43.454
  STEP: Confirm that the generated APIService has been deleted @ 05/22/23 05:51:43.457
  May 22 05:51:43.457: INFO: Requesting list of APIServices to confirm quantity
  May 22 05:51:43.461: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  May 22 05:51:43.461: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  May 22 05:51:43.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-8047" for this suite. @ 05/22/23 05:51:43.512
• [24.798 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 05/22/23 05:51:43.518
  May 22 05:51:43.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 05:51:43.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:51:43.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:51:43.529
  May 22 05:51:43.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4628 create -f -'
  May 22 05:51:44.529: INFO: stderr: ""
  May 22 05:51:44.529: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  May 22 05:51:44.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4628 create -f -'
  May 22 05:51:44.822: INFO: stderr: ""
  May 22 05:51:44.822: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/22/23 05:51:44.822
  May 22 05:51:45.825: INFO: Selector matched 1 pods for map[app:agnhost]
  May 22 05:51:45.825: INFO: Found 1 / 1
  May 22 05:51:45.825: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 22 05:51:45.826: INFO: Selector matched 1 pods for map[app:agnhost]
  May 22 05:51:45.826: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 22 05:51:45.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4628 describe pod agnhost-primary-8556b'
  May 22 05:51:45.891: INFO: stderr: ""
  May 22 05:51:45.891: INFO: stdout: "Name:             agnhost-primary-8556b\nNamespace:        kubectl-4628\nPriority:         0\nService Account:  default\nNode:             node2/192.168.33.122\nStart Time:       Mon, 22 May 2023 05:51:44 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 61088b3552873d7b722d8bf9ca85e3d1084de97ee6de0fa541517db79d367fd9\n                  cni.projectcalico.org/podIP: 192.168.104.49/32\n                  cni.projectcalico.org/podIPs: 192.168.104.49/32\nStatus:           Running\nIP:               192.168.104.49\nIPs:\n  IP:           192.168.104.49\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://a5d8a3cc2aa50c7b77624e7956fd8d9c2eeff51f81cc81e60e2c1c653701a53c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 22 May 2023 05:51:45 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m4dl9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-m4dl9:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-4628/agnhost-primary-8556b to node2\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
  May 22 05:51:45.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4628 describe rc agnhost-primary'
  May 22 05:51:45.953: INFO: stderr: ""
  May 22 05:51:45.953: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4628\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-8556b\n"
  May 22 05:51:45.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4628 describe service agnhost-primary'
  May 22 05:51:46.014: INFO: stderr: ""
  May 22 05:51:46.014: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4628\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.98.125.149\nIPs:               10.98.125.149\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.104.49:6379\nSession Affinity:  None\nEvents:            <none>\n"
  May 22 05:51:46.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4628 describe node node1'
  May 22 05:51:46.096: INFO: stderr: ""
  May 22 05:51:46.096: INFO: stdout: "Name:               node1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=node1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.tigera.io\":\"node1\",\"rbd.csi.ceph.com\":\"node1\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.168.20.121/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 192.168.166.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 17 May 2023 07:19:23 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  node1\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 22 May 2023 05:51:46 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 19 May 2023 08:44:11 +0000   Fri, 19 May 2023 08:44:11 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 22 May 2023 05:49:56 +0000   Fri, 19 May 2023 08:29:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 22 May 2023 05:49:56 +0000   Fri, 19 May 2023 08:29:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 22 May 2023 05:49:56 +0000   Fri, 19 May 2023 08:29:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 22 May 2023 05:49:56 +0000   Fri, 19 May 2023 11:57:28 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.33.121\n  Hostname:    node1\nCapacity:\n  cpu:                    8\n  ephemeral-storage:      49222292Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16392528Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    8\n  ephemeral-storage:      45363264233\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 16290128Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 388f064eb9d542a0b199f3d2b05c8406\n  System UUID:                e1df0142-2636-2723-5381-7a7f5f816f14\n  Boot ID:                    8860e6ee-4362-4905-8129-53d484adf1c5\n  Kernel Version:             5.4.0-146-generic\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.21\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nPodCIDR:                      192.168.0.0/24\nPodCIDRs:                     192.168.0.0/24\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-apiserver            calico-apiserver-666fc8f69-6l8kk                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d20h\n  calico-system               calico-node-k76wg                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d21h\n  calico-system               csi-node-driver-vg8gt                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d21h\n  default                     csi-rbdplugin-hjnwl                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d17h\n  default                     csi-rbdplugin-provisioner-6d5864d5f5-2scr8                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d17h\n  kube-system                 coredns-5d78c9869d-nckgt                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     4d22h\n  kube-system                 coredns-5d78c9869d-wrhg2                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     4d22h\n  kube-system                 etcd-node1                                                 100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         4d22h\n  kube-system                 kube-apiserver-node1                                       250m (3%)     0 (0%)      0 (0%)           0 (0%)         4d22h\n  kube-system                 kube-controller-manager-node1                              200m (2%)     0 (0%)      0 (0%)           0 (0%)         4d22h\n  kube-system                 kube-proxy-qx9ss                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d22h\n  kube-system                 kube-scheduler-node1                                       100m (1%)     0 (0%)      0 (0%)           0 (0%)         4d22h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-bdnbl    0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    850m (10%)  0 (0%)\n  memory                 240Mi (1%)  340Mi (2%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:                  <none>\n"
  May 22 05:51:46.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-4628 describe namespace kubectl-4628'
  May 22 05:51:46.155: INFO: stderr: ""
  May 22 05:51:46.155: INFO: stdout: "Name:         kubectl-4628\nLabels:       e2e-framework=kubectl\n              e2e-run=58979e28-e9ea-4c59-9bf7-7b6c51b0e65a\n              kubernetes.io/metadata.name=kubectl-4628\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  May 22 05:51:46.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4628" for this suite. @ 05/22/23 05:51:46.158
• [2.643 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/22/23 05:51:46.163
  May 22 05:51:46.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 05:51:46.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:51:46.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:51:46.173
  STEP: creating the pod @ 05/22/23 05:51:46.175
  STEP: submitting the pod to kubernetes @ 05/22/23 05:51:46.175
  STEP: verifying QOS class is set on the pod @ 05/22/23 05:51:46.18
  May 22 05:51:46.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3267" for this suite. @ 05/22/23 05:51:46.185
• [0.024 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 05/22/23 05:51:46.188
  May 22 05:51:46.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 05:51:46.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:51:46.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:51:46.197
  STEP: Creating a ResourceQuota with best effort scope @ 05/22/23 05:51:46.199
  STEP: Ensuring ResourceQuota status is calculated @ 05/22/23 05:51:46.201
  STEP: Creating a ResourceQuota with not best effort scope @ 05/22/23 05:51:48.204
  STEP: Ensuring ResourceQuota status is calculated @ 05/22/23 05:51:48.206
  STEP: Creating a best-effort pod @ 05/22/23 05:51:50.209
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/22/23 05:51:50.219
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/22/23 05:51:52.222
  STEP: Deleting the pod @ 05/22/23 05:51:54.224
  STEP: Ensuring resource quota status released the pod usage @ 05/22/23 05:51:54.231
  STEP: Creating a not best-effort pod @ 05/22/23 05:51:56.233
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/22/23 05:51:56.24
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/22/23 05:51:58.242
  STEP: Deleting the pod @ 05/22/23 05:52:00.246
  STEP: Ensuring resource quota status released the pod usage @ 05/22/23 05:52:00.252
  May 22 05:52:02.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4784" for this suite. @ 05/22/23 05:52:02.259
• [16.074 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 05/22/23 05:52:02.262
  May 22 05:52:02.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-probe @ 05/22/23 05:52:02.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:52:02.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:52:02.271
  STEP: Creating pod test-grpc-1b4753dd-c73b-4a4b-9b38-c73bb1f3c2f3 in namespace container-probe-3490 @ 05/22/23 05:52:02.273
  May 22 05:52:04.282: INFO: Started pod test-grpc-1b4753dd-c73b-4a4b-9b38-c73bb1f3c2f3 in namespace container-probe-3490
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/22/23 05:52:04.282
  May 22 05:52:04.285: INFO: Initial restart count of pod test-grpc-1b4753dd-c73b-4a4b-9b38-c73bb1f3c2f3 is 0
  May 22 05:53:08.410: INFO: Restart count of pod container-probe-3490/test-grpc-1b4753dd-c73b-4a4b-9b38-c73bb1f3c2f3 is now 1 (1m4.125030105s elapsed)
  May 22 05:53:08.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 05:53:08.413
  STEP: Destroying namespace "container-probe-3490" for this suite. @ 05/22/23 05:53:08.421
• [66.162 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 05/22/23 05:53:08.425
  May 22 05:53:08.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replicaset @ 05/22/23 05:53:08.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:53:08.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:53:08.437
  STEP: Create a Replicaset @ 05/22/23 05:53:08.442
  STEP: Verify that the required pods have come up. @ 05/22/23 05:53:08.444
  May 22 05:53:08.446: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 22 05:53:13.453: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/22/23 05:53:13.453
  STEP: Getting /status @ 05/22/23 05:53:13.453
  May 22 05:53:13.455: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/22/23 05:53:13.455
  May 22 05:53:13.459: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/22/23 05:53:13.459
  May 22 05:53:13.461: INFO: Observed &ReplicaSet event: ADDED
  May 22 05:53:13.461: INFO: Observed &ReplicaSet event: MODIFIED
  May 22 05:53:13.461: INFO: Observed &ReplicaSet event: MODIFIED
  May 22 05:53:13.461: INFO: Observed &ReplicaSet event: MODIFIED
  May 22 05:53:13.461: INFO: Found replicaset test-rs in namespace replicaset-7595 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 22 05:53:13.461: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/22/23 05:53:13.461
  May 22 05:53:13.461: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 22 05:53:13.464: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/22/23 05:53:13.464
  May 22 05:53:13.466: INFO: Observed &ReplicaSet event: ADDED
  May 22 05:53:13.466: INFO: Observed &ReplicaSet event: MODIFIED
  May 22 05:53:13.466: INFO: Observed &ReplicaSet event: MODIFIED
  May 22 05:53:13.466: INFO: Observed &ReplicaSet event: MODIFIED
  May 22 05:53:13.466: INFO: Observed replicaset test-rs in namespace replicaset-7595 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 22 05:53:13.466: INFO: Observed &ReplicaSet event: MODIFIED
  May 22 05:53:13.466: INFO: Found replicaset test-rs in namespace replicaset-7595 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May 22 05:53:13.466: INFO: Replicaset test-rs has a patched status
  May 22 05:53:13.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7595" for this suite. @ 05/22/23 05:53:13.469
• [5.050 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 05/22/23 05:53:13.474
  May 22 05:53:13.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:53:13.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:53:13.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:53:13.483
  STEP: Creating configMap with name projected-configmap-test-volume-map-912a1f67-b78f-4e70-89ab-5cd81f3455bb @ 05/22/23 05:53:13.485
  STEP: Creating a pod to test consume configMaps @ 05/22/23 05:53:13.487
  STEP: Saw pod success @ 05/22/23 05:53:17.527
  May 22 05:53:17.529: INFO: Trying to get logs from node node3 pod pod-projected-configmaps-a2a62a84-46ab-4458-ad5f-cbf2ba9efcb2 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 05:53:17.54
  May 22 05:53:17.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6023" for this suite. @ 05/22/23 05:53:17.55
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 05/22/23 05:53:17.554
  May 22 05:53:17.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename endpointslice @ 05/22/23 05:53:17.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:53:17.594
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:53:17.596
  STEP: referencing a single matching pod @ 05/22/23 05:53:22.641
  STEP: referencing matching pods with named port @ 05/22/23 05:53:27.648
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/22/23 05:53:32.653
  STEP: recreating EndpointSlices after they've been deleted @ 05/22/23 05:53:37.657
  May 22 05:53:37.666: INFO: EndpointSlice for Service endpointslice-1863/example-named-port not found
  May 22 05:53:47.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1863" for this suite. @ 05/22/23 05:53:47.675
• [30.125 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 05/22/23 05:53:47.679
  May 22 05:53:47.679: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/22/23 05:53:47.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:53:47.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:53:47.69
  May 22 05:53:47.692: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:53:53.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-574" for this suite. @ 05/22/23 05:53:53.944
• [6.267 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/22/23 05:53:53.947
  May 22 05:53:53.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename deployment @ 05/22/23 05:53:53.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:53:53.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:53:53.957
  May 22 05:53:53.959: INFO: Creating deployment "test-recreate-deployment"
  May 22 05:53:53.965: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  May 22 05:53:53.968: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  May 22 05:53:55.974: INFO: Waiting deployment "test-recreate-deployment" to complete
  May 22 05:53:55.975: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  May 22 05:53:55.981: INFO: Updating deployment test-recreate-deployment
  May 22 05:53:55.981: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  May 22 05:53:56.020: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-6531  9c61f152-7000-433a-a243-fdbdeaaa4dd8 884201 2 2023-05-22 05:53:53 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-22 05:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 05:53:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046674d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-22 05:53:56 +0000 UTC,LastTransitionTime:2023-05-22 05:53:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-05-22 05:53:56 +0000 UTC,LastTransitionTime:2023-05-22 05:53:53 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  May 22 05:53:56.023: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-6531  dfbcda83-edfa-455b-83c9-a3fab5b4767b 884197 1 2023-05-22 05:53:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9c61f152-7000-433a-a243-fdbdeaaa4dd8 0xc004b6c017 0xc004b6c018}] [] [{kube-controller-manager Update apps/v1 2023-05-22 05:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c61f152-7000-433a-a243-fdbdeaaa4dd8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 05:53:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b6c128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 22 05:53:56.023: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  May 22 05:53:56.023: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-6531  88e8984a-a764-4483-b29d-eade1bfdd04a 884186 2 2023-05-22 05:53:53 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9c61f152-7000-433a-a243-fdbdeaaa4dd8 0xc004b6c257 0xc004b6c258}] [] [{kube-controller-manager Update apps/v1 2023-05-22 05:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c61f152-7000-433a-a243-fdbdeaaa4dd8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 05:53:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b6c408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 22 05:53:56.025: INFO: Pod "test-recreate-deployment-54757ffd6c-qctqn" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-qctqn test-recreate-deployment-54757ffd6c- deployment-6531  f131ab80-212b-4112-aa00-26538dd21a05 884200 0 2023-05-22 05:53:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c dfbcda83-edfa-455b-83c9-a3fab5b4767b 0xc0049bf8d7 0xc0049bf8d8}] [] [{kube-controller-manager Update v1 2023-05-22 05:53:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfbcda83-edfa-455b-83c9-a3fab5b4767b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 05:53:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gq5ks,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gq5ks,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:53:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:53:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:53:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 05:53:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:,StartTime:2023-05-22 05:53:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 05:53:56.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6531" for this suite. @ 05/22/23 05:53:56.028
• [2.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 05/22/23 05:53:56.032
  May 22 05:53:56.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:53:56.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:53:56.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:53:56.042
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 05:53:56.043
  STEP: Saw pod success @ 05/22/23 05:54:00.056
  May 22 05:54:00.057: INFO: Trying to get logs from node node2 pod downwardapi-volume-74ad75e3-1abe-4884-b0b7-b09ce42ea4d1 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 05:54:00.068
  May 22 05:54:00.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6860" for this suite. @ 05/22/23 05:54:00.078
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/22/23 05:54:00.081
  May 22 05:54:00.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-runtime @ 05/22/23 05:54:00.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:54:00.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:54:00.095
  STEP: create the container @ 05/22/23 05:54:00.097
  W0522 05:54:00.101618      25 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/22/23 05:54:00.101
  STEP: get the container status @ 05/22/23 05:54:03.111
  STEP: the container should be terminated @ 05/22/23 05:54:03.113
  STEP: the termination message should be set @ 05/22/23 05:54:03.113
  May 22 05:54:03.113: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/22/23 05:54:03.113
  May 22 05:54:03.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1537" for this suite. @ 05/22/23 05:54:03.123
• [3.045 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 05/22/23 05:54:03.127
  May 22 05:54:03.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename daemonsets @ 05/22/23 05:54:03.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:54:03.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:54:03.136
  STEP: Creating simple DaemonSet "daemon-set" @ 05/22/23 05:54:03.15
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/22/23 05:54:03.153
  May 22 05:54:03.157: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 05:54:03.157: INFO: Node node1 is running 0 daemon pod, expected 1
  May 22 05:54:04.162: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 22 05:54:04.162: INFO: Node node1 is running 0 daemon pod, expected 1
  May 22 05:54:05.162: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 22 05:54:05.162: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 05/22/23 05:54:05.163
  May 22 05:54:05.165: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/22/23 05:54:05.165
  May 22 05:54:05.171: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/22/23 05:54:05.171
  May 22 05:54:05.172: INFO: Observed &DaemonSet event: ADDED
  May 22 05:54:05.172: INFO: Observed &DaemonSet event: MODIFIED
  May 22 05:54:05.172: INFO: Observed &DaemonSet event: MODIFIED
  May 22 05:54:05.172: INFO: Observed &DaemonSet event: MODIFIED
  May 22 05:54:05.173: INFO: Observed &DaemonSet event: MODIFIED
  May 22 05:54:05.173: INFO: Found daemon set daemon-set in namespace daemonsets-789 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 22 05:54:05.173: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/22/23 05:54:05.173
  STEP: watching for the daemon set status to be patched @ 05/22/23 05:54:05.176
  May 22 05:54:05.177: INFO: Observed &DaemonSet event: ADDED
  May 22 05:54:05.178: INFO: Observed &DaemonSet event: MODIFIED
  May 22 05:54:05.178: INFO: Observed &DaemonSet event: MODIFIED
  May 22 05:54:05.178: INFO: Observed &DaemonSet event: MODIFIED
  May 22 05:54:05.178: INFO: Observed &DaemonSet event: MODIFIED
  May 22 05:54:05.178: INFO: Observed daemon set daemon-set in namespace daemonsets-789 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 22 05:54:05.178: INFO: Observed &DaemonSet event: MODIFIED
  May 22 05:54:05.178: INFO: Found daemon set daemon-set in namespace daemonsets-789 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  May 22 05:54:05.178: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/22/23 05:54:05.18
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-789, will wait for the garbage collector to delete the pods @ 05/22/23 05:54:05.18
  May 22 05:54:05.235: INFO: Deleting DaemonSet.extensions daemon-set took: 3.351546ms
  May 22 05:54:05.336: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.615678ms
  May 22 05:54:07.738: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 05:54:07.738: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 22 05:54:07.739: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"884399"},"items":null}

  May 22 05:54:07.741: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"884399"},"items":null}

  May 22 05:54:07.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-789" for this suite. @ 05/22/23 05:54:07.751
• [4.627 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 05/22/23 05:54:07.754
  May 22 05:54:07.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replication-controller @ 05/22/23 05:54:07.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:54:07.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:54:07.764
  STEP: Given a ReplicationController is created @ 05/22/23 05:54:07.766
  STEP: When the matched label of one of its pods change @ 05/22/23 05:54:07.769
  May 22 05:54:07.770: INFO: Pod name pod-release: Found 0 pods out of 1
  May 22 05:54:12.776: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/22/23 05:54:12.782
  May 22 05:54:13.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1177" for this suite. @ 05/22/23 05:54:13.79
• [6.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 05/22/23 05:54:13.798
  May 22 05:54:13.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 05:54:13.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:54:13.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:54:13.811
  STEP: Setting up server cert @ 05/22/23 05:54:13.824
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 05:54:14.517
  STEP: Deploying the webhook pod @ 05/22/23 05:54:14.522
  STEP: Wait for the deployment to be ready @ 05/22/23 05:54:14.529
  May 22 05:54:14.536: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/22/23 05:54:16.542
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 05:54:16.55
  May 22 05:54:17.551: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 22 05:54:17.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1830-crds.webhook.example.com via the AdmissionRegistration API @ 05/22/23 05:54:18.061
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/22/23 05:54:18.073
  May 22 05:54:20.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9658" for this suite. @ 05/22/23 05:54:20.636
  STEP: Destroying namespace "webhook-markers-3946" for this suite. @ 05/22/23 05:54:20.639
• [6.844 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 05/22/23 05:54:20.643
  May 22 05:54:20.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename server-version @ 05/22/23 05:54:20.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:54:20.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:54:20.651
  STEP: Request ServerVersion @ 05/22/23 05:54:20.653
  STEP: Confirm major version @ 05/22/23 05:54:20.654
  May 22 05:54:20.654: INFO: Major version: 1
  STEP: Confirm minor version @ 05/22/23 05:54:20.654
  May 22 05:54:20.654: INFO: cleanMinorVersion: 27
  May 22 05:54:20.655: INFO: Minor version: 27
  May 22 05:54:20.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-7618" for this suite. @ 05/22/23 05:54:20.658
• [0.018 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 05/22/23 05:54:20.662
  May 22 05:54:20.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-probe @ 05/22/23 05:54:20.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:54:20.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:54:20.671
  STEP: Creating pod liveness-ff0e6203-2990-4c89-9ae2-4ca523dca556 in namespace container-probe-8921 @ 05/22/23 05:54:20.673
  May 22 05:54:22.686: INFO: Started pod liveness-ff0e6203-2990-4c89-9ae2-4ca523dca556 in namespace container-probe-8921
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/22/23 05:54:22.687
  May 22 05:54:22.689: INFO: Initial restart count of pod liveness-ff0e6203-2990-4c89-9ae2-4ca523dca556 is 0
  May 22 05:54:42.726: INFO: Restart count of pod container-probe-8921/liveness-ff0e6203-2990-4c89-9ae2-4ca523dca556 is now 1 (20.03681083s elapsed)
  May 22 05:55:02.760: INFO: Restart count of pod container-probe-8921/liveness-ff0e6203-2990-4c89-9ae2-4ca523dca556 is now 2 (40.071376798s elapsed)
  May 22 05:55:22.789: INFO: Restart count of pod container-probe-8921/liveness-ff0e6203-2990-4c89-9ae2-4ca523dca556 is now 3 (1m0.100191208s elapsed)
  May 22 05:55:42.821: INFO: Restart count of pod container-probe-8921/liveness-ff0e6203-2990-4c89-9ae2-4ca523dca556 is now 4 (1m20.132504423s elapsed)
  May 22 05:56:46.932: INFO: Restart count of pod container-probe-8921/liveness-ff0e6203-2990-4c89-9ae2-4ca523dca556 is now 5 (2m24.24332162s elapsed)
  May 22 05:56:46.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 05:56:46.936
  STEP: Destroying namespace "container-probe-8921" for this suite. @ 05/22/23 05:56:46.942
• [146.284 seconds]
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 05/22/23 05:56:46.946
  May 22 05:56:46.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pod-network-test @ 05/22/23 05:56:46.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:56:46.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:56:46.957
  STEP: Performing setup for networking test in namespace pod-network-test-3462 @ 05/22/23 05:56:46.959
  STEP: creating a selector @ 05/22/23 05:56:46.96
  STEP: Creating the service pods in kubernetes @ 05/22/23 05:56:46.96
  May 22 05:56:46.960: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/22/23 05:56:59.009
  May 22 05:57:01.021: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 22 05:57:01.021: INFO: Breadth first check of 192.168.166.191 on host 192.168.33.121...
  May 22 05:57:01.022: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.104.53:9080/dial?request=hostname&protocol=http&host=192.168.166.191&port=8083&tries=1'] Namespace:pod-network-test-3462 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:57:01.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:57:01.023: INFO: ExecWithOptions: Clientset creation
  May 22 05:57:01.023: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3462/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.104.53%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.166.191%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 22 05:57:01.097: INFO: Waiting for responses: map[]
  May 22 05:57:01.097: INFO: reached 192.168.166.191 after 0/1 tries
  May 22 05:57:01.097: INFO: Breadth first check of 192.168.104.28 on host 192.168.33.122...
  May 22 05:57:01.099: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.104.53:9080/dial?request=hostname&protocol=http&host=192.168.104.28&port=8083&tries=1'] Namespace:pod-network-test-3462 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:57:01.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:57:01.100: INFO: ExecWithOptions: Clientset creation
  May 22 05:57:01.100: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3462/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.104.53%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.104.28%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 22 05:57:01.164: INFO: Waiting for responses: map[]
  May 22 05:57:01.164: INFO: reached 192.168.104.28 after 0/1 tries
  May 22 05:57:01.164: INFO: Breadth first check of 192.168.135.28 on host 192.168.33.123...
  May 22 05:57:01.166: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.104.53:9080/dial?request=hostname&protocol=http&host=192.168.135.28&port=8083&tries=1'] Namespace:pod-network-test-3462 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:57:01.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:57:01.167: INFO: ExecWithOptions: Clientset creation
  May 22 05:57:01.167: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3462/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.104.53%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.135.28%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 22 05:57:01.215: INFO: Waiting for responses: map[]
  May 22 05:57:01.215: INFO: reached 192.168.135.28 after 0/1 tries
  May 22 05:57:01.215: INFO: Going to retry 0 out of 3 pods....
  May 22 05:57:01.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3462" for this suite. @ 05/22/23 05:57:01.219
• [14.278 seconds]
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/22/23 05:57:01.224
  May 22 05:57:01.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubelet-test @ 05/22/23 05:57:01.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:57:01.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:57:01.235
  STEP: Waiting for pod completion @ 05/22/23 05:57:01.242
  May 22 05:57:05.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-447" for this suite. @ 05/22/23 05:57:05.267
• [4.047 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 05/22/23 05:57:05.27
  May 22 05:57:05.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename field-validation @ 05/22/23 05:57:05.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:57:05.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:57:05.28
  May 22 05:57:05.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  W0522 05:57:07.817311      25 warnings.go:70] unknown field "alpha"
  W0522 05:57:07.817334      25 warnings.go:70] unknown field "beta"
  W0522 05:57:07.817341      25 warnings.go:70] unknown field "delta"
  W0522 05:57:07.817344      25 warnings.go:70] unknown field "epsilon"
  W0522 05:57:07.817347      25 warnings.go:70] unknown field "gamma"
  May 22 05:57:07.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2736" for this suite. @ 05/22/23 05:57:07.833
• [2.566 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 05/22/23 05:57:07.838
  May 22 05:57:07.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 05:57:07.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:57:07.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:57:07.848
  STEP: Setting up server cert @ 05/22/23 05:57:07.859
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 05:57:08.176
  STEP: Deploying the webhook pod @ 05/22/23 05:57:08.181
  STEP: Wait for the deployment to be ready @ 05/22/23 05:57:08.187
  May 22 05:57:08.190: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/22/23 05:57:10.198
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 05:57:10.206
  May 22 05:57:11.206: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 22 05:57:11.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1884-crds.webhook.example.com via the AdmissionRegistration API @ 05/22/23 05:57:11.715
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/22/23 05:57:11.726
  May 22 05:57:13.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6012" for this suite. @ 05/22/23 05:57:14.271
  STEP: Destroying namespace "webhook-markers-9794" for this suite. @ 05/22/23 05:57:14.274
• [6.439 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 05/22/23 05:57:14.277
  May 22 05:57:14.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 05:57:14.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:57:14.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:57:14.288
  STEP: Creating Pod @ 05/22/23 05:57:14.29
  STEP: Reading file content from the nginx-container @ 05/22/23 05:57:16.301
  May 22 05:57:16.301: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3534 PodName:pod-sharedvolume-221715b6-579b-422e-9417-57011c707829 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 05:57:16.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 05:57:16.301: INFO: ExecWithOptions: Clientset creation
  May 22 05:57:16.301: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-3534/pods/pod-sharedvolume-221715b6-579b-422e-9417-57011c707829/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  May 22 05:57:16.374: INFO: Exec stderr: ""
  May 22 05:57:16.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3534" for this suite. @ 05/22/23 05:57:16.378
• [2.105 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/22/23 05:57:16.383
  May 22 05:57:16.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/22/23 05:57:16.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:57:16.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:57:16.392
  May 22 05:57:18.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 05/22/23 05:57:18.414
  STEP: Cleaning up the configmap @ 05/22/23 05:57:18.417
  STEP: Cleaning up the pod @ 05/22/23 05:57:18.419
  STEP: Destroying namespace "emptydir-wrapper-232" for this suite. @ 05/22/23 05:57:18.425
• [2.045 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 05/22/23 05:57:18.429
  May 22 05:57:18.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 05:57:18.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:57:18.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:57:18.437
  STEP: creating a Service @ 05/22/23 05:57:18.441
  STEP: watching for the Service to be added @ 05/22/23 05:57:18.449
  May 22 05:57:18.451: INFO: Found Service test-service-xtwkn in namespace services-4033 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  May 22 05:57:18.451: INFO: Service test-service-xtwkn created
  STEP: Getting /status @ 05/22/23 05:57:18.451
  May 22 05:57:18.453: INFO: Service test-service-xtwkn has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/22/23 05:57:18.453
  STEP: watching for the Service to be patched @ 05/22/23 05:57:18.457
  May 22 05:57:18.458: INFO: observed Service test-service-xtwkn in namespace services-4033 with annotations: map[] & LoadBalancer: {[]}
  May 22 05:57:18.459: INFO: Found Service test-service-xtwkn in namespace services-4033 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  May 22 05:57:18.459: INFO: Service test-service-xtwkn has service status patched
  STEP: updating the ServiceStatus @ 05/22/23 05:57:18.459
  May 22 05:57:18.464: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/22/23 05:57:18.464
  May 22 05:57:18.466: INFO: Observed Service test-service-xtwkn in namespace services-4033 with annotations: map[] & Conditions: {[]}
  May 22 05:57:18.466: INFO: Observed event: &Service{ObjectMeta:{test-service-xtwkn  services-4033  0b6675ff-e453-4d4e-bb4b-4c00750a127a 885535 0 2023-05-22 05:57:18 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-22 05:57:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-22 05:57:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.101.249.161,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.101.249.161],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  May 22 05:57:18.466: INFO: Found Service test-service-xtwkn in namespace services-4033 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 22 05:57:18.466: INFO: Service test-service-xtwkn has service status updated
  STEP: patching the service @ 05/22/23 05:57:18.466
  STEP: watching for the Service to be patched @ 05/22/23 05:57:18.469
  May 22 05:57:18.470: INFO: observed Service test-service-xtwkn in namespace services-4033 with labels: map[test-service-static:true]
  May 22 05:57:18.471: INFO: observed Service test-service-xtwkn in namespace services-4033 with labels: map[test-service-static:true]
  May 22 05:57:18.471: INFO: observed Service test-service-xtwkn in namespace services-4033 with labels: map[test-service-static:true]
  May 22 05:57:18.471: INFO: Found Service test-service-xtwkn in namespace services-4033 with labels: map[test-service:patched test-service-static:true]
  May 22 05:57:18.471: INFO: Service test-service-xtwkn patched
  STEP: deleting the service @ 05/22/23 05:57:18.471
  STEP: watching for the Service to be deleted @ 05/22/23 05:57:18.479
  May 22 05:57:18.481: INFO: Observed event: ADDED
  May 22 05:57:18.481: INFO: Observed event: MODIFIED
  May 22 05:57:18.481: INFO: Observed event: MODIFIED
  May 22 05:57:18.481: INFO: Observed event: MODIFIED
  May 22 05:57:18.481: INFO: Found Service test-service-xtwkn in namespace services-4033 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  May 22 05:57:18.481: INFO: Service test-service-xtwkn deleted
  May 22 05:57:18.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4033" for this suite. @ 05/22/23 05:57:18.483
• [0.057 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 05/22/23 05:57:18.487
  May 22 05:57:18.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename svcaccounts @ 05/22/23 05:57:18.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:57:18.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:57:18.496
  May 22 05:57:18.505: INFO: created pod
  STEP: Saw pod success @ 05/22/23 05:57:22.515
  May 22 05:57:52.517: INFO: polling logs
  May 22 05:57:52.531: INFO: Pod logs: 
  I0522 05:57:19.166539       1 log.go:198] OK: Got token
  I0522 05:57:19.166573       1 log.go:198] validating with in-cluster discovery
  I0522 05:57:19.166976       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0522 05:57:19.167005       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6088:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684735638, NotBefore:1684735038, IssuedAt:1684735038, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6088", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"35225ec2-d640-4127-a5a9-eea73c576b75"}}}
  I0522 05:57:19.175918       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0522 05:57:19.181202       1 log.go:198] OK: Validated signature on JWT
  I0522 05:57:19.181279       1 log.go:198] OK: Got valid claims from token!
  I0522 05:57:19.181297       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6088:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684735638, NotBefore:1684735038, IssuedAt:1684735038, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6088", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"35225ec2-d640-4127-a5a9-eea73c576b75"}}}

  May 22 05:57:52.532: INFO: completed pod
  May 22 05:57:52.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6088" for this suite. @ 05/22/23 05:57:52.538
• [34.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 05/22/23 05:57:52.543
  May 22 05:57:52.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 05:57:52.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:57:52.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:57:52.555
  STEP: Creating configMap with name projected-configmap-test-volume-map-af224883-0514-4fd3-9ea0-40c5a46a9d32 @ 05/22/23 05:57:52.558
  STEP: Creating a pod to test consume configMaps @ 05/22/23 05:57:52.561
  STEP: Saw pod success @ 05/22/23 05:57:56.574
  May 22 05:57:56.576: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-72dbceb9-0ce8-438d-a25d-de75af34f855 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 05:57:56.581
  May 22 05:57:56.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4944" for this suite. @ 05/22/23 05:57:56.592
• [4.052 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/22/23 05:57:56.595
  May 22 05:57:56.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubelet-test @ 05/22/23 05:57:56.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:57:56.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:57:56.604
  May 22 05:57:58.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8089" for this suite. @ 05/22/23 05:57:58.624
• [2.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 05/22/23 05:57:58.627
  May 22 05:57:58.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 05:57:58.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:57:58.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:57:58.636
  STEP: Setting up server cert @ 05/22/23 05:57:58.688
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 05:57:59.244
  STEP: Deploying the webhook pod @ 05/22/23 05:57:59.25
  STEP: Wait for the deployment to be ready @ 05/22/23 05:57:59.256
  May 22 05:57:59.261: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/22/23 05:58:01.268
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 05:58:01.276
  May 22 05:58:02.276: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/22/23 05:58:02.278
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/22/23 05:58:02.289
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/22/23 05:58:02.297
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/22/23 05:58:02.303
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/22/23 05:58:02.307
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/22/23 05:58:02.312
  May 22 05:58:02.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5760" for this suite. @ 05/22/23 05:58:02.34
  STEP: Destroying namespace "webhook-markers-5032" for this suite. @ 05/22/23 05:58:02.342
• [3.718 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 05/22/23 05:58:02.346
  May 22 05:58:02.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename podtemplate @ 05/22/23 05:58:02.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:58:02.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:58:02.355
  STEP: Create a pod template @ 05/22/23 05:58:02.357
  STEP: Replace a pod template @ 05/22/23 05:58:02.359
  May 22 05:58:02.363: INFO: Found updated podtemplate annotation: "true"

  May 22 05:58:02.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-9949" for this suite. @ 05/22/23 05:58:02.365
• [0.022 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 05/22/23 05:58:02.368
  May 22 05:58:02.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 05:58:02.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:58:02.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:58:02.376
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/22/23 05:58:02.378
  STEP: Saw pod success @ 05/22/23 05:58:04.385
  May 22 05:58:04.387: INFO: Trying to get logs from node node2 pod pod-e0e748db-c2cd-474d-93cb-433e6bd001c1 container test-container: <nil>
  STEP: delete the pod @ 05/22/23 05:58:04.391
  May 22 05:58:04.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5776" for this suite. @ 05/22/23 05:58:04.4
• [2.034 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 05/22/23 05:58:04.403
  May 22 05:58:04.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 05:58:04.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:58:04.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:58:04.412
  STEP: creating a replication controller @ 05/22/23 05:58:04.414
  May 22 05:58:04.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 create -f -'
  May 22 05:58:05.368: INFO: stderr: ""
  May 22 05:58:05.368: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/22/23 05:58:05.368
  May 22 05:58:05.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 22 05:58:05.428: INFO: stderr: ""
  May 22 05:58:05.428: INFO: stdout: "update-demo-nautilus-smjfw update-demo-nautilus-zvkn2 "
  May 22 05:58:05.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-smjfw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 22 05:58:05.483: INFO: stderr: ""
  May 22 05:58:05.483: INFO: stdout: ""
  May 22 05:58:05.483: INFO: update-demo-nautilus-smjfw is created but not running
  May 22 05:58:10.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 22 05:58:10.544: INFO: stderr: ""
  May 22 05:58:10.544: INFO: stdout: "update-demo-nautilus-smjfw update-demo-nautilus-zvkn2 "
  May 22 05:58:10.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-smjfw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 22 05:58:10.600: INFO: stderr: ""
  May 22 05:58:10.600: INFO: stdout: "true"
  May 22 05:58:10.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-smjfw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 22 05:58:10.658: INFO: stderr: ""
  May 22 05:58:10.658: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 22 05:58:10.658: INFO: validating pod update-demo-nautilus-smjfw
  May 22 05:58:10.661: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 22 05:58:10.661: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 22 05:58:10.661: INFO: update-demo-nautilus-smjfw is verified up and running
  May 22 05:58:10.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-zvkn2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 22 05:58:10.719: INFO: stderr: ""
  May 22 05:58:10.719: INFO: stdout: "true"
  May 22 05:58:10.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-zvkn2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 22 05:58:10.777: INFO: stderr: ""
  May 22 05:58:10.777: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 22 05:58:10.777: INFO: validating pod update-demo-nautilus-zvkn2
  May 22 05:58:10.781: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 22 05:58:10.781: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 22 05:58:10.781: INFO: update-demo-nautilus-zvkn2 is verified up and running
  STEP: scaling down the replication controller @ 05/22/23 05:58:10.781
  May 22 05:58:10.782: INFO: scanned /root for discovery docs: <nil>
  May 22 05:58:10.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  May 22 05:58:11.850: INFO: stderr: ""
  May 22 05:58:11.850: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/22/23 05:58:11.85
  May 22 05:58:11.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 22 05:58:11.910: INFO: stderr: ""
  May 22 05:58:11.910: INFO: stdout: "update-demo-nautilus-zvkn2 "
  May 22 05:58:11.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-zvkn2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 22 05:58:11.973: INFO: stderr: ""
  May 22 05:58:11.973: INFO: stdout: "true"
  May 22 05:58:11.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-zvkn2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 22 05:58:12.032: INFO: stderr: ""
  May 22 05:58:12.032: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 22 05:58:12.032: INFO: validating pod update-demo-nautilus-zvkn2
  May 22 05:58:12.035: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 22 05:58:12.035: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 22 05:58:12.035: INFO: update-demo-nautilus-zvkn2 is verified up and running
  STEP: scaling up the replication controller @ 05/22/23 05:58:12.035
  May 22 05:58:12.035: INFO: scanned /root for discovery docs: <nil>
  May 22 05:58:12.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  May 22 05:58:13.102: INFO: stderr: ""
  May 22 05:58:13.102: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/22/23 05:58:13.102
  May 22 05:58:13.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 22 05:58:13.163: INFO: stderr: ""
  May 22 05:58:13.163: INFO: stdout: "update-demo-nautilus-6vrw2 update-demo-nautilus-zvkn2 "
  May 22 05:58:13.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-6vrw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 22 05:58:13.219: INFO: stderr: ""
  May 22 05:58:13.219: INFO: stdout: ""
  May 22 05:58:13.219: INFO: update-demo-nautilus-6vrw2 is created but not running
  May 22 05:58:18.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 22 05:58:18.281: INFO: stderr: ""
  May 22 05:58:18.281: INFO: stdout: "update-demo-nautilus-6vrw2 update-demo-nautilus-zvkn2 "
  May 22 05:58:18.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-6vrw2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 22 05:58:18.337: INFO: stderr: ""
  May 22 05:58:18.337: INFO: stdout: "true"
  May 22 05:58:18.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-6vrw2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 22 05:58:18.393: INFO: stderr: ""
  May 22 05:58:18.393: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 22 05:58:18.393: INFO: validating pod update-demo-nautilus-6vrw2
  May 22 05:58:18.396: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 22 05:58:18.396: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 22 05:58:18.396: INFO: update-demo-nautilus-6vrw2 is verified up and running
  May 22 05:58:18.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-zvkn2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 22 05:58:18.450: INFO: stderr: ""
  May 22 05:58:18.450: INFO: stdout: "true"
  May 22 05:58:18.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods update-demo-nautilus-zvkn2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 22 05:58:18.508: INFO: stderr: ""
  May 22 05:58:18.508: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 22 05:58:18.508: INFO: validating pod update-demo-nautilus-zvkn2
  May 22 05:58:18.510: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 22 05:58:18.510: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 22 05:58:18.510: INFO: update-demo-nautilus-zvkn2 is verified up and running
  STEP: using delete to clean up resources @ 05/22/23 05:58:18.51
  May 22 05:58:18.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 delete --grace-period=0 --force -f -'
  May 22 05:58:18.567: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 22 05:58:18.567: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 22 05:58:18.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get rc,svc -l name=update-demo --no-headers'
  May 22 05:58:18.638: INFO: stderr: "No resources found in kubectl-2958 namespace.\n"
  May 22 05:58:18.638: INFO: stdout: ""
  May 22 05:58:18.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2958 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 22 05:58:18.709: INFO: stderr: ""
  May 22 05:58:18.709: INFO: stdout: ""
  May 22 05:58:18.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2958" for this suite. @ 05/22/23 05:58:18.712
• [14.312 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 05/22/23 05:58:18.715
  May 22 05:58:18.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 05:58:18.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:58:18.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:58:18.727
  STEP: Creating configMap with name configmap-test-volume-map-8856290e-af2d-4b93-995d-a6e45048e775 @ 05/22/23 05:58:18.73
  STEP: Creating a pod to test consume configMaps @ 05/22/23 05:58:18.735
  STEP: Saw pod success @ 05/22/23 05:58:22.749
  May 22 05:58:22.751: INFO: Trying to get logs from node node2 pod pod-configmaps-854e74e6-38a4-484f-9b8a-6dfabbaf7b91 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 05:58:22.757
  May 22 05:58:22.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1432" for this suite. @ 05/22/23 05:58:22.769
• [4.057 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 05/22/23 05:58:22.773
  May 22 05:58:22.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename job @ 05/22/23 05:58:22.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:58:22.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:58:22.782
  STEP: Creating Indexed job @ 05/22/23 05:58:22.783
  STEP: Ensuring job reaches completions @ 05/22/23 05:58:22.787
  STEP: Ensuring pods with index for job exist @ 05/22/23 05:58:30.791
  May 22 05:58:30.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7834" for this suite. @ 05/22/23 05:58:30.796
• [8.026 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/22/23 05:58:30.799
  May 22 05:58:30.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-runtime @ 05/22/23 05:58:30.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:58:30.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:58:30.81
  STEP: create the container @ 05/22/23 05:58:30.812
  W0522 05:58:30.817471      25 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/22/23 05:58:30.817
  STEP: get the container status @ 05/22/23 05:58:33.827
  STEP: the container should be terminated @ 05/22/23 05:58:33.828
  STEP: the termination message should be set @ 05/22/23 05:58:33.828
  May 22 05:58:33.828: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/22/23 05:58:33.829
  May 22 05:58:33.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4077" for this suite. @ 05/22/23 05:58:33.839
• [3.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 05/22/23 05:58:33.842
  May 22 05:58:33.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename var-expansion @ 05/22/23 05:58:33.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 05:58:33.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 05:58:33.852
  STEP: creating the pod with failed condition @ 05/22/23 05:58:33.855
  STEP: updating the pod @ 05/22/23 06:00:33.86
  May 22 06:00:34.369: INFO: Successfully updated pod "var-expansion-aa282110-2bfa-4989-bc5c-6edc5415aad9"
  STEP: waiting for pod running @ 05/22/23 06:00:34.369
  STEP: deleting the pod gracefully @ 05/22/23 06:00:36.375
  May 22 06:00:36.375: INFO: Deleting pod "var-expansion-aa282110-2bfa-4989-bc5c-6edc5415aad9" in namespace "var-expansion-4743"
  May 22 06:00:36.379: INFO: Wait up to 5m0s for pod "var-expansion-aa282110-2bfa-4989-bc5c-6edc5415aad9" to be fully deleted
  May 22 06:01:08.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4743" for this suite. @ 05/22/23 06:01:08.452
• [154.612 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 05/22/23 06:01:08.455
  May 22 06:01:08.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:01:08.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:01:08.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:01:08.465
  STEP: creating a collection of services @ 05/22/23 06:01:08.467
  May 22 06:01:08.467: INFO: Creating e2e-svc-a-psb22
  May 22 06:01:08.475: INFO: Creating e2e-svc-b-tddvq
  May 22 06:01:08.483: INFO: Creating e2e-svc-c-v682j
  STEP: deleting service collection @ 05/22/23 06:01:08.494
  May 22 06:01:08.512: INFO: Collection of services has been deleted
  May 22 06:01:08.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7540" for this suite. @ 05/22/23 06:01:08.514
• [0.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 05/22/23 06:01:08.518
  May 22 06:01:08.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename gc @ 05/22/23 06:01:08.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:01:08.526
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:01:08.528
  May 22 06:01:08.549: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"2f3b4d8e-f078-4894-a861-0d861ae97bcf", Controller:(*bool)(0xc0086787ba), BlockOwnerDeletion:(*bool)(0xc0086787bb)}}
  May 22 06:01:08.553: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"92643dc2-dfd5-4c65-ab12-50a5d39d9c02", Controller:(*bool)(0xc0086789e2), BlockOwnerDeletion:(*bool)(0xc0086789e3)}}
  May 22 06:01:08.558: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e0ee846b-53af-4486-b15f-8e825ddfbaa5", Controller:(*bool)(0xc008678c12), BlockOwnerDeletion:(*bool)(0xc008678c13)}}
  May 22 06:01:13.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9913" for this suite. @ 05/22/23 06:01:13.569
• [5.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 05/22/23 06:01:13.573
  May 22 06:01:13.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 06:01:13.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:01:13.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:01:13.585
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/22/23 06:01:13.587
  STEP: Saw pod success @ 05/22/23 06:01:17.6
  May 22 06:01:17.602: INFO: Trying to get logs from node node2 pod pod-506476d1-1be7-452a-8409-badcc245b41b container test-container: <nil>
  STEP: delete the pod @ 05/22/23 06:01:17.613
  May 22 06:01:17.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-768" for this suite. @ 05/22/23 06:01:17.626
• [4.056 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 05/22/23 06:01:17.63
  May 22 06:01:17.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:01:17.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:01:17.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:01:17.64
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:01:17.642
  STEP: Saw pod success @ 05/22/23 06:01:21.656
  May 22 06:01:21.658: INFO: Trying to get logs from node node2 pod downwardapi-volume-33430e46-9109-4b5a-acde-477bd4df26b3 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:01:21.662
  May 22 06:01:21.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4153" for this suite. @ 05/22/23 06:01:21.672
• [4.044 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 05/22/23 06:01:21.675
  May 22 06:01:21.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:01:21.675
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:01:21.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:01:21.684
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:01:21.698
  STEP: Saw pod success @ 05/22/23 06:01:25.726
  May 22 06:01:25.728: INFO: Trying to get logs from node node2 pod downwardapi-volume-12d7e370-731a-4d40-9317-0d97f03b5116 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:01:25.733
  May 22 06:01:25.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3266" for this suite. @ 05/22/23 06:01:25.743
• [4.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 05/22/23 06:01:25.747
  May 22 06:01:25.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 06:01:25.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:01:25.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:01:25.756
  STEP: Counting existing ResourceQuota @ 05/22/23 06:01:25.758
  STEP: Creating a ResourceQuota @ 05/22/23 06:01:30.76
  STEP: Ensuring resource quota status is calculated @ 05/22/23 06:01:30.764
  STEP: Creating a Service @ 05/22/23 06:01:32.767
  STEP: Creating a NodePort Service @ 05/22/23 06:01:32.778
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/22/23 06:01:32.792
  STEP: Ensuring resource quota status captures service creation @ 05/22/23 06:01:32.81
  STEP: Deleting Services @ 05/22/23 06:01:34.814
  STEP: Ensuring resource quota status released usage @ 05/22/23 06:01:34.837
  May 22 06:01:36.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6416" for this suite. @ 05/22/23 06:01:36.844
• [11.100 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 05/22/23 06:01:36.847
  May 22 06:01:36.847: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename statefulset @ 05/22/23 06:01:36.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:01:36.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:01:36.857
  STEP: Creating service test in namespace statefulset-7439 @ 05/22/23 06:01:36.859
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/22/23 06:01:36.862
  STEP: Creating stateful set ss in namespace statefulset-7439 @ 05/22/23 06:01:36.864
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7439 @ 05/22/23 06:01:36.867
  May 22 06:01:36.869: INFO: Found 0 stateful pods, waiting for 1
  May 22 06:01:46.874: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/22/23 06:01:46.875
  May 22 06:01:46.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-7439 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 22 06:01:46.995: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 22 06:01:46.995: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 22 06:01:46.995: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 22 06:01:46.997: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May 22 06:01:57.003: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 22 06:01:57.003: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 06:01:57.011: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999827s
  May 22 06:01:58.014: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997774717s
  May 22 06:01:59.016: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994970581s
  May 22 06:02:00.019: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.992551487s
  May 22 06:02:01.022: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.990011247s
  May 22 06:02:02.025: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.98719582s
  May 22 06:02:03.028: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.984302433s
  May 22 06:02:04.030: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.981401935s
  May 22 06:02:05.033: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978771562s
  May 22 06:02:06.036: INFO: Verifying statefulset ss doesn't scale past 1 for another 975.862956ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7439 @ 05/22/23 06:02:07.036
  May 22 06:02:07.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-7439 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 22 06:02:07.168: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 22 06:02:07.168: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 22 06:02:07.168: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 22 06:02:07.170: INFO: Found 1 stateful pods, waiting for 3
  May 22 06:02:17.174: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 22 06:02:17.174: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 22 06:02:17.174: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/22/23 06:02:17.174
  STEP: Scale down will halt with unhealthy stateful pod @ 05/22/23 06:02:17.174
  May 22 06:02:17.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-7439 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 22 06:02:17.300: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 22 06:02:17.300: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 22 06:02:17.300: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 22 06:02:17.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-7439 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 22 06:02:17.429: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 22 06:02:17.429: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 22 06:02:17.429: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 22 06:02:17.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-7439 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 22 06:02:17.568: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 22 06:02:17.569: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 22 06:02:17.569: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 22 06:02:17.569: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 06:02:17.571: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  May 22 06:02:27.576: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 22 06:02:27.576: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 22 06:02:27.576: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 22 06:02:27.584: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999747s
  May 22 06:02:28.587: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997975791s
  May 22 06:02:29.591: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994143631s
  May 22 06:02:30.594: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991385352s
  May 22 06:02:31.597: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988523832s
  May 22 06:02:32.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984966861s
  May 22 06:02:33.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982148649s
  May 22 06:02:34.608: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.9791912s
  May 22 06:02:35.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974550639s
  May 22 06:02:36.614: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.517653ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7439 @ 05/22/23 06:02:37.614
  May 22 06:02:37.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-7439 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 22 06:02:37.745: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 22 06:02:37.745: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 22 06:02:37.745: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 22 06:02:37.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-7439 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 22 06:02:37.877: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 22 06:02:37.877: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 22 06:02:37.877: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 22 06:02:37.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-7439 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 22 06:02:38.020: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 22 06:02:38.020: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 22 06:02:38.020: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 22 06:02:38.020: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/22/23 06:02:48.035
  May 22 06:02:48.035: INFO: Deleting all statefulset in ns statefulset-7439
  May 22 06:02:48.037: INFO: Scaling statefulset ss to 0
  May 22 06:02:48.043: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 06:02:48.044: INFO: Deleting statefulset ss
  May 22 06:02:48.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7439" for this suite. @ 05/22/23 06:02:48.053
• [71.209 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 05/22/23 06:02:48.057
  May 22 06:02:48.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 06:02:48.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:02:48.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:02:48.067
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/22/23 06:02:48.069
  STEP: Saw pod success @ 05/22/23 06:02:52.082
  May 22 06:02:52.084: INFO: Trying to get logs from node node2 pod pod-65e2321d-70e4-4861-af00-f2de75d96def container test-container: <nil>
  STEP: delete the pod @ 05/22/23 06:02:52.09
  May 22 06:02:52.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7702" for this suite. @ 05/22/23 06:02:52.1
• [4.046 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 05/22/23 06:02:52.103
  May 22 06:02:52.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename var-expansion @ 05/22/23 06:02:52.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:02:52.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:02:52.114
  STEP: Creating a pod to test substitution in container's args @ 05/22/23 06:02:52.116
  STEP: Saw pod success @ 05/22/23 06:02:54.125
  May 22 06:02:54.127: INFO: Trying to get logs from node node2 pod var-expansion-eff55566-7d76-42b5-b466-f63d83c27575 container dapi-container: <nil>
  STEP: delete the pod @ 05/22/23 06:02:54.131
  May 22 06:02:54.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5539" for this suite. @ 05/22/23 06:02:54.142
• [2.041 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 05/22/23 06:02:54.145
  May 22 06:02:54.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename init-container @ 05/22/23 06:02:54.146
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:02:54.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:02:54.155
  STEP: creating the pod @ 05/22/23 06:02:54.157
  May 22 06:02:54.157: INFO: PodSpec: initContainers in spec.initContainers
  May 22 06:03:37.934: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c86f7443-0408-4f62-8e34-34ea18480c64", GenerateName:"", Namespace:"init-container-2646", SelfLink:"", UID:"5311e3e3-bc1a-4811-9f00-cd04c95e7887", ResourceVersion:"887909", Generation:0, CreationTimestamp:time.Date(2023, time.May, 22, 6, 2, 54, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"157557949"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"9eeb9fe19c3cc82705fed93844536e3343731ed624b4b3541e14aa0f4b1e0469", "cni.projectcalico.org/podIP":"192.168.104.34/32", "cni.projectcalico.org/podIPs":"192.168.104.34/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 22, 6, 2, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0091ae138), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 22, 6, 2, 54, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0091ae168), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 22, 6, 3, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0091ae198), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-sr799", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0045fc060), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sr799", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sr799", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-sr799", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0047d4118), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000462000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0047d41c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0047d41e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0047d41e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0047d41ec), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e9c240), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 22, 6, 2, 54, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 22, 6, 2, 54, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 22, 6, 2, 54, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 22, 6, 2, 54, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.33.122", PodIP:"192.168.104.34", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.104.34"}}, StartTime:time.Date(2023, time.May, 22, 6, 2, 54, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000462150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004621c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://544e09f9374d159d9f28fd20163870815e67bcca1ce2b1d287dd9b71251b4b1d", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0045fc160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0045fc120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0047d4264), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  May 22 06:03:37.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2646" for this suite. @ 05/22/23 06:03:37.938
• [43.796 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/22/23 06:03:37.941
  May 22 06:03:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:03:37.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:03:37.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:03:37.95
  STEP: Creating secret with name secret-test-d3679e66-584b-48f3-8cb9-ef02e4b2a776 @ 05/22/23 06:03:37.96
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:03:37.962
  STEP: Saw pod success @ 05/22/23 06:03:41.974
  May 22 06:03:41.976: INFO: Trying to get logs from node node3 pod pod-secrets-1e43c4e0-2184-45df-8156-5552e148d381 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 06:03:41.99
  May 22 06:03:41.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4624" for this suite. @ 05/22/23 06:03:42.001
  STEP: Destroying namespace "secret-namespace-6843" for this suite. @ 05/22/23 06:03:42.004
• [4.065 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 05/22/23 06:03:42.007
  May 22 06:03:42.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename subpath @ 05/22/23 06:03:42.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:03:42.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:03:42.017
  STEP: Setting up data @ 05/22/23 06:03:42.019
  STEP: Creating pod pod-subpath-test-projected-x5c8 @ 05/22/23 06:03:42.024
  STEP: Creating a pod to test atomic-volume-subpath @ 05/22/23 06:03:42.024
  STEP: Saw pod success @ 05/22/23 06:04:06.069
  May 22 06:04:06.071: INFO: Trying to get logs from node node3 pod pod-subpath-test-projected-x5c8 container test-container-subpath-projected-x5c8: <nil>
  STEP: delete the pod @ 05/22/23 06:04:06.075
  STEP: Deleting pod pod-subpath-test-projected-x5c8 @ 05/22/23 06:04:06.084
  May 22 06:04:06.084: INFO: Deleting pod "pod-subpath-test-projected-x5c8" in namespace "subpath-4557"
  May 22 06:04:06.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4557" for this suite. @ 05/22/23 06:04:06.088
• [24.085 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 05/22/23 06:04:06.092
  May 22 06:04:06.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:04:06.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:04:06.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:04:06.1
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:04:06.102
  STEP: Saw pod success @ 05/22/23 06:04:10.116
  May 22 06:04:10.118: INFO: Trying to get logs from node node2 pod downwardapi-volume-edd49c68-d9fb-46af-a18a-184f7287eb5f container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:04:10.122
  May 22 06:04:10.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-738" for this suite. @ 05/22/23 06:04:10.133
• [4.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 05/22/23 06:04:10.14
  May 22 06:04:10.140: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:04:10.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:04:10.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:04:10.15
  STEP: creating service multi-endpoint-test in namespace services-608 @ 05/22/23 06:04:10.152
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-608 to expose endpoints map[] @ 05/22/23 06:04:10.159
  May 22 06:04:10.162: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  May 22 06:04:11.167: INFO: successfully validated that service multi-endpoint-test in namespace services-608 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-608 @ 05/22/23 06:04:11.167
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-608 to expose endpoints map[pod1:[100]] @ 05/22/23 06:04:13.18
  May 22 06:04:13.185: INFO: successfully validated that service multi-endpoint-test in namespace services-608 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-608 @ 05/22/23 06:04:13.185
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-608 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/22/23 06:04:15.196
  May 22 06:04:15.202: INFO: successfully validated that service multi-endpoint-test in namespace services-608 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/22/23 06:04:15.202
  May 22 06:04:15.202: INFO: Creating new exec pod
  May 22 06:04:18.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-608 exec execpod47cb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  May 22 06:04:18.337: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  May 22 06:04:18.337: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:04:18.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-608 exec execpod47cb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.8.230 80'
  May 22 06:04:18.467: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.8.230 80\nConnection to 10.108.8.230 80 port [tcp/http] succeeded!\n"
  May 22 06:04:18.467: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:04:18.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-608 exec execpod47cb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  May 22 06:04:18.596: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  May 22 06:04:18.596: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:04:18.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-608 exec execpod47cb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.8.230 81'
  May 22 06:04:18.716: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.8.230 81\nConnection to 10.108.8.230 81 port [tcp/*] succeeded!\n"
  May 22 06:04:18.716: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-608 @ 05/22/23 06:04:18.716
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-608 to expose endpoints map[pod2:[101]] @ 05/22/23 06:04:18.723
  May 22 06:04:18.730: INFO: successfully validated that service multi-endpoint-test in namespace services-608 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-608 @ 05/22/23 06:04:18.73
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-608 to expose endpoints map[] @ 05/22/23 06:04:18.74
  May 22 06:04:18.748: INFO: successfully validated that service multi-endpoint-test in namespace services-608 exposes endpoints map[]
  May 22 06:04:18.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-608" for this suite. @ 05/22/23 06:04:18.761
• [8.625 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/22/23 06:04:18.765
  May 22 06:04:18.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubelet-test @ 05/22/23 06:04:18.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:04:18.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:04:18.774
  May 22 06:04:20.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3476" for this suite. @ 05/22/23 06:04:20.794
• [2.033 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 05/22/23 06:04:20.798
  May 22 06:04:20.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename statefulset @ 05/22/23 06:04:20.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:04:20.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:04:20.808
  STEP: Creating service test in namespace statefulset-6623 @ 05/22/23 06:04:20.81
  STEP: Creating statefulset ss in namespace statefulset-6623 @ 05/22/23 06:04:20.813
  May 22 06:04:20.818: INFO: Found 0 stateful pods, waiting for 1
  May 22 06:04:30.821: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/22/23 06:04:30.825
  STEP: updating a scale subresource @ 05/22/23 06:04:30.826
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/22/23 06:04:30.831
  STEP: Patch a scale subresource @ 05/22/23 06:04:30.833
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/22/23 06:04:30.842
  May 22 06:04:30.845: INFO: Deleting all statefulset in ns statefulset-6623
  May 22 06:04:30.849: INFO: Scaling statefulset ss to 0
  May 22 06:04:40.859: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 06:04:40.861: INFO: Deleting statefulset ss
  May 22 06:04:40.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6623" for this suite. @ 05/22/23 06:04:40.871
• [20.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 05/22/23 06:04:40.875
  May 22 06:04:40.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename job @ 05/22/23 06:04:40.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:04:40.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:04:40.885
  STEP: Creating a job @ 05/22/23 06:04:40.887
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/22/23 06:04:40.89
  STEP: patching /status @ 05/22/23 06:04:42.893
  STEP: updating /status @ 05/22/23 06:04:42.898
  STEP: get /status @ 05/22/23 06:04:42.919
  May 22 06:04:42.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7350" for this suite. @ 05/22/23 06:04:42.923
• [2.051 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 05/22/23 06:04:42.927
  May 22 06:04:42.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 06:04:42.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:04:42.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:04:42.936
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/22/23 06:04:42.938
  STEP: Saw pod success @ 05/22/23 06:04:46.95
  May 22 06:04:46.952: INFO: Trying to get logs from node node2 pod pod-b20a185d-48d6-44bc-8e44-8bdb2f7627b4 container test-container: <nil>
  STEP: delete the pod @ 05/22/23 06:04:46.956
  May 22 06:04:46.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8270" for this suite. @ 05/22/23 06:04:46.965
• [4.041 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 05/22/23 06:04:46.968
  May 22 06:04:46.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replication-controller @ 05/22/23 06:04:46.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:04:46.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:04:46.976
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/22/23 06:04:46.978
  STEP: When a replication controller with a matching selector is created @ 05/22/23 06:04:48.989
  STEP: Then the orphan pod is adopted @ 05/22/23 06:04:48.992
  May 22 06:04:49.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7474" for this suite. @ 05/22/23 06:04:49.998
• [3.034 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 05/22/23 06:04:50.002
  May 22 06:04:50.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:04:50.003
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:04:50.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:04:50.012
  STEP: creating an Endpoint @ 05/22/23 06:04:50.015
  STEP: waiting for available Endpoint @ 05/22/23 06:04:50.017
  STEP: listing all Endpoints @ 05/22/23 06:04:50.018
  STEP: updating the Endpoint @ 05/22/23 06:04:50.02
  STEP: fetching the Endpoint @ 05/22/23 06:04:50.023
  STEP: patching the Endpoint @ 05/22/23 06:04:50.024
  STEP: fetching the Endpoint @ 05/22/23 06:04:50.027
  STEP: deleting the Endpoint by Collection @ 05/22/23 06:04:50.028
  STEP: waiting for Endpoint deletion @ 05/22/23 06:04:50.031
  STEP: fetching the Endpoint @ 05/22/23 06:04:50.032
  May 22 06:04:50.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4712" for this suite. @ 05/22/23 06:04:50.035
• [0.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/22/23 06:04:50.04
  May 22 06:04:50.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename containers @ 05/22/23 06:04:50.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:04:50.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:04:50.048
  STEP: Creating a pod to test override arguments @ 05/22/23 06:04:50.049
  STEP: Saw pod success @ 05/22/23 06:04:54.061
  May 22 06:04:54.063: INFO: Trying to get logs from node node3 pod client-containers-a0658c35-628c-43dd-9e28-9cbfdfdf6ef0 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:04:54.066
  May 22 06:04:54.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1280" for this suite. @ 05/22/23 06:04:54.075
• [4.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 05/22/23 06:04:54.079
  May 22 06:04:54.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename job @ 05/22/23 06:04:54.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:04:54.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:04:54.091
  STEP: Creating a job @ 05/22/23 06:04:54.093
  STEP: Ensuring active pods == parallelism @ 05/22/23 06:04:54.096
  STEP: delete a job @ 05/22/23 06:04:56.101
  STEP: deleting Job.batch foo in namespace job-9959, will wait for the garbage collector to delete the pods @ 05/22/23 06:04:56.101
  May 22 06:04:56.158: INFO: Deleting Job.batch foo took: 4.060703ms
  May 22 06:04:56.259: INFO: Terminating Job.batch foo pods took: 101.003144ms
  STEP: Ensuring job was deleted @ 05/22/23 06:05:28.459
  May 22 06:05:28.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9959" for this suite. @ 05/22/23 06:05:28.464
• [34.397 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 05/22/23 06:05:28.477
  May 22 06:05:28.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:05:28.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:05:28.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:05:28.499
  STEP: Creating the pod @ 05/22/23 06:05:28.502
  May 22 06:05:31.035: INFO: Successfully updated pod "labelsupdatec28d6111-28f6-4e04-bd9e-8ac23613cab1"
  May 22 06:05:35.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8689" for this suite. @ 05/22/23 06:05:35.111
• [6.638 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 05/22/23 06:05:35.115
  May 22 06:05:35.115: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename daemonsets @ 05/22/23 06:05:35.115
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:05:35.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:05:35.123
  May 22 06:05:35.135: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/22/23 06:05:35.138
  May 22 06:05:35.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:05:35.141: INFO: Node node1 is running 0 daemon pod, expected 1
  May 22 06:05:36.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:05:36.147: INFO: Node node1 is running 0 daemon pod, expected 1
  May 22 06:05:37.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 22 06:05:37.147: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/22/23 06:05:37.154
  STEP: Check that daemon pods images are updated. @ 05/22/23 06:05:37.16
  May 22 06:05:37.163: INFO: Wrong image for pod: daemon-set-7hsmf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:37.163: INFO: Wrong image for pod: daemon-set-9g44w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:37.163: INFO: Wrong image for pod: daemon-set-twmd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:38.169: INFO: Wrong image for pod: daemon-set-9g44w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:38.169: INFO: Wrong image for pod: daemon-set-twmd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:39.169: INFO: Wrong image for pod: daemon-set-9g44w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:39.169: INFO: Wrong image for pod: daemon-set-twmd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:40.168: INFO: Wrong image for pod: daemon-set-9g44w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:40.168: INFO: Wrong image for pod: daemon-set-twmd7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:40.168: INFO: Pod daemon-set-tzhnv is not available
  May 22 06:05:41.169: INFO: Wrong image for pod: daemon-set-9g44w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:42.169: INFO: Wrong image for pod: daemon-set-9g44w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 22 06:05:42.169: INFO: Pod daemon-set-rjds6 is not available
  May 22 06:05:44.169: INFO: Pod daemon-set-m6zxl is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/22/23 06:05:44.172
  May 22 06:05:44.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 22 06:05:44.177: INFO: Node node2 is running 0 daemon pod, expected 1
  May 22 06:05:45.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 22 06:05:45.183: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/22/23 06:05:45.192
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1744, will wait for the garbage collector to delete the pods @ 05/22/23 06:05:45.192
  May 22 06:05:45.248: INFO: Deleting DaemonSet.extensions daemon-set took: 3.713295ms
  May 22 06:05:45.349: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.013197ms
  May 22 06:05:47.352: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:05:47.352: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 22 06:05:47.354: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"889139"},"items":null}

  May 22 06:05:47.356: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"889139"},"items":null}

  May 22 06:05:47.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1744" for this suite. @ 05/22/23 06:05:47.367
• [12.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 05/22/23 06:05:47.37
  May 22 06:05:47.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replicaset @ 05/22/23 06:05:47.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:05:47.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:05:47.38
  May 22 06:05:47.382: INFO: Creating ReplicaSet my-hostname-basic-0e9e1698-c74d-4a68-b9be-f0221e33d228
  May 22 06:05:47.388: INFO: Pod name my-hostname-basic-0e9e1698-c74d-4a68-b9be-f0221e33d228: Found 0 pods out of 1
  May 22 06:05:52.390: INFO: Pod name my-hostname-basic-0e9e1698-c74d-4a68-b9be-f0221e33d228: Found 1 pods out of 1
  May 22 06:05:52.390: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0e9e1698-c74d-4a68-b9be-f0221e33d228" is running
  May 22 06:05:52.392: INFO: Pod "my-hostname-basic-0e9e1698-c74d-4a68-b9be-f0221e33d228-vnftb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-22 06:05:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-22 06:05:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-22 06:05:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-22 06:05:47 +0000 UTC Reason: Message:}])
  May 22 06:05:52.392: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/22/23 06:05:52.392
  May 22 06:05:52.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8044" for this suite. @ 05/22/23 06:05:52.4
• [5.033 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 05/22/23 06:05:52.404
  May 22 06:05:52.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:05:52.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:05:52.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:05:52.413
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:05:52.414
  STEP: Saw pod success @ 05/22/23 06:05:56.427
  May 22 06:05:56.428: INFO: Trying to get logs from node node2 pod downwardapi-volume-019bece0-affd-4f62-ae1b-4b621731684e container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:05:56.433
  May 22 06:05:56.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-434" for this suite. @ 05/22/23 06:05:56.443
• [4.042 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 05/22/23 06:05:56.446
  May 22 06:05:56.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:05:56.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:05:56.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:05:56.455
  STEP: creating secret secrets-3659/secret-test-b13ed4fd-de2b-4646-8d14-32ad7ed687d5 @ 05/22/23 06:05:56.457
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:05:56.459
  STEP: Saw pod success @ 05/22/23 06:05:58.469
  May 22 06:05:58.471: INFO: Trying to get logs from node node2 pod pod-configmaps-9ae68af2-732a-470b-a7fd-7ab9f24eb7b4 container env-test: <nil>
  STEP: delete the pod @ 05/22/23 06:05:58.476
  May 22 06:05:58.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3659" for this suite. @ 05/22/23 06:05:58.486
• [2.043 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/22/23 06:05:58.489
  May 22 06:05:58.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:05:58.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:05:58.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:05:58.5
  STEP: Creating secret with name secret-test-06be7ff8-913f-4bf4-8b19-4c52ef00ce8a @ 05/22/23 06:05:58.503
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:05:58.505
  STEP: Saw pod success @ 05/22/23 06:06:02.517
  May 22 06:06:02.519: INFO: Trying to get logs from node node2 pod pod-secrets-f0afe7c0-d486-4597-a2ff-5565c72840e3 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 06:06:02.523
  May 22 06:06:02.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1842" for this suite. @ 05/22/23 06:06:02.533
• [4.046 seconds]
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 05/22/23 06:06:02.536
  May 22 06:06:02.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename endpointslice @ 05/22/23 06:06:02.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:06:02.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:06:02.544
  May 22 06:06:04.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2669" for this suite. @ 05/22/23 06:06:04.583
• [2.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 05/22/23 06:06:04.587
  May 22 06:06:04.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename security-context @ 05/22/23 06:06:04.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:06:04.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:06:04.597
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/22/23 06:06:04.598
  STEP: Saw pod success @ 05/22/23 06:06:08.612
  May 22 06:06:08.614: INFO: Trying to get logs from node node2 pod security-context-163455b0-443f-4868-a29c-0e4a9c6f05bf container test-container: <nil>
  STEP: delete the pod @ 05/22/23 06:06:08.622
  May 22 06:06:08.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3557" for this suite. @ 05/22/23 06:06:08.633
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 05/22/23 06:06:08.637
  May 22 06:06:08.637: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:06:08.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:06:08.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:06:08.645
  STEP: creating service in namespace services-3300 @ 05/22/23 06:06:08.647
  STEP: creating service affinity-clusterip in namespace services-3300 @ 05/22/23 06:06:08.647
  STEP: creating replication controller affinity-clusterip in namespace services-3300 @ 05/22/23 06:06:08.653
  I0522 06:06:08.658229      25 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-3300, replica count: 3
  I0522 06:06:11.710401      25 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 22 06:06:11.714: INFO: Creating new exec pod
  May 22 06:06:14.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3300 exec execpod-affinityzx4dj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  May 22 06:06:14.837: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  May 22 06:06:14.837: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:06:14.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3300 exec execpod-affinityzx4dj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.17.136 80'
  May 22 06:06:14.948: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.17.136 80\nConnection to 10.101.17.136 80 port [tcp/http] succeeded!\n"
  May 22 06:06:14.948: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:06:14.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-3300 exec execpod-affinityzx4dj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.101.17.136:80/ ; done'
  May 22 06:06:15.120: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.101.17.136:80/\n"
  May 22 06:06:15.120: INFO: stdout: "\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn\naffinity-clusterip-d6dsn"
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Received response from host: affinity-clusterip-d6dsn
  May 22 06:06:15.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 06:06:15.124: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-3300, will wait for the garbage collector to delete the pods @ 05/22/23 06:06:15.13
  May 22 06:06:15.186: INFO: Deleting ReplicationController affinity-clusterip took: 3.610131ms
  May 22 06:06:15.287: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.128048ms
  STEP: Destroying namespace "services-3300" for this suite. @ 05/22/23 06:06:17.596
• [8.962 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 05/22/23 06:06:17.6
  May 22 06:06:17.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/22/23 06:06:17.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:06:17.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:06:17.609
  May 22 06:06:17.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/22/23 06:06:19.051
  May 22 06:06:19.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 create -f -'
  May 22 06:06:19.892: INFO: stderr: ""
  May 22 06:06:19.892: INFO: stdout: "e2e-test-crd-publish-openapi-9945-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 22 06:06:19.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 delete e2e-test-crd-publish-openapi-9945-crds test-cr'
  May 22 06:06:19.950: INFO: stderr: ""
  May 22 06:06:19.950: INFO: stdout: "e2e-test-crd-publish-openapi-9945-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  May 22 06:06:19.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 apply -f -'
  May 22 06:06:20.187: INFO: stderr: ""
  May 22 06:06:20.187: INFO: stdout: "e2e-test-crd-publish-openapi-9945-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 22 06:06:20.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 delete e2e-test-crd-publish-openapi-9945-crds test-cr'
  May 22 06:06:20.246: INFO: stderr: ""
  May 22 06:06:20.246: INFO: stdout: "e2e-test-crd-publish-openapi-9945-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/22/23 06:06:20.246
  May 22 06:06:20.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-5028 explain e2e-test-crd-publish-openapi-9945-crds'
  May 22 06:06:20.469: INFO: stderr: ""
  May 22 06:06:20.469: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-9945-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  May 22 06:06:21.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5028" for this suite. @ 05/22/23 06:06:21.888
• [4.291 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 05/22/23 06:06:21.892
  May 22 06:06:21.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 06:06:21.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:06:21.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:06:21.903
  STEP: Counting existing ResourceQuota @ 05/22/23 06:06:38.909
  STEP: Creating a ResourceQuota @ 05/22/23 06:06:43.911
  STEP: Ensuring resource quota status is calculated @ 05/22/23 06:06:43.915
  STEP: Creating a ConfigMap @ 05/22/23 06:06:45.92
  STEP: Ensuring resource quota status captures configMap creation @ 05/22/23 06:06:45.926
  STEP: Deleting a ConfigMap @ 05/22/23 06:06:47.929
  STEP: Ensuring resource quota status released usage @ 05/22/23 06:06:47.933
  May 22 06:06:49.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-441" for this suite. @ 05/22/23 06:06:49.94
• [28.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 05/22/23 06:06:49.945
  May 22 06:06:49.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename field-validation @ 05/22/23 06:06:49.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:06:49.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:06:49.957
  May 22 06:06:49.960: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  W0522 06:06:52.520746      25 warnings.go:70] unknown field "alpha"
  W0522 06:06:52.520768      25 warnings.go:70] unknown field "beta"
  W0522 06:06:52.520772      25 warnings.go:70] unknown field "delta"
  W0522 06:06:52.520775      25 warnings.go:70] unknown field "epsilon"
  W0522 06:06:52.520779      25 warnings.go:70] unknown field "gamma"
  May 22 06:06:52.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3559" for this suite. @ 05/22/23 06:06:52.542
• [2.600 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 05/22/23 06:06:52.545
  May 22 06:06:52.545: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 06:06:52.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:06:52.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:06:52.557
  STEP: Starting the proxy @ 05/22/23 06:06:52.561
  May 22 06:06:52.561: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-5188 proxy --unix-socket=/tmp/kubectl-proxy-unix946120219/test'
  STEP: retrieving proxy /api/ output @ 05/22/23 06:06:52.618
  May 22 06:06:52.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5188" for this suite. @ 05/22/23 06:06:52.622
• [0.081 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 05/22/23 06:06:52.627
  May 22 06:06:52.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename dns @ 05/22/23 06:06:52.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:06:52.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:06:52.636
  STEP: Creating a test headless service @ 05/22/23 06:06:52.638
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9201.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9201.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9201.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9201.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9201.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9201.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9201.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9201.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9201.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 70.56.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.56.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.56.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.56.70_tcp@PTR;sleep 1; done
   @ 05/22/23 06:06:52.65
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9201.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9201.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9201.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9201.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9201.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9201.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9201.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9201.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9201.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9201.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 70.56.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.56.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.56.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.56.70_tcp@PTR;sleep 1; done
   @ 05/22/23 06:06:52.65
  STEP: creating a pod to probe DNS @ 05/22/23 06:06:52.65
  STEP: submitting the pod to kubernetes @ 05/22/23 06:06:52.65
  STEP: retrieving the pod @ 05/22/23 06:06:54.663
  STEP: looking for the results for each expected name from probers @ 05/22/23 06:06:54.665
  May 22 06:06:54.667: INFO: Unable to read wheezy_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:54.669: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:54.671: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:54.672: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:54.680: INFO: Unable to read jessie_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:54.682: INFO: Unable to read jessie_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:54.684: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:54.685: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:54.692: INFO: Lookups using dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4 failed for: [wheezy_udp@dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_udp@dns-test-service.dns-9201.svc.cluster.local jessie_tcp@dns-test-service.dns-9201.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local]

  May 22 06:06:59.696: INFO: Unable to read wheezy_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:59.698: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:59.700: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:59.701: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:59.711: INFO: Unable to read jessie_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:59.713: INFO: Unable to read jessie_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:59.714: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:59.716: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:06:59.724: INFO: Lookups using dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4 failed for: [wheezy_udp@dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_udp@dns-test-service.dns-9201.svc.cluster.local jessie_tcp@dns-test-service.dns-9201.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local]

  May 22 06:07:04.695: INFO: Unable to read wheezy_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:04.697: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:04.699: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:04.701: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:04.711: INFO: Unable to read jessie_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:04.713: INFO: Unable to read jessie_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:04.715: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:04.717: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:04.724: INFO: Lookups using dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4 failed for: [wheezy_udp@dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_udp@dns-test-service.dns-9201.svc.cluster.local jessie_tcp@dns-test-service.dns-9201.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local]

  May 22 06:07:09.696: INFO: Unable to read wheezy_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:09.698: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:09.700: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:09.702: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:09.711: INFO: Unable to read jessie_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:09.712: INFO: Unable to read jessie_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:09.714: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:09.716: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:09.722: INFO: Lookups using dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4 failed for: [wheezy_udp@dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_udp@dns-test-service.dns-9201.svc.cluster.local jessie_tcp@dns-test-service.dns-9201.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local]

  May 22 06:07:14.696: INFO: Unable to read wheezy_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:14.699: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:14.701: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:14.704: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:14.715: INFO: Unable to read jessie_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:14.717: INFO: Unable to read jessie_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:14.720: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:14.721: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:14.727: INFO: Lookups using dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4 failed for: [wheezy_udp@dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_udp@dns-test-service.dns-9201.svc.cluster.local jessie_tcp@dns-test-service.dns-9201.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local]

  May 22 06:07:19.696: INFO: Unable to read wheezy_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:19.698: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:19.700: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:19.702: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:19.711: INFO: Unable to read jessie_udp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:19.713: INFO: Unable to read jessie_tcp@dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:19.715: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:19.717: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local from pod dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4: the server could not find the requested resource (get pods dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4)
  May 22 06:07:19.726: INFO: Lookups using dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4 failed for: [wheezy_udp@dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@dns-test-service.dns-9201.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_udp@dns-test-service.dns-9201.svc.cluster.local jessie_tcp@dns-test-service.dns-9201.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9201.svc.cluster.local]

  May 22 06:07:24.722: INFO: DNS probes using dns-9201/dns-test-346a6d25-97a0-4deb-854a-c8e74ec994b4 succeeded

  May 22 06:07:24.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 06:07:24.724
  STEP: deleting the test service @ 05/22/23 06:07:24.734
  STEP: deleting the test headless service @ 05/22/23 06:07:24.748
  STEP: Destroying namespace "dns-9201" for this suite. @ 05/22/23 06:07:24.753
• [32.130 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 05/22/23 06:07:24.756
  May 22 06:07:24.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:07:24.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:07:24.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:07:24.766
  STEP: creating a secret @ 05/22/23 06:07:24.768
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/22/23 06:07:24.771
  STEP: patching the secret @ 05/22/23 06:07:24.772
  STEP: deleting the secret using a LabelSelector @ 05/22/23 06:07:24.777
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/22/23 06:07:24.78
  May 22 06:07:24.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6800" for this suite. @ 05/22/23 06:07:24.784
• [0.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 05/22/23 06:07:24.787
  May 22 06:07:24.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename svcaccounts @ 05/22/23 06:07:24.788
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:07:24.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:07:24.795
  STEP: Creating a pod to test service account token:  @ 05/22/23 06:07:24.797
  STEP: Saw pod success @ 05/22/23 06:07:26.806
  May 22 06:07:26.807: INFO: Trying to get logs from node node2 pod test-pod-4a8eb713-02cf-455c-a0c9-700f7ad684bf container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:07:26.814
  May 22 06:07:26.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1478" for this suite. @ 05/22/23 06:07:26.823
• [2.040 seconds]
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 05/22/23 06:07:26.827
  May 22 06:07:26.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename disruption @ 05/22/23 06:07:26.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:07:26.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:07:26.835
  STEP: Creating a kubernetes client @ 05/22/23 06:07:26.837
  May 22 06:07:26.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename disruption-2 @ 05/22/23 06:07:26.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:07:26.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:07:26.846
  STEP: Waiting for the pdb to be processed @ 05/22/23 06:07:26.85
  STEP: Waiting for the pdb to be processed @ 05/22/23 06:07:28.858
  STEP: Waiting for the pdb to be processed @ 05/22/23 06:07:30.866
  STEP: listing a collection of PDBs across all namespaces @ 05/22/23 06:07:32.869
  STEP: listing a collection of PDBs in namespace disruption-9024 @ 05/22/23 06:07:32.871
  STEP: deleting a collection of PDBs @ 05/22/23 06:07:32.873
  STEP: Waiting for the PDB collection to be deleted @ 05/22/23 06:07:32.877
  May 22 06:07:32.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 06:07:32.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-7846" for this suite. @ 05/22/23 06:07:32.883
  STEP: Destroying namespace "disruption-9024" for this suite. @ 05/22/23 06:07:32.887
• [6.063 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 05/22/23 06:07:32.89
  May 22 06:07:32.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:07:32.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:07:32.897
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:07:32.899
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-175 @ 05/22/23 06:07:32.901
  STEP: changing the ExternalName service to type=NodePort @ 05/22/23 06:07:32.905
  STEP: creating replication controller externalname-service in namespace services-175 @ 05/22/23 06:07:32.916
  I0522 06:07:32.921109      25 runners.go:194] Created replication controller with name: externalname-service, namespace: services-175, replica count: 2
  I0522 06:07:35.972675      25 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 22 06:07:35.972: INFO: Creating new exec pod
  May 22 06:07:38.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-175 exec execpodrlvhq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 22 06:07:39.110: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 22 06:07:39.110: INFO: stdout: ""
  May 22 06:07:40.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-175 exec execpodrlvhq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 22 06:07:40.227: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 22 06:07:40.227: INFO: stdout: "externalname-service-s4qqx"
  May 22 06:07:40.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-175 exec execpodrlvhq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.233.41 80'
  May 22 06:07:40.340: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.233.41 80\nConnection to 10.103.233.41 80 port [tcp/http] succeeded!\n"
  May 22 06:07:40.340: INFO: stdout: "externalname-service-6lg6g"
  May 22 06:07:40.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-175 exec execpodrlvhq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.123 30253'
  May 22 06:07:40.470: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.123 30253\nConnection to 192.168.33.123 30253 port [tcp/*] succeeded!\n"
  May 22 06:07:40.470: INFO: stdout: "externalname-service-6lg6g"
  May 22 06:07:40.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-175 exec execpodrlvhq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.121 30253'
  May 22 06:07:40.599: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.121 30253\nConnection to 192.168.33.121 30253 port [tcp/*] succeeded!\n"
  May 22 06:07:40.599: INFO: stdout: "externalname-service-6lg6g"
  May 22 06:07:40.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 06:07:40.603: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-175" for this suite. @ 05/22/23 06:07:40.615
• [7.728 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 05/22/23 06:07:40.62
  May 22 06:07:40.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename limitrange @ 05/22/23 06:07:40.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:07:40.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:07:40.674
  STEP: Creating a LimitRange @ 05/22/23 06:07:40.676
  STEP: Setting up watch @ 05/22/23 06:07:40.676
  STEP: Submitting a LimitRange @ 05/22/23 06:07:40.779
  STEP: Verifying LimitRange creation was observed @ 05/22/23 06:07:40.783
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/22/23 06:07:40.783
  May 22 06:07:40.785: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 22 06:07:40.785: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/22/23 06:07:40.785
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/22/23 06:07:40.788
  May 22 06:07:40.790: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 22 06:07:40.790: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/22/23 06:07:40.79
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/22/23 06:07:40.793
  May 22 06:07:40.796: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  May 22 06:07:40.796: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/22/23 06:07:40.796
  STEP: Failing to create a Pod with more than max resources @ 05/22/23 06:07:40.8
  STEP: Updating a LimitRange @ 05/22/23 06:07:40.802
  STEP: Verifying LimitRange updating is effective @ 05/22/23 06:07:40.804
  STEP: Creating a Pod with less than former min resources @ 05/22/23 06:07:42.81
  STEP: Failing to create a Pod with more than max resources @ 05/22/23 06:07:42.814
  STEP: Deleting a LimitRange @ 05/22/23 06:07:42.815
  STEP: Verifying the LimitRange was deleted @ 05/22/23 06:07:42.819
  May 22 06:07:47.821: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/22/23 06:07:47.822
  May 22 06:07:47.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-4838" for this suite. @ 05/22/23 06:07:47.832
• [7.214 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/22/23 06:07:47.836
  May 22 06:07:47.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename hostport @ 05/22/23 06:07:47.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:07:47.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:07:47.845
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/22/23 06:07:47.849
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.33.123 on the node which pod1 resides and expect scheduled @ 05/22/23 06:07:49.858
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.33.123 but use UDP protocol on the node which pod2 resides @ 05/22/23 06:07:51.868
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/22/23 06:07:55.887
  May 22 06:07:55.887: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.33.123 http://127.0.0.1:54323/hostname] Namespace:hostport-9327 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:07:55.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:07:55.887: INFO: ExecWithOptions: Clientset creation
  May 22 06:07:55.887: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9327/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.33.123+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.33.123, port: 54323 @ 05/22/23 06:07:55.95
  May 22 06:07:55.950: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.33.123:54323/hostname] Namespace:hostport-9327 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:07:55.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:07:55.950: INFO: ExecWithOptions: Clientset creation
  May 22 06:07:55.950: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9327/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.33.123%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.33.123, port: 54323 UDP @ 05/22/23 06:07:55.991
  May 22 06:07:55.991: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.33.123 54323] Namespace:hostport-9327 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:07:55.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:07:55.992: INFO: ExecWithOptions: Clientset creation
  May 22 06:07:55.992: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-9327/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.33.123+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  May 22 06:08:01.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-9327" for this suite. @ 05/22/23 06:08:01.051
• [13.219 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/22/23 06:08:01.055
  May 22 06:08:01.055: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename deployment @ 05/22/23 06:08:01.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:08:01.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:08:01.064
  May 22 06:08:01.075: INFO: Pod name rollover-pod: Found 0 pods out of 1
  May 22 06:08:06.078: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/22/23 06:08:06.078
  May 22 06:08:06.078: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  May 22 06:08:08.081: INFO: Creating deployment "test-rollover-deployment"
  May 22 06:08:08.086: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  May 22 06:08:10.091: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  May 22 06:08:10.094: INFO: Ensure that both replica sets have 1 created replica
  May 22 06:08:10.097: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  May 22 06:08:10.102: INFO: Updating deployment test-rollover-deployment
  May 22 06:08:10.102: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  May 22 06:08:12.106: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  May 22 06:08:12.110: INFO: Make sure deployment "test-rollover-deployment" is complete
  May 22 06:08:12.114: INFO: all replica sets need to contain the pod-template-hash label
  May 22 06:08:12.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 6, 8, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 06:08:14.120: INFO: all replica sets need to contain the pod-template-hash label
  May 22 06:08:14.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 6, 8, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 06:08:16.120: INFO: all replica sets need to contain the pod-template-hash label
  May 22 06:08:16.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 6, 8, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 06:08:18.121: INFO: all replica sets need to contain the pod-template-hash label
  May 22 06:08:18.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 6, 8, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 06:08:20.120: INFO: all replica sets need to contain the pod-template-hash label
  May 22 06:08:20.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 22, 6, 8, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 22, 6, 8, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 22 06:08:22.120: INFO: 
  May 22 06:08:22.120: INFO: Ensure that both old replica sets have no replicas
  May 22 06:08:22.124: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3080  0b1b3cd4-87b0-4b40-b139-9eeacea86aa6 890545 2 2023-05-22 06:08:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-22 06:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:08:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00643e8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-22 06:08:08 +0000 UTC,LastTransitionTime:2023-05-22 06:08:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-05-22 06:08:21 +0000 UTC,LastTransitionTime:2023-05-22 06:08:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 22 06:08:22.126: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-3080  e72d6b18-1c5d-4b01-8761-d00bfa1acd87 890534 2 2023-05-22 06:08:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0b1b3cd4-87b0-4b40-b139-9eeacea86aa6 0xc006308b67 0xc006308b68}] [] [{kube-controller-manager Update apps/v1 2023-05-22 06:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b1b3cd4-87b0-4b40-b139-9eeacea86aa6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:08:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006308c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 22 06:08:22.126: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  May 22 06:08:22.127: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3080  bdd598e4-429f-4d49-810e-726ebd46f752 890544 2 2023-05-22 06:08:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0b1b3cd4-87b0-4b40-b139-9eeacea86aa6 0xc006308a37 0xc006308a38}] [] [{e2e.test Update apps/v1 2023-05-22 06:08:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:08:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b1b3cd4-87b0-4b40-b139-9eeacea86aa6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:08:21 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006308af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 22 06:08:22.127: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-3080  0e7b8cff-5ce4-4fd4-b9df-9e4f61480327 890477 2 2023-05-22 06:08:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0b1b3cd4-87b0-4b40-b139-9eeacea86aa6 0xc006308c87 0xc006308c88}] [] [{kube-controller-manager Update apps/v1 2023-05-22 06:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b1b3cd4-87b0-4b40-b139-9eeacea86aa6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:08:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006308d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 22 06:08:22.129: INFO: Pod "test-rollover-deployment-57777854c9-854nh" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-854nh test-rollover-deployment-57777854c9- deployment-3080  90956263-9f6e-4530-85a0-d391bf29ea7d 890496 0 2023-05-22 06:08:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[cni.projectcalico.org/containerID:b62149bff1ad3e6ad4907de253bb18d7056aca9be9735e95a4922128480d6d51 cni.projectcalico.org/podIP:192.168.104.7/32 cni.projectcalico.org/podIPs:192.168.104.7/32] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 e72d6b18-1c5d-4b01-8761-d00bfa1acd87 0xc0063092c7 0xc0063092c8}] [] [{calico Update v1 2023-05-22 06:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e72d6b18-1c5d-4b01-8761-d00bfa1acd87\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:08:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nrzrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nrzrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:08:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:08:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:08:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:08:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.7,StartTime:2023-05-22 06:08:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:08:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://6c24f8e4196c439ec7c4112ee673b92859702153986e400159a5887fc1362005,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.7,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:08:22.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3080" for this suite. @ 05/22/23 06:08:22.131
• [21.080 seconds]
------------------------------
S
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 05/22/23 06:08:22.135
  May 22 06:08:22.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:08:22.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:08:22.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:08:22.151
  STEP: creating service in namespace services-5437 @ 05/22/23 06:08:22.152
  STEP: creating service affinity-nodeport in namespace services-5437 @ 05/22/23 06:08:22.152
  STEP: creating replication controller affinity-nodeport in namespace services-5437 @ 05/22/23 06:08:22.161
  I0522 06:08:22.164577      25 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-5437, replica count: 3
  I0522 06:08:25.216200      25 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 22 06:08:25.222: INFO: Creating new exec pod
  May 22 06:08:28.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-5437 exec execpod-affinitycwvhn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  May 22 06:08:28.355: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  May 22 06:08:28.355: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:08:28.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-5437 exec execpod-affinitycwvhn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.38.246 80'
  May 22 06:08:28.485: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.38.246 80\nConnection to 10.102.38.246 80 port [tcp/http] succeeded!\n"
  May 22 06:08:28.485: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:08:28.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-5437 exec execpod-affinitycwvhn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.122 30763'
  May 22 06:08:28.601: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.122 30763\nConnection to 192.168.33.122 30763 port [tcp/*] succeeded!\n"
  May 22 06:08:28.601: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:08:28.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-5437 exec execpod-affinitycwvhn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.123 30763'
  May 22 06:08:28.737: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.123 30763\nConnection to 192.168.33.123 30763 port [tcp/*] succeeded!\n"
  May 22 06:08:28.737: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:08:28.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-5437 exec execpod-affinitycwvhn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.33.121:30763/ ; done'
  May 22 06:08:28.922: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30763/\n"
  May 22 06:08:28.922: INFO: stdout: "\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp\naffinity-nodeport-77kdp"
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Received response from host: affinity-nodeport-77kdp
  May 22 06:08:28.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 06:08:28.925: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-5437, will wait for the garbage collector to delete the pods @ 05/22/23 06:08:28.932
  May 22 06:08:28.988: INFO: Deleting ReplicationController affinity-nodeport took: 3.580198ms
  May 22 06:08:29.088: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.585905ms
  STEP: Destroying namespace "services-5437" for this suite. @ 05/22/23 06:08:31.101
• [8.969 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 05/22/23 06:08:31.104
  May 22 06:08:31.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/22/23 06:08:31.105
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:08:31.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:08:31.116
  May 22 06:08:31.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/22/23 06:08:32.512
  May 22 06:08:32.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-1081 --namespace=crd-publish-openapi-1081 create -f -'
  May 22 06:08:33.362: INFO: stderr: ""
  May 22 06:08:33.362: INFO: stdout: "e2e-test-crd-publish-openapi-9772-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 22 06:08:33.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-1081 --namespace=crd-publish-openapi-1081 delete e2e-test-crd-publish-openapi-9772-crds test-cr'
  May 22 06:08:33.424: INFO: stderr: ""
  May 22 06:08:33.424: INFO: stdout: "e2e-test-crd-publish-openapi-9772-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  May 22 06:08:33.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-1081 --namespace=crd-publish-openapi-1081 apply -f -'
  May 22 06:08:33.654: INFO: stderr: ""
  May 22 06:08:33.654: INFO: stdout: "e2e-test-crd-publish-openapi-9772-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 22 06:08:33.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-1081 --namespace=crd-publish-openapi-1081 delete e2e-test-crd-publish-openapi-9772-crds test-cr'
  May 22 06:08:33.711: INFO: stderr: ""
  May 22 06:08:33.711: INFO: stdout: "e2e-test-crd-publish-openapi-9772-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/22/23 06:08:33.711
  May 22 06:08:33.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-1081 explain e2e-test-crd-publish-openapi-9772-crds'
  May 22 06:08:33.932: INFO: stderr: ""
  May 22 06:08:33.932: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-9772-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  May 22 06:08:35.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1081" for this suite. @ 05/22/23 06:08:35.332
• [4.231 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 05/22/23 06:08:35.335
  May 22 06:08:35.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sysctl @ 05/22/23 06:08:35.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:08:35.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:08:35.345
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/22/23 06:08:35.347
  May 22 06:08:35.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-445" for this suite. @ 05/22/23 06:08:35.353
• [0.020 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 05/22/23 06:08:35.356
  May 22 06:08:35.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/22/23 06:08:35.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:08:35.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:08:35.364
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/22/23 06:08:35.366
  May 22 06:08:35.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/22/23 06:08:41.796
  May 22 06:08:41.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:08:43.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:08:48.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2177" for this suite. @ 05/22/23 06:08:48.918
• [13.565 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 05/22/23 06:08:48.922
  May 22 06:08:48.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename svcaccounts @ 05/22/23 06:08:48.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:08:48.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:08:48.932
  May 22 06:08:48.936: INFO: Got root ca configmap in namespace "svcaccounts-6571"
  May 22 06:08:48.938: INFO: Deleted root ca configmap in namespace "svcaccounts-6571"
  STEP: waiting for a new root ca configmap created @ 05/22/23 06:08:49.439
  May 22 06:08:49.441: INFO: Recreated root ca configmap in namespace "svcaccounts-6571"
  May 22 06:08:49.445: INFO: Updated root ca configmap in namespace "svcaccounts-6571"
  STEP: waiting for the root ca configmap reconciled @ 05/22/23 06:08:49.946
  May 22 06:08:49.948: INFO: Reconciled root ca configmap in namespace "svcaccounts-6571"
  May 22 06:08:49.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6571" for this suite. @ 05/22/23 06:08:49.951
• [1.032 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 05/22/23 06:08:49.954
  May 22 06:08:49.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-probe @ 05/22/23 06:08:49.955
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:08:49.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:08:49.963
  STEP: Creating pod liveness-9e5339fc-7caa-4019-a9a0-071e50a1e19d in namespace container-probe-4516 @ 05/22/23 06:08:49.965
  May 22 06:08:51.974: INFO: Started pod liveness-9e5339fc-7caa-4019-a9a0-071e50a1e19d in namespace container-probe-4516
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/22/23 06:08:51.974
  May 22 06:08:51.976: INFO: Initial restart count of pod liveness-9e5339fc-7caa-4019-a9a0-071e50a1e19d is 0
  May 22 06:12:52.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 06:12:52.381
  STEP: Destroying namespace "container-probe-4516" for this suite. @ 05/22/23 06:12:52.389
• [242.438 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 05/22/23 06:12:52.392
  May 22 06:12:52.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:12:52.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:12:52.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:12:52.401
  STEP: Creating configMap that has name configmap-test-emptyKey-6492d491-9d0b-46eb-bf00-21cb0c881f64 @ 05/22/23 06:12:52.403
  May 22 06:12:52.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2296" for this suite. @ 05/22/23 06:12:52.407
• [0.018 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 05/22/23 06:12:52.411
  May 22 06:12:52.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename svcaccounts @ 05/22/23 06:12:52.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:12:52.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:12:52.422
  May 22 06:12:52.434: INFO: created pod pod-service-account-defaultsa
  May 22 06:12:52.435: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  May 22 06:12:52.440: INFO: created pod pod-service-account-mountsa
  May 22 06:12:52.440: INFO: pod pod-service-account-mountsa service account token volume mount: true
  May 22 06:12:52.443: INFO: created pod pod-service-account-nomountsa
  May 22 06:12:52.443: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  May 22 06:12:52.450: INFO: created pod pod-service-account-defaultsa-mountspec
  May 22 06:12:52.450: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  May 22 06:12:52.453: INFO: created pod pod-service-account-mountsa-mountspec
  May 22 06:12:52.453: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  May 22 06:12:52.459: INFO: created pod pod-service-account-nomountsa-mountspec
  May 22 06:12:52.459: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  May 22 06:12:52.463: INFO: created pod pod-service-account-defaultsa-nomountspec
  May 22 06:12:52.463: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  May 22 06:12:52.471: INFO: created pod pod-service-account-mountsa-nomountspec
  May 22 06:12:52.471: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  May 22 06:12:52.474: INFO: created pod pod-service-account-nomountsa-nomountspec
  May 22 06:12:52.474: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  May 22 06:12:52.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-551" for this suite. @ 05/22/23 06:12:52.478
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/22/23 06:12:52.484
  May 22 06:12:52.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 06:12:52.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:12:52.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:12:52.496
  STEP: creating a Pod with a static label @ 05/22/23 06:12:52.506
  STEP: watching for Pod to be ready @ 05/22/23 06:12:52.511
  May 22 06:12:52.513: INFO: observed Pod pod-test in namespace pods-1396 in phase Pending with labels: map[test-pod-static:true] & conditions []
  May 22 06:12:52.514: INFO: observed Pod pod-test in namespace pods-1396 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC  }]
  May 22 06:12:52.523: INFO: observed Pod pod-test in namespace pods-1396 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC  }]
  May 22 06:12:53.002: INFO: observed Pod pod-test in namespace pods-1396 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC  }]
  May 22 06:12:53.196: INFO: Found Pod pod-test in namespace pods-1396 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-22 06:12:52 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 05/22/23 06:12:53.198
  STEP: getting the Pod and ensuring that it's patched @ 05/22/23 06:12:53.204
  STEP: replacing the Pod's status Ready condition to False @ 05/22/23 06:12:53.206
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/22/23 06:12:53.214
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/22/23 06:12:53.215
  STEP: watching for the Pod to be deleted @ 05/22/23 06:12:53.219
  May 22 06:12:53.221: INFO: observed event type MODIFIED
  May 22 06:12:55.200: INFO: observed event type MODIFIED
  May 22 06:12:55.349: INFO: observed event type MODIFIED
  May 22 06:12:55.479: INFO: observed event type MODIFIED
  May 22 06:12:56.204: INFO: observed event type MODIFIED
  May 22 06:12:56.209: INFO: observed event type MODIFIED
  May 22 06:12:56.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1396" for this suite. @ 05/22/23 06:12:56.214
• [3.733 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 05/22/23 06:12:56.218
  May 22 06:12:56.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename gc @ 05/22/23 06:12:56.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:12:56.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:12:56.271
  STEP: create the rc @ 05/22/23 06:12:56.273
  W0522 06:12:56.276379      25 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/22/23 06:13:01.279
  STEP: wait for all pods to be garbage collected @ 05/22/23 06:13:01.282
  STEP: Gathering metrics @ 05/22/23 06:13:06.287
  May 22 06:13:06.358: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 22 06:13:06.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6686" for this suite. @ 05/22/23 06:13:06.362
• [10.147 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 05/22/23 06:13:06.369
  May 22 06:13:06.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename security-context-test @ 05/22/23 06:13:06.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:13:06.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:13:06.379
  May 22 06:13:12.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5832" for this suite. @ 05/22/23 06:13:12.411
• [6.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 05/22/23 06:13:12.414
  May 22 06:13:12.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:13:12.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:13:12.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:13:12.424
  STEP: Creating configMap with name configmap-test-upd-ef68c516-e94e-46aa-971f-1b5ae6876d0b @ 05/22/23 06:13:12.428
  STEP: Creating the pod @ 05/22/23 06:13:12.431
  STEP: Updating configmap configmap-test-upd-ef68c516-e94e-46aa-971f-1b5ae6876d0b @ 05/22/23 06:13:14.446
  STEP: waiting to observe update in volume @ 05/22/23 06:13:14.448
  May 22 06:14:42.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8962" for this suite. @ 05/22/23 06:14:42.729
• [90.319 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 05/22/23 06:14:42.734
  May 22 06:14:42.734: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:14:42.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:14:42.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:14:42.743
  STEP: Creating the pod @ 05/22/23 06:14:42.745
  May 22 06:14:45.268: INFO: Successfully updated pod "labelsupdate60043d44-3a60-44e3-8b2b-20bc035d7c84"
  May 22 06:14:49.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2124" for this suite. @ 05/22/23 06:14:49.286
• [6.557 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 05/22/23 06:14:49.291
  May 22 06:14:49.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename namespaces @ 05/22/23 06:14:49.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:14:49.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:14:49.304
  STEP: creating a Namespace @ 05/22/23 06:14:49.306
  STEP: patching the Namespace @ 05/22/23 06:14:49.314
  STEP: get the Namespace and ensuring it has the label @ 05/22/23 06:14:49.318
  May 22 06:14:49.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4364" for this suite. @ 05/22/23 06:14:49.322
  STEP: Destroying namespace "nspatchtest-d1ce1f7c-49ab-4ef5-b6f2-887e993edb6f-3200" for this suite. @ 05/22/23 06:14:49.325
• [0.037 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 05/22/23 06:14:49.328
  May 22 06:14:49.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 06:14:49.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:14:49.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:14:49.337
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/22/23 06:14:49.339
  STEP: Saw pod success @ 05/22/23 06:14:53.352
  May 22 06:14:53.354: INFO: Trying to get logs from node node2 pod pod-bd4e929b-f888-42ef-ac3a-49fae9651878 container test-container: <nil>
  STEP: delete the pod @ 05/22/23 06:14:53.359
  May 22 06:14:53.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1544" for this suite. @ 05/22/23 06:14:53.369
• [4.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 05/22/23 06:14:53.373
  May 22 06:14:53.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename field-validation @ 05/22/23 06:14:53.373
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:14:53.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:14:53.383
  STEP: apply creating a deployment @ 05/22/23 06:14:53.386
  May 22 06:14:53.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3844" for this suite. @ 05/22/23 06:14:53.395
• [0.026 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 05/22/23 06:14:53.399
  May 22 06:14:53.399: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/22/23 06:14:53.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:14:53.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:14:53.408
  May 22 06:14:53.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:14:54.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6251" for this suite. @ 05/22/23 06:14:54.426
• [1.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 05/22/23 06:14:54.431
  May 22 06:14:54.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:14:54.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:14:54.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:14:54.44
  STEP: Creating configMap with name projected-configmap-test-volume-a38f92b8-123b-4d0f-bbb7-4ceae891fd1c @ 05/22/23 06:14:54.442
  STEP: Creating a pod to test consume configMaps @ 05/22/23 06:14:54.445
  STEP: Saw pod success @ 05/22/23 06:14:58.46
  May 22 06:14:58.462: INFO: Trying to get logs from node node3 pod pod-projected-configmaps-ea100227-8065-4bab-a3bd-526a68a5683b container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:14:58.475
  May 22 06:14:58.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9432" for this suite. @ 05/22/23 06:14:58.487
• [4.059 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 05/22/23 06:14:58.49
  May 22 06:14:58.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename init-container @ 05/22/23 06:14:58.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:14:58.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:14:58.499
  STEP: creating the pod @ 05/22/23 06:14:58.501
  May 22 06:14:58.501: INFO: PodSpec: initContainers in spec.initContainers
  May 22 06:15:01.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7022" for this suite. @ 05/22/23 06:15:01.498
• [3.011 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 05/22/23 06:15:01.502
  May 22 06:15:01.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:15:01.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:01.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:01.514
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-1030 @ 05/22/23 06:15:01.515
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/22/23 06:15:01.526
  STEP: creating service externalsvc in namespace services-1030 @ 05/22/23 06:15:01.526
  STEP: creating replication controller externalsvc in namespace services-1030 @ 05/22/23 06:15:01.533
  I0522 06:15:01.537175      25 runners.go:194] Created replication controller with name: externalsvc, namespace: services-1030, replica count: 2
  I0522 06:15:04.588458      25 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/22/23 06:15:04.59
  May 22 06:15:04.602: INFO: Creating new exec pod
  May 22 06:15:06.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-1030 exec execpodf89l9 -- /bin/sh -x -c nslookup nodeport-service.services-1030.svc.cluster.local'
  May 22 06:15:06.752: INFO: stderr: "+ nslookup nodeport-service.services-1030.svc.cluster.local\n"
  May 22 06:15:06.752: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-1030.svc.cluster.local\tcanonical name = externalsvc.services-1030.svc.cluster.local.\nName:\texternalsvc.services-1030.svc.cluster.local\nAddress: 10.111.143.70\n\n"
  May 22 06:15:06.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-1030, will wait for the garbage collector to delete the pods @ 05/22/23 06:15:06.756
  May 22 06:15:06.811: INFO: Deleting ReplicationController externalsvc took: 2.411303ms
  May 22 06:15:06.911: INFO: Terminating ReplicationController externalsvc pods took: 100.249841ms
  May 22 06:15:08.820: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-1030" for this suite. @ 05/22/23 06:15:08.825
• [7.326 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 05/22/23 06:15:08.828
  May 22 06:15:08.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replication-controller @ 05/22/23 06:15:08.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:08.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:08.838
  STEP: Creating ReplicationController "e2e-rc-fh8fq" @ 05/22/23 06:15:08.839
  May 22 06:15:08.842: INFO: Get Replication Controller "e2e-rc-fh8fq" to confirm replicas
  May 22 06:15:09.844: INFO: Get Replication Controller "e2e-rc-fh8fq" to confirm replicas
  May 22 06:15:09.847: INFO: Found 1 replicas for "e2e-rc-fh8fq" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-fh8fq" @ 05/22/23 06:15:09.847
  STEP: Updating a scale subresource @ 05/22/23 06:15:09.849
  STEP: Verifying replicas where modified for replication controller "e2e-rc-fh8fq" @ 05/22/23 06:15:09.852
  May 22 06:15:09.852: INFO: Get Replication Controller "e2e-rc-fh8fq" to confirm replicas
  May 22 06:15:10.854: INFO: Get Replication Controller "e2e-rc-fh8fq" to confirm replicas
  May 22 06:15:10.856: INFO: Found 2 replicas for "e2e-rc-fh8fq" replication controller
  May 22 06:15:10.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7968" for this suite. @ 05/22/23 06:15:10.86
• [2.034 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/22/23 06:15:10.863
  May 22 06:15:10.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename subjectreview @ 05/22/23 06:15:10.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:10.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:10.872
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-924" @ 05/22/23 06:15:10.874
  May 22 06:15:10.876: INFO: saUsername: "system:serviceaccount:subjectreview-924:e2e"
  May 22 06:15:10.876: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-924"}
  May 22 06:15:10.876: INFO: saUID: "50b5fdb4-4d19-4e88-84f4-bb07d9833e5a"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-924:e2e" @ 05/22/23 06:15:10.876
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-924:e2e" @ 05/22/23 06:15:10.876
  May 22 06:15:10.877: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-924:e2e" api 'list' configmaps in "subjectreview-924" namespace @ 05/22/23 06:15:10.877
  May 22 06:15:10.878: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-924:e2e" @ 05/22/23 06:15:10.878
  May 22 06:15:10.880: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  May 22 06:15:10.880: INFO: LocalSubjectAccessReview has been verified
  May 22 06:15:10.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-924" for this suite. @ 05/22/23 06:15:10.882
• [0.023 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 05/22/23 06:15:10.886
  May 22 06:15:10.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename dns @ 05/22/23 06:15:10.887
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:10.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:10.896
  STEP: Creating a test headless service @ 05/22/23 06:15:10.898
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9357.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9357.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local;sleep 1; done
   @ 05/22/23 06:15:10.901
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9357.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9357.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local;sleep 1; done
   @ 05/22/23 06:15:10.901
  STEP: creating a pod to probe DNS @ 05/22/23 06:15:10.901
  STEP: submitting the pod to kubernetes @ 05/22/23 06:15:10.901
  STEP: retrieving the pod @ 05/22/23 06:15:12.912
  STEP: looking for the results for each expected name from probers @ 05/22/23 06:15:12.913
  May 22 06:15:12.920: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:12.921: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:12.923: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:12.925: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:12.927: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:12.929: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:12.929: INFO: Lookups using dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a failed for: [wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9357.svc.cluster.local jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local]

  May 22 06:15:17.938: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:17.940: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:17.945: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:17.947: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:17.947: INFO: Lookups using dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a failed for: [wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local]

  May 22 06:15:22.937: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:22.938: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:22.945: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:22.947: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:22.947: INFO: Lookups using dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a failed for: [wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local]

  May 22 06:15:27.939: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:27.941: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:27.946: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:27.948: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:27.948: INFO: Lookups using dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a failed for: [wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local]

  May 22 06:15:32.937: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:32.939: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:32.945: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:32.947: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:32.947: INFO: Lookups using dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a failed for: [wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local]

  May 22 06:15:37.941: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:37.943: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:37.951: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:37.953: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local from pod dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a: the server could not find the requested resource (get pods dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a)
  May 22 06:15:37.953: INFO: Lookups using dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a failed for: [wheezy_udp@dns-test-service-2.dns-9357.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9357.svc.cluster.local jessie_udp@dns-test-service-2.dns-9357.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9357.svc.cluster.local]

  May 22 06:15:42.946: INFO: DNS probes using dns-9357/dns-test-396cd0bf-e308-4689-a5de-a0a3a9becf9a succeeded

  May 22 06:15:42.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 06:15:42.949
  STEP: deleting the test headless service @ 05/22/23 06:15:42.956
  STEP: Destroying namespace "dns-9357" for this suite. @ 05/22/23 06:15:42.962
• [32.083 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 05/22/23 06:15:42.969
  May 22 06:15:42.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename var-expansion @ 05/22/23 06:15:42.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:42.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:42.98
  May 22 06:15:44.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 06:15:44.997: INFO: Deleting pod "var-expansion-2aa73e53-d007-4e44-816e-90d3f783f11b" in namespace "var-expansion-1441"
  May 22 06:15:45.001: INFO: Wait up to 5m0s for pod "var-expansion-2aa73e53-d007-4e44-816e-90d3f783f11b" to be fully deleted
  STEP: Destroying namespace "var-expansion-1441" for this suite. @ 05/22/23 06:15:47.006
• [4.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 05/22/23 06:15:47.012
  May 22 06:15:47.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:15:47.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:47.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:47.022
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:15:47.024
  STEP: Saw pod success @ 05/22/23 06:15:51.038
  May 22 06:15:51.040: INFO: Trying to get logs from node node2 pod downwardapi-volume-a45387a6-e098-4454-87ae-1189dfc3249a container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:15:51.044
  May 22 06:15:51.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1847" for this suite. @ 05/22/23 06:15:51.054
• [4.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 05/22/23 06:15:51.057
  May 22 06:15:51.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:15:51.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:51.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:51.067
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:15:51.069
  STEP: Saw pod success @ 05/22/23 06:15:55.08
  May 22 06:15:55.081: INFO: Trying to get logs from node node2 pod downwardapi-volume-f0ce41ba-779e-40c2-8aae-1d654959a717 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:15:55.086
  May 22 06:15:55.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4974" for this suite. @ 05/22/23 06:15:55.096
• [4.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 05/22/23 06:15:55.101
  May 22 06:15:55.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename runtimeclass @ 05/22/23 06:15:55.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:55.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:55.113
  STEP: getting /apis @ 05/22/23 06:15:55.114
  STEP: getting /apis/node.k8s.io @ 05/22/23 06:15:55.118
  STEP: getting /apis/node.k8s.io/v1 @ 05/22/23 06:15:55.118
  STEP: creating @ 05/22/23 06:15:55.119
  STEP: watching @ 05/22/23 06:15:55.126
  May 22 06:15:55.126: INFO: starting watch
  STEP: getting @ 05/22/23 06:15:55.13
  STEP: listing @ 05/22/23 06:15:55.131
  STEP: patching @ 05/22/23 06:15:55.132
  STEP: updating @ 05/22/23 06:15:55.135
  May 22 06:15:55.137: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 05/22/23 06:15:55.137
  STEP: deleting a collection @ 05/22/23 06:15:55.142
  May 22 06:15:55.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8253" for this suite. @ 05/22/23 06:15:55.151
• [0.053 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/22/23 06:15:55.154
  May 22 06:15:55.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sched-pred @ 05/22/23 06:15:55.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:55.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:55.163
  May 22 06:15:55.165: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 22 06:15:55.169: INFO: Waiting for terminating namespaces to be deleted...
  May 22 06:15:55.171: INFO: 
  Logging pods the apiserver thinks is on node node1 before test
  May 22 06:15:55.176: INFO: calico-apiserver-666fc8f69-6l8kk from calico-apiserver started at 2023-05-19 09:15:01 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 22 06:15:55.176: INFO: calico-node-k76wg from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container calico-node ready: true, restart count 0
  May 22 06:15:55.176: INFO: csi-node-driver-vg8gt from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 06:15:55.176: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 06:15:55.176: INFO: csi-rbdplugin-hjnwl from default started at 2023-05-19 12:08:20 +0000 UTC (3 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:15:55.176: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 06:15:55.176: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:15:55.176: INFO: csi-rbdplugin-provisioner-6d5864d5f5-2scr8 from default started at 2023-05-19 12:08:07 +0000 UTC (7 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container csi-attacher ready: true, restart count 0
  May 22 06:15:55.176: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 22 06:15:55.176: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:15:55.176: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 22 06:15:55.176: INFO: 	Container csi-resizer ready: true, restart count 0
  May 22 06:15:55.176: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 06:15:55.176: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:15:55.176: INFO: coredns-5d78c9869d-nckgt from kube-system started at 2023-05-17 07:47:08 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container coredns ready: true, restart count 0
  May 22 06:15:55.176: INFO: coredns-5d78c9869d-wrhg2 from kube-system started at 2023-05-17 07:47:08 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container coredns ready: true, restart count 0
  May 22 06:15:55.176: INFO: etcd-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container etcd ready: true, restart count 0
  May 22 06:15:55.176: INFO: kube-apiserver-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container kube-apiserver ready: true, restart count 0
  May 22 06:15:55.176: INFO: kube-controller-manager-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container kube-controller-manager ready: true, restart count 1
  May 22 06:15:55.176: INFO: kube-proxy-qx9ss from kube-system started at 2023-05-17 07:19:40 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 06:15:55.176: INFO: kube-scheduler-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container kube-scheduler ready: true, restart count 1
  May 22 06:15:55.176: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-bdnbl from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 06:15:55.176: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 06:15:55.176: INFO: 	Container systemd-logs ready: true, restart count 0
  May 22 06:15:55.176: INFO: 
  Logging pods the apiserver thinks is on node node2 before test
  May 22 06:15:55.181: INFO: calico-node-s8lxt from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.181: INFO: 	Container calico-node ready: true, restart count 0
  May 22 06:15:55.181: INFO: calico-typha-cf4d768f9-99xbv from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.181: INFO: 	Container calico-typha ready: true, restart count 0
  May 22 06:15:55.181: INFO: csi-node-driver-ffpxl from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 06:15:55.181: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 06:15:55.181: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 06:15:55.181: INFO: csi-rbdplugin-provisioner-6d5864d5f5-8kmgv from default started at 2023-05-22 05:47:24 +0000 UTC (7 container statuses recorded)
  May 22 06:15:55.181: INFO: 	Container csi-attacher ready: true, restart count 0
  May 22 06:15:55.181: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 22 06:15:55.181: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:15:55.181: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 22 06:15:55.181: INFO: 	Container csi-resizer ready: true, restart count 0
  May 22 06:15:55.181: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 06:15:55.181: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:15:55.181: INFO: csi-rbdplugin-sv5xd from default started at 2023-05-22 05:47:23 +0000 UTC (3 container statuses recorded)
  May 22 06:15:55.181: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:15:55.181: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 06:15:55.181: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:15:55.181: INFO: kube-proxy-k5zhn from kube-system started at 2023-05-17 07:50:57 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.181: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 06:15:55.181: INFO: sonobuoy from sonobuoy started at 2023-05-22 05:20:17 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.181: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 22 06:15:55.181: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-hm7kk from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 06:15:55.181: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 06:15:55.181: INFO: 	Container systemd-logs ready: true, restart count 0
  May 22 06:15:55.181: INFO: tigera-operator-549d4f9bdb-g29xs from tigera-operator started at 2023-05-19 08:43:58 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.181: INFO: 	Container tigera-operator ready: true, restart count 2
  May 22 06:15:55.181: INFO: 
  Logging pods the apiserver thinks is on node node3 before test
  May 22 06:15:55.187: INFO: calico-apiserver-666fc8f69-48crp from calico-apiserver started at 2023-05-19 12:51:48 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.187: INFO: 	Container calico-apiserver ready: true, restart count 1
  May 22 06:15:55.187: INFO: calico-kube-controllers-789dc4c76b-qqbbj from calico-system started at 2023-05-19 12:51:48 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.187: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 22 06:15:55.187: INFO: calico-node-g6cnn from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.187: INFO: 	Container calico-node ready: true, restart count 0
  May 22 06:15:55.187: INFO: calico-typha-cf4d768f9-fgk49 from calico-system started at 2023-05-19 08:44:10 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.187: INFO: 	Container calico-typha ready: true, restart count 0
  May 22 06:15:55.187: INFO: csi-node-driver-jjbvg from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 06:15:55.187: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 06:15:55.187: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 06:15:55.187: INFO: csi-rbdplugin-nkgs7 from default started at 2023-05-19 12:08:20 +0000 UTC (3 container statuses recorded)
  May 22 06:15:55.187: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:15:55.188: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 06:15:55.188: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:15:55.188: INFO: csi-rbdplugin-provisioner-6d5864d5f5-cn9zx from default started at 2023-05-19 12:08:07 +0000 UTC (7 container statuses recorded)
  May 22 06:15:55.188: INFO: 	Container csi-attacher ready: true, restart count 2
  May 22 06:15:55.188: INFO: 	Container csi-provisioner ready: true, restart count 1
  May 22 06:15:55.188: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:15:55.188: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 1
  May 22 06:15:55.188: INFO: 	Container csi-resizer ready: true, restart count 2
  May 22 06:15:55.188: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 06:15:55.188: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:15:55.188: INFO: kube-proxy-kgvhd from kube-system started at 2023-05-17 07:51:09 +0000 UTC (1 container statuses recorded)
  May 22 06:15:55.188: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 06:15:55.188: INFO: sonobuoy-e2e-job-3f7387b6fe3c48b5 from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 06:15:55.188: INFO: 	Container e2e ready: true, restart count 0
  May 22 06:15:55.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 06:15:55.188: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-d6c6x from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 06:15:55.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 06:15:55.188: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/22/23 06:15:55.188
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/22/23 06:15:57.199
  STEP: Trying to apply a random label on the found node. @ 05/22/23 06:15:57.206
  STEP: verifying the node has the label kubernetes.io/e2e-f8dcef81-e95a-46d3-9217-85b2445c7587 42 @ 05/22/23 06:15:57.213
  STEP: Trying to relaunch the pod, now with labels. @ 05/22/23 06:15:57.216
  STEP: removing the label kubernetes.io/e2e-f8dcef81-e95a-46d3-9217-85b2445c7587 off the node node2 @ 05/22/23 06:15:59.226
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-f8dcef81-e95a-46d3-9217-85b2445c7587 @ 05/22/23 06:15:59.236
  May 22 06:15:59.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3037" for this suite. @ 05/22/23 06:15:59.243
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 05/22/23 06:15:59.247
  May 22 06:15:59.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 06:15:59.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:59.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:59.257
  STEP: Creating a ResourceQuota @ 05/22/23 06:15:59.259
  STEP: Getting a ResourceQuota @ 05/22/23 06:15:59.262
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/22/23 06:15:59.264
  STEP: Patching the ResourceQuota @ 05/22/23 06:15:59.266
  STEP: Deleting a Collection of ResourceQuotas @ 05/22/23 06:15:59.269
  STEP: Verifying the deleted ResourceQuota @ 05/22/23 06:15:59.274
  May 22 06:15:59.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6555" for this suite. @ 05/22/23 06:15:59.279
• [0.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 05/22/23 06:15:59.282
  May 22 06:15:59.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename runtimeclass @ 05/22/23 06:15:59.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:15:59.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:15:59.293
  May 22 06:16:01.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7578" for this suite. @ 05/22/23 06:16:01.31
• [2.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 05/22/23 06:16:01.313
  May 22 06:16:01.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:16:01.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:16:01.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:16:01.321
  STEP: Creating a pod to test downward api env vars @ 05/22/23 06:16:01.323
  STEP: Saw pod success @ 05/22/23 06:16:05.336
  May 22 06:16:05.338: INFO: Trying to get logs from node node3 pod downward-api-af60b63a-126c-44e2-83fe-220edba16942 container dapi-container: <nil>
  STEP: delete the pod @ 05/22/23 06:16:05.342
  May 22 06:16:05.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2646" for this suite. @ 05/22/23 06:16:05.353
• [4.042 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/22/23 06:16:05.357
  May 22 06:16:05.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename tables @ 05/22/23 06:16:05.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:16:05.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:16:05.37
  May 22 06:16:05.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-9267" for this suite. @ 05/22/23 06:16:05.376
• [0.022 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 05/22/23 06:16:05.381
  May 22 06:16:05.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-webhook @ 05/22/23 06:16:05.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:16:05.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:16:05.389
  STEP: Setting up server cert @ 05/22/23 06:16:05.391
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/22/23 06:16:05.819
  STEP: Deploying the custom resource conversion webhook pod @ 05/22/23 06:16:05.823
  STEP: Wait for the deployment to be ready @ 05/22/23 06:16:05.83
  May 22 06:16:05.833: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/22/23 06:16:07.841
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 06:16:07.848
  May 22 06:16:08.848: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 22 06:16:08.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Creating a v1 custom resource @ 05/22/23 06:16:11.397
  STEP: Create a v2 custom resource @ 05/22/23 06:16:11.408
  STEP: List CRs in v1 @ 05/22/23 06:16:11.431
  STEP: List CRs in v2 @ 05/22/23 06:16:11.434
  May 22 06:16:11.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-4431" for this suite. @ 05/22/23 06:16:11.969
• [6.594 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 05/22/23 06:16:11.976
  May 22 06:16:11.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pod-network-test @ 05/22/23 06:16:11.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:16:11.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:16:11.986
  STEP: Performing setup for networking test in namespace pod-network-test-5188 @ 05/22/23 06:16:11.989
  STEP: creating a selector @ 05/22/23 06:16:11.989
  STEP: Creating the service pods in kubernetes @ 05/22/23 06:16:11.989
  May 22 06:16:11.989: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/22/23 06:16:24.038
  May 22 06:16:26.053: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 22 06:16:26.053: INFO: Going to poll 192.168.166.135 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 22 06:16:26.054: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.166.135 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5188 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:16:26.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:16:26.055: INFO: ExecWithOptions: Clientset creation
  May 22 06:16:26.055: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5188/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.166.135+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 22 06:16:27.113: INFO: Found all 1 expected endpoints: [netserver-0]
  May 22 06:16:27.113: INFO: Going to poll 192.168.104.58 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 22 06:16:27.116: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.104.58 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5188 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:16:27.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:16:27.117: INFO: ExecWithOptions: Clientset creation
  May 22 06:16:27.117: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5188/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.104.58+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 22 06:16:28.174: INFO: Found all 1 expected endpoints: [netserver-1]
  May 22 06:16:28.174: INFO: Going to poll 192.168.135.32 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 22 06:16:28.176: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.135.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5188 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:16:28.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:16:28.177: INFO: ExecWithOptions: Clientset creation
  May 22 06:16:28.177: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5188/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.135.32+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 22 06:16:29.240: INFO: Found all 1 expected endpoints: [netserver-2]
  May 22 06:16:29.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5188" for this suite. @ 05/22/23 06:16:29.243
• [17.270 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 05/22/23 06:16:29.247
  May 22 06:16:29.247: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 06:16:29.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:16:29.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:16:29.257
  STEP: creating Agnhost RC @ 05/22/23 06:16:29.259
  May 22 06:16:29.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-7431 create -f -'
  May 22 06:16:30.300: INFO: stderr: ""
  May 22 06:16:30.300: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/22/23 06:16:30.3
  May 22 06:16:31.303: INFO: Selector matched 1 pods for map[app:agnhost]
  May 22 06:16:31.303: INFO: Found 0 / 1
  May 22 06:16:32.304: INFO: Selector matched 1 pods for map[app:agnhost]
  May 22 06:16:32.304: INFO: Found 1 / 1
  May 22 06:16:32.304: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/22/23 06:16:32.304
  May 22 06:16:32.306: INFO: Selector matched 1 pods for map[app:agnhost]
  May 22 06:16:32.306: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 22 06:16:32.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-7431 patch pod agnhost-primary-2j4g6 -p {"metadata":{"annotations":{"x":"y"}}}'
  May 22 06:16:32.369: INFO: stderr: ""
  May 22 06:16:32.369: INFO: stdout: "pod/agnhost-primary-2j4g6 patched\n"
  STEP: checking annotations @ 05/22/23 06:16:32.37
  May 22 06:16:32.372: INFO: Selector matched 1 pods for map[app:agnhost]
  May 22 06:16:32.372: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 22 06:16:32.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7431" for this suite. @ 05/22/23 06:16:32.375
• [3.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 05/22/23 06:16:32.379
  May 22 06:16:32.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename job @ 05/22/23 06:16:32.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:16:32.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:16:32.388
  STEP: Creating a job @ 05/22/23 06:16:32.39
  STEP: Ensuring active pods == parallelism @ 05/22/23 06:16:32.393
  STEP: Orphaning one of the Job's Pods @ 05/22/23 06:16:34.41
  May 22 06:16:34.922: INFO: Successfully updated pod "adopt-release-2t6w7"
  STEP: Checking that the Job readopts the Pod @ 05/22/23 06:16:34.922
  STEP: Removing the labels from the Job's Pod @ 05/22/23 06:16:36.926
  May 22 06:16:37.434: INFO: Successfully updated pod "adopt-release-2t6w7"
  STEP: Checking that the Job releases the Pod @ 05/22/23 06:16:37.434
  May 22 06:16:39.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6781" for this suite. @ 05/22/23 06:16:39.441
• [7.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 05/22/23 06:16:39.445
  May 22 06:16:39.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/22/23 06:16:39.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:16:39.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:16:39.455
  May 22 06:16:39.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/22/23 06:16:40.846
  May 22 06:16:40.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-6558 --namespace=crd-publish-openapi-6558 create -f -'
  May 22 06:16:41.737: INFO: stderr: ""
  May 22 06:16:41.737: INFO: stdout: "e2e-test-crd-publish-openapi-1213-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 22 06:16:41.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-6558 --namespace=crd-publish-openapi-6558 delete e2e-test-crd-publish-openapi-1213-crds test-cr'
  May 22 06:16:41.809: INFO: stderr: ""
  May 22 06:16:41.809: INFO: stdout: "e2e-test-crd-publish-openapi-1213-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  May 22 06:16:41.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-6558 --namespace=crd-publish-openapi-6558 apply -f -'
  May 22 06:16:42.050: INFO: stderr: ""
  May 22 06:16:42.050: INFO: stdout: "e2e-test-crd-publish-openapi-1213-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 22 06:16:42.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-6558 --namespace=crd-publish-openapi-6558 delete e2e-test-crd-publish-openapi-1213-crds test-cr'
  May 22 06:16:42.108: INFO: stderr: ""
  May 22 06:16:42.108: INFO: stdout: "e2e-test-crd-publish-openapi-1213-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/22/23 06:16:42.108
  May 22 06:16:42.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-6558 explain e2e-test-crd-publish-openapi-1213-crds'
  May 22 06:16:42.335: INFO: stderr: ""
  May 22 06:16:42.335: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-1213-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  May 22 06:16:43.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6558" for this suite. @ 05/22/23 06:16:43.742
• [4.300 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 05/22/23 06:16:43.746
  May 22 06:16:43.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename dns @ 05/22/23 06:16:43.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:16:43.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:16:43.757
  STEP: Creating a test headless service @ 05/22/23 06:16:43.759
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-611.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-611.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/22/23 06:16:43.761
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-611.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-611.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/22/23 06:16:43.761
  STEP: creating a pod to probe DNS @ 05/22/23 06:16:43.761
  STEP: submitting the pod to kubernetes @ 05/22/23 06:16:43.761
  STEP: retrieving the pod @ 05/22/23 06:16:45.771
  STEP: looking for the results for each expected name from probers @ 05/22/23 06:16:45.773
  May 22 06:16:45.782: INFO: DNS probes using dns-611/dns-test-96f5d4e0-d8da-41af-84a4-8278be129e8c succeeded

  May 22 06:16:45.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 06:16:45.785
  STEP: deleting the test headless service @ 05/22/23 06:16:45.792
  STEP: Destroying namespace "dns-611" for this suite. @ 05/22/23 06:16:45.797
• [2.055 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 05/22/23 06:16:45.801
  May 22 06:16:45.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename statefulset @ 05/22/23 06:16:45.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:16:45.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:16:45.811
  STEP: Creating service test in namespace statefulset-7400 @ 05/22/23 06:16:45.813
  STEP: Creating statefulset ss in namespace statefulset-7400 @ 05/22/23 06:16:45.818
  May 22 06:16:45.823: INFO: Found 0 stateful pods, waiting for 1
  May 22 06:16:55.827: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/22/23 06:16:55.831
  STEP: Getting /status @ 05/22/23 06:16:55.836
  May 22 06:16:55.838: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/22/23 06:16:55.838
  May 22 06:16:55.844: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/22/23 06:16:55.844
  May 22 06:16:55.845: INFO: Observed &StatefulSet event: ADDED
  May 22 06:16:55.845: INFO: Found Statefulset ss in namespace statefulset-7400 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 22 06:16:55.845: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/22/23 06:16:55.845
  May 22 06:16:55.846: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 22 06:16:55.850: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/22/23 06:16:55.85
  May 22 06:16:55.851: INFO: Observed &StatefulSet event: ADDED
  May 22 06:16:55.851: INFO: Observed Statefulset ss in namespace statefulset-7400 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 22 06:16:55.851: INFO: Observed &StatefulSet event: MODIFIED
  May 22 06:16:55.851: INFO: Deleting all statefulset in ns statefulset-7400
  May 22 06:16:55.853: INFO: Scaling statefulset ss to 0
  May 22 06:17:05.863: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 06:17:05.864: INFO: Deleting statefulset ss
  May 22 06:17:05.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7400" for this suite. @ 05/22/23 06:17:05.873
• [20.076 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 05/22/23 06:17:05.877
  May 22 06:17:05.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename field-validation @ 05/22/23 06:17:05.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:17:05.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:17:05.887
  May 22 06:17:05.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  W0522 06:17:05.890506      25 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc006027900 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0522 06:17:08.427981      25 warnings.go:70] unknown field "alpha"
  W0522 06:17:08.428021      25 warnings.go:70] unknown field "beta"
  W0522 06:17:08.428026      25 warnings.go:70] unknown field "delta"
  W0522 06:17:08.428031      25 warnings.go:70] unknown field "epsilon"
  W0522 06:17:08.428044      25 warnings.go:70] unknown field "gamma"
  May 22 06:17:08.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-169" for this suite. @ 05/22/23 06:17:08.45
• [2.578 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 05/22/23 06:17:08.457
  May 22 06:17:08.457: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename namespaces @ 05/22/23 06:17:08.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:17:08.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:17:08.469
  STEP: Creating a test namespace @ 05/22/23 06:17:08.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:17:08.479
  STEP: Creating a pod in the namespace @ 05/22/23 06:17:08.481
  STEP: Waiting for the pod to have running status @ 05/22/23 06:17:08.486
  STEP: Deleting the namespace @ 05/22/23 06:17:10.492
  STEP: Waiting for the namespace to be removed. @ 05/22/23 06:17:10.497
  STEP: Recreating the namespace @ 05/22/23 06:17:21.499
  STEP: Verifying there are no pods in the namespace @ 05/22/23 06:17:21.508
  May 22 06:17:21.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7979" for this suite. @ 05/22/23 06:17:21.512
  STEP: Destroying namespace "nsdeletetest-286" for this suite. @ 05/22/23 06:17:21.515
  May 22 06:17:21.516: INFO: Namespace nsdeletetest-286 was already deleted
  STEP: Destroying namespace "nsdeletetest-234" for this suite. @ 05/22/23 06:17:21.516
• [13.061 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/22/23 06:17:21.519
  May 22 06:17:21.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-watch @ 05/22/23 06:17:21.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:17:21.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:17:21.527
  May 22 06:17:21.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Creating first CR  @ 05/22/23 06:17:24.054
  May 22 06:17:24.057: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-22T06:17:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-22T06:17:24Z]] name:name1 resourceVersion:894151 uid:4eb4a3f5-6d66-406e-8dd9-075768aa8ef8] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 05/22/23 06:17:34.061
  May 22 06:17:34.065: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-22T06:17:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-22T06:17:34Z]] name:name2 resourceVersion:894193 uid:93aed936-cc40-4968-b3e9-fc9c97879dc9] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 05/22/23 06:17:44.067
  May 22 06:17:44.072: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-22T06:17:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-22T06:17:44Z]] name:name1 resourceVersion:894227 uid:4eb4a3f5-6d66-406e-8dd9-075768aa8ef8] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 05/22/23 06:17:54.072
  May 22 06:17:54.078: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-22T06:17:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-22T06:17:54Z]] name:name2 resourceVersion:894261 uid:93aed936-cc40-4968-b3e9-fc9c97879dc9] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 05/22/23 06:18:04.082
  May 22 06:18:04.087: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-22T06:17:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-22T06:17:44Z]] name:name1 resourceVersion:894295 uid:4eb4a3f5-6d66-406e-8dd9-075768aa8ef8] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 05/22/23 06:18:14.089
  May 22 06:18:14.094: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-22T06:17:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-22T06:17:54Z]] name:name2 resourceVersion:894329 uid:93aed936-cc40-4968-b3e9-fc9c97879dc9] num:map[num1:9223372036854775807 num2:1000000]]}
  May 22 06:18:24.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-4520" for this suite. @ 05/22/23 06:18:24.609
• [63.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 05/22/23 06:18:24.613
  May 22 06:18:24.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 06:18:24.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:18:24.621
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:18:24.623
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/22/23 06:18:24.625
  May 22 06:18:24.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2762 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 22 06:18:24.684: INFO: stderr: ""
  May 22 06:18:24.684: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/22/23 06:18:24.684
  May 22 06:18:24.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2762 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  May 22 06:18:24.743: INFO: stderr: ""
  May 22 06:18:24.743: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/22/23 06:18:24.743
  May 22 06:18:24.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-2762 delete pods e2e-test-httpd-pod'
  May 22 06:18:27.022: INFO: stderr: ""
  May 22 06:18:27.022: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 22 06:18:27.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2762" for this suite. @ 05/22/23 06:18:27.025
• [2.414 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 05/22/23 06:18:27.028
  May 22 06:18:27.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:18:27.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:18:27.035
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:18:27.037
  STEP: Creating configMap with name cm-test-opt-del-b3abf15e-03d1-440a-b5e8-5a98b4423a8d @ 05/22/23 06:18:27.041
  STEP: Creating configMap with name cm-test-opt-upd-0ddd577d-a7bb-4dff-94be-c3ed159b35a9 @ 05/22/23 06:18:27.043
  STEP: Creating the pod @ 05/22/23 06:18:27.045
  STEP: Deleting configmap cm-test-opt-del-b3abf15e-03d1-440a-b5e8-5a98b4423a8d @ 05/22/23 06:18:29.078
  STEP: Updating configmap cm-test-opt-upd-0ddd577d-a7bb-4dff-94be-c3ed159b35a9 @ 05/22/23 06:18:29.081
  STEP: Creating configMap with name cm-test-opt-create-0cfbb754-8bad-4e6b-918e-f8bf4269a4fd @ 05/22/23 06:18:29.085
  STEP: waiting to observe update in volume @ 05/22/23 06:18:29.087
  May 22 06:18:31.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7138" for this suite. @ 05/22/23 06:18:31.106
• [4.081 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/22/23 06:18:31.109
  May 22 06:18:31.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:18:31.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:18:31.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:18:31.118
  STEP: Creating secret with name secret-test-f7764d6f-0c58-4bba-a0fe-b3a8771cfbb3 @ 05/22/23 06:18:31.12
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:18:31.122
  STEP: Saw pod success @ 05/22/23 06:18:35.134
  May 22 06:18:35.136: INFO: Trying to get logs from node node3 pod pod-secrets-abd1194b-b3f6-42ce-9902-572a8e1b0e5a container secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 06:18:35.147
  May 22 06:18:35.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1988" for this suite. @ 05/22/23 06:18:35.158
• [4.052 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 05/22/23 06:18:35.161
  May 22 06:18:35.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename var-expansion @ 05/22/23 06:18:35.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:18:35.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:18:35.17
  STEP: Creating a pod to test substitution in volume subpath @ 05/22/23 06:18:35.172
  STEP: Saw pod success @ 05/22/23 06:18:39.184
  May 22 06:18:39.186: INFO: Trying to get logs from node node3 pod var-expansion-2c9dee9a-36cd-454e-ac90-edb11946a70e container dapi-container: <nil>
  STEP: delete the pod @ 05/22/23 06:18:39.189
  May 22 06:18:39.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6165" for this suite. @ 05/22/23 06:18:39.2
• [4.041 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/22/23 06:18:39.203
  May 22 06:18:39.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename proxy @ 05/22/23 06:18:39.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:18:39.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:18:39.212
  May 22 06:18:39.214: INFO: Creating pod...
  May 22 06:18:41.222: INFO: Creating service...
  May 22 06:18:41.231: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/pods/agnhost/proxy/some/path/with/DELETE
  May 22 06:18:41.233: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 22 06:18:41.233: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/pods/agnhost/proxy/some/path/with/GET
  May 22 06:18:41.235: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 22 06:18:41.235: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/pods/agnhost/proxy/some/path/with/HEAD
  May 22 06:18:41.237: INFO: http.Client request:HEAD | StatusCode:200
  May 22 06:18:41.237: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/pods/agnhost/proxy/some/path/with/OPTIONS
  May 22 06:18:41.238: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 22 06:18:41.238: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/pods/agnhost/proxy/some/path/with/PATCH
  May 22 06:18:41.240: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 22 06:18:41.240: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/pods/agnhost/proxy/some/path/with/POST
  May 22 06:18:41.242: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 22 06:18:41.242: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/pods/agnhost/proxy/some/path/with/PUT
  May 22 06:18:41.243: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 22 06:18:41.243: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/services/test-service/proxy/some/path/with/DELETE
  May 22 06:18:41.246: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 22 06:18:41.246: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/services/test-service/proxy/some/path/with/GET
  May 22 06:18:41.248: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 22 06:18:41.248: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/services/test-service/proxy/some/path/with/HEAD
  May 22 06:18:41.250: INFO: http.Client request:HEAD | StatusCode:200
  May 22 06:18:41.250: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/services/test-service/proxy/some/path/with/OPTIONS
  May 22 06:18:41.252: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 22 06:18:41.252: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/services/test-service/proxy/some/path/with/PATCH
  May 22 06:18:41.255: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 22 06:18:41.255: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/services/test-service/proxy/some/path/with/POST
  May 22 06:18:41.257: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 22 06:18:41.257: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9482/services/test-service/proxy/some/path/with/PUT
  May 22 06:18:41.259: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 22 06:18:41.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-9482" for this suite. @ 05/22/23 06:18:41.262
• [2.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 05/22/23 06:18:41.266
  May 22 06:18:41.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:18:41.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:18:41.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:18:41.277
  STEP: Creating configMap with name configmap-test-volume-map-7e19fc46-bb56-4d2e-8c19-c8c23e5abe3e @ 05/22/23 06:18:41.279
  STEP: Creating a pod to test consume configMaps @ 05/22/23 06:18:41.282
  STEP: Saw pod success @ 05/22/23 06:18:43.29
  May 22 06:18:43.293: INFO: Trying to get logs from node node2 pod pod-configmaps-64db48b4-4691-4c4e-b9f4-91696f8178c6 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:18:43.297
  May 22 06:18:43.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6714" for this suite. @ 05/22/23 06:18:43.307
• [2.045 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 05/22/23 06:18:43.311
  May 22 06:18:43.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:18:43.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:18:43.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:18:43.321
  STEP: creating service in namespace services-468 @ 05/22/23 06:18:43.323
  STEP: creating service affinity-nodeport-transition in namespace services-468 @ 05/22/23 06:18:43.323
  STEP: creating replication controller affinity-nodeport-transition in namespace services-468 @ 05/22/23 06:18:43.331
  I0522 06:18:43.334721      25 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-468, replica count: 3
  I0522 06:18:46.386847      25 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 22 06:18:46.393: INFO: Creating new exec pod
  May 22 06:18:49.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-468 exec execpod-affinityltl7q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  May 22 06:18:49.521: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  May 22 06:18:49.521: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:18:49.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-468 exec execpod-affinityltl7q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.247.133 80'
  May 22 06:18:49.641: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.247.133 80\nConnection to 10.96.247.133 80 port [tcp/http] succeeded!\n"
  May 22 06:18:49.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:18:49.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-468 exec execpod-affinityltl7q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.123 30713'
  May 22 06:18:49.764: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.123 30713\nConnection to 192.168.33.123 30713 port [tcp/*] succeeded!\n"
  May 22 06:18:49.764: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:18:49.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-468 exec execpod-affinityltl7q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.33.122 30713'
  May 22 06:18:49.880: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.33.122 30713\nConnection to 192.168.33.122 30713 port [tcp/*] succeeded!\n"
  May 22 06:18:49.880: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:18:49.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-468 exec execpod-affinityltl7q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.33.121:30713/ ; done'
  May 22 06:18:50.067: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n"
  May 22 06:18:50.067: INFO: stdout: "\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-5vcxm\naffinity-nodeport-transition-5vcxm\naffinity-nodeport-transition-jlpq8\naffinity-nodeport-transition-jlpq8\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-5vcxm\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-5vcxm\naffinity-nodeport-transition-jlpq8\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-5vcxm\naffinity-nodeport-transition-5vcxm\naffinity-nodeport-transition-jlpq8\naffinity-nodeport-transition-cr48j"
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-5vcxm
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-5vcxm
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-jlpq8
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-jlpq8
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-5vcxm
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-5vcxm
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-jlpq8
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-5vcxm
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-5vcxm
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-jlpq8
  May 22 06:18:50.067: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-468 exec execpod-affinityltl7q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.33.121:30713/ ; done'
  May 22 06:18:50.248: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.33.121:30713/\n"
  May 22 06:18:50.248: INFO: stdout: "\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j\naffinity-nodeport-transition-cr48j"
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Received response from host: affinity-nodeport-transition-cr48j
  May 22 06:18:50.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 06:18:50.251: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-468, will wait for the garbage collector to delete the pods @ 05/22/23 06:18:50.257
  May 22 06:18:50.313: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.215622ms
  May 22 06:18:50.413: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.085184ms
  STEP: Destroying namespace "services-468" for this suite. @ 05/22/23 06:18:52.425
• [9.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 05/22/23 06:18:52.429
  May 22 06:18:52.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 06:18:52.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:18:52.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:18:52.44
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/22/23 06:18:52.442
  STEP: Saw pod success @ 05/22/23 06:18:56.455
  May 22 06:18:56.457: INFO: Trying to get logs from node node2 pod pod-e4746c95-37fb-458b-bcab-edbb3eabd508 container test-container: <nil>
  STEP: delete the pod @ 05/22/23 06:18:56.46
  May 22 06:18:56.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6415" for this suite. @ 05/22/23 06:18:56.471
• [4.045 seconds]
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/22/23 06:18:56.474
  May 22 06:18:56.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:18:56.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:18:56.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:18:56.484
  May 22 06:18:56.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5164" for this suite. @ 05/22/23 06:18:56.508
• [0.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 05/22/23 06:18:56.515
  May 22 06:18:56.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 06:18:56.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:18:56.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:18:56.523
  STEP: Setting up server cert @ 05/22/23 06:18:56.533
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 06:18:57.043
  STEP: Deploying the webhook pod @ 05/22/23 06:18:57.048
  STEP: Wait for the deployment to be ready @ 05/22/23 06:18:57.054
  May 22 06:18:57.057: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/22/23 06:18:59.064
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 06:18:59.071
  May 22 06:19:00.072: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/22/23 06:19:00.074
  STEP: create a configmap that should be updated by the webhook @ 05/22/23 06:19:00.085
  May 22 06:19:00.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1900" for this suite. @ 05/22/23 06:19:00.117
  STEP: Destroying namespace "webhook-markers-1495" for this suite. @ 05/22/23 06:19:00.12
• [3.607 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 05/22/23 06:19:00.123
  May 22 06:19:00.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sched-preemption @ 05/22/23 06:19:00.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:19:00.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:19:00.133
  May 22 06:19:00.142: INFO: Waiting up to 1m0s for all nodes to be ready
  May 22 06:20:00.170: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/22/23 06:20:00.172
  May 22 06:20:00.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/22/23 06:20:00.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:00.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:00.182
  STEP: Finding an available node @ 05/22/23 06:20:00.184
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/22/23 06:20:00.184
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/22/23 06:20:02.196
  May 22 06:20:02.201: INFO: found a healthy node: node2
  May 22 06:20:08.241: INFO: pods created so far: [1 1 1]
  May 22 06:20:08.241: INFO: length of pods created so far: 3
  May 22 06:20:10.248: INFO: pods created so far: [2 2 1]
  May 22 06:20:17.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 06:20:17.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-8351" for this suite. @ 05/22/23 06:20:17.313
  STEP: Destroying namespace "sched-preemption-697" for this suite. @ 05/22/23 06:20:17.317
• [77.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 05/22/23 06:20:17.321
  May 22 06:20:17.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replication-controller @ 05/22/23 06:20:17.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:17.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:17.335
  STEP: creating a ReplicationController @ 05/22/23 06:20:17.339
  STEP: waiting for RC to be added @ 05/22/23 06:20:17.341
  STEP: waiting for available Replicas @ 05/22/23 06:20:17.341
  STEP: patching ReplicationController @ 05/22/23 06:20:18.417
  STEP: waiting for RC to be modified @ 05/22/23 06:20:18.421
  STEP: patching ReplicationController status @ 05/22/23 06:20:18.421
  STEP: waiting for RC to be modified @ 05/22/23 06:20:18.424
  STEP: waiting for available Replicas @ 05/22/23 06:20:18.424
  STEP: fetching ReplicationController status @ 05/22/23 06:20:18.428
  STEP: patching ReplicationController scale @ 05/22/23 06:20:18.43
  STEP: waiting for RC to be modified @ 05/22/23 06:20:18.432
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/22/23 06:20:18.432
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/22/23 06:20:19.311
  STEP: updating ReplicationController status @ 05/22/23 06:20:19.313
  STEP: waiting for RC to be modified @ 05/22/23 06:20:19.316
  STEP: listing all ReplicationControllers @ 05/22/23 06:20:19.316
  STEP: checking that ReplicationController has expected values @ 05/22/23 06:20:19.318
  STEP: deleting ReplicationControllers by collection @ 05/22/23 06:20:19.318
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/22/23 06:20:19.321
  May 22 06:20:19.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0522 06:20:19.355923      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-2463" for this suite. @ 05/22/23 06:20:19.358
• [2.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 05/22/23 06:20:19.361
  May 22 06:20:19.362: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/22/23 06:20:19.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:19.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:19.369
  STEP: create the container to handle the HTTPGet hook request. @ 05/22/23 06:20:19.373
  E0522 06:20:20.355988      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:21.356405      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/22/23 06:20:21.385
  E0522 06:20:22.356721      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:23.356950      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/22/23 06:20:23.396
  STEP: delete the pod with lifecycle hook @ 05/22/23 06:20:23.401
  E0522 06:20:24.357267      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:25.357557      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:20:25.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6700" for this suite. @ 05/22/23 06:20:25.413
• [6.055 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 05/22/23 06:20:25.417
  May 22 06:20:25.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename dns @ 05/22/23 06:20:25.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:25.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:25.428
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/22/23 06:20:25.43
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/22/23 06:20:25.43
  STEP: creating a pod to probe DNS @ 05/22/23 06:20:25.431
  STEP: submitting the pod to kubernetes @ 05/22/23 06:20:25.431
  E0522 06:20:26.358090      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:27.358209      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/22/23 06:20:27.441
  STEP: looking for the results for each expected name from probers @ 05/22/23 06:20:27.443
  May 22 06:20:27.451: INFO: DNS probes using dns-7527/dns-test-6faaa8dc-6e20-42d1-beae-d3b39715b04a succeeded

  May 22 06:20:27.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 06:20:27.453
  STEP: Destroying namespace "dns-7527" for this suite. @ 05/22/23 06:20:27.461
• [2.047 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 05/22/23 06:20:27.464
  May 22 06:20:27.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 06:20:27.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:27.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:27.472
  STEP: creating the pod @ 05/22/23 06:20:27.474
  May 22 06:20:27.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-5571 create -f -'
  E0522 06:20:28.358914      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:20:28.449: INFO: stderr: ""
  May 22 06:20:28.449: INFO: stdout: "pod/pause created\n"
  E0522 06:20:29.359897      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:30.360231      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/22/23 06:20:30.455
  May 22 06:20:30.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-5571 label pods pause testing-label=testing-label-value'
  May 22 06:20:30.520: INFO: stderr: ""
  May 22 06:20:30.520: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/22/23 06:20:30.52
  May 22 06:20:30.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-5571 get pod pause -L testing-label'
  May 22 06:20:30.579: INFO: stderr: ""
  May 22 06:20:30.579: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/22/23 06:20:30.579
  May 22 06:20:30.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-5571 label pods pause testing-label-'
  May 22 06:20:30.646: INFO: stderr: ""
  May 22 06:20:30.646: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/22/23 06:20:30.646
  May 22 06:20:30.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-5571 get pod pause -L testing-label'
  May 22 06:20:30.711: INFO: stderr: ""
  May 22 06:20:30.711: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 05/22/23 06:20:30.711
  May 22 06:20:30.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-5571 delete --grace-period=0 --force -f -'
  May 22 06:20:30.775: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 22 06:20:30.775: INFO: stdout: "pod \"pause\" force deleted\n"
  May 22 06:20:30.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-5571 get rc,svc -l name=pause --no-headers'
  May 22 06:20:30.845: INFO: stderr: "No resources found in kubectl-5571 namespace.\n"
  May 22 06:20:30.845: INFO: stdout: ""
  May 22 06:20:30.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-5571 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 22 06:20:30.901: INFO: stderr: ""
  May 22 06:20:30.901: INFO: stdout: ""
  May 22 06:20:30.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5571" for this suite. @ 05/22/23 06:20:30.904
• [3.444 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 05/22/23 06:20:30.908
  May 22 06:20:30.908: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename ingressclass @ 05/22/23 06:20:30.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:30.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:30.917
  STEP: getting /apis @ 05/22/23 06:20:30.919
  STEP: getting /apis/networking.k8s.io @ 05/22/23 06:20:30.923
  STEP: getting /apis/networking.k8s.iov1 @ 05/22/23 06:20:30.924
  STEP: creating @ 05/22/23 06:20:30.924
  STEP: getting @ 05/22/23 06:20:30.93
  STEP: listing @ 05/22/23 06:20:30.931
  STEP: watching @ 05/22/23 06:20:30.933
  May 22 06:20:30.933: INFO: starting watch
  STEP: patching @ 05/22/23 06:20:30.933
  STEP: updating @ 05/22/23 06:20:30.936
  May 22 06:20:30.938: INFO: waiting for watch events with expected annotations
  May 22 06:20:30.938: INFO: saw patched and updated annotations
  STEP: deleting @ 05/22/23 06:20:30.938
  STEP: deleting a collection @ 05/22/23 06:20:30.942
  May 22 06:20:30.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-7445" for this suite. @ 05/22/23 06:20:30.951
• [0.045 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 05/22/23 06:20:30.953
  May 22 06:20:30.953: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 06:20:30.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:30.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:30.963
  STEP: starting the proxy server @ 05/22/23 06:20:30.965
  May 22 06:20:30.965: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-7554 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 05/22/23 06:20:31.008
  May 22 06:20:31.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7554" for this suite. @ 05/22/23 06:20:31.018
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 05/22/23 06:20:31.023
  May 22 06:20:31.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 06:20:31.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:31.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:31.032
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/22/23 06:20:31.034
  E0522 06:20:31.360893      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:32.361208      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:33.361507      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:34.362025      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:20:35.047
  May 22 06:20:35.049: INFO: Trying to get logs from node node2 pod pod-076bcac1-854d-4061-b537-1f42acdcb2d6 container test-container: <nil>
  STEP: delete the pod @ 05/22/23 06:20:35.053
  May 22 06:20:35.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9083" for this suite. @ 05/22/23 06:20:35.064
• [4.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/22/23 06:20:35.068
  May 22 06:20:35.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 06:20:35.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:35.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:35.076
  STEP: creating the pod @ 05/22/23 06:20:35.081
  STEP: submitting the pod to kubernetes @ 05/22/23 06:20:35.081
  E0522 06:20:35.362829      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:36.363096      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 05/22/23 06:20:37.092
  STEP: updating the pod @ 05/22/23 06:20:37.094
  E0522 06:20:37.363964      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:20:37.602: INFO: Successfully updated pod "pod-update-a6be10fb-c2f1-43b4-803f-061305f37fbf"
  STEP: verifying the updated pod is in kubernetes @ 05/22/23 06:20:37.604
  May 22 06:20:37.605: INFO: Pod update OK
  May 22 06:20:37.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5783" for this suite. @ 05/22/23 06:20:37.608
• [2.543 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 05/22/23 06:20:37.617
  May 22 06:20:37.617: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 06:20:37.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:37.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:37.628
  STEP: Setting up server cert @ 05/22/23 06:20:37.639
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 06:20:38.048
  STEP: Deploying the webhook pod @ 05/22/23 06:20:38.053
  STEP: Wait for the deployment to be ready @ 05/22/23 06:20:38.06
  May 22 06:20:38.064: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0522 06:20:38.364089      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:39.365044      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/22/23 06:20:40.07
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 06:20:40.077
  E0522 06:20:40.365905      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:20:41.077: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/22/23 06:20:41.108
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/22/23 06:20:41.133
  STEP: Deleting the collection of validation webhooks @ 05/22/23 06:20:41.154
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/22/23 06:20:41.17
  May 22 06:20:41.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7456" for this suite. @ 05/22/23 06:20:41.2
  STEP: Destroying namespace "webhook-markers-1191" for this suite. @ 05/22/23 06:20:41.204
• [3.589 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/22/23 06:20:41.208
  May 22 06:20:41.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename watch @ 05/22/23 06:20:41.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:41.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:41.217
  STEP: creating a new configmap @ 05/22/23 06:20:41.219
  STEP: modifying the configmap once @ 05/22/23 06:20:41.221
  STEP: modifying the configmap a second time @ 05/22/23 06:20:41.225
  STEP: deleting the configmap @ 05/22/23 06:20:41.229
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/22/23 06:20:41.231
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/22/23 06:20:41.232
  May 22 06:20:41.232: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8928  05e2519c-348d-4784-b200-1449c4a64949 895880 0 2023-05-22 06:20:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-22 06:20:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:20:41.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8928  05e2519c-348d-4784-b200-1449c4a64949 895881 0 2023-05-22 06:20:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-22 06:20:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:20:41.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8928" for this suite. @ 05/22/23 06:20:41.235
• [0.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 05/22/23 06:20:41.239
  May 22 06:20:41.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pod-network-test @ 05/22/23 06:20:41.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:41.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:41.249
  STEP: Performing setup for networking test in namespace pod-network-test-7447 @ 05/22/23 06:20:41.251
  STEP: creating a selector @ 05/22/23 06:20:41.251
  STEP: Creating the service pods in kubernetes @ 05/22/23 06:20:41.251
  May 22 06:20:41.251: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0522 06:20:41.366446      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:42.366810      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:43.366854      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:44.367676      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:45.367949      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:46.368264      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:47.368576      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:48.368850      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:49.369845      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:50.370027      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:51.370931      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:52.371189      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/22/23 06:20:53.299
  E0522 06:20:53.371246      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:54.372235      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:20:55.315: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 22 06:20:55.315: INFO: Going to poll 192.168.166.136 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 22 06:20:55.316: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.166.136:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7447 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:20:55.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:20:55.317: INFO: ExecWithOptions: Clientset creation
  May 22 06:20:55.317: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7447/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.166.136%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0522 06:20:55.372343      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:20:55.378: INFO: Found all 1 expected endpoints: [netserver-0]
  May 22 06:20:55.378: INFO: Going to poll 192.168.104.12 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 22 06:20:55.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.104.12:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7447 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:20:55.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:20:55.380: INFO: ExecWithOptions: Clientset creation
  May 22 06:20:55.380: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7447/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.104.12%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 22 06:20:55.436: INFO: Found all 1 expected endpoints: [netserver-1]
  May 22 06:20:55.436: INFO: Going to poll 192.168.135.16 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 22 06:20:55.438: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.135.16:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7447 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:20:55.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:20:55.439: INFO: ExecWithOptions: Clientset creation
  May 22 06:20:55.439: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7447/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.135.16%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 22 06:20:55.523: INFO: Found all 1 expected endpoints: [netserver-2]
  May 22 06:20:55.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7447" for this suite. @ 05/22/23 06:20:55.528
• [14.293 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 05/22/23 06:20:55.533
  May 22 06:20:55.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename gc @ 05/22/23 06:20:55.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:20:55.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:20:55.544
  STEP: create the rc @ 05/22/23 06:20:55.549
  W0522 06:20:55.551886      25 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0522 06:20:56.372845      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:57.372968      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:58.373473      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:20:59.375527      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:00.376516      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:01.376932      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/22/23 06:21:01.554
  STEP: wait for the rc to be deleted @ 05/22/23 06:21:01.558
  E0522 06:21:02.377035      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:02.570: INFO: 80 pods remaining
  May 22 06:21:02.570: INFO: 80 pods has nil DeletionTimestamp
  May 22 06:21:02.570: INFO: 
  E0522 06:21:03.377153      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:03.575: INFO: 71 pods remaining
  May 22 06:21:03.575: INFO: 71 pods has nil DeletionTimestamp
  May 22 06:21:03.575: INFO: 
  E0522 06:21:04.377692      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:04.570: INFO: 60 pods remaining
  May 22 06:21:04.570: INFO: 59 pods has nil DeletionTimestamp
  May 22 06:21:04.570: INFO: 
  E0522 06:21:05.378714      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:05.569: INFO: 40 pods remaining
  May 22 06:21:05.569: INFO: 40 pods has nil DeletionTimestamp
  May 22 06:21:05.569: INFO: 
  E0522 06:21:06.378371      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:06.570: INFO: 31 pods remaining
  May 22 06:21:06.570: INFO: 31 pods has nil DeletionTimestamp
  May 22 06:21:06.570: INFO: 
  E0522 06:21:07.378591      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:07.565: INFO: 20 pods remaining
  May 22 06:21:07.565: INFO: 19 pods has nil DeletionTimestamp
  May 22 06:21:07.565: INFO: 
  E0522 06:21:08.378805      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/22/23 06:21:08.564
  May 22 06:21:08.682: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 22 06:21:08.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8744" for this suite. @ 05/22/23 06:21:08.686
• [13.156 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 05/22/23 06:21:08.69
  May 22 06:21:08.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:21:08.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:08.71
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:08.714
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:21:08.72
  E0522 06:21:09.379368      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:10.379508      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:11.379686      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:12.379785      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:21:12.735
  May 22 06:21:12.737: INFO: Trying to get logs from node node2 pod downwardapi-volume-5249e34a-043c-464c-ac57-ea8f62631182 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:21:12.741
  May 22 06:21:12.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9935" for this suite. @ 05/22/23 06:21:12.752
• [4.065 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/22/23 06:21:12.755
  May 22 06:21:12.755: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/22/23 06:21:12.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:12.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:12.765
  STEP: mirroring a new custom Endpoint @ 05/22/23 06:21:12.774
  May 22 06:21:12.779: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0522 06:21:13.379900      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:14.380385      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 05/22/23 06:21:14.782
  May 22 06:21:14.786: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0522 06:21:15.380871      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:16.381092      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 05/22/23 06:21:16.79
  May 22 06:21:16.794: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0522 06:21:17.381634      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:18.381901      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:18.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-6016" for this suite. @ 05/22/23 06:21:18.8
• [6.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 05/22/23 06:21:18.808
  May 22 06:21:18.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename daemonsets @ 05/22/23 06:21:18.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:18.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:18.818
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/22/23 06:21:18.829
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/22/23 06:21:18.831
  May 22 06:21:18.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:21:18.835: INFO: Node node1 is running 0 daemon pod, expected 1
  E0522 06:21:19.382416      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:19.841: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 22 06:21:19.841: INFO: Node node1 is running 0 daemon pod, expected 1
  E0522 06:21:20.382725      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:20.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 22 06:21:20.843: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/22/23 06:21:20.845
  May 22 06:21:20.858: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 22 06:21:20.858: INFO: Node node2 is running 0 daemon pod, expected 1
  E0522 06:21:21.382812      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:21.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 22 06:21:21.863: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/22/23 06:21:21.863
  STEP: Deleting DaemonSet "daemon-set" @ 05/22/23 06:21:21.867
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8808, will wait for the garbage collector to delete the pods @ 05/22/23 06:21:21.867
  May 22 06:21:21.922: INFO: Deleting DaemonSet.extensions daemon-set took: 3.588428ms
  May 22 06:21:22.022: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.657583ms
  E0522 06:21:22.383205      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:23.383281      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:24.384037      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:24.726: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:21:24.726: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 22 06:21:24.727: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"898713"},"items":null}

  May 22 06:21:24.729: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"898713"},"items":null}

  May 22 06:21:24.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8808" for this suite. @ 05/22/23 06:21:24.738
• [5.932 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 05/22/23 06:21:24.741
  May 22 06:21:24.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename lease-test @ 05/22/23 06:21:24.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:24.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:24.75
  May 22 06:21:24.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-8774" for this suite. @ 05/22/23 06:21:24.776
• [0.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/22/23 06:21:24.779
  May 22 06:21:24.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 06:21:24.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:24.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:24.789
  E0522 06:21:25.384450      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:26.384778      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:27.385866      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:28.386042      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:29.386613      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:30.386751      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:21:30.822
  May 22 06:21:30.824: INFO: Trying to get logs from node node2 pod client-envvars-78c63ed7-1eac-4fbd-b21a-2ffb28de685d container env3cont: <nil>
  STEP: delete the pod @ 05/22/23 06:21:30.829
  May 22 06:21:30.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6874" for this suite. @ 05/22/23 06:21:30.839
• [6.062 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 05/22/23 06:21:30.843
  May 22 06:21:30.843: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replicaset @ 05/22/23 06:21:30.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:30.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:30.856
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/22/23 06:21:30.859
  May 22 06:21:30.864: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0522 06:21:31.387683      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:32.387962      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:33.388801      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:34.389568      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:35.390100      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:35.868: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/22/23 06:21:35.868
  STEP: getting scale subresource @ 05/22/23 06:21:35.869
  STEP: updating a scale subresource @ 05/22/23 06:21:35.871
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/22/23 06:21:35.874
  STEP: Patch a scale subresource @ 05/22/23 06:21:35.875
  May 22 06:21:35.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4642" for this suite. @ 05/22/23 06:21:35.888
• [5.050 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 05/22/23 06:21:35.894
  May 22 06:21:35.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:21:35.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:35.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:35.905
  STEP: Creating configMap with name configmap-test-upd-bf87729e-f265-47c6-8df9-2c8ffe99580d @ 05/22/23 06:21:35.912
  STEP: Creating the pod @ 05/22/23 06:21:35.935
  E0522 06:21:36.391024      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:37.391933      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 05/22/23 06:21:37.944
  STEP: Waiting for pod with binary data @ 05/22/23 06:21:37.949
  May 22 06:21:37.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5982" for this suite. @ 05/22/23 06:21:37.957
• [2.067 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/22/23 06:21:37.961
  May 22 06:21:37.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 06:21:37.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:37.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:37.972
  May 22 06:21:37.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: creating the pod @ 05/22/23 06:21:37.975
  STEP: submitting the pod to kubernetes @ 05/22/23 06:21:37.975
  E0522 06:21:38.392570      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:39.392951      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:40.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9737" for this suite. @ 05/22/23 06:21:40.007
• [2.049 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 05/22/23 06:21:40.01
  May 22 06:21:40.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename limitrange @ 05/22/23 06:21:40.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:40.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:40.02
  STEP: Creating LimitRange "e2e-limitrange-l9gr9" in namespace "limitrange-3870" @ 05/22/23 06:21:40.022
  STEP: Creating another limitRange in another namespace @ 05/22/23 06:21:40.024
  May 22 06:21:40.031: INFO: Namespace "e2e-limitrange-l9gr9-3681" created
  May 22 06:21:40.031: INFO: Creating LimitRange "e2e-limitrange-l9gr9" in namespace "e2e-limitrange-l9gr9-3681"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-l9gr9" @ 05/22/23 06:21:40.033
  May 22 06:21:40.035: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-l9gr9" in "limitrange-3870" namespace @ 05/22/23 06:21:40.035
  May 22 06:21:40.037: INFO: LimitRange "e2e-limitrange-l9gr9" has been patched
  STEP: Delete LimitRange "e2e-limitrange-l9gr9" by Collection with labelSelector: "e2e-limitrange-l9gr9=patched" @ 05/22/23 06:21:40.037
  STEP: Confirm that the limitRange "e2e-limitrange-l9gr9" has been deleted @ 05/22/23 06:21:40.04
  May 22 06:21:40.040: INFO: Requesting list of LimitRange to confirm quantity
  May 22 06:21:40.041: INFO: Found 0 LimitRange with label "e2e-limitrange-l9gr9=patched"
  May 22 06:21:40.041: INFO: LimitRange "e2e-limitrange-l9gr9" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-l9gr9" @ 05/22/23 06:21:40.041
  May 22 06:21:40.043: INFO: Found 1 limitRange
  May 22 06:21:40.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-3870" for this suite. @ 05/22/23 06:21:40.045
  STEP: Destroying namespace "e2e-limitrange-l9gr9-3681" for this suite. @ 05/22/23 06:21:40.048
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 05/22/23 06:21:40.051
  May 22 06:21:40.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 06:21:40.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:40.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:40.061
  STEP: Setting up server cert @ 05/22/23 06:21:40.072
  E0522 06:21:40.393526      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 06:21:40.537
  STEP: Deploying the webhook pod @ 05/22/23 06:21:40.543
  STEP: Wait for the deployment to be ready @ 05/22/23 06:21:40.549
  May 22 06:21:40.555: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0522 06:21:41.393949      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:42.394233      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/22/23 06:21:42.561
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 06:21:42.569
  E0522 06:21:43.394685      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:43.570: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/22/23 06:21:43.572
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/22/23 06:21:43.583
  STEP: Creating a dummy validating-webhook-configuration object @ 05/22/23 06:21:43.593
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/22/23 06:21:43.598
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/22/23 06:21:43.6
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/22/23 06:21:43.605
  May 22 06:21:43.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8346" for this suite. @ 05/22/23 06:21:43.633
  STEP: Destroying namespace "webhook-markers-1292" for this suite. @ 05/22/23 06:21:43.636
• [3.588 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 05/22/23 06:21:43.64
  May 22 06:21:43.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename svcaccounts @ 05/22/23 06:21:43.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:43.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:43.65
  STEP: Creating ServiceAccount "e2e-sa-t8x5l"  @ 05/22/23 06:21:43.653
  May 22 06:21:43.655: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-t8x5l"  @ 05/22/23 06:21:43.655
  May 22 06:21:43.661: INFO: AutomountServiceAccountToken: true
  May 22 06:21:43.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8207" for this suite. @ 05/22/23 06:21:43.663
• [0.026 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/22/23 06:21:43.666
  May 22 06:21:43.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:21:43.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:43.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:43.678
  STEP: Creating secret with name secret-test-map-130849a1-a347-433a-9b8f-818a7a0f85d2 @ 05/22/23 06:21:43.68
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:21:43.682
  E0522 06:21:44.395584      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:45.395905      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:46.396389      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:47.396560      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:21:47.696
  May 22 06:21:47.697: INFO: Trying to get logs from node node2 pod pod-secrets-11c87042-6551-4cc0-8b70-dbe0e0edcc2c container secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 06:21:47.701
  May 22 06:21:47.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1561" for this suite. @ 05/22/23 06:21:47.711
• [4.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 05/22/23 06:21:47.716
  May 22 06:21:47.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename gc @ 05/22/23 06:21:47.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:47.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:47.725
  STEP: create the deployment @ 05/22/23 06:21:47.726
  W0522 06:21:47.729143      25 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/22/23 06:21:47.729
  STEP: delete the deployment @ 05/22/23 06:21:48.232
  STEP: wait for all rs to be garbage collected @ 05/22/23 06:21:48.235
  STEP: expected 0 rs, got 1 rs @ 05/22/23 06:21:48.239
  STEP: expected 0 pods, got 2 pods @ 05/22/23 06:21:48.241
  E0522 06:21:48.397582      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/22/23 06:21:48.746
  May 22 06:21:48.807: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 22 06:21:48.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7662" for this suite. @ 05/22/23 06:21:48.809
• [1.096 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 05/22/23 06:21:48.812
  May 22 06:21:48.812: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:21:48.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:48.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:48.822
  STEP: creating service in namespace services-5413 @ 05/22/23 06:21:48.824
  STEP: creating service affinity-clusterip-transition in namespace services-5413 @ 05/22/23 06:21:48.824
  STEP: creating replication controller affinity-clusterip-transition in namespace services-5413 @ 05/22/23 06:21:48.831
  I0522 06:21:48.838843      25 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-5413, replica count: 3
  E0522 06:21:49.397908      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:50.398857      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:51.399038      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0522 06:21:51.889696      25 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 22 06:21:51.893: INFO: Creating new exec pod
  E0522 06:21:52.399937      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:53.400388      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:54.400558      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:54.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-5413 exec execpod-affinityvlfbc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  May 22 06:21:55.015: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  May 22 06:21:55.015: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:21:55.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-5413 exec execpod-affinityvlfbc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.214.70 80'
  May 22 06:21:55.127: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.214.70 80\nConnection to 10.110.214.70 80 port [tcp/http] succeeded!\n"
  May 22 06:21:55.127: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:21:55.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-5413 exec execpod-affinityvlfbc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.214.70:80/ ; done'
  May 22 06:21:55.321: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n"
  May 22 06:21:55.321: INFO: stdout: "\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-m6nt6\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-rrc7c\naffinity-clusterip-transition-m6nt6\naffinity-clusterip-transition-m6nt6\naffinity-clusterip-transition-rrc7c\naffinity-clusterip-transition-rrc7c\naffinity-clusterip-transition-m6nt6\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-rrc7c\naffinity-clusterip-transition-m6nt6"
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-m6nt6
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-rrc7c
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-m6nt6
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-m6nt6
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-rrc7c
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-rrc7c
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-m6nt6
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-rrc7c
  May 22 06:21:55.321: INFO: Received response from host: affinity-clusterip-transition-m6nt6
  May 22 06:21:55.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-5413 exec execpod-affinityvlfbc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.110.214.70:80/ ; done'
  E0522 06:21:55.401257      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:21:55.531: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.110.214.70:80/\n"
  May 22 06:21:55.531: INFO: stdout: "\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln\naffinity-clusterip-transition-wrfln"
  May 22 06:21:55.531: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.531: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.531: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.531: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.531: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.531: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Received response from host: affinity-clusterip-transition-wrfln
  May 22 06:21:55.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 06:21:55.535: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5413, will wait for the garbage collector to delete the pods @ 05/22/23 06:21:55.542
  May 22 06:21:55.598: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.451478ms
  May 22 06:21:55.699: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.017048ms
  E0522 06:21:56.401713      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:57.402048      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5413" for this suite. @ 05/22/23 06:21:57.947
• [9.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 05/22/23 06:21:57.959
  May 22 06:21:57.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename ingress @ 05/22/23 06:21:57.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:57.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:57.969
  STEP: getting /apis @ 05/22/23 06:21:57.971
  STEP: getting /apis/networking.k8s.io @ 05/22/23 06:21:57.974
  STEP: getting /apis/networking.k8s.iov1 @ 05/22/23 06:21:57.975
  STEP: creating @ 05/22/23 06:21:57.975
  STEP: getting @ 05/22/23 06:21:57.982
  STEP: listing @ 05/22/23 06:21:57.983
  STEP: watching @ 05/22/23 06:21:57.985
  May 22 06:21:57.985: INFO: starting watch
  STEP: cluster-wide listing @ 05/22/23 06:21:57.986
  STEP: cluster-wide watching @ 05/22/23 06:21:57.987
  May 22 06:21:57.987: INFO: starting watch
  STEP: patching @ 05/22/23 06:21:57.988
  STEP: updating @ 05/22/23 06:21:57.991
  May 22 06:21:57.995: INFO: waiting for watch events with expected annotations
  May 22 06:21:57.995: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/22/23 06:21:57.995
  STEP: updating /status @ 05/22/23 06:21:57.999
  STEP: get /status @ 05/22/23 06:21:58.002
  STEP: deleting @ 05/22/23 06:21:58.004
  STEP: deleting a collection @ 05/22/23 06:21:58.009
  May 22 06:21:58.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-3646" for this suite. @ 05/22/23 06:21:58.016
• [0.059 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 05/22/23 06:21:58.019
  May 22 06:21:58.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:21:58.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:21:58.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:21:58.031
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:21:58.033
  E0522 06:21:58.403031      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:21:59.403866      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:22:00.043
  May 22 06:22:00.045: INFO: Trying to get logs from node node2 pod downwardapi-volume-03e8064e-2b9a-452c-bcf6-dafc695ce033 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:22:00.049
  May 22 06:22:00.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2661" for this suite. @ 05/22/23 06:22:00.061
• [2.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/22/23 06:22:00.067
  May 22 06:22:00.067: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-runtime @ 05/22/23 06:22:00.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:22:00.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:22:00.078
  STEP: create the container @ 05/22/23 06:22:00.079
  W0522 06:22:00.083960      25 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/22/23 06:22:00.084
  E0522 06:22:00.404018      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:01.405111      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:02.405793      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/22/23 06:22:03.093
  STEP: the container should be terminated @ 05/22/23 06:22:03.096
  STEP: the termination message should be set @ 05/22/23 06:22:03.096
  May 22 06:22:03.096: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 05/22/23 06:22:03.096
  May 22 06:22:03.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-16" for this suite. @ 05/22/23 06:22:03.108
• [3.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/22/23 06:22:03.112
  May 22 06:22:03.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename deployment @ 05/22/23 06:22:03.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:22:03.119
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:22:03.121
  May 22 06:22:03.123: INFO: Creating deployment "webserver-deployment"
  May 22 06:22:03.127: INFO: Waiting for observed generation 1
  E0522 06:22:03.405958      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:04.406454      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:22:05.131: INFO: Waiting for all required pods to come up
  May 22 06:22:05.135: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/22/23 06:22:05.135
  May 22 06:22:05.135: INFO: Waiting for deployment "webserver-deployment" to complete
  May 22 06:22:05.138: INFO: Updating deployment "webserver-deployment" with a non-existent image
  May 22 06:22:05.143: INFO: Updating deployment webserver-deployment
  May 22 06:22:05.143: INFO: Waiting for observed generation 2
  E0522 06:22:05.406616      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:06.406961      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:22:07.149: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  May 22 06:22:07.151: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  May 22 06:22:07.153: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 22 06:22:07.158: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  May 22 06:22:07.158: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  May 22 06:22:07.160: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 22 06:22:07.165: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  May 22 06:22:07.165: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  May 22 06:22:07.171: INFO: Updating deployment webserver-deployment
  May 22 06:22:07.171: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  May 22 06:22:07.175: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  May 22 06:22:07.177: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  E0522 06:22:07.407610      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:08.407772      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:22:09.185: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-5202  a837b9f7-0af8-4b82-bf90-3e63fc497878 899987 3 2023-05-22 06:22:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:22:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002564e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:20,UnavailableReplicas:13,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-22 06:22:07 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-05-22 06:22:09 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,},},ReadyReplicas:20,CollisionCount:nil,},}

  May 22 06:22:09.187: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-5202  b8dc95de-2df4-4fda-9286-2d01627a1b0a 899766 3 2023-05-22 06:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment a837b9f7-0af8-4b82-bf90-3e63fc497878 0xc005439037 0xc005439038}] [] [{kube-controller-manager Update apps/v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a837b9f7-0af8-4b82-bf90-3e63fc497878\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054390d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 22 06:22:09.187: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  May 22 06:22:09.187: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-5202  50f1f8da-7499-4b04-b3c0-a2e9f4629897 899986 3 2023-05-22 06:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment a837b9f7-0af8-4b82-bf90-3e63fc497878 0xc005438f47 0xc005438f48}] [] [{kube-controller-manager Update apps/v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a837b9f7-0af8-4b82-bf90-3e63fc497878\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:22:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005438fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:20,AvailableReplicas:20,Conditions:[]ReplicaSetCondition{},},}
  May 22 06:22:09.192: INFO: Pod "webserver-deployment-67bd4bf6dc-4j6x5" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4j6x5 webserver-deployment-67bd4bf6dc- deployment-5202  68382eb9-a578-486a-b8a0-499502480221 899984 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:6d788bf33bb5ce8cecac6456999cdffd438b1b2d42ae420f26ffd57d1c3a1360 cni.projectcalico.org/podIP:192.168.166.155/32 cni.projectcalico.org/podIPs:192.168.166.155/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc0054395c7 0xc0054395c8}] [] [{kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-22 06:22:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz9vf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz9vf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.121,PodIP:192.168.166.155,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3bbc64b2e4e25394fa1d0a2e4de8e4ecf0c28c1416377cd6849585bf32f1a6d1,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.155,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.192: INFO: Pod "webserver-deployment-67bd4bf6dc-4wk69" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4wk69 webserver-deployment-67bd4bf6dc- deployment-5202  ba92ecb5-2480-4818-b76b-88fe0807336f 899591 0 2023-05-22 06:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:2c103ee5598edd8d026fbbea69b32d755bb5ad11d8c010432ad41814f5254db1 cni.projectcalico.org/podIP:192.168.135.47/32 cni.projectcalico.org/podIPs:192.168.135.47/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc0054397d7 0xc0054397d8}] [] [{calico Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lg2hp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lg2hp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.47,StartTime:2023-05-22 06:22:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://65fd0909d4fe9df0a2e9ebd3ff26902347ad2483c30e9573dd0511e1915b11b5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.47,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.192: INFO: Pod "webserver-deployment-67bd4bf6dc-6pp9z" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6pp9z webserver-deployment-67bd4bf6dc- deployment-5202  6d95ef91-44e6-463d-8c35-7e397a545d84 899955 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:d4975b77fe0554e4c1f6cb37f3b8fb5900b3bee11cdada9cd53afca263944103 cni.projectcalico.org/podIP:192.168.104.40/32 cni.projectcalico.org/podIPs:192.168.104.40/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc0054399d7 0xc0054399d8}] [] [{kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h9srz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h9srz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.40,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dd1c2482404a82675f68e6a23f180771f6712bc628d8c2a2d251c70e970b0f02,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.40,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.193: INFO: Pod "webserver-deployment-67bd4bf6dc-6pqhz" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6pqhz webserver-deployment-67bd4bf6dc- deployment-5202  8a1a35e6-1f4d-4f31-be43-4beae518452f 899565 0 2023-05-22 06:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:6da97d0a220e0a4df1da50d299cec8c87a40c04d200bfecca6c3dc8e4e8be120 cni.projectcalico.org/podIP:192.168.166.188/32 cni.projectcalico.org/podIPs:192.168.166.188/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc005439c07 0xc005439c08}] [] [{calico Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94hqr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94hqr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.121,PodIP:192.168.166.188,StartTime:2023-05-22 06:22:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a9bd61a214edbb9ab946d690cd1177953e77c1d4c3086be686ea1aa43b54f9c9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.188,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.193: INFO: Pod "webserver-deployment-67bd4bf6dc-6w2wk" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6w2wk webserver-deployment-67bd4bf6dc- deployment-5202  31d3bc97-cec4-4d4e-a34c-6c06993ea834 899953 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:e15d013a6d71428402df37cbe14a85ed7a261435ef7135ff08a713152473b34c cni.projectcalico.org/podIP:192.168.104.8/32 cni.projectcalico.org/podIPs:192.168.104.8/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc005439e27 0xc005439e28}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m88s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m88s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.8,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6d7bb2e3618770b9afc118a73c7d21758c50e46f4f693520d7d8a4e635b48c39,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.8,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.193: INFO: Pod "webserver-deployment-67bd4bf6dc-8v8h7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8v8h7 webserver-deployment-67bd4bf6dc- deployment-5202  005fdea0-d7ce-4926-a003-14786f34cd8f 899593 0 2023-05-22 06:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:2a68b69eb67f537eb06ad4bf4e5c1846efd10e3af47bb81a73920019fd3cc8ed cni.projectcalico.org/podIP:192.168.135.54/32 cni.projectcalico.org/podIPs:192.168.135.54/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a98027 0xc004a98028}] [] [{calico Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6q9xf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6q9xf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.54,StartTime:2023-05-22 06:22:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0a6456f1b86ba3f749ed1a4f8463d7bfcca29d5802b662cfc1ef40b3ab7b6b32,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.54,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.193: INFO: Pod "webserver-deployment-67bd4bf6dc-bhvw6" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bhvw6 webserver-deployment-67bd4bf6dc- deployment-5202  bcb4643d-efda-4d44-bbbd-701a33d7aa21 899596 0 2023-05-22 06:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:aef06ea64f560cb3cd979436409a92efb00d7aff66844951c165e171578b266d cni.projectcalico.org/podIP:192.168.135.43/32 cni.projectcalico.org/podIPs:192.168.135.43/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a98247 0xc004a98248}] [] [{calico Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-96f5t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-96f5t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.43,StartTime:2023-05-22 06:22:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bab81822931196cd0836b49dfa5da0962c23e3cfc75a4e79dba3799185c57b50,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.43,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.193: INFO: Pod "webserver-deployment-67bd4bf6dc-c2brl" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-c2brl webserver-deployment-67bd4bf6dc- deployment-5202  53b8f5d2-eacf-43f9-a137-d2c91df510b1 899600 0 2023-05-22 06:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:93b984fab7ab8bc8fc5cbf1022044352e60b287049d715bfa9a718ff7308bac3 cni.projectcalico.org/podIP:192.168.166.163/32 cni.projectcalico.org/podIPs:192.168.166.163/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a98447 0xc004a98448}] [] [{calico Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzw4n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzw4n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.121,PodIP:192.168.166.163,StartTime:2023-05-22 06:22:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bf5f68ec30421d262a9901ca5aa52dc413c6033bea5527928179bcd663730841,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.163,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.194: INFO: Pod "webserver-deployment-67bd4bf6dc-c7gqn" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-c7gqn webserver-deployment-67bd4bf6dc- deployment-5202  283d78a7-dacb-4d86-af9b-5cbe08f5ff84 899603 0 2023-05-22 06:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:9524bc12c96cffdadad50a5d4c23a6c0c3bd1d7525e6d1c7f896ef5cade8c4f0 cni.projectcalico.org/podIP:192.168.166.183/32 cni.projectcalico.org/podIPs:192.168.166.183/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a98647 0xc004a98648}] [] [{calico Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnsj7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnsj7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.121,PodIP:192.168.166.183,StartTime:2023-05-22 06:22:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a4222c3b4d61f01b45d975ceb68d8e61eefe713ecf3039bcd71a81a68b2b37e3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.183,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.194: INFO: Pod "webserver-deployment-67bd4bf6dc-d8w7g" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-d8w7g webserver-deployment-67bd4bf6dc- deployment-5202  ef725fc4-fb85-4ca1-ab2f-bfcd1594a87c 899587 0 2023-05-22 06:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:437a63d174cf304d6cf48b29b86967fdddf1f9e3e0fbb2c9aea4a8ee30325667 cni.projectcalico.org/podIP:192.168.104.61/32 cni.projectcalico.org/podIPs:192.168.104.61/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a98847 0xc004a98848}] [] [{calico Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wd96b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wd96b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.61,StartTime:2023-05-22 06:22:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://76699d3fc3e0b2b664f3f5fd2fba557e9171347574969b8de3b487fd664da139,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.61,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.194: INFO: Pod "webserver-deployment-67bd4bf6dc-g76ps" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-g76ps webserver-deployment-67bd4bf6dc- deployment-5202  c0223ac2-5036-4185-9575-43376a29191c 899584 0 2023-05-22 06:22:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:a450305fae7c266aad0f9bdb1a5521c0a199c0087f768ab48011fcb682753331 cni.projectcalico.org/podIP:192.168.104.11/32 cni.projectcalico.org/podIPs:192.168.104.11/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a98a47 0xc004a98a48}] [] [{calico Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9r69j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9r69j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.11,StartTime:2023-05-22 06:22:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4ff47084f702976744aff8f808aedcc2b9f8a80ff81040f3813a5bc686b35e71,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.11,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.194: INFO: Pod "webserver-deployment-67bd4bf6dc-gq7c9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gq7c9 webserver-deployment-67bd4bf6dc- deployment-5202  0df1839a-5b57-4847-bd79-9c4562d0ac61 899972 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:b3e8d7a3aec33991f04ee67138c7dbb6c70fbc10058b61b3f85c4dec75d875fe cni.projectcalico.org/podIP:192.168.135.52/32 cni.projectcalico.org/podIPs:192.168.135.52/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a98c47 0xc004a98c48}] [] [{kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vnkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vnkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.52,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ad6b1b84e9cdd37b11a3e588f9b2bad83df1b33f1fff92ed674a18a1f56b504d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.52,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.195: INFO: Pod "webserver-deployment-67bd4bf6dc-hzl6k" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hzl6k webserver-deployment-67bd4bf6dc- deployment-5202  9c727614-607e-468a-801b-ce3a253f7101 899948 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:fb370966f18dfb40deee6f42f55870279b37e639a4db609c6422121f6e591905 cni.projectcalico.org/podIP:192.168.104.63/32 cni.projectcalico.org/podIPs:192.168.104.63/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a98e57 0xc004a98e58}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kxxr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kxxr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.63,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9e0674b13cb39635a3071b43f259dc55cca3473741654d447b5ec0459deee4bb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.63,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.195: INFO: Pod "webserver-deployment-67bd4bf6dc-kx4nq" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-kx4nq webserver-deployment-67bd4bf6dc- deployment-5202  d74e2118-4c69-4a82-b872-7fdc9c29f1a0 899970 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:5412ff4fb455310b5c2d6381f47de16e45f56b7183e6ebe11c1a7fdfa26db7d6 cni.projectcalico.org/podIP:192.168.135.57/32 cni.projectcalico.org/podIPs:192.168.135.57/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a99067 0xc004a99068}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwrb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwrb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.57,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9ab5af2f385b4def9ea13074bc5ce0b0c4a2923e056d878c41c1dfeb52df028a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.57,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.195: INFO: Pod "webserver-deployment-67bd4bf6dc-n4p7h" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-n4p7h webserver-deployment-67bd4bf6dc- deployment-5202  fc867043-f7f7-4c95-a82b-21ebd0c31f7e 899944 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:6f9d39813a7d36828bfd09540a90955e18b1b7617bb7ebe5e4ee9f1339efa4b5 cni.projectcalico.org/podIP:192.168.104.12/32 cni.projectcalico.org/podIPs:192.168.104.12/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a99267 0xc004a99268}] [] [{kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q8p6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q8p6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.12,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://975f90d739988140fa39dd50e676300d58909b7822fa646009742bf988b632fe,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.12,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.195: INFO: Pod "webserver-deployment-67bd4bf6dc-nvlxp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nvlxp webserver-deployment-67bd4bf6dc- deployment-5202  a74ab90f-31ae-47fe-a1ce-2343f508278f 899959 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:f2747fdd949c59f607e24cf4c394ea0e901e5c7b615bd31042a560fd52d4d693 cni.projectcalico.org/podIP:192.168.104.39/32 cni.projectcalico.org/podIPs:192.168.104.39/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a99477 0xc004a99478}] [] [{kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wct6r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wct6r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.39,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1972a1d4a6f6f0d01a4a6c5bbb17e994d26bd8ee365836a10c329ccbed2a88c8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.39,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.195: INFO: Pod "webserver-deployment-67bd4bf6dc-nwcn4" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nwcn4 webserver-deployment-67bd4bf6dc- deployment-5202  018d8675-dc9c-4f76-977e-f2c81d5fb279 899981 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:234a2a39f6a59f7143a379b1e24882ac8aebc16abd7ec4b4ebe1700196fcd868 cni.projectcalico.org/podIP:192.168.166.143/32 cni.projectcalico.org/podIPs:192.168.166.143/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a99687 0xc004a99688}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jtr8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jtr8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.121,PodIP:192.168.166.143,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6b3711f2c54bf893c1534a39b5fe9c9f4215236bb9609f93d576d3ee5237c5e1,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.143,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.196: INFO: Pod "webserver-deployment-67bd4bf6dc-r5xxg" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-r5xxg webserver-deployment-67bd4bf6dc- deployment-5202  4c391d80-77e8-4b07-999e-97085f11c53c 899965 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:24c8983d390156eb2bba954b7a920e68903b1c047a8c66ebfe0a28ed5718817e cni.projectcalico.org/podIP:192.168.135.26/32 cni.projectcalico.org/podIPs:192.168.135.26/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a99887 0xc004a99888}] [] [{kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68bg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68bg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.26,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://efb9697d0b97a3118d92edd050a65f1d0b4b8d652a88bbf9cc0ccf864ae3cf25,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.26,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.196: INFO: Pod "webserver-deployment-67bd4bf6dc-rf4wx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rf4wx webserver-deployment-67bd4bf6dc- deployment-5202  497c5b62-2447-4f4f-9670-b01a0c87b663 899962 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:ca6fc5d4a9ffe712c0116a91a981a701f559e1f30c9266cd77a4f6c8636b87fe cni.projectcalico.org/podIP:192.168.135.37/32 cni.projectcalico.org/podIPs:192.168.135.37/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a99a97 0xc004a99a98}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqr72,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqr72,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.37,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://66ec2ada77bc70252591890532694d457fb4c11d9c9e913c77e4eb5ad12cb6b5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.37,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.196: INFO: Pod "webserver-deployment-67bd4bf6dc-x9nx9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-x9nx9 webserver-deployment-67bd4bf6dc- deployment-5202  5726396e-ec57-4581-894b-a70ac63e79d0 899979 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:4e0fd72541a48f3686602de1e23eca39f1b5e859da1c9182defc95ab3693c6c7 cni.projectcalico.org/podIP:192.168.166.140/32 cni.projectcalico.org/podIPs:192.168.166.140/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 50f1f8da-7499-4b04-b3c0-a2e9f4629897 0xc004a99c97 0xc004a99c98}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50f1f8da-7499-4b04-b3c0-a2e9f4629897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-klpsw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-klpsw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.121,PodIP:192.168.166.140,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:22:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://41ee8fb3e9dfc56f49e3e398bbc83cad0efd2f7cc41d9ea589b161fdc3a375f8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.140,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.196: INFO: Pod "webserver-deployment-7b75d79cf5-26xnz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-26xnz webserver-deployment-7b75d79cf5- deployment-5202  97f21f95-0b96-498a-9dcb-492680fd0b28 899876 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:6704461775863c1de44c625738c376aa93661c52a8a593c9384eedd3402f49e0 cni.projectcalico.org/podIP:192.168.166.178/32 cni.projectcalico.org/podIPs:192.168.166.178/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc004a99e97 0xc004a99e98}] [] [{kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rxl47,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rxl47,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.121,PodIP:,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.197: INFO: Pod "webserver-deployment-7b75d79cf5-4pszv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-4pszv webserver-deployment-7b75d79cf5- deployment-5202  fbee3bc4-6131-4ee2-9d97-dac687231122 899870 0 2023-05-22 06:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:d0325fe3e652862823ad7f3b481f50fbb50852d33376fe99c2a85e21b57f4a86 cni.projectcalico.org/podIP:192.168.166.145/32 cni.projectcalico.org/podIPs:192.168.166.145/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a40a7 0xc0054a40a8}] [] [{calico Update v1 2023-05-22 06:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6kl29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6kl29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.121,PodIP:192.168.166.145,StartTime:2023-05-22 06:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.166.145,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.197: INFO: Pod "webserver-deployment-7b75d79cf5-8vh67" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8vh67 webserver-deployment-7b75d79cf5- deployment-5202  a0280b48-1f56-4aaf-bd54-57e8c3428881 899951 0 2023-05-22 06:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:7e4499437db7291b6d368f9ac90419b2cf58c88f529d4a6f763612356044603c cni.projectcalico.org/podIP:192.168.104.60/32 cni.projectcalico.org/podIPs:192.168.104.60/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a42d7 0xc0054a42d8}] [] [{calico Update v1 2023-05-22 06:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kdkv2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kdkv2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.60,StartTime:2023-05-22 06:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.60,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.197: INFO: Pod "webserver-deployment-7b75d79cf5-9llcf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-9llcf webserver-deployment-7b75d79cf5- deployment-5202  27b61cdb-b24c-4856-a675-0475cd75f5b4 899887 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:324da57ce1f33914f18e9e36e64e79eedc03e0db38b9e9a569011b8fd3473b9e cni.projectcalico.org/podIP:192.168.104.35/32 cni.projectcalico.org/podIPs:192.168.104.35/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a4507 0xc0054a4508}] [] [{kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzmdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzmdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.197: INFO: Pod "webserver-deployment-7b75d79cf5-bpslz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-bpslz webserver-deployment-7b75d79cf5- deployment-5202  73dca737-3a59-4cef-bddb-a308c0887d88 899873 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:d08f2e0783410d2f46e7e99d34515ddc1052c7c5eb7fd3003b31c8a4bc668643 cni.projectcalico.org/podIP:192.168.104.1/32 cni.projectcalico.org/podIPs:192.168.104.1/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a4737 0xc0054a4738}] [] [{kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vn9mb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vn9mb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.197: INFO: Pod "webserver-deployment-7b75d79cf5-dpkvw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-dpkvw webserver-deployment-7b75d79cf5- deployment-5202  e5508399-8b12-460d-9fe0-b91c30efdddb 899831 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:857eafa4359c10416997b4f9656dad918d08fdfb67d671ef8b1374d2947b065d cni.projectcalico.org/podIP:192.168.104.44/32 cni.projectcalico.org/podIPs:192.168.104.44/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a4947 0xc0054a4948}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sw6zs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sw6zs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.198: INFO: Pod "webserver-deployment-7b75d79cf5-g8mrk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-g8mrk webserver-deployment-7b75d79cf5- deployment-5202  1ebc3972-c77d-46b4-97ac-33a5ecb637bd 899828 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:c353230b8e886952a91dfd35c1d6e6e302ffbd4f33e720c24466fbb0ea059f55 cni.projectcalico.org/podIP:192.168.135.35/32 cni.projectcalico.org/podIPs:192.168.135.35/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a4b47 0xc0054a4b48}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8jqlk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8jqlk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.198: INFO: Pod "webserver-deployment-7b75d79cf5-j8glw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-j8glw webserver-deployment-7b75d79cf5- deployment-5202  0b2964a5-9e89-4549-a734-b3e68665512f 899830 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:c1649dcf81a2baaf223621c0edc6d24c3fc346b81dcea38e57ebfeb67f200c44 cni.projectcalico.org/podIP:192.168.166.179/32 cni.projectcalico.org/podIPs:192.168.166.179/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a4d47 0xc0054a4d48}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xcmsg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xcmsg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.121,PodIP:,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.198: INFO: Pod "webserver-deployment-7b75d79cf5-nk7zf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-nk7zf webserver-deployment-7b75d79cf5- deployment-5202  0667d0f1-a930-4c93-a458-f8dec1c32fea 899815 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:47c2b5646691f406ad475e38456a38710b4eb05df15239c78ee0fcd542899781 cni.projectcalico.org/podIP:192.168.166.150/32 cni.projectcalico.org/podIPs:192.168.166.150/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a4f47 0xc0054a4f48}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8xgs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8xgs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.121,PodIP:,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.198: INFO: Pod "webserver-deployment-7b75d79cf5-qqb6q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qqb6q webserver-deployment-7b75d79cf5- deployment-5202  96a6b899-47b6-4548-98fb-151a257a7828 899848 0 2023-05-22 06:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:45ddbf4b6f6b68a02b8c0cde0088bb6949d6330d892d423d5d9f5b895fa218fd cni.projectcalico.org/podIP:192.168.135.61/32 cni.projectcalico.org/podIPs:192.168.135.61/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a5147 0xc0054a5148}] [] [{calico Update v1 2023-05-22 06:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ck9nz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ck9nz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.61,StartTime:2023-05-22 06:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.61,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.198: INFO: Pod "webserver-deployment-7b75d79cf5-tpjb8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tpjb8 webserver-deployment-7b75d79cf5- deployment-5202  e4340f69-5afe-491c-acbd-4aa8d9cba266 899836 0 2023-05-22 06:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:f68326a08e2c950e4570fdf40cd821dcfa02fbe2f1c1dd8483639241517be2da cni.projectcalico.org/podIP:192.168.104.16/32 cni.projectcalico.org/podIPs:192.168.104.16/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a5377 0xc0054a5378}] [] [{calico Update v1 2023-05-22 06:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzv2s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzv2s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.16,StartTime:2023-05-22 06:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.16,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.199: INFO: Pod "webserver-deployment-7b75d79cf5-v6g5w" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-v6g5w webserver-deployment-7b75d79cf5- deployment-5202  061ba977-cd7d-4c62-a746-44b4b1e4f6a0 899854 0 2023-05-22 06:22:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:34676a5285dfc7fcfaa1f5df3cbba282cc4c8154d50e986448e6d399579f919f cni.projectcalico.org/podIP:192.168.135.40/32 cni.projectcalico.org/podIPs:192.168.135.40/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a55a7 0xc0054a55a8}] [] [{calico Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hkzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hkzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:,StartTime:2023-05-22 06:22:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.199: INFO: Pod "webserver-deployment-7b75d79cf5-vbc5t" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vbc5t webserver-deployment-7b75d79cf5- deployment-5202  d8b30310-ec15-46df-83cb-60cf47a9d187 899976 0 2023-05-22 06:22:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:46e6de1b25c7c6b25f7d34f12941cc70a51f8165f5d3346ac13c1dc2c78585dc cni.projectcalico.org/podIP:192.168.135.58/32 cni.projectcalico.org/podIPs:192.168.135.58/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b8dc95de-2df4-4fda-9286-2d01627a1b0a 0xc0054a57a7 0xc0054a57a8}] [] [{calico Update v1 2023-05-22 06:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:22:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8dc95de-2df4-4fda-9286-2d01627a1b0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:22:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5tmbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5tmbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.58,StartTime:2023-05-22 06:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.58,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:22:09.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5202" for this suite. @ 05/22/23 06:22:09.202
• [6.092 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/22/23 06:22:09.205
  May 22 06:22:09.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubelet-test @ 05/22/23 06:22:09.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:22:09.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:22:09.215
  E0522 06:22:09.408307      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:10.408830      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:11.409001      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:12.409231      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:22:13.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6952" for this suite. @ 05/22/23 06:22:13.228
• [4.026 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 05/22/23 06:22:13.232
  May 22 06:22:13.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename daemonsets @ 05/22/23 06:22:13.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:22:13.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:22:13.241
  STEP: Creating simple DaemonSet "daemon-set" @ 05/22/23 06:22:13.254
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/22/23 06:22:13.258
  May 22 06:22:13.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:22:13.263: INFO: Node node1 is running 0 daemon pod, expected 1
  E0522 06:22:13.409938      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:22:14.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 22 06:22:14.269: INFO: Node node2 is running 0 daemon pod, expected 1
  E0522 06:22:14.410716      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:22:15.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 22 06:22:15.292: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/22/23 06:22:15.294
  STEP: DeleteCollection of the DaemonSets @ 05/22/23 06:22:15.303
  STEP: Verify that ReplicaSets have been deleted @ 05/22/23 06:22:15.318
  May 22 06:22:15.338: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"900396"},"items":null}

  May 22 06:22:15.355: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"900402"},"items":[{"metadata":{"name":"daemon-set-45szm","generateName":"daemon-set-","namespace":"daemonsets-5955","uid":"b7e85dd5-9992-46f4-a644-31397e34d5a8","resourceVersion":"900402","creationTimestamp":"2023-05-22T06:22:13Z","deletionTimestamp":"2023-05-22T06:22:45Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"25429d433e54f22a6d5ae430e2cda6ddde6f34d1f3e8941b48b003fbc635e8be","cni.projectcalico.org/podIP":"192.168.135.32/32","cni.projectcalico.org/podIPs":"192.168.135.32/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a58f2dc4-b9be-4f1a-a7c4-1201f2c3ce73","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-22T06:22:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-22T06:22:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a58f2dc4-b9be-4f1a-a7c4-1201f2c3ce73\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-22T06:22:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qh2xn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qh2xn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:13Z"}],"hostIP":"192.168.33.123","podIP":"192.168.135.32","podIPs":[{"ip":"192.168.135.32"}],"startTime":"2023-05-22T06:22:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-22T06:22:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://580c78fe3a06d584b0647890ba2f6965ea6ca53136b00ae5d5a5db29cd2dc0d9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-5pz6g","generateName":"daemon-set-","namespace":"daemonsets-5955","uid":"dba710a3-9897-497c-9a39-024fd2faec55","resourceVersion":"900398","creationTimestamp":"2023-05-22T06:22:13Z","deletionTimestamp":"2023-05-22T06:22:45Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"feca079ead5cd372cc513763eece02a312abe16e23a988682d88f35928a2f306","cni.projectcalico.org/podIP":"192.168.104.49/32","cni.projectcalico.org/podIPs":"192.168.104.49/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a58f2dc4-b9be-4f1a-a7c4-1201f2c3ce73","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-22T06:22:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-22T06:22:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a58f2dc4-b9be-4f1a-a7c4-1201f2c3ce73\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-22T06:22:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.49\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l4p5q","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l4p5q","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:13Z"}],"hostIP":"192.168.33.122","podIP":"192.168.104.49","podIPs":[{"ip":"192.168.104.49"}],"startTime":"2023-05-22T06:22:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-22T06:22:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://6b8e481b6343f930fd2c1e7e0a25db0d04d763fbc993d0daf53e8ff462ecc8aa","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-cf9hm","generateName":"daemon-set-","namespace":"daemonsets-5955","uid":"d61dc414-991b-4ed9-a849-489569836738","resourceVersion":"900399","creationTimestamp":"2023-05-22T06:22:13Z","deletionTimestamp":"2023-05-22T06:22:45Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3e31493500c7260dbf136cea9d9c29ffd2b372ff8558fda6daa01cff24ddff7c","cni.projectcalico.org/podIP":"192.168.166.142/32","cni.projectcalico.org/podIPs":"192.168.166.142/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a58f2dc4-b9be-4f1a-a7c4-1201f2c3ce73","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-22T06:22:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-22T06:22:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a58f2dc4-b9be-4f1a-a7c4-1201f2c3ce73\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-22T06:22:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.166.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-dnwnw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-dnwnw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"node1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["node1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:13Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-22T06:22:13Z"}],"hostIP":"192.168.33.121","podIP":"192.168.166.142","podIPs":[{"ip":"192.168.166.142"}],"startTime":"2023-05-22T06:22:13Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-22T06:22:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e77bc523638afc839fdcec723fa55015d65d210044e0d178b1c577484d389341","started":true}],"qosClass":"BestEffort"}}]}

  May 22 06:22:15.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5955" for this suite. @ 05/22/23 06:22:15.372
• [2.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 05/22/23 06:22:15.382
  May 22 06:22:15.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:22:15.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:22:15.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:22:15.401
  STEP: creating service endpoint-test2 in namespace services-2464 @ 05/22/23 06:22:15.404
  E0522 06:22:15.411386      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2464 to expose endpoints map[] @ 05/22/23 06:22:15.413
  May 22 06:22:15.416: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0522 06:22:16.411404      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:22:16.421: INFO: successfully validated that service endpoint-test2 in namespace services-2464 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2464 @ 05/22/23 06:22:16.421
  E0522 06:22:17.411729      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:18.411930      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2464 to expose endpoints map[pod1:[80]] @ 05/22/23 06:22:18.433
  May 22 06:22:18.439: INFO: successfully validated that service endpoint-test2 in namespace services-2464 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/22/23 06:22:18.44
  May 22 06:22:18.440: INFO: Creating new exec pod
  E0522 06:22:19.412484      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:20.412601      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:21.412856      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:22:21.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-2464 exec execpodjxjmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 22 06:22:21.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 22 06:22:21.569: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:22:21.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-2464 exec execpodjxjmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.173.227 80'
  May 22 06:22:21.703: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.173.227 80\nConnection to 10.106.173.227 80 port [tcp/http] succeeded!\n"
  May 22 06:22:21.704: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-2464 @ 05/22/23 06:22:21.704
  E0522 06:22:22.413113      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:23.413727      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2464 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/22/23 06:22:23.715
  May 22 06:22:23.721: INFO: successfully validated that service endpoint-test2 in namespace services-2464 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/22/23 06:22:23.721
  E0522 06:22:24.414679      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:22:24.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-2464 exec execpodjxjmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 22 06:22:24.853: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 22 06:22:24.854: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:22:24.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-2464 exec execpodjxjmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.173.227 80'
  May 22 06:22:24.969: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.173.227 80\nConnection to 10.106.173.227 80 port [tcp/http] succeeded!\n"
  May 22 06:22:24.969: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2464 @ 05/22/23 06:22:24.969
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2464 to expose endpoints map[pod2:[80]] @ 05/22/23 06:22:24.978
  May 22 06:22:24.986: INFO: successfully validated that service endpoint-test2 in namespace services-2464 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/22/23 06:22:24.986
  E0522 06:22:25.415451      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:22:25.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-2464 exec execpodjxjmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 22 06:22:26.113: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 22 06:22:26.113: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 22 06:22:26.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=services-2464 exec execpodjxjmf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.173.227 80'
  May 22 06:22:26.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.173.227 80\nConnection to 10.106.173.227 80 port [tcp/http] succeeded!\n"
  May 22 06:22:26.236: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-2464 @ 05/22/23 06:22:26.236
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2464 to expose endpoints map[] @ 05/22/23 06:22:26.244
  May 22 06:22:26.251: INFO: successfully validated that service endpoint-test2 in namespace services-2464 exposes endpoints map[]
  May 22 06:22:26.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2464" for this suite. @ 05/22/23 06:22:26.265
• [10.886 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 05/22/23 06:22:26.269
  May 22 06:22:26.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename gc @ 05/22/23 06:22:26.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:22:26.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:22:26.279
  STEP: create the rc @ 05/22/23 06:22:26.283
  W0522 06:22:26.286022      25 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0522 06:22:26.415824      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:27.416564      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:28.417233      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:29.417757      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:30.418285      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:31.418822      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/22/23 06:22:32.289
  STEP: wait for the rc to be deleted @ 05/22/23 06:22:32.293
  E0522 06:22:32.419556      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:33.419653      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:34.419677      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:35.419839      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:36.420051      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/22/23 06:22:37.296
  E0522 06:22:37.420591      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:38.420945      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:39.421871      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:40.422029      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:41.422248      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:42.422377      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:43.422704      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:44.423625      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:45.423756      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:46.423886      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:47.424025      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:48.424180      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:49.424676      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:50.424888      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:51.424999      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:52.425202      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:53.425245      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:54.425584      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:55.425691      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:56.426052      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:57.426452      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:58.427468      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:22:59.428027      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:00.428696      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:01.429611      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:02.429891      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:03.430067      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:04.430489      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:05.430701      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:06.430896      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/22/23 06:23:07.307
  May 22 06:23:07.373: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 22 06:23:07.373: INFO: Deleting pod "simpletest.rc-247mw" in namespace "gc-6006"
  May 22 06:23:07.385: INFO: Deleting pod "simpletest.rc-2p6wg" in namespace "gc-6006"
  May 22 06:23:07.391: INFO: Deleting pod "simpletest.rc-4fwn2" in namespace "gc-6006"
  May 22 06:23:07.397: INFO: Deleting pod "simpletest.rc-4j48n" in namespace "gc-6006"
  May 22 06:23:07.404: INFO: Deleting pod "simpletest.rc-4jxzb" in namespace "gc-6006"
  May 22 06:23:07.411: INFO: Deleting pod "simpletest.rc-4nzgw" in namespace "gc-6006"
  May 22 06:23:07.416: INFO: Deleting pod "simpletest.rc-4pkqv" in namespace "gc-6006"
  May 22 06:23:07.423: INFO: Deleting pod "simpletest.rc-4px84" in namespace "gc-6006"
  May 22 06:23:07.430: INFO: Deleting pod "simpletest.rc-5fdpn" in namespace "gc-6006"
  E0522 06:23:07.431852      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:23:07.436: INFO: Deleting pod "simpletest.rc-5tgzq" in namespace "gc-6006"
  May 22 06:23:07.442: INFO: Deleting pod "simpletest.rc-69n22" in namespace "gc-6006"
  May 22 06:23:07.451: INFO: Deleting pod "simpletest.rc-6bgdr" in namespace "gc-6006"
  May 22 06:23:07.456: INFO: Deleting pod "simpletest.rc-6cczn" in namespace "gc-6006"
  May 22 06:23:07.464: INFO: Deleting pod "simpletest.rc-6fhg8" in namespace "gc-6006"
  May 22 06:23:07.477: INFO: Deleting pod "simpletest.rc-6qd5h" in namespace "gc-6006"
  May 22 06:23:07.495: INFO: Deleting pod "simpletest.rc-6zsr9" in namespace "gc-6006"
  May 22 06:23:07.504: INFO: Deleting pod "simpletest.rc-7cglh" in namespace "gc-6006"
  May 22 06:23:07.513: INFO: Deleting pod "simpletest.rc-7ltxg" in namespace "gc-6006"
  May 22 06:23:07.534: INFO: Deleting pod "simpletest.rc-7sjlx" in namespace "gc-6006"
  May 22 06:23:07.540: INFO: Deleting pod "simpletest.rc-7xp7j" in namespace "gc-6006"
  May 22 06:23:07.545: INFO: Deleting pod "simpletest.rc-8hcpp" in namespace "gc-6006"
  May 22 06:23:07.554: INFO: Deleting pod "simpletest.rc-9fxw9" in namespace "gc-6006"
  May 22 06:23:07.562: INFO: Deleting pod "simpletest.rc-9lc9n" in namespace "gc-6006"
  May 22 06:23:07.574: INFO: Deleting pod "simpletest.rc-9qts2" in namespace "gc-6006"
  May 22 06:23:07.588: INFO: Deleting pod "simpletest.rc-bsmwp" in namespace "gc-6006"
  May 22 06:23:07.604: INFO: Deleting pod "simpletest.rc-cc4fq" in namespace "gc-6006"
  May 22 06:23:07.613: INFO: Deleting pod "simpletest.rc-clvdh" in namespace "gc-6006"
  May 22 06:23:07.640: INFO: Deleting pod "simpletest.rc-clwr9" in namespace "gc-6006"
  May 22 06:23:07.663: INFO: Deleting pod "simpletest.rc-cmsst" in namespace "gc-6006"
  May 22 06:23:07.677: INFO: Deleting pod "simpletest.rc-cx4gt" in namespace "gc-6006"
  May 22 06:23:07.698: INFO: Deleting pod "simpletest.rc-d7mmm" in namespace "gc-6006"
  May 22 06:23:07.724: INFO: Deleting pod "simpletest.rc-df2hl" in namespace "gc-6006"
  May 22 06:23:07.783: INFO: Deleting pod "simpletest.rc-dfcks" in namespace "gc-6006"
  May 22 06:23:07.806: INFO: Deleting pod "simpletest.rc-dgj5v" in namespace "gc-6006"
  May 22 06:23:07.826: INFO: Deleting pod "simpletest.rc-dvb54" in namespace "gc-6006"
  May 22 06:23:07.845: INFO: Deleting pod "simpletest.rc-dxw8d" in namespace "gc-6006"
  May 22 06:23:07.878: INFO: Deleting pod "simpletest.rc-fmrqf" in namespace "gc-6006"
  May 22 06:23:07.955: INFO: Deleting pod "simpletest.rc-fsqrj" in namespace "gc-6006"
  May 22 06:23:07.984: INFO: Deleting pod "simpletest.rc-g7dpn" in namespace "gc-6006"
  May 22 06:23:08.017: INFO: Deleting pod "simpletest.rc-gtqpl" in namespace "gc-6006"
  May 22 06:23:08.062: INFO: Deleting pod "simpletest.rc-h6nbk" in namespace "gc-6006"
  May 22 06:23:08.086: INFO: Deleting pod "simpletest.rc-h84jv" in namespace "gc-6006"
  May 22 06:23:08.119: INFO: Deleting pod "simpletest.rc-hb9tb" in namespace "gc-6006"
  May 22 06:23:08.142: INFO: Deleting pod "simpletest.rc-hbjv4" in namespace "gc-6006"
  May 22 06:23:08.164: INFO: Deleting pod "simpletest.rc-hdjl6" in namespace "gc-6006"
  May 22 06:23:08.210: INFO: Deleting pod "simpletest.rc-hgb2m" in namespace "gc-6006"
  May 22 06:23:08.259: INFO: Deleting pod "simpletest.rc-hlml7" in namespace "gc-6006"
  May 22 06:23:08.290: INFO: Deleting pod "simpletest.rc-j4f2r" in namespace "gc-6006"
  May 22 06:23:08.306: INFO: Deleting pod "simpletest.rc-jhvzl" in namespace "gc-6006"
  May 22 06:23:08.364: INFO: Deleting pod "simpletest.rc-jshtw" in namespace "gc-6006"
  May 22 06:23:08.400: INFO: Deleting pod "simpletest.rc-khqh8" in namespace "gc-6006"
  E0522 06:23:08.436987      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:23:08.437: INFO: Deleting pod "simpletest.rc-krvwf" in namespace "gc-6006"
  May 22 06:23:08.473: INFO: Deleting pod "simpletest.rc-ksb5t" in namespace "gc-6006"
  May 22 06:23:08.501: INFO: Deleting pod "simpletest.rc-l6l47" in namespace "gc-6006"
  May 22 06:23:08.526: INFO: Deleting pod "simpletest.rc-lpf2w" in namespace "gc-6006"
  May 22 06:23:08.556: INFO: Deleting pod "simpletest.rc-ls7xl" in namespace "gc-6006"
  May 22 06:23:08.583: INFO: Deleting pod "simpletest.rc-m5gdk" in namespace "gc-6006"
  May 22 06:23:08.617: INFO: Deleting pod "simpletest.rc-m964d" in namespace "gc-6006"
  May 22 06:23:08.661: INFO: Deleting pod "simpletest.rc-mgdvd" in namespace "gc-6006"
  May 22 06:23:08.681: INFO: Deleting pod "simpletest.rc-mtj42" in namespace "gc-6006"
  May 22 06:23:08.709: INFO: Deleting pod "simpletest.rc-n2cbc" in namespace "gc-6006"
  May 22 06:23:08.737: INFO: Deleting pod "simpletest.rc-n2dvt" in namespace "gc-6006"
  May 22 06:23:08.757: INFO: Deleting pod "simpletest.rc-n2lrh" in namespace "gc-6006"
  May 22 06:23:08.765: INFO: Deleting pod "simpletest.rc-n4pxj" in namespace "gc-6006"
  May 22 06:23:08.780: INFO: Deleting pod "simpletest.rc-n77zj" in namespace "gc-6006"
  May 22 06:23:08.850: INFO: Deleting pod "simpletest.rc-npb9k" in namespace "gc-6006"
  May 22 06:23:08.880: INFO: Deleting pod "simpletest.rc-nszwt" in namespace "gc-6006"
  May 22 06:23:08.898: INFO: Deleting pod "simpletest.rc-nwbcg" in namespace "gc-6006"
  May 22 06:23:08.929: INFO: Deleting pod "simpletest.rc-pd79m" in namespace "gc-6006"
  May 22 06:23:08.946: INFO: Deleting pod "simpletest.rc-pls2f" in namespace "gc-6006"
  May 22 06:23:08.971: INFO: Deleting pod "simpletest.rc-psg2r" in namespace "gc-6006"
  May 22 06:23:08.987: INFO: Deleting pod "simpletest.rc-q4dpw" in namespace "gc-6006"
  May 22 06:23:09.028: INFO: Deleting pod "simpletest.rc-qcb5r" in namespace "gc-6006"
  May 22 06:23:09.045: INFO: Deleting pod "simpletest.rc-qgjs5" in namespace "gc-6006"
  May 22 06:23:09.057: INFO: Deleting pod "simpletest.rc-qx5ph" in namespace "gc-6006"
  May 22 06:23:09.075: INFO: Deleting pod "simpletest.rc-rd628" in namespace "gc-6006"
  May 22 06:23:09.107: INFO: Deleting pod "simpletest.rc-rh75p" in namespace "gc-6006"
  May 22 06:23:09.150: INFO: Deleting pod "simpletest.rc-rvg9z" in namespace "gc-6006"
  May 22 06:23:09.197: INFO: Deleting pod "simpletest.rc-sflcc" in namespace "gc-6006"
  May 22 06:23:09.227: INFO: Deleting pod "simpletest.rc-spqmc" in namespace "gc-6006"
  May 22 06:23:09.249: INFO: Deleting pod "simpletest.rc-sw5j9" in namespace "gc-6006"
  May 22 06:23:09.288: INFO: Deleting pod "simpletest.rc-tn5zm" in namespace "gc-6006"
  May 22 06:23:09.309: INFO: Deleting pod "simpletest.rc-tv7fh" in namespace "gc-6006"
  May 22 06:23:09.336: INFO: Deleting pod "simpletest.rc-tzb62" in namespace "gc-6006"
  May 22 06:23:09.368: INFO: Deleting pod "simpletest.rc-vbpj9" in namespace "gc-6006"
  May 22 06:23:09.392: INFO: Deleting pod "simpletest.rc-vc48v" in namespace "gc-6006"
  May 22 06:23:09.426: INFO: Deleting pod "simpletest.rc-vfb9q" in namespace "gc-6006"
  E0522 06:23:09.437381      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:23:09.451: INFO: Deleting pod "simpletest.rc-vlzjj" in namespace "gc-6006"
  May 22 06:23:09.480: INFO: Deleting pod "simpletest.rc-vmnlm" in namespace "gc-6006"
  May 22 06:23:09.506: INFO: Deleting pod "simpletest.rc-vnh2f" in namespace "gc-6006"
  May 22 06:23:09.530: INFO: Deleting pod "simpletest.rc-vpbnk" in namespace "gc-6006"
  May 22 06:23:09.560: INFO: Deleting pod "simpletest.rc-wdtvc" in namespace "gc-6006"
  May 22 06:23:09.631: INFO: Deleting pod "simpletest.rc-wss59" in namespace "gc-6006"
  May 22 06:23:09.660: INFO: Deleting pod "simpletest.rc-x7dvm" in namespace "gc-6006"
  May 22 06:23:09.719: INFO: Deleting pod "simpletest.rc-xdgz9" in namespace "gc-6006"
  May 22 06:23:09.771: INFO: Deleting pod "simpletest.rc-xp9tx" in namespace "gc-6006"
  May 22 06:23:09.836: INFO: Deleting pod "simpletest.rc-zhffd" in namespace "gc-6006"
  May 22 06:23:09.870: INFO: Deleting pod "simpletest.rc-zhrhr" in namespace "gc-6006"
  May 22 06:23:09.920: INFO: Deleting pod "simpletest.rc-znfl2" in namespace "gc-6006"
  May 22 06:23:09.960: INFO: Deleting pod "simpletest.rc-zqg4d" in namespace "gc-6006"
  May 22 06:23:10.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6006" for this suite. @ 05/22/23 06:23:10.075
• [43.834 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 05/22/23 06:23:10.103
  May 22 06:23:10.103: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:23:10.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:10.113
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:10.117
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:23:10.121
  E0522 06:23:10.438380      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:11.438484      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:12.438984      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:13.439861      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:23:14.147
  May 22 06:23:14.148: INFO: Trying to get logs from node node2 pod downwardapi-volume-93c41436-9bd5-40ca-99ac-55e8c502146b container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:23:14.152
  May 22 06:23:14.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4953" for this suite. @ 05/22/23 06:23:14.161
• [4.061 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/22/23 06:23:14.164
  May 22 06:23:14.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename deployment @ 05/22/23 06:23:14.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:14.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:14.174
  STEP: creating a Deployment @ 05/22/23 06:23:14.179
  STEP: waiting for Deployment to be created @ 05/22/23 06:23:14.181
  STEP: waiting for all Replicas to be Ready @ 05/22/23 06:23:14.182
  May 22 06:23:14.183: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 22 06:23:14.183: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 22 06:23:14.188: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 22 06:23:14.188: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 22 06:23:14.196: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 22 06:23:14.196: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 22 06:23:14.205: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 22 06:23:14.205: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0522 06:23:14.439915      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:23:15.328: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 22 06:23:15.328: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 22 06:23:15.351: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/22/23 06:23:15.351
  W0522 06:23:15.357812      25 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 22 06:23:15.359: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/22/23 06:23:15.359
  May 22 06:23:15.360: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0
  May 22 06:23:15.360: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0
  May 22 06:23:15.360: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0
  May 22 06:23:15.360: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0
  May 22 06:23:15.360: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0
  May 22 06:23:15.361: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0
  May 22 06:23:15.361: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0
  May 22 06:23:15.361: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 0
  May 22 06:23:15.361: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  May 22 06:23:15.361: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  May 22 06:23:15.361: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:15.361: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:15.361: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:15.361: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:15.364: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:15.364: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:15.372: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:15.372: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:15.378: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  May 22 06:23:15.378: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  May 22 06:23:15.385: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  May 22 06:23:15.385: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  E0522 06:23:15.440500      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:23:16.337: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:16.337: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:16.345: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  STEP: listing Deployments @ 05/22/23 06:23:16.345
  May 22 06:23:16.347: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/22/23 06:23:16.347
  May 22 06:23:16.356: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/22/23 06:23:16.356
  May 22 06:23:16.361: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 22 06:23:16.363: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 22 06:23:16.371: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 22 06:23:16.377: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 22 06:23:16.386: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0522 06:23:16.441454      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:23:17.344: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 22 06:23:17.365: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 22 06:23:17.369: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0522 06:23:17.441714      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:23:18.370: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/22/23 06:23:18.388
  STEP: fetching the DeploymentStatus @ 05/22/23 06:23:18.393
  May 22 06:23:18.397: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  May 22 06:23:18.397: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  May 22 06:23:18.399: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  May 22 06:23:18.399: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  May 22 06:23:18.399: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 1
  May 22 06:23:18.399: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:18.399: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:18.399: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 2
  May 22 06:23:18.399: INFO: observed Deployment test-deployment in namespace deployment-9068 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/22/23 06:23:18.399
  May 22 06:23:18.404: INFO: observed event type MODIFIED
  May 22 06:23:18.405: INFO: observed event type MODIFIED
  May 22 06:23:18.405: INFO: observed event type MODIFIED
  May 22 06:23:18.405: INFO: observed event type MODIFIED
  May 22 06:23:18.405: INFO: observed event type MODIFIED
  May 22 06:23:18.405: INFO: observed event type MODIFIED
  May 22 06:23:18.405: INFO: observed event type MODIFIED
  May 22 06:23:18.405: INFO: observed event type MODIFIED
  May 22 06:23:18.405: INFO: observed event type MODIFIED
  May 22 06:23:18.405: INFO: observed event type MODIFIED
  May 22 06:23:18.408: INFO: Log out all the ReplicaSets if there is no deployment created
  May 22 06:23:18.411: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-9068  3e353edd-41f9-4e13-b738-8142dadd4c60 903344 3 2023-05-22 06:23:14 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment edc23df3-b9bc-49e1-9167-12cab005121a 0xc00510d3d7 0xc00510d3d8}] [] [{kube-controller-manager Update apps/v1 2023-05-22 06:23:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"edc23df3-b9bc-49e1-9167-12cab005121a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:23:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00510d460 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 22 06:23:18.413: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-9068  9bcc079a-c697-4494-a86a-a8e136c4b487 903442 4 2023-05-22 06:23:15 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment edc23df3-b9bc-49e1-9167-12cab005121a 0xc00510d4c7 0xc00510d4c8}] [] [{kube-controller-manager Update apps/v1 2023-05-22 06:23:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"edc23df3-b9bc-49e1-9167-12cab005121a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:23:18 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00510d550 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 22 06:23:18.416: INFO: pod: "test-deployment-5b5dcbcd95-ch77c":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-ch77c test-deployment-5b5dcbcd95- deployment-9068  a40b1205-3985-4148-bce7-2cdd739ff8d0 903435 0 2023-05-22 06:23:15 +0000 UTC 2023-05-22 06:23:19 +0000 UTC 0xc00510d8a8 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:45dda5025cf1c0b1e3db9d84a4df0b44168a5152caf663c10f4098f57e690e29 cni.projectcalico.org/podIP:192.168.104.51/32 cni.projectcalico.org/podIPs:192.168.104.51/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 9bcc079a-c697-4494-a86a-a8e136c4b487 0xc00510d8d7 0xc00510d8d8}] [] [{calico Update v1 2023-05-22 06:23:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:23:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9bcc079a-c697-4494-a86a-a8e136c4b487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:23:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.51\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9gx47,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9gx47,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.51,StartTime:2023-05-22 06:23:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:23:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://9a0a698554112aac2c120496de48ce8eaf86990c8ab932e655da135237836405,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.51,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 22 06:23:18.416: INFO: pod: "test-deployment-5b5dcbcd95-mq552":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-mq552 test-deployment-5b5dcbcd95- deployment-9068  4caf64de-e790-4231-8c6a-f44d395d1799 903402 0 2023-05-22 06:23:16 +0000 UTC 2023-05-22 06:23:18 +0000 UTC 0xc00510dac0 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:3eb116e27d649cce40e5adaac5298cb50f85dd278403f51f9fd13af3f289e332 cni.projectcalico.org/podIP:192.168.135.21/32 cni.projectcalico.org/podIPs:192.168.135.21/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 9bcc079a-c697-4494-a86a-a8e136c4b487 0xc00510daf7 0xc00510daf8}] [] [{calico Update v1 2023-05-22 06:23:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:23:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9bcc079a-c697-4494-a86a-a8e136c4b487\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:23:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hjb64,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hjb64,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.21,StartTime:2023-05-22 06:23:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:23:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://0e2af2e03dc14b793e2628ecb3cd6ab894672f3745123e879015a4846ff38a77,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.21,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 22 06:23:18.416: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-9068  d5152a7d-ec55-4c8a-bd26-22237b2c8134 903430 2 2023-05-22 06:23:16 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment edc23df3-b9bc-49e1-9167-12cab005121a 0xc00510d5b7 0xc00510d5b8}] [] [{kube-controller-manager Update apps/v1 2023-05-22 06:23:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"edc23df3-b9bc-49e1-9167-12cab005121a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:23:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00510d640 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  May 22 06:23:18.418: INFO: pod: "test-deployment-6fc78d85c6-9s2xc":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-9s2xc test-deployment-6fc78d85c6- deployment-9068  c210d831-54d3-45e4-8688-4694ea412683 903429 0 2023-05-22 06:23:17 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:a6e4d466248a535e0c0de4dde82d1ab73d50f0ff21b12cd8c236eb97fca15094 cni.projectcalico.org/podIP:192.168.135.24/32 cni.projectcalico.org/podIPs:192.168.135.24/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 d5152a7d-ec55-4c8a-bd26-22237b2c8134 0xc00905ad57 0xc00905ad58}] [] [{calico Update v1 2023-05-22 06:23:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:23:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5152a7d-ec55-4c8a-bd26-22237b2c8134\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:23:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blcq7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blcq7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.24,StartTime:2023-05-22 06:23:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:23:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d1129de1b86e7d26613a070f6893c98898b9ea02f40682214922eef23ff56ec2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.24,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 22 06:23:18.418: INFO: pod: "test-deployment-6fc78d85c6-vxrfn":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-vxrfn test-deployment-6fc78d85c6- deployment-9068  24f9e50e-a343-4773-bf98-01c9d21f97c7 903387 0 2023-05-22 06:23:16 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:1900043bd8526d61a7cd55ac5133fc67d898dca04798b79eb63ca31d7214b599 cni.projectcalico.org/podIP:192.168.104.34/32 cni.projectcalico.org/podIPs:192.168.104.34/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 d5152a7d-ec55-4c8a-bd26-22237b2c8134 0xc00905b047 0xc00905b048}] [] [{calico Update v1 2023-05-22 06:23:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:23:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5152a7d-ec55-4c8a-bd26-22237b2c8134\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:23:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.34\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vg6d5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vg6d5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:23:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.34,StartTime:2023-05-22 06:23:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:23:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://240f51f24f264573ee54db4c02b4113807ddd6b7c24222c321b177538355399b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.34,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 22 06:23:18.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9068" for this suite. @ 05/22/23 06:23:18.422
• [4.261 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 05/22/23 06:23:18.425
  May 22 06:23:18.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:23:18.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:18.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:18.434
  STEP: Creating configMap with name configmap-test-volume-map-a41d86e5-26fb-412a-8481-3ece53bda92d @ 05/22/23 06:23:18.438
  E0522 06:23:18.441679      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test consume configMaps @ 05/22/23 06:23:18.441
  E0522 06:23:19.441943      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:20.442096      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:21.442759      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:22.443168      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:23:22.456
  May 22 06:23:22.458: INFO: Trying to get logs from node node2 pod pod-configmaps-fc1cc81f-cca4-4db1-9a15-4145336b436a container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:23:22.462
  May 22 06:23:22.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8043" for this suite. @ 05/22/23 06:23:22.471
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 05/22/23 06:23:22.475
  May 22 06:23:22.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename runtimeclass @ 05/22/23 06:23:22.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:22.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:22.485
  STEP: Deleting RuntimeClass runtimeclass-3098-delete-me @ 05/22/23 06:23:22.49
  STEP: Waiting for the RuntimeClass to disappear @ 05/22/23 06:23:22.492
  May 22 06:23:22.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3098" for this suite. @ 05/22/23 06:23:22.5
• [0.027 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 05/22/23 06:23:22.503
  May 22 06:23:22.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 06:23:22.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:22.51
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:22.511
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/22/23 06:23:22.513
  E0522 06:23:23.443561      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:24.443953      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:25.444097      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:26.444262      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:23:26.527
  May 22 06:23:26.530: INFO: Trying to get logs from node node2 pod pod-1ab10c86-a0f2-453f-90ea-00c64f668d5d container test-container: <nil>
  STEP: delete the pod @ 05/22/23 06:23:26.533
  May 22 06:23:26.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2398" for this suite. @ 05/22/23 06:23:26.543
• [4.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 05/22/23 06:23:26.547
  May 22 06:23:26.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename endpointslice @ 05/22/23 06:23:26.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:26.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:26.556
  STEP: getting /apis @ 05/22/23 06:23:26.558
  STEP: getting /apis/discovery.k8s.io @ 05/22/23 06:23:26.562
  STEP: getting /apis/discovery.k8s.iov1 @ 05/22/23 06:23:26.563
  STEP: creating @ 05/22/23 06:23:26.564
  STEP: getting @ 05/22/23 06:23:26.571
  STEP: listing @ 05/22/23 06:23:26.572
  STEP: watching @ 05/22/23 06:23:26.574
  May 22 06:23:26.574: INFO: starting watch
  STEP: cluster-wide listing @ 05/22/23 06:23:26.575
  STEP: cluster-wide watching @ 05/22/23 06:23:26.576
  May 22 06:23:26.576: INFO: starting watch
  STEP: patching @ 05/22/23 06:23:26.577
  STEP: updating @ 05/22/23 06:23:26.58
  May 22 06:23:26.584: INFO: waiting for watch events with expected annotations
  May 22 06:23:26.584: INFO: saw patched and updated annotations
  STEP: deleting @ 05/22/23 06:23:26.584
  STEP: deleting a collection @ 05/22/23 06:23:26.588
  May 22 06:23:26.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2016" for this suite. @ 05/22/23 06:23:26.598
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 05/22/23 06:23:26.601
  May 22 06:23:26.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 06:23:26.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:26.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:26.61
  STEP: Setting up server cert @ 05/22/23 06:23:26.622
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 06:23:27.219
  STEP: Deploying the webhook pod @ 05/22/23 06:23:27.223
  STEP: Wait for the deployment to be ready @ 05/22/23 06:23:27.229
  May 22 06:23:27.232: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0522 06:23:27.444950      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:28.445209      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/22/23 06:23:29.241
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 06:23:29.25
  E0522 06:23:29.446307      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:23:30.250: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/22/23 06:23:30.253
  STEP: create a namespace for the webhook @ 05/22/23 06:23:30.266
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/22/23 06:23:30.274
  May 22 06:23:30.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-63" for this suite. @ 05/22/23 06:23:30.314
  STEP: Destroying namespace "webhook-markers-1351" for this suite. @ 05/22/23 06:23:30.317
  STEP: Destroying namespace "fail-closed-namespace-9265" for this suite. @ 05/22/23 06:23:30.32
• [3.722 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 05/22/23 06:23:30.324
  May 22 06:23:30.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 06:23:30.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:30.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:30.333
  STEP: Setting up server cert @ 05/22/23 06:23:30.344
  E0522 06:23:30.447212      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 06:23:30.629
  STEP: Deploying the webhook pod @ 05/22/23 06:23:30.633
  STEP: Wait for the deployment to be ready @ 05/22/23 06:23:30.639
  May 22 06:23:30.642: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0522 06:23:31.447879      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:32.448703      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/22/23 06:23:32.648
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 06:23:32.655
  E0522 06:23:33.449358      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:23:33.655: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/22/23 06:23:33.658
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/22/23 06:23:33.658
  E0522 06:23:34.449522      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/22/23 06:23:34.696
  E0522 06:23:35.450153      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/22/23 06:23:35.703
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/22/23 06:23:35.703
  E0522 06:23:36.450946      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 05/22/23 06:23:36.722
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/22/23 06:23:36.722
  E0522 06:23:37.450887      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:38.451870      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:39.452639      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:40.453141      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:41.453642      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/22/23 06:23:41.743
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/22/23 06:23:41.744
  E0522 06:23:42.454159      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:43.454375      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:44.454617      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:45.454863      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:46.455548      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:23:46.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-430" for this suite. @ 05/22/23 06:23:46.792
  STEP: Destroying namespace "webhook-markers-9160" for this suite. @ 05/22/23 06:23:46.795
• [16.475 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 05/22/23 06:23:46.8
  May 22 06:23:46.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:23:46.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:46.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:46.809
  STEP: Creating configMap with name projected-configmap-test-volume-9e74c598-05e1-43b9-ad22-3ae3d4f37a8f @ 05/22/23 06:23:46.811
  STEP: Creating a pod to test consume configMaps @ 05/22/23 06:23:46.814
  E0522 06:23:47.456569      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:48.456747      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:49.457688      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:50.457842      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:23:50.826
  May 22 06:23:50.828: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-766e1d60-0bd7-43cb-9a5a-279ff17cb127 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:23:50.834
  May 22 06:23:50.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8480" for this suite. @ 05/22/23 06:23:50.846
• [4.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 05/22/23 06:23:50.852
  May 22 06:23:50.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:23:50.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:50.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:50.863
  STEP: Creating secret with name secret-test-beea28ef-c32f-4937-ab96-c989b9999b59 @ 05/22/23 06:23:50.866
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:23:50.869
  E0522 06:23:51.458887      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:52.459287      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:53.459417      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:54.459551      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:23:54.881
  May 22 06:23:54.883: INFO: Trying to get logs from node node2 pod pod-secrets-e4eeb3bb-15e0-4373-95c3-26cf9eced83e container secret-env-test: <nil>
  STEP: delete the pod @ 05/22/23 06:23:54.887
  May 22 06:23:54.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7906" for this suite. @ 05/22/23 06:23:54.898
• [4.048 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 05/22/23 06:23:54.901
  May 22 06:23:54.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename subpath @ 05/22/23 06:23:54.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:23:54.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:23:54.911
  STEP: Setting up data @ 05/22/23 06:23:54.913
  STEP: Creating pod pod-subpath-test-configmap-q9n2 @ 05/22/23 06:23:54.918
  STEP: Creating a pod to test atomic-volume-subpath @ 05/22/23 06:23:54.918
  E0522 06:23:55.459832      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:56.460116      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:57.461188      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:58.461508      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:23:59.461920      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:00.462134      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:01.462876      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:02.463001      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:03.463884      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:04.464756      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:05.465795      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:06.466002      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:07.466935      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:08.467288      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:09.467703      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:10.467926      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:11.468021      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:12.468120      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:13.468191      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:14.468593      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:15.469413      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:16.469754      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:17.470503      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:18.470948      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:24:18.964
  May 22 06:24:18.965: INFO: Trying to get logs from node node2 pod pod-subpath-test-configmap-q9n2 container test-container-subpath-configmap-q9n2: <nil>
  STEP: delete the pod @ 05/22/23 06:24:18.971
  STEP: Deleting pod pod-subpath-test-configmap-q9n2 @ 05/22/23 06:24:18.977
  May 22 06:24:18.978: INFO: Deleting pod "pod-subpath-test-configmap-q9n2" in namespace "subpath-1432"
  May 22 06:24:18.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1432" for this suite. @ 05/22/23 06:24:18.982
• [24.083 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 05/22/23 06:24:18.985
  May 22 06:24:18.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename init-container @ 05/22/23 06:24:18.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:24:18.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:24:18.993
  STEP: creating the pod @ 05/22/23 06:24:18.995
  May 22 06:24:18.995: INFO: PodSpec: initContainers in spec.initContainers
  E0522 06:24:19.471186      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:20.472082      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:21.472286      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:22.472756      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:24:22.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1353" for this suite. @ 05/22/23 06:24:22.791
• [3.810 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 05/22/23 06:24:22.795
  May 22 06:24:22.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:24:22.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:24:22.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:24:22.803
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:24:22.805
  E0522 06:24:23.473661      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:24.474406      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:25.475024      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:26.475283      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:24:26.818
  May 22 06:24:26.820: INFO: Trying to get logs from node node2 pod downwardapi-volume-f269f96a-cc60-4c04-8179-7162711f5366 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:24:26.824
  May 22 06:24:26.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8292" for this suite. @ 05/22/23 06:24:26.833
• [4.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 05/22/23 06:24:26.837
  May 22 06:24:26.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:24:26.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:24:26.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:24:26.846
  STEP: Creating projection with secret that has name secret-emptykey-test-a241b9fe-abf3-402f-b992-1982ce55c539 @ 05/22/23 06:24:26.847
  May 22 06:24:26.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7648" for this suite. @ 05/22/23 06:24:26.851
• [0.016 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 05/22/23 06:24:26.853
  May 22 06:24:26.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 06:24:26.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:24:26.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:24:26.862
  STEP: Counting existing ResourceQuota @ 05/22/23 06:24:26.864
  E0522 06:24:27.475986      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:28.476825      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:29.477481      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:30.477716      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:31.478370      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/22/23 06:24:31.866
  STEP: Ensuring resource quota status is calculated @ 05/22/23 06:24:31.869
  E0522 06:24:32.479231      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:33.480504      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 05/22/23 06:24:33.872
  STEP: Ensuring resource quota status captures replication controller creation @ 05/22/23 06:24:33.88
  E0522 06:24:34.480604      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:35.480726      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 05/22/23 06:24:35.884
  STEP: Ensuring resource quota status released usage @ 05/22/23 06:24:35.889
  E0522 06:24:36.481236      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:37.481802      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:24:37.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8165" for this suite. @ 05/22/23 06:24:37.894
• [11.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 05/22/23 06:24:37.899
  May 22 06:24:37.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename gc @ 05/22/23 06:24:37.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:24:37.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:24:37.908
  STEP: create the deployment @ 05/22/23 06:24:37.911
  W0522 06:24:37.913764      25 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/22/23 06:24:37.913
  STEP: delete the deployment @ 05/22/23 06:24:38.421
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/22/23 06:24:38.424
  E0522 06:24:38.482046      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/22/23 06:24:38.934
  May 22 06:24:39.004: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 22 06:24:39.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3515" for this suite. @ 05/22/23 06:24:39.007
• [1.112 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 05/22/23 06:24:39.011
  May 22 06:24:39.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 06:24:39.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:24:39.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:24:39.02
  STEP: Setting up server cert @ 05/22/23 06:24:39.032
  E0522 06:24:39.482748      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 06:24:39.87
  STEP: Deploying the webhook pod @ 05/22/23 06:24:39.875
  STEP: Wait for the deployment to be ready @ 05/22/23 06:24:39.882
  May 22 06:24:39.885: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0522 06:24:40.483458      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:41.483687      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/22/23 06:24:41.892
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 06:24:41.899
  E0522 06:24:42.483817      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:24:42.900: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/22/23 06:24:42.933
  STEP: Creating a configMap that should be mutated @ 05/22/23 06:24:42.943
  STEP: Deleting the collection of validation webhooks @ 05/22/23 06:24:42.961
  STEP: Creating a configMap that should not be mutated @ 05/22/23 06:24:42.98
  May 22 06:24:42.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2744" for this suite. @ 05/22/23 06:24:43.007
  STEP: Destroying namespace "webhook-markers-1719" for this suite. @ 05/22/23 06:24:43.01
• [4.009 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/22/23 06:24:43.022
  May 22 06:24:43.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename watch @ 05/22/23 06:24:43.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:24:43.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:24:43.032
  STEP: getting a starting resourceVersion @ 05/22/23 06:24:43.033
  STEP: starting a background goroutine to produce watch events @ 05/22/23 06:24:43.035
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/22/23 06:24:43.035
  E0522 06:24:43.484670      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:44.485763      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:45.485714      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:24:45.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8386" for this suite. @ 05/22/23 06:24:45.876
• [2.905 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/22/23 06:24:45.928
  May 22 06:24:45.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:24:45.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:24:45.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:24:45.94
  STEP: Creating projection with secret that has name projected-secret-test-5908ef24-736e-4801-a5c3-1e53810f2ab0 @ 05/22/23 06:24:45.941
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:24:45.944
  E0522 06:24:46.486427      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:47.486746      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:48.487785      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:49.488679      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:24:49.955
  May 22 06:24:49.957: INFO: Trying to get logs from node node2 pod pod-projected-secrets-dde6f1bb-4856-4429-98c2-4855d5720eb9 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 06:24:49.961
  May 22 06:24:49.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8870" for this suite. @ 05/22/23 06:24:49.971
• [4.047 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/22/23 06:24:49.975
  May 22 06:24:49.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename containers @ 05/22/23 06:24:49.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:24:49.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:24:49.984
  STEP: Creating a pod to test override all @ 05/22/23 06:24:49.985
  E0522 06:24:50.489733      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:51.490126      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:52.491076      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:53.491661      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:24:53.998
  May 22 06:24:54.000: INFO: Trying to get logs from node node2 pod client-containers-55af5b2e-019f-4b16-bcf3-98d580c4b1c3 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:24:54.004
  May 22 06:24:54.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8577" for this suite. @ 05/22/23 06:24:54.013
• [4.042 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/22/23 06:24:54.017
  May 22 06:24:54.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename cronjob @ 05/22/23 06:24:54.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:24:54.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:24:54.028
  STEP: Creating a cronjob @ 05/22/23 06:24:54.03
  STEP: Ensuring more than one job is running at a time @ 05/22/23 06:24:54.033
  E0522 06:24:54.492193      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:55.492724      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:56.492880      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:57.493123      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:58.493900      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:24:59.494832      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:00.495772      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:01.496028      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:02.496626      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:03.496848      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:04.497627      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:05.497891      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:06.498762      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:07.499016      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:08.499111      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:09.500029      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:10.500510      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:11.500908      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:12.501836      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:13.502000      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:14.502374      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:15.502641      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:16.503596      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:17.503906      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:18.504847      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:19.505790      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:20.506539      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:21.507622      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:22.508007      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:23.508194      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:24.508665      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:25.508854      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:26.509574      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:27.510595      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:28.511182      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:29.512265      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:30.512746      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:31.513123      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:32.514027      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:33.514243      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:34.514868      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:35.515281      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:36.515930      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:37.516168      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:38.517113      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:39.517270      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:40.518186      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:41.518459      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:42.518868      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:43.519125      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:44.519784      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:45.519977      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:46.520256      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:47.520458      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:48.521086      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:49.521603      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:50.522079      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:51.522270      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:52.522865      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:53.523668      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:54.524606      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:55.525153      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:56.526020      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:57.526226      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:58.526716      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:25:59.527571      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:00.528321      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:01.528596      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/22/23 06:26:02.038
  STEP: Removing cronjob @ 05/22/23 06:26:02.04
  May 22 06:26:02.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8473" for this suite. @ 05/22/23 06:26:02.047
• [68.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 05/22/23 06:26:02.052
  May 22 06:26:02.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename controllerrevisions @ 05/22/23 06:26:02.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:26:02.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:26:02.063
  STEP: Creating DaemonSet "e2e-thkdb-daemon-set" @ 05/22/23 06:26:02.076
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/22/23 06:26:02.08
  May 22 06:26:02.084: INFO: Number of nodes with available pods controlled by daemonset e2e-thkdb-daemon-set: 0
  May 22 06:26:02.084: INFO: Node node1 is running 0 daemon pod, expected 1
  E0522 06:26:02.529144      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:26:03.090: INFO: Number of nodes with available pods controlled by daemonset e2e-thkdb-daemon-set: 1
  May 22 06:26:03.090: INFO: Node node1 is running 0 daemon pod, expected 1
  E0522 06:26:03.529909      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:26:04.092: INFO: Number of nodes with available pods controlled by daemonset e2e-thkdb-daemon-set: 2
  May 22 06:26:04.092: INFO: Node node1 is running 0 daemon pod, expected 1
  E0522 06:26:04.530607      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:26:05.090: INFO: Number of nodes with available pods controlled by daemonset e2e-thkdb-daemon-set: 3
  May 22 06:26:05.090: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-thkdb-daemon-set
  STEP: Confirm DaemonSet "e2e-thkdb-daemon-set" successfully created with "daemonset-name=e2e-thkdb-daemon-set" label @ 05/22/23 06:26:05.091
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-thkdb-daemon-set" @ 05/22/23 06:26:05.094
  May 22 06:26:05.097: INFO: Located ControllerRevision: "e2e-thkdb-daemon-set-7b6cb949fd"
  STEP: Patching ControllerRevision "e2e-thkdb-daemon-set-7b6cb949fd" @ 05/22/23 06:26:05.099
  May 22 06:26:05.102: INFO: e2e-thkdb-daemon-set-7b6cb949fd has been patched
  STEP: Create a new ControllerRevision @ 05/22/23 06:26:05.102
  May 22 06:26:05.104: INFO: Created ControllerRevision: e2e-thkdb-daemon-set-68d9bc475b
  STEP: Confirm that there are two ControllerRevisions @ 05/22/23 06:26:05.104
  May 22 06:26:05.104: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 22 06:26:05.106: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-thkdb-daemon-set-7b6cb949fd" @ 05/22/23 06:26:05.106
  STEP: Confirm that there is only one ControllerRevision @ 05/22/23 06:26:05.108
  May 22 06:26:05.108: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 22 06:26:05.110: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-thkdb-daemon-set-68d9bc475b" @ 05/22/23 06:26:05.111
  May 22 06:26:05.116: INFO: e2e-thkdb-daemon-set-68d9bc475b has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/22/23 06:26:05.116
  W0522 06:26:05.119587      25 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/22/23 06:26:05.119
  May 22 06:26:05.119: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0522 06:26:05.531561      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:26:06.121: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 22 06:26:06.123: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-thkdb-daemon-set-68d9bc475b=updated" @ 05/22/23 06:26:06.123
  STEP: Confirm that there is only one ControllerRevision @ 05/22/23 06:26:06.126
  May 22 06:26:06.126: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 22 06:26:06.128: INFO: Found 1 ControllerRevisions
  May 22 06:26:06.129: INFO: ControllerRevision "e2e-thkdb-daemon-set-7bc95c9945" has revision 3
  STEP: Deleting DaemonSet "e2e-thkdb-daemon-set" @ 05/22/23 06:26:06.13
  STEP: deleting DaemonSet.extensions e2e-thkdb-daemon-set in namespace controllerrevisions-5652, will wait for the garbage collector to delete the pods @ 05/22/23 06:26:06.131
  May 22 06:26:06.185: INFO: Deleting DaemonSet.extensions e2e-thkdb-daemon-set took: 3.411195ms
  May 22 06:26:06.286: INFO: Terminating DaemonSet.extensions e2e-thkdb-daemon-set pods took: 100.346848ms
  E0522 06:26:06.532136      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:07.533097      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:26:07.989: INFO: Number of nodes with available pods controlled by daemonset e2e-thkdb-daemon-set: 0
  May 22 06:26:07.989: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-thkdb-daemon-set
  May 22 06:26:07.991: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"905031"},"items":null}

  May 22 06:26:07.992: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"905031"},"items":null}

  May 22 06:26:08.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-5652" for this suite. @ 05/22/23 06:26:08.002
• [5.953 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 05/22/23 06:26:08.005
  May 22 06:26:08.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/22/23 06:26:08.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:26:08.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:26:08.015
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/22/23 06:26:08.017
  May 22 06:26:08.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  E0522 06:26:08.533421      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:26:09.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  E0522 06:26:09.533490      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:10.534315      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:11.534330      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:12.535036      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:13.536071      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:14.536413      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:15.536771      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:26:15.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8001" for this suite. @ 05/22/23 06:26:15.864
• [7.862 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/22/23 06:26:15.869
  May 22 06:26:15.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-runtime @ 05/22/23 06:26:15.869
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:26:15.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:26:15.879
  STEP: create the container @ 05/22/23 06:26:15.88
  W0522 06:26:15.885164      25 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/22/23 06:26:15.885
  E0522 06:26:16.537734      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:17.538762      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:18.539677      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/22/23 06:26:18.895
  STEP: the container should be terminated @ 05/22/23 06:26:18.897
  STEP: the termination message should be set @ 05/22/23 06:26:18.897
  May 22 06:26:18.897: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/22/23 06:26:18.897
  May 22 06:26:18.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7563" for this suite. @ 05/22/23 06:26:18.908
• [3.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 05/22/23 06:26:18.912
  May 22 06:26:18.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename statefulset @ 05/22/23 06:26:18.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:26:18.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:26:18.921
  STEP: Creating service test in namespace statefulset-5825 @ 05/22/23 06:26:18.923
  STEP: Creating a new StatefulSet @ 05/22/23 06:26:18.925
  May 22 06:26:18.931: INFO: Found 0 stateful pods, waiting for 3
  E0522 06:26:19.540733      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:20.540840      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:21.541116      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:22.541475      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:23.541631      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:24.541978      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:25.542355      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:26.542525      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:27.542939      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:28.543312      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:26:28.936: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 22 06:26:28.936: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 22 06:26:28.936: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  May 22 06:26:28.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5825 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 22 06:26:29.104: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 22 06:26:29.104: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 22 06:26:29.104: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0522 06:26:29.544219      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:30.544433      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:31.544641      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:32.544844      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:33.545027      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:34.545351      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:35.545490      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:36.545903      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:37.546097      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:38.546607      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/22/23 06:26:39.114
  May 22 06:26:39.130: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/22/23 06:26:39.13
  E0522 06:26:39.547474      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:40.547586      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:41.547778      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:42.547989      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:43.548200      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:44.548807      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:45.549037      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:46.549218      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:47.549332      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:48.550283      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 05/22/23 06:26:49.139
  May 22 06:26:49.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5825 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 22 06:26:49.270: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 22 06:26:49.270: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 22 06:26:49.270: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0522 06:26:49.551067      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:50.551173      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:51.551962      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:52.552317      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:53.552450      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:54.552581      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:55.552700      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:56.552851      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:57.553091      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:26:58.553409      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 05/22/23 06:26:59.283
  May 22 06:26:59.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5825 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 22 06:26:59.406: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 22 06:26:59.406: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 22 06:26:59.406: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0522 06:26:59.554400      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:00.555300      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:01.555532      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:02.555827      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:03.556040      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:04.556331      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:05.556511      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:06.556692      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:07.556876      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:08.557109      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:27:09.430: INFO: Updating stateful set ss2
  E0522 06:27:09.557744      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:10.558120      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:11.558509      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:12.558777      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:13.558958      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:14.559229      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:15.559416      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:16.559733      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:17.559904      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:18.560303      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 05/22/23 06:27:19.44
  May 22 06:27:19.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=statefulset-5825 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0522 06:27:19.560961      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:27:19.572: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 22 06:27:19.572: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 22 06:27:19.572: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0522 06:27:20.561016      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:21.561191      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:22.561655      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:23.561911      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:24.562765      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:25.564997      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:26.565329      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:27.565469      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:28.565768      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:29.566415      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:27:29.587: INFO: Deleting all statefulset in ns statefulset-5825
  May 22 06:27:29.588: INFO: Scaling statefulset ss2 to 0
  E0522 06:27:30.567004      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:31.567225      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:32.567513      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:33.567935      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:34.568279      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:35.568623      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:36.568874      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:37.569294      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:38.569639      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:39.570513      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:27:39.598: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 06:27:39.600: INFO: Deleting statefulset ss2
  May 22 06:27:39.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5825" for this suite. @ 05/22/23 06:27:39.609
• [80.700 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 05/22/23 06:27:39.613
  May 22 06:27:39.613: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-probe @ 05/22/23 06:27:39.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:27:39.621
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:27:39.623
  STEP: Creating pod test-webserver-6583aa1b-829e-46f8-9104-521b0c2d6426 in namespace container-probe-2309 @ 05/22/23 06:27:39.625
  E0522 06:27:40.570797      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:41.571009      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:27:41.636: INFO: Started pod test-webserver-6583aa1b-829e-46f8-9104-521b0c2d6426 in namespace container-probe-2309
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/22/23 06:27:41.636
  May 22 06:27:41.638: INFO: Initial restart count of pod test-webserver-6583aa1b-829e-46f8-9104-521b0c2d6426 is 0
  E0522 06:27:42.571675      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:43.571853      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:44.572841      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:45.573062      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:46.573746      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:47.574065      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:48.575133      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:49.575679      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:50.575888      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:51.576257      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:52.577341      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:53.577584      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:54.577831      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:55.578088      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:56.578610      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:57.578813      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:58.579859      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:27:59.580709      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:00.581436      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:01.581791      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:02.582524      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:03.583147      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:04.584138      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:05.584378      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:06.584922      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:07.585401      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:08.585759      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:09.586587      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:10.586763      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:11.586993      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:12.587431      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:13.587636      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:14.588565      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:15.588823      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:16.589414      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:17.589618      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:18.590629      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:19.590874      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:20.591404      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:21.592264      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:22.592751      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:23.593049      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:24.593699      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:25.594891      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:26.594928      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:27.595222      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:28.595468      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:29.596066      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:30.596241      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:31.596433      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:32.597192      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:33.597291      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:34.597375      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:35.597558      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:36.598313      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:37.598635      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:38.598707      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:39.599361      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:40.599638      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:41.600487      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:42.600640      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:43.600873      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:44.601913      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:45.602085      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:46.602775      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:47.603111      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:48.603403      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:49.603969      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:50.604090      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:51.604335      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:52.605083      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:53.605259      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:54.605340      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:55.605480      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:56.605631      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:57.605867      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:58.606099      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:28:59.606605      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:00.607633      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:01.607752      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:02.607919      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:03.608040      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:04.608903      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:05.609066      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:06.609755      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:07.609881      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:08.610763      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:09.611312      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:10.612145      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:11.612317      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:12.612833      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:13.613026      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:14.613800      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:15.613953      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:16.614149      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:17.614286      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:18.614437      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:19.614568      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:20.614721      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:21.614830      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:22.614991      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:23.615063      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:24.615852      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:25.616042      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:26.616223      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:27.616421      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:28.616858      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:29.617267      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:30.617328      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:31.617485      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:32.617632      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:33.617767      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:34.618559      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:35.618768      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:36.618899      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:37.619224      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:38.619360      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:39.620167      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:40.621191      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:41.621389      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:42.621760      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:43.621992      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:44.622988      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:45.623195      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:46.623876      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:47.624101      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:48.625102      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:49.625565      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:50.626094      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:51.626285      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:52.626912      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:53.627113      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:54.627575      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:55.627770      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:56.628059      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:57.628258      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:58.628372      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:29:59.628763      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:00.629027      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:01.629221      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:02.629361      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:03.629433      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:04.630069      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:05.630569      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:06.631138      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:07.631448      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:08.632313      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:09.633027      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:10.633251      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:11.633569      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:12.634571      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:13.634822      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:14.634960      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:15.635242      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:16.635832      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:17.636249      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:18.637035      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:19.637783      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:20.637919      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:21.638268      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:22.639081      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:23.639365      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:24.640440      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:25.640767      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:26.641430      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:27.641635      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:28.641804      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:29.642481      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:30.642764      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:31.643854      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:32.644348      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:33.644606      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:34.645246      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:35.645546      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:36.646457      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:37.646683      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:38.647491      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:39.648272      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:40.648493      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:41.648693      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:42.649008      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:43.649210      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:44.649931      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:45.650147      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:46.650538      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:47.650761      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:48.651347      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:49.651834      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:50.652312      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:51.652544      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:52.652806      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:53.652977      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:54.653653      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:55.653810      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:56.654310      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:57.654504      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:58.655016      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:30:59.655506      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:00.655590      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:01.655743      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:02.655823      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:03.656019      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:04.656821      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:05.657018      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:06.657547      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:07.657768      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:08.658802      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:09.659358      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:10.660125      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:11.660389      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:12.660530      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:13.660710      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:14.661720      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:15.661874      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:16.662138      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:17.662319      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:18.663237      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:19.663656      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:20.663853      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:21.664113      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:22.664821      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:23.665749      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:24.666585      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:25.666871      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:26.667794      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:27.667954      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:28.668517      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:29.669152      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:30.669155      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:31.669633      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:32.669889      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:33.670117      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:34.671061      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:35.671886      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:36.672595      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:37.673020      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:38.673742      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:39.674543      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:40.674975      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:41.675238      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:31:42.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 06:31:42.027
  STEP: Destroying namespace "container-probe-2309" for this suite. @ 05/22/23 06:31:42.034
• [242.424 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 05/22/23 06:31:42.037
  May 22 06:31:42.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/22/23 06:31:42.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:31:42.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:31:42.049
  May 22 06:31:42.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  E0522 06:31:42.675481      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/22/23 06:31:43.463
  May 22 06:31:43.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 --namespace=crd-publish-openapi-7888 create -f -'
  E0522 06:31:43.675854      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:31:44.350: INFO: stderr: ""
  May 22 06:31:44.350: INFO: stdout: "e2e-test-crd-publish-openapi-435-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 22 06:31:44.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 --namespace=crd-publish-openapi-7888 delete e2e-test-crd-publish-openapi-435-crds test-foo'
  May 22 06:31:44.412: INFO: stderr: ""
  May 22 06:31:44.412: INFO: stdout: "e2e-test-crd-publish-openapi-435-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  May 22 06:31:44.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 --namespace=crd-publish-openapi-7888 apply -f -'
  May 22 06:31:44.647: INFO: stderr: ""
  May 22 06:31:44.647: INFO: stdout: "e2e-test-crd-publish-openapi-435-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 22 06:31:44.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 --namespace=crd-publish-openapi-7888 delete e2e-test-crd-publish-openapi-435-crds test-foo'
  E0522 06:31:44.676301      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:31:44.711: INFO: stderr: ""
  May 22 06:31:44.711: INFO: stdout: "e2e-test-crd-publish-openapi-435-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/22/23 06:31:44.711
  May 22 06:31:44.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 --namespace=crd-publish-openapi-7888 create -f -'
  May 22 06:31:44.950: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/22/23 06:31:44.95
  May 22 06:31:44.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 --namespace=crd-publish-openapi-7888 create -f -'
  May 22 06:31:45.194: INFO: rc: 1
  May 22 06:31:45.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 --namespace=crd-publish-openapi-7888 apply -f -'
  May 22 06:31:45.422: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/22/23 06:31:45.422
  May 22 06:31:45.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 --namespace=crd-publish-openapi-7888 create -f -'
  May 22 06:31:45.655: INFO: rc: 1
  May 22 06:31:45.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 --namespace=crd-publish-openapi-7888 apply -f -'
  E0522 06:31:45.677192      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:31:45.889: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/22/23 06:31:45.889
  May 22 06:31:45.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 explain e2e-test-crd-publish-openapi-435-crds'
  May 22 06:31:46.117: INFO: stderr: ""
  May 22 06:31:46.117: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-435-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/22/23 06:31:46.117
  May 22 06:31:46.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 explain e2e-test-crd-publish-openapi-435-crds.metadata'
  May 22 06:31:46.352: INFO: stderr: ""
  May 22 06:31:46.352: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-435-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  May 22 06:31:46.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 explain e2e-test-crd-publish-openapi-435-crds.spec'
  May 22 06:31:46.595: INFO: stderr: ""
  May 22 06:31:46.595: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-435-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  May 22 06:31:46.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 explain e2e-test-crd-publish-openapi-435-crds.spec.bars'
  E0522 06:31:46.677984      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:31:46.822: INFO: stderr: ""
  May 22 06:31:46.822: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-435-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/22/23 06:31:46.822
  May 22 06:31:46.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=crd-publish-openapi-7888 explain e2e-test-crd-publish-openapi-435-crds.spec.bars2'
  May 22 06:31:47.067: INFO: rc: 1
  E0522 06:31:47.678004      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:31:48.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7888" for this suite. @ 05/22/23 06:31:48.451
• [6.417 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 05/22/23 06:31:48.454
  May 22 06:31:48.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:31:48.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:31:48.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:31:48.465
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:31:48.467
  E0522 06:31:48.679099      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:49.679755      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:50.680434      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:51.680572      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:31:52.48
  May 22 06:31:52.482: INFO: Trying to get logs from node node2 pod downwardapi-volume-05c5cdf1-a992-4718-98cd-0301663a7fe1 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:31:52.495
  May 22 06:31:52.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9009" for this suite. @ 05/22/23 06:31:52.507
• [4.056 seconds]
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 05/22/23 06:31:52.511
  May 22 06:31:52.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-probe @ 05/22/23 06:31:52.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:31:52.52
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:31:52.521
  STEP: Creating pod busybox-b3ff35a8-446d-452b-93d3-73a336e06c2a in namespace container-probe-7130 @ 05/22/23 06:31:52.523
  E0522 06:31:52.681269      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:53.681531      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:31:54.533: INFO: Started pod busybox-b3ff35a8-446d-452b-93d3-73a336e06c2a in namespace container-probe-7130
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/22/23 06:31:54.533
  May 22 06:31:54.534: INFO: Initial restart count of pod busybox-b3ff35a8-446d-452b-93d3-73a336e06c2a is 0
  E0522 06:31:54.681728      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:55.681868      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:56.682493      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:57.682715      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:58.682945      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:31:59.683667      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:00.684153      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:01.684385      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:02.685403      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:03.685606      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:04.685755      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:05.685885      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:06.686343      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:07.686911      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:08.687205      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:09.687384      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:10.687783      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:11.688016      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:12.688414      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:13.688775      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:14.689488      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:15.690089      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:16.690346      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:17.690731      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:18.691696      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:19.692180      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:20.692270      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:21.692467      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:22.693274      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:23.693490      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:24.694099      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:25.694558      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:26.694543      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:27.694803      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:28.695426      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:29.696189      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:30.696889      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:31.697198      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:32.697913      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:33.698184      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:34.698986      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:35.699053      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:36.699542      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:37.700138      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:38.701120      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:39.701964      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:40.702801      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:41.703154      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:42.703982      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:43.704471      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:44.705421      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:45.705627      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:46.705744      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:47.705948      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:48.707097      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:49.707992      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:50.708208      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:51.708462      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:52.708987      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:53.709406      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:54.709511      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:55.709733      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:56.710498      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:57.710845      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:58.711468      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:32:59.711618      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:00.712469      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:01.712696      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:02.713747      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:03.714172      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:04.715237      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:05.715431      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:06.715517      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:07.715719      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:08.716024      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:09.716160      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:10.716409      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:11.716513      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:12.717105      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:13.717248      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:14.717261      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:15.717496      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:16.718433      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:17.718684      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:18.719464      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:19.719602      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:20.720535      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:21.720627      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:22.721013      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:23.721248      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:24.721510      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:25.721694      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:26.722423      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:27.722627      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:28.722870      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:29.722996      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:30.723823      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:31.724014      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:32.725105      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:33.725309      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:34.726390      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:35.726577      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:36.727547      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:37.727819      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:38.728622      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:39.728550      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:40.729420      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:41.729823      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:42.730439      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:43.730640      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:44.731661      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:45.731859      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:46.732765      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:47.732967      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:48.733558      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:49.734273      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:50.734541      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:51.734784      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:52.735869      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:53.736136      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:54.736924      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:55.737124      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:56.737595      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:57.737792      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:58.738859      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:33:59.739551      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:00.739988      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:01.740092      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:02.740299      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:03.740800      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:04.741144      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:05.741436      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:06.741601      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:07.742482      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:08.742688      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:09.743571      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:10.743715      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:11.743858      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:12.743970      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:13.744145      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:14.744578      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:15.745320      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:16.745446      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:17.745627      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:18.746061      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:19.746060      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:20.746361      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:21.746601      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:22.746958      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:23.748004      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:24.748381      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:25.749064      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:26.749287      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:27.749981      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:28.750375      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:29.750404      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:30.750591      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:31.751198      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:32.751421      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:33.752086      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:34.752743      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:35.752845      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:36.752992      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:37.753959      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:38.754222      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:39.754795      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:40.755058      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:41.755705      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:42.755952      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:43.756154      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:44.756238      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:45.756565      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:46.756795      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:47.757366      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:48.757629      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:49.757726      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:50.757942      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:51.758785      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:52.759774      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:53.760770      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:54.761154      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:55.761686      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:56.761899      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:57.762403      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:58.762883      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:34:59.763820      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:00.764049      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:01.764108      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:02.764353      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:03.764498      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:04.764677      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:05.765482      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:06.765702      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:07.765810      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:08.766302      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:09.766450      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:10.766672      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:11.766710      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:12.766792      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:13.766905      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:14.766938      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:15.767036      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:16.767167      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:17.767286      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:18.767391      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:19.768306      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:20.768531      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:21.769371      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:22.769566      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:23.770647      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:24.770958      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:25.771321      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:26.771522      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:27.771824      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:28.771888      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:29.772988      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:30.773300      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:31.773421      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:32.773630      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:33.774441      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:34.774826      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:35.775352      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:36.775996      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:37.776656      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:38.776801      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:39.777827      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:40.778017      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:41.778874      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:42.779089      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:43.779972      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:44.780269      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:45.781068      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:46.781142      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:47.781194      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:48.781281      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:49.781483      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:50.781846      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:51.782163      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:52.782292      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:53.783059      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:54.783624      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:35:54.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 06:35:54.926
  STEP: Destroying namespace "container-probe-7130" for this suite. @ 05/22/23 06:35:54.933
• [242.426 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/22/23 06:35:54.937
  May 22 06:35:54.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 06:35:54.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:35:54.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:35:54.946
  STEP: creating the pod @ 05/22/23 06:35:54.948
  STEP: setting up watch @ 05/22/23 06:35:54.948
  STEP: submitting the pod to kubernetes @ 05/22/23 06:35:55.05
  STEP: verifying the pod is in kubernetes @ 05/22/23 06:35:55.055
  STEP: verifying pod creation was observed @ 05/22/23 06:35:55.057
  E0522 06:35:55.784241      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:56.784382      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 05/22/23 06:35:57.063
  STEP: verifying pod deletion was observed @ 05/22/23 06:35:57.067
  E0522 06:35:57.784894      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:35:58.785851      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:35:59.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3164" for this suite. @ 05/22/23 06:35:59.026
• [4.092 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 05/22/23 06:35:59.029
  May 22 06:35:59.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename discovery @ 05/22/23 06:35:59.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:35:59.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:35:59.037
  STEP: Setting up server cert @ 05/22/23 06:35:59.04
  May 22 06:35:59.683: INFO: Checking APIGroup: apiregistration.k8s.io
  May 22 06:35:59.684: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  May 22 06:35:59.684: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  May 22 06:35:59.684: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  May 22 06:35:59.684: INFO: Checking APIGroup: apps
  May 22 06:35:59.685: INFO: PreferredVersion.GroupVersion: apps/v1
  May 22 06:35:59.685: INFO: Versions found [{apps/v1 v1}]
  May 22 06:35:59.685: INFO: apps/v1 matches apps/v1
  May 22 06:35:59.685: INFO: Checking APIGroup: events.k8s.io
  May 22 06:35:59.686: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  May 22 06:35:59.686: INFO: Versions found [{events.k8s.io/v1 v1}]
  May 22 06:35:59.686: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  May 22 06:35:59.686: INFO: Checking APIGroup: authentication.k8s.io
  May 22 06:35:59.687: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  May 22 06:35:59.687: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  May 22 06:35:59.687: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  May 22 06:35:59.687: INFO: Checking APIGroup: authorization.k8s.io
  May 22 06:35:59.688: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  May 22 06:35:59.688: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  May 22 06:35:59.688: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  May 22 06:35:59.688: INFO: Checking APIGroup: autoscaling
  May 22 06:35:59.689: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  May 22 06:35:59.689: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  May 22 06:35:59.689: INFO: autoscaling/v2 matches autoscaling/v2
  May 22 06:35:59.689: INFO: Checking APIGroup: batch
  May 22 06:35:59.689: INFO: PreferredVersion.GroupVersion: batch/v1
  May 22 06:35:59.689: INFO: Versions found [{batch/v1 v1}]
  May 22 06:35:59.689: INFO: batch/v1 matches batch/v1
  May 22 06:35:59.690: INFO: Checking APIGroup: certificates.k8s.io
  May 22 06:35:59.690: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  May 22 06:35:59.690: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  May 22 06:35:59.690: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  May 22 06:35:59.690: INFO: Checking APIGroup: networking.k8s.io
  May 22 06:35:59.691: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  May 22 06:35:59.691: INFO: Versions found [{networking.k8s.io/v1 v1}]
  May 22 06:35:59.691: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  May 22 06:35:59.691: INFO: Checking APIGroup: policy
  May 22 06:35:59.692: INFO: PreferredVersion.GroupVersion: policy/v1
  May 22 06:35:59.692: INFO: Versions found [{policy/v1 v1}]
  May 22 06:35:59.692: INFO: policy/v1 matches policy/v1
  May 22 06:35:59.692: INFO: Checking APIGroup: rbac.authorization.k8s.io
  May 22 06:35:59.693: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  May 22 06:35:59.693: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  May 22 06:35:59.693: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  May 22 06:35:59.693: INFO: Checking APIGroup: storage.k8s.io
  May 22 06:35:59.693: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  May 22 06:35:59.693: INFO: Versions found [{storage.k8s.io/v1 v1}]
  May 22 06:35:59.693: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  May 22 06:35:59.693: INFO: Checking APIGroup: admissionregistration.k8s.io
  May 22 06:35:59.694: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  May 22 06:35:59.694: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  May 22 06:35:59.694: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  May 22 06:35:59.694: INFO: Checking APIGroup: apiextensions.k8s.io
  May 22 06:35:59.695: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  May 22 06:35:59.695: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  May 22 06:35:59.695: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  May 22 06:35:59.695: INFO: Checking APIGroup: scheduling.k8s.io
  May 22 06:35:59.696: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  May 22 06:35:59.696: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  May 22 06:35:59.696: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  May 22 06:35:59.696: INFO: Checking APIGroup: coordination.k8s.io
  May 22 06:35:59.696: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  May 22 06:35:59.696: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  May 22 06:35:59.696: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  May 22 06:35:59.696: INFO: Checking APIGroup: node.k8s.io
  May 22 06:35:59.697: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  May 22 06:35:59.697: INFO: Versions found [{node.k8s.io/v1 v1}]
  May 22 06:35:59.697: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  May 22 06:35:59.697: INFO: Checking APIGroup: discovery.k8s.io
  May 22 06:35:59.698: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  May 22 06:35:59.698: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  May 22 06:35:59.698: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  May 22 06:35:59.698: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  May 22 06:35:59.699: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  May 22 06:35:59.699: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  May 22 06:35:59.699: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  May 22 06:35:59.699: INFO: Checking APIGroup: projectcalico.org
  May 22 06:35:59.699: INFO: PreferredVersion.GroupVersion: projectcalico.org/v3
  May 22 06:35:59.699: INFO: Versions found [{projectcalico.org/v3 v3}]
  May 22 06:35:59.699: INFO: projectcalico.org/v3 matches projectcalico.org/v3
  May 22 06:35:59.699: INFO: Checking APIGroup: crd.projectcalico.org
  May 22 06:35:59.700: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
  May 22 06:35:59.700: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
  May 22 06:35:59.700: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
  May 22 06:35:59.700: INFO: Checking APIGroup: operator.tigera.io
  May 22 06:35:59.701: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
  May 22 06:35:59.701: INFO: Versions found [{operator.tigera.io/v1 v1}]
  May 22 06:35:59.701: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
  May 22 06:35:59.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-5525" for this suite. @ 05/22/23 06:35:59.704
• [0.678 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 05/22/23 06:35:59.71
  May 22 06:35:59.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename cronjob @ 05/22/23 06:35:59.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:35:59.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:35:59.72
  STEP: Creating a cronjob @ 05/22/23 06:35:59.722
  STEP: creating @ 05/22/23 06:35:59.722
  STEP: getting @ 05/22/23 06:35:59.724
  STEP: listing @ 05/22/23 06:35:59.726
  STEP: watching @ 05/22/23 06:35:59.727
  May 22 06:35:59.727: INFO: starting watch
  STEP: cluster-wide listing @ 05/22/23 06:35:59.728
  STEP: cluster-wide watching @ 05/22/23 06:35:59.729
  May 22 06:35:59.729: INFO: starting watch
  STEP: patching @ 05/22/23 06:35:59.73
  STEP: updating @ 05/22/23 06:35:59.734
  May 22 06:35:59.739: INFO: waiting for watch events with expected annotations
  May 22 06:35:59.739: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/22/23 06:35:59.739
  STEP: updating /status @ 05/22/23 06:35:59.742
  STEP: get /status @ 05/22/23 06:35:59.746
  STEP: deleting @ 05/22/23 06:35:59.748
  STEP: deleting a collection @ 05/22/23 06:35:59.754
  May 22 06:35:59.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9888" for this suite. @ 05/22/23 06:35:59.763
• [0.056 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 05/22/23 06:35:59.766
  May 22 06:35:59.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 06:35:59.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:35:59.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:35:59.776
  E0522 06:35:59.786005      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 05/22/23 06:35:59.786
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 06:36:00.038
  STEP: Deploying the webhook pod @ 05/22/23 06:36:00.041
  STEP: Wait for the deployment to be ready @ 05/22/23 06:36:00.048
  May 22 06:36:00.050: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0522 06:36:00.786804      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:01.786980      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/22/23 06:36:02.058
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 06:36:02.066
  E0522 06:36:02.787051      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:36:03.066: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/22/23 06:36:03.068
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/22/23 06:36:03.069
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/22/23 06:36:03.069
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/22/23 06:36:03.069
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/22/23 06:36:03.07
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/22/23 06:36:03.07
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/22/23 06:36:03.071
  May 22 06:36:03.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-993" for this suite. @ 05/22/23 06:36:03.093
  STEP: Destroying namespace "webhook-markers-4466" for this suite. @ 05/22/23 06:36:03.097
• [3.334 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/22/23 06:36:03.101
  May 22 06:36:03.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename deployment @ 05/22/23 06:36:03.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:36:03.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:36:03.111
  May 22 06:36:03.118: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0522 06:36:03.787987      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:04.788540      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:05.788793      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:06.788961      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:07.789190      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:36:08.120: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/22/23 06:36:08.12
  May 22 06:36:08.120: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/22/23 06:36:08.126
  E0522 06:36:08.789563      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:09.789949      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:36:10.137: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5020  930892aa-6937-465d-ac27-5cdaf800477c 907856 1 2023-05-22 06:36:08 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-22 06:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:36:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004667de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-22 06:36:08 +0000 UTC,LastTransitionTime:2023-05-22 06:36:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-68b75d69f8" has successfully progressed.,LastUpdateTime:2023-05-22 06:36:09 +0000 UTC,LastTransitionTime:2023-05-22 06:36:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 22 06:36:10.139: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-5020  93b3178c-90dc-42d2-8b06-08f90e30c79d 907846 1 2023-05-22 06:36:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 930892aa-6937-465d-ac27-5cdaf800477c 0xc002da5647 0xc002da5648}] [] [{kube-controller-manager Update apps/v1 2023-05-22 06:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"930892aa-6937-465d-ac27-5cdaf800477c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:36:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002da56f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 22 06:36:10.141: INFO: Pod "test-cleanup-deployment-68b75d69f8-8pscl" is available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-8pscl test-cleanup-deployment-68b75d69f8- deployment-5020  6c0e5d7c-e90d-487d-88a7-7fbbc9725437 907845 0 2023-05-22 06:36:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[cni.projectcalico.org/containerID:43dbba1cb7d39df945a0add3bdee0b139f21243533628a96c564794211147f65 cni.projectcalico.org/podIP:192.168.104.18/32 cni.projectcalico.org/podIPs:192.168.104.18/32] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 93b3178c-90dc-42d2-8b06-08f90e30c79d 0xc002da5a97 0xc002da5a98}] [] [{calico Update v1 2023-05-22 06:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-22 06:36:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"93b3178c-90dc-42d2-8b06-08f90e30c79d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-22 06:36:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.104.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wzttl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wzttl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:36:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:36:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:36:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:36:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.122,PodIP:192.168.104.18,StartTime:2023-05-22 06:36:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:36:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://333dcbcae44f218dea1ba511babff6500fb35560aea14078a9deaf69004d543e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.104.18,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:36:10.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5020" for this suite. @ 05/22/23 06:36:10.143
• [7.045 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 05/22/23 06:36:10.147
  May 22 06:36:10.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:36:10.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:36:10.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:36:10.16
  STEP: Creating configMap with name cm-test-opt-del-913c07e7-a602-403f-af66-718f9d7a91b0 @ 05/22/23 06:36:10.164
  STEP: Creating configMap with name cm-test-opt-upd-d35fda39-27b5-44e8-a309-b043e2c91b5f @ 05/22/23 06:36:10.166
  STEP: Creating the pod @ 05/22/23 06:36:10.169
  E0522 06:36:10.790810      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:11.791454      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-913c07e7-a602-403f-af66-718f9d7a91b0 @ 05/22/23 06:36:12.207
  STEP: Updating configmap cm-test-opt-upd-d35fda39-27b5-44e8-a309-b043e2c91b5f @ 05/22/23 06:36:12.209
  STEP: Creating configMap with name cm-test-opt-create-44fea49d-fc5a-4014-9d15-597904419192 @ 05/22/23 06:36:12.212
  STEP: waiting to observe update in volume @ 05/22/23 06:36:12.214
  E0522 06:36:12.792489      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:13.792701      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:14.793091      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:15.793333      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:16.793384      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:17.793593      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:18.794476      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:19.794631      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:20.795406      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:21.795613      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:22.796019      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:23.796172      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:24.796323      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:25.796523      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:26.796705      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:27.796960      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:28.797816      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:29.798477      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:30.798592      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:31.798798      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:32.798838      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:33.799061      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:34.799405      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:35.799668      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:36.799831      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:37.800057      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:38.800366      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:39.800839      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:40.801068      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:41.801205      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:42.801553      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:43.801736      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:44.801926      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:45.802212      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:46.803310      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:47.803644      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:48.804533      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:49.805012      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:50.805383      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:51.805667      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:52.806453      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:53.806709      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:54.806724      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:55.807037      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:56.807548      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:57.807767      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:58.808088      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:36:59.808680      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:00.809689      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:01.809913      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:02.810097      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:03.810312      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:04.810571      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:05.810875      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:06.810882      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:07.811041      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:08.811215      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:09.812067      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:10.812937      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:11.813394      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:12.814020      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:13.814227      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:14.815313      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:15.815511      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:16.815662      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:17.815786      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:18.816590      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:19.817100      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:20.817396      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:21.817619      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:22.818316      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:23.818560      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:24.819492      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:25.819889      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:37:26.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4968" for this suite. @ 05/22/23 06:37:26.465
• [76.321 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 05/22/23 06:37:26.468
  May 22 06:37:26.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 06:37:26.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:26.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:26.478
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/22/23 06:37:26.48
  May 22 06:37:26.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-7928 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  May 22 06:37:26.543: INFO: stderr: ""
  May 22 06:37:26.543: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/22/23 06:37:26.543
  May 22 06:37:26.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-7928 delete pods e2e-test-httpd-pod'
  E0522 06:37:26.820837      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:27.821073      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:37:28.254: INFO: stderr: ""
  May 22 06:37:28.255: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 22 06:37:28.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7928" for this suite. @ 05/22/23 06:37:28.258
• [1.792 seconds]
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/22/23 06:37:28.266
  May 22 06:37:28.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename events @ 05/22/23 06:37:28.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:28.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:28.276
  STEP: Create set of events @ 05/22/23 06:37:28.278
  STEP: get a list of Events with a label in the current namespace @ 05/22/23 06:37:28.283
  STEP: delete a list of events @ 05/22/23 06:37:28.285
  May 22 06:37:28.285: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/22/23 06:37:28.291
  May 22 06:37:28.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6682" for this suite. @ 05/22/23 06:37:28.296
• [0.032 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 05/22/23 06:37:28.3
  May 22 06:37:28.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replicaset @ 05/22/23 06:37:28.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:28.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:28.308
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/22/23 06:37:28.31
  E0522 06:37:28.822792      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:29.823852      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 05/22/23 06:37:30.321
  STEP: Then the orphan pod is adopted @ 05/22/23 06:37:30.324
  E0522 06:37:30.824006      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 05/22/23 06:37:31.329
  May 22 06:37:31.331: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/22/23 06:37:31.337
  E0522 06:37:31.824143      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:37:32.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1372" for this suite. @ 05/22/23 06:37:32.344
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 05/22/23 06:37:32.349
  May 22 06:37:32.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename namespaces @ 05/22/23 06:37:32.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:32.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:32.358
  STEP: Creating namespace "e2e-ns-xsxs4" @ 05/22/23 06:37:32.36
  May 22 06:37:32.366: INFO: Namespace "e2e-ns-xsxs4-721" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-xsxs4-721" @ 05/22/23 06:37:32.366
  May 22 06:37:32.370: INFO: Namespace "e2e-ns-xsxs4-721" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-xsxs4-721" @ 05/22/23 06:37:32.37
  May 22 06:37:32.373: INFO: Namespace "e2e-ns-xsxs4-721" has []v1.FinalizerName{"kubernetes"}
  May 22 06:37:32.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3421" for this suite. @ 05/22/23 06:37:32.376
  STEP: Destroying namespace "e2e-ns-xsxs4-721" for this suite. @ 05/22/23 06:37:32.379
• [0.032 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 05/22/23 06:37:32.382
  May 22 06:37:32.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 06:37:32.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:32.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:32.391
  STEP: validating api versions @ 05/22/23 06:37:32.393
  May 22 06:37:32.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-8141 api-versions'
  May 22 06:37:32.447: INFO: stderr: ""
  May 22 06:37:32.447: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.tigera.io/v1\npolicy/v1\nprojectcalico.org/v3\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  May 22 06:37:32.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8141" for this suite. @ 05/22/23 06:37:32.451
• [0.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/22/23 06:37:32.456
  May 22 06:37:32.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename proxy @ 05/22/23 06:37:32.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:32.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:32.465
  STEP: starting an echo server on multiple ports @ 05/22/23 06:37:32.473
  STEP: creating replication controller proxy-service-tvkzz in namespace proxy-2934 @ 05/22/23 06:37:32.474
  I0522 06:37:32.477919      25 runners.go:194] Created replication controller with name: proxy-service-tvkzz, namespace: proxy-2934, replica count: 1
  E0522 06:37:32.825088      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0522 06:37:33.529276      25 runners.go:194] proxy-service-tvkzz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0522 06:37:33.825867      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0522 06:37:34.530342      25 runners.go:194] proxy-service-tvkzz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 22 06:37:34.533: INFO: setup took 2.065685207s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/22/23 06:37:34.533
  May 22 06:37:34.538: INFO: (0) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 5.132399ms)
  May 22 06:37:34.538: INFO: (0) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 5.148722ms)
  May 22 06:37:34.538: INFO: (0) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 5.129626ms)
  May 22 06:37:34.538: INFO: (0) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 5.506854ms)
  May 22 06:37:34.538: INFO: (0) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 5.412383ms)
  May 22 06:37:34.540: INFO: (0) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 6.968782ms)
  May 22 06:37:34.540: INFO: (0) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 7.168276ms)
  May 22 06:37:34.540: INFO: (0) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 7.04999ms)
  May 22 06:37:34.540: INFO: (0) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 7.042192ms)
  May 22 06:37:34.540: INFO: (0) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 7.030324ms)
  May 22 06:37:34.540: INFO: (0) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 7.097924ms)
  May 22 06:37:34.544: INFO: (0) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 11.136354ms)
  May 22 06:37:34.544: INFO: (0) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 11.21214ms)
  May 22 06:37:34.544: INFO: (0) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 11.140319ms)
  May 22 06:37:34.545: INFO: (0) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 11.958837ms)
  May 22 06:37:34.545: INFO: (0) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 11.926702ms)
  May 22 06:37:34.548: INFO: (1) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.070855ms)
  May 22 06:37:34.548: INFO: (1) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 3.212702ms)
  May 22 06:37:34.548: INFO: (1) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 3.290608ms)
  May 22 06:37:34.548: INFO: (1) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 3.260312ms)
  May 22 06:37:34.548: INFO: (1) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 3.396444ms)
  May 22 06:37:34.548: INFO: (1) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.424555ms)
  May 22 06:37:34.548: INFO: (1) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.300704ms)
  May 22 06:37:34.548: INFO: (1) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 3.467318ms)
  May 22 06:37:34.548: INFO: (1) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.514899ms)
  May 22 06:37:34.549: INFO: (1) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 3.737366ms)
  May 22 06:37:34.549: INFO: (1) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.388103ms)
  May 22 06:37:34.549: INFO: (1) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 4.441972ms)
  May 22 06:37:34.549: INFO: (1) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 4.4802ms)
  May 22 06:37:34.549: INFO: (1) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 4.448118ms)
  May 22 06:37:34.549: INFO: (1) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 4.511396ms)
  May 22 06:37:34.550: INFO: (1) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.533971ms)
  May 22 06:37:34.552: INFO: (2) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 2.153734ms)
  May 22 06:37:34.552: INFO: (2) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 2.638681ms)
  May 22 06:37:34.554: INFO: (2) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 4.318731ms)
  May 22 06:37:34.554: INFO: (2) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 4.425799ms)
  May 22 06:37:34.554: INFO: (2) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 3.529554ms)
  May 22 06:37:34.554: INFO: (2) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 3.998746ms)
  May 22 06:37:34.554: INFO: (2) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 4.542198ms)
  May 22 06:37:34.555: INFO: (2) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 5.052831ms)
  May 22 06:37:34.555: INFO: (2) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 4.859319ms)
  May 22 06:37:34.555: INFO: (2) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 4.621096ms)
  May 22 06:37:34.555: INFO: (2) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 4.732598ms)
  May 22 06:37:34.555: INFO: (2) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 5.488831ms)
  May 22 06:37:34.555: INFO: (2) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 4.632443ms)
  May 22 06:37:34.555: INFO: (2) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 5.182513ms)
  May 22 06:37:34.555: INFO: (2) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.515225ms)
  May 22 06:37:34.555: INFO: (2) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.619245ms)
  May 22 06:37:34.558: INFO: (3) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 2.173252ms)
  May 22 06:37:34.558: INFO: (3) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 2.492708ms)
  May 22 06:37:34.558: INFO: (3) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 2.48631ms)
  May 22 06:37:34.558: INFO: (3) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.480718ms)
  May 22 06:37:34.558: INFO: (3) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.548038ms)
  May 22 06:37:34.558: INFO: (3) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 2.622386ms)
  May 22 06:37:34.558: INFO: (3) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 2.997643ms)
  May 22 06:37:34.558: INFO: (3) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 3.199231ms)
  May 22 06:37:34.558: INFO: (3) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 3.16329ms)
  May 22 06:37:34.559: INFO: (3) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 3.879687ms)
  May 22 06:37:34.559: INFO: (3) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 3.947869ms)
  May 22 06:37:34.559: INFO: (3) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 3.869995ms)
  May 22 06:37:34.559: INFO: (3) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 3.929422ms)
  May 22 06:37:34.559: INFO: (3) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 3.880725ms)
  May 22 06:37:34.560: INFO: (3) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.263908ms)
  May 22 06:37:34.560: INFO: (3) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.297091ms)
  May 22 06:37:34.563: INFO: (4) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 2.665558ms)
  May 22 06:37:34.563: INFO: (4) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 2.790096ms)
  May 22 06:37:34.563: INFO: (4) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 2.983002ms)
  May 22 06:37:34.563: INFO: (4) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 2.928156ms)
  May 22 06:37:34.563: INFO: (4) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.9092ms)
  May 22 06:37:34.563: INFO: (4) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 2.914135ms)
  May 22 06:37:34.563: INFO: (4) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 2.882077ms)
  May 22 06:37:34.563: INFO: (4) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.020681ms)
  May 22 06:37:34.563: INFO: (4) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.056192ms)
  May 22 06:37:34.563: INFO: (4) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 3.338474ms)
  May 22 06:37:34.564: INFO: (4) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 3.750332ms)
  May 22 06:37:34.564: INFO: (4) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 3.99614ms)
  May 22 06:37:34.564: INFO: (4) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 4.009151ms)
  May 22 06:37:34.564: INFO: (4) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.205652ms)
  May 22 06:37:34.564: INFO: (4) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 4.31832ms)
  May 22 06:37:34.564: INFO: (4) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.388837ms)
  May 22 06:37:34.570: INFO: (5) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 5.928696ms)
  May 22 06:37:34.570: INFO: (5) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 5.957367ms)
  May 22 06:37:34.570: INFO: (5) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 6.04257ms)
  May 22 06:37:34.570: INFO: (5) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 6.167803ms)
  May 22 06:37:34.570: INFO: (5) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 6.249094ms)
  May 22 06:37:34.570: INFO: (5) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 6.12088ms)
  May 22 06:37:34.571: INFO: (5) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 6.191472ms)
  May 22 06:37:34.571: INFO: (5) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 6.389178ms)
  May 22 06:37:34.571: INFO: (5) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 6.3628ms)
  May 22 06:37:34.571: INFO: (5) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 6.396195ms)
  May 22 06:37:34.571: INFO: (5) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 6.323284ms)
  May 22 06:37:34.571: INFO: (5) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 6.404932ms)
  May 22 06:37:34.571: INFO: (5) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 6.506159ms)
  May 22 06:37:34.571: INFO: (5) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 6.608776ms)
  May 22 06:37:34.571: INFO: (5) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 6.642596ms)
  May 22 06:37:34.571: INFO: (5) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 6.802413ms)
  May 22 06:37:34.574: INFO: (6) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 3.021123ms)
  May 22 06:37:34.574: INFO: (6) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.124805ms)
  May 22 06:37:34.574: INFO: (6) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 3.218929ms)
  May 22 06:37:34.575: INFO: (6) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 3.753813ms)
  May 22 06:37:34.576: INFO: (6) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 4.817771ms)
  May 22 06:37:34.576: INFO: (6) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 4.943711ms)
  May 22 06:37:34.576: INFO: (6) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.869594ms)
  May 22 06:37:34.576: INFO: (6) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 5.133222ms)
  May 22 06:37:34.576: INFO: (6) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 5.022811ms)
  May 22 06:37:34.576: INFO: (6) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 5.015307ms)
  May 22 06:37:34.577: INFO: (6) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 5.010156ms)
  May 22 06:37:34.577: INFO: (6) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 5.233005ms)
  May 22 06:37:34.577: INFO: (6) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 5.568533ms)
  May 22 06:37:34.577: INFO: (6) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 5.496866ms)
  May 22 06:37:34.577: INFO: (6) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 5.896049ms)
  May 22 06:37:34.577: INFO: (6) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 5.802896ms)
  May 22 06:37:34.586: INFO: (7) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 8.257938ms)
  May 22 06:37:34.586: INFO: (7) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 8.359303ms)
  May 22 06:37:34.586: INFO: (7) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 8.218203ms)
  May 22 06:37:34.586: INFO: (7) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 8.241367ms)
  May 22 06:37:34.586: INFO: (7) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 8.267941ms)
  May 22 06:37:34.586: INFO: (7) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 8.299658ms)
  May 22 06:37:34.586: INFO: (7) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 8.383886ms)
  May 22 06:37:34.586: INFO: (7) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 8.345807ms)
  May 22 06:37:34.586: INFO: (7) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 8.269603ms)
  May 22 06:37:34.586: INFO: (7) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 8.472062ms)
  May 22 06:37:34.588: INFO: (7) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 10.599031ms)
  May 22 06:37:34.588: INFO: (7) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 10.810715ms)
  May 22 06:37:34.588: INFO: (7) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 10.716268ms)
  May 22 06:37:34.588: INFO: (7) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 10.835631ms)
  May 22 06:37:34.588: INFO: (7) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 10.861103ms)
  May 22 06:37:34.589: INFO: (7) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 10.96263ms)
  May 22 06:37:34.592: INFO: (8) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.849742ms)
  May 22 06:37:34.592: INFO: (8) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 3.462358ms)
  May 22 06:37:34.592: INFO: (8) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.264214ms)
  May 22 06:37:34.592: INFO: (8) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 3.351355ms)
  May 22 06:37:34.592: INFO: (8) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 3.268867ms)
  May 22 06:37:34.592: INFO: (8) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 3.443069ms)
  May 22 06:37:34.593: INFO: (8) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 3.590926ms)
  May 22 06:37:34.593: INFO: (8) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.710413ms)
  May 22 06:37:34.593: INFO: (8) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 4.148734ms)
  May 22 06:37:34.593: INFO: (8) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 4.407253ms)
  May 22 06:37:34.594: INFO: (8) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 5.217646ms)
  May 22 06:37:34.594: INFO: (8) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 5.29626ms)
  May 22 06:37:34.594: INFO: (8) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 5.205485ms)
  May 22 06:37:34.594: INFO: (8) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 5.544297ms)
  May 22 06:37:34.594: INFO: (8) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 5.593134ms)
  May 22 06:37:34.594: INFO: (8) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 5.215709ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.011902ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 3.857667ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.955368ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 4.24262ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 4.3587ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 3.220444ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 3.552489ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 4.546335ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 4.234764ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 3.882265ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.391599ms)
  May 22 06:37:34.599: INFO: (9) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 4.555408ms)
  May 22 06:37:34.600: INFO: (9) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.824773ms)
  May 22 06:37:34.600: INFO: (9) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 4.605655ms)
  May 22 06:37:34.600: INFO: (9) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 4.506092ms)
  May 22 06:37:34.600: INFO: (9) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.784078ms)
  May 22 06:37:34.603: INFO: (10) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 2.299598ms)
  May 22 06:37:34.603: INFO: (10) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 2.878099ms)
  May 22 06:37:34.603: INFO: (10) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 2.593218ms)
  May 22 06:37:34.603: INFO: (10) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.574947ms)
  May 22 06:37:34.603: INFO: (10) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 3.159008ms)
  May 22 06:37:34.603: INFO: (10) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 2.913687ms)
  May 22 06:37:34.603: INFO: (10) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.053505ms)
  May 22 06:37:34.604: INFO: (10) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 3.081334ms)
  May 22 06:37:34.604: INFO: (10) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.141958ms)
  May 22 06:37:34.604: INFO: (10) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 3.398805ms)
  May 22 06:37:34.604: INFO: (10) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 3.090553ms)
  May 22 06:37:34.604: INFO: (10) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 3.390829ms)
  May 22 06:37:34.604: INFO: (10) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 3.558853ms)
  May 22 06:37:34.604: INFO: (10) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 3.236161ms)
  May 22 06:37:34.604: INFO: (10) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.866021ms)
  May 22 06:37:34.604: INFO: (10) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 3.827651ms)
  May 22 06:37:34.607: INFO: (11) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 2.611188ms)
  May 22 06:37:34.607: INFO: (11) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 2.726627ms)
  May 22 06:37:34.607: INFO: (11) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 2.731643ms)
  May 22 06:37:34.607: INFO: (11) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 3.164399ms)
  May 22 06:37:34.607: INFO: (11) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.088267ms)
  May 22 06:37:34.607: INFO: (11) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.111662ms)
  May 22 06:37:34.607: INFO: (11) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 3.087895ms)
  May 22 06:37:34.607: INFO: (11) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.202981ms)
  May 22 06:37:34.607: INFO: (11) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.152997ms)
  May 22 06:37:34.607: INFO: (11) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 3.25281ms)
  May 22 06:37:34.608: INFO: (11) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 3.860441ms)
  May 22 06:37:34.608: INFO: (11) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 4.007075ms)
  May 22 06:37:34.608: INFO: (11) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.057546ms)
  May 22 06:37:34.608: INFO: (11) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 4.103904ms)
  May 22 06:37:34.608: INFO: (11) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 4.177307ms)
  May 22 06:37:34.608: INFO: (11) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.063124ms)
  May 22 06:37:34.611: INFO: (12) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 2.452826ms)
  May 22 06:37:34.611: INFO: (12) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.848511ms)
  May 22 06:37:34.611: INFO: (12) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 2.725944ms)
  May 22 06:37:34.611: INFO: (12) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 2.784665ms)
  May 22 06:37:34.611: INFO: (12) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 2.966575ms)
  May 22 06:37:34.612: INFO: (12) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 3.76989ms)
  May 22 06:37:34.612: INFO: (12) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 3.679862ms)
  May 22 06:37:34.612: INFO: (12) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 3.809074ms)
  May 22 06:37:34.612: INFO: (12) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 3.739843ms)
  May 22 06:37:34.612: INFO: (12) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 3.810713ms)
  May 22 06:37:34.613: INFO: (12) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 3.928113ms)
  May 22 06:37:34.613: INFO: (12) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 4.068686ms)
  May 22 06:37:34.613: INFO: (12) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 4.167289ms)
  May 22 06:37:34.613: INFO: (12) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 4.327871ms)
  May 22 06:37:34.613: INFO: (12) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 4.27311ms)
  May 22 06:37:34.613: INFO: (12) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 4.333348ms)
  May 22 06:37:34.617: INFO: (13) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 4.26092ms)
  May 22 06:37:34.617: INFO: (13) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.175666ms)
  May 22 06:37:34.617: INFO: (13) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 4.317066ms)
  May 22 06:37:34.617: INFO: (13) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 4.289823ms)
  May 22 06:37:34.617: INFO: (13) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 4.243006ms)
  May 22 06:37:34.617: INFO: (13) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.266303ms)
  May 22 06:37:34.617: INFO: (13) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 4.241774ms)
  May 22 06:37:34.617: INFO: (13) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 4.343033ms)
  May 22 06:37:34.617: INFO: (13) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 4.268647ms)
  May 22 06:37:34.618: INFO: (13) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 4.319886ms)
  May 22 06:37:34.618: INFO: (13) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 4.344877ms)
  May 22 06:37:34.618: INFO: (13) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 4.576716ms)
  May 22 06:37:34.618: INFO: (13) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 4.563612ms)
  May 22 06:37:34.618: INFO: (13) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 4.525253ms)
  May 22 06:37:34.618: INFO: (13) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 4.542786ms)
  May 22 06:37:34.618: INFO: (13) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 4.540967ms)
  May 22 06:37:34.620: INFO: (14) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 2.718768ms)
  May 22 06:37:34.620: INFO: (14) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 2.647199ms)
  May 22 06:37:34.620: INFO: (14) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 2.706677ms)
  May 22 06:37:34.621: INFO: (14) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 3.019331ms)
  May 22 06:37:34.621: INFO: (14) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.190032ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 3.793454ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 3.786272ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 3.981262ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 4.052461ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.124943ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 4.126004ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 4.17297ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 4.316481ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 4.460789ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.429914ms)
  May 22 06:37:34.622: INFO: (14) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 4.475886ms)
  May 22 06:37:34.625: INFO: (15) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.253136ms)
  May 22 06:37:34.625: INFO: (15) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 2.337173ms)
  May 22 06:37:34.625: INFO: (15) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 2.510877ms)
  May 22 06:37:34.625: INFO: (15) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 2.879627ms)
  May 22 06:37:34.625: INFO: (15) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 2.855888ms)
  May 22 06:37:34.626: INFO: (15) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 2.546862ms)
  May 22 06:37:34.626: INFO: (15) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.619751ms)
  May 22 06:37:34.626: INFO: (15) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 3.25338ms)
  May 22 06:37:34.626: INFO: (15) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 3.158823ms)
  May 22 06:37:34.626: INFO: (15) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 3.106082ms)
  May 22 06:37:34.626: INFO: (15) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 3.551802ms)
  May 22 06:37:34.627: INFO: (15) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 4.091379ms)
  May 22 06:37:34.627: INFO: (15) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 4.057523ms)
  May 22 06:37:34.627: INFO: (15) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 3.71985ms)
  May 22 06:37:34.627: INFO: (15) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.264389ms)
  May 22 06:37:34.627: INFO: (15) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.139683ms)
  May 22 06:37:34.630: INFO: (16) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 2.803529ms)
  May 22 06:37:34.630: INFO: (16) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 2.898209ms)
  May 22 06:37:34.630: INFO: (16) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 2.893983ms)
  May 22 06:37:34.630: INFO: (16) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.294201ms)
  May 22 06:37:34.631: INFO: (16) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 3.436156ms)
  May 22 06:37:34.631: INFO: (16) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 3.572647ms)
  May 22 06:37:34.631: INFO: (16) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 3.675129ms)
  May 22 06:37:34.631: INFO: (16) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 3.796887ms)
  May 22 06:37:34.631: INFO: (16) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.988484ms)
  May 22 06:37:34.631: INFO: (16) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 4.133668ms)
  May 22 06:37:34.631: INFO: (16) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 4.152216ms)
  May 22 06:37:34.631: INFO: (16) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 4.246917ms)
  May 22 06:37:34.632: INFO: (16) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 4.590793ms)
  May 22 06:37:34.632: INFO: (16) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.713889ms)
  May 22 06:37:34.632: INFO: (16) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.636423ms)
  May 22 06:37:34.632: INFO: (16) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 4.719122ms)
  May 22 06:37:34.633: INFO: (17) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 1.448851ms)
  May 22 06:37:34.634: INFO: (17) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 1.623636ms)
  May 22 06:37:34.634: INFO: (17) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 1.626767ms)
  May 22 06:37:34.635: INFO: (17) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 2.378139ms)
  May 22 06:37:34.635: INFO: (17) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 2.191093ms)
  May 22 06:37:34.635: INFO: (17) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 2.179961ms)
  May 22 06:37:34.635: INFO: (17) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 2.689252ms)
  May 22 06:37:34.635: INFO: (17) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.51805ms)
  May 22 06:37:34.636: INFO: (17) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 3.052015ms)
  May 22 06:37:34.636: INFO: (17) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.933833ms)
  May 22 06:37:34.636: INFO: (17) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.227004ms)
  May 22 06:37:34.636: INFO: (17) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 3.262746ms)
  May 22 06:37:34.636: INFO: (17) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 3.44995ms)
  May 22 06:37:34.636: INFO: (17) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 3.701823ms)
  May 22 06:37:34.636: INFO: (17) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 3.690177ms)
  May 22 06:37:34.636: INFO: (17) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 3.594735ms)
  May 22 06:37:34.639: INFO: (18) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 2.4893ms)
  May 22 06:37:34.639: INFO: (18) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.436396ms)
  May 22 06:37:34.639: INFO: (18) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 2.491203ms)
  May 22 06:37:34.639: INFO: (18) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 3.066819ms)
  May 22 06:37:34.639: INFO: (18) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.036327ms)
  May 22 06:37:34.639: INFO: (18) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.07164ms)
  May 22 06:37:34.639: INFO: (18) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 3.10393ms)
  May 22 06:37:34.640: INFO: (18) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 3.378034ms)
  May 22 06:37:34.640: INFO: (18) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.345589ms)
  May 22 06:37:34.640: INFO: (18) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 3.497657ms)
  May 22 06:37:34.641: INFO: (18) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 4.337036ms)
  May 22 06:37:34.641: INFO: (18) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 4.345659ms)
  May 22 06:37:34.641: INFO: (18) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.321244ms)
  May 22 06:37:34.641: INFO: (18) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 4.517943ms)
  May 22 06:37:34.641: INFO: (18) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.525959ms)
  May 22 06:37:34.641: INFO: (18) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 4.51824ms)
  May 22 06:37:34.643: INFO: (19) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:460/proxy/: tls baz (200; 2.168727ms)
  May 22 06:37:34.643: INFO: (19) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">... (200; 2.266424ms)
  May 22 06:37:34.643: INFO: (19) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 2.13514ms)
  May 22 06:37:34.643: INFO: (19) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:462/proxy/: tls qux (200; 2.160325ms)
  May 22 06:37:34.643: INFO: (19) /api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/https:proxy-service-tvkzz-c6xhr:443/proxy/tlsrewritem... (200; 2.228321ms)
  May 22 06:37:34.643: INFO: (19) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 2.29705ms)
  May 22 06:37:34.644: INFO: (19) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr/proxy/rewriteme">test</a> (200; 3.10561ms)
  May 22 06:37:34.644: INFO: (19) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:162/proxy/: bar (200; 3.012184ms)
  May 22 06:37:34.645: INFO: (19) /api/v1/namespaces/proxy-2934/pods/http:proxy-service-tvkzz-c6xhr:160/proxy/: foo (200; 3.731509ms)
  May 22 06:37:34.645: INFO: (19) /api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/: <a href="/api/v1/namespaces/proxy-2934/pods/proxy-service-tvkzz-c6xhr:1080/proxy/rewriteme">test<... (200; 4.13381ms)
  May 22 06:37:34.645: INFO: (19) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname1/proxy/: foo (200; 4.194666ms)
  May 22 06:37:34.645: INFO: (19) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname1/proxy/: foo (200; 4.287935ms)
  May 22 06:37:34.646: INFO: (19) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname1/proxy/: tls baz (200; 4.026563ms)
  May 22 06:37:34.646: INFO: (19) /api/v1/namespaces/proxy-2934/services/proxy-service-tvkzz:portname2/proxy/: bar (200; 4.459361ms)
  May 22 06:37:34.646: INFO: (19) /api/v1/namespaces/proxy-2934/services/https:proxy-service-tvkzz:tlsportname2/proxy/: tls qux (200; 4.434507ms)
  May 22 06:37:34.646: INFO: (19) /api/v1/namespaces/proxy-2934/services/http:proxy-service-tvkzz:portname2/proxy/: bar (200; 4.328341ms)
  May 22 06:37:34.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-tvkzz in namespace proxy-2934, will wait for the garbage collector to delete the pods @ 05/22/23 06:37:34.648
  May 22 06:37:34.705: INFO: Deleting ReplicationController proxy-service-tvkzz took: 4.371838ms
  May 22 06:37:34.805: INFO: Terminating ReplicationController proxy-service-tvkzz pods took: 100.188044ms
  E0522 06:37:34.826153      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:35.826489      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:36.827114      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-2934" for this suite. @ 05/22/23 06:37:37.306
• [4.854 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 05/22/23 06:37:37.311
  May 22 06:37:37.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:37:37.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:37.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:37.32
  STEP: Creating configMap with name projected-configmap-test-volume-map-24df5c87-ce54-4e4b-ba4a-f0b3e3dad75c @ 05/22/23 06:37:37.322
  STEP: Creating a pod to test consume configMaps @ 05/22/23 06:37:37.324
  E0522 06:37:37.828009      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:38.828424      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:39.828852      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:40.829225      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:37:41.337
  May 22 06:37:41.339: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-474baf70-3d34-44e4-aecb-74c9b42ccaf9 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:37:41.343
  May 22 06:37:41.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5360" for this suite. @ 05/22/23 06:37:41.354
• [4.046 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 05/22/23 06:37:41.357
  May 22 06:37:41.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/22/23 06:37:41.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:41.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:41.366
  STEP: creating @ 05/22/23 06:37:41.368
  STEP: getting @ 05/22/23 06:37:41.375
  STEP: listing @ 05/22/23 06:37:41.378
  STEP: deleting @ 05/22/23 06:37:41.379
  May 22 06:37:41.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-888" for this suite. @ 05/22/23 06:37:41.388
• [0.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 05/22/23 06:37:41.392
  May 22 06:37:41.392: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replication-controller @ 05/22/23 06:37:41.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:41.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:41.401
  May 22 06:37:41.403: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0522 06:37:41.829267      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/22/23 06:37:42.408
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/22/23 06:37:42.411
  E0522 06:37:42.830000      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/22/23 06:37:43.416
  May 22 06:37:43.421: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/22/23 06:37:43.421
  E0522 06:37:43.830781      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:37:44.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4562" for this suite. @ 05/22/23 06:37:44.429
• [3.040 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 05/22/23 06:37:44.432
  May 22 06:37:44.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename svcaccounts @ 05/22/23 06:37:44.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:44.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:44.443
  E0522 06:37:44.831691      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:45.831980      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 05/22/23 06:37:46.455
  May 22 06:37:46.455: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6160 pod-service-account-028722ca-5761-4a5b-8943-8b7259881db7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 05/22/23 06:37:46.588
  May 22 06:37:46.588: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6160 pod-service-account-028722ca-5761-4a5b-8943-8b7259881db7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 05/22/23 06:37:46.712
  May 22 06:37:46.712: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6160 pod-service-account-028722ca-5761-4a5b-8943-8b7259881db7 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  E0522 06:37:46.832998      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:37:46.842: INFO: Got root ca configmap in namespace "svcaccounts-6160"
  May 22 06:37:46.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6160" for this suite. @ 05/22/23 06:37:46.847
• [2.418 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 05/22/23 06:37:46.852
  May 22 06:37:46.852: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename namespaces @ 05/22/23 06:37:46.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:46.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:46.861
  STEP: Updating Namespace "namespaces-4327" @ 05/22/23 06:37:46.863
  May 22 06:37:46.867: INFO: Namespace "namespaces-4327" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"58979e28-e9ea-4c59-9bf7-7b6c51b0e65a", "kubernetes.io/metadata.name":"namespaces-4327", "namespaces-4327":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  May 22 06:37:46.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4327" for this suite. @ 05/22/23 06:37:46.869
• [0.021 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 05/22/23 06:37:46.873
  May 22 06:37:46.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:37:46.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:46.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:46.882
  STEP: Creating configMap configmap-5830/configmap-test-cff2ac04-2b76-4307-b2e3-037aeddc46b6 @ 05/22/23 06:37:46.884
  STEP: Creating a pod to test consume configMaps @ 05/22/23 06:37:46.886
  E0522 06:37:47.833800      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:48.846867      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:49.847901      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:50.848041      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:37:50.899
  May 22 06:37:50.901: INFO: Trying to get logs from node node2 pod pod-configmaps-518d0a4a-9cdb-4388-9125-1c479d1254a3 container env-test: <nil>
  STEP: delete the pod @ 05/22/23 06:37:50.905
  May 22 06:37:50.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5830" for this suite. @ 05/22/23 06:37:50.915
• [4.045 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 05/22/23 06:37:50.918
  May 22 06:37:50.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-webhook @ 05/22/23 06:37:50.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:50.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:50.927
  STEP: Setting up server cert @ 05/22/23 06:37:50.929
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/22/23 06:37:51.485
  STEP: Deploying the custom resource conversion webhook pod @ 05/22/23 06:37:51.49
  STEP: Wait for the deployment to be ready @ 05/22/23 06:37:51.497
  May 22 06:37:51.501: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0522 06:37:51.848202      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:52.848446      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/22/23 06:37:53.508
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 06:37:53.517
  E0522 06:37:53.848693      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:37:54.517: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 22 06:37:54.520: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  E0522 06:37:54.849538      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:55.850245      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:56.850571      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/22/23 06:37:57.066
  STEP: v2 custom resource should be converted @ 05/22/23 06:37:57.069
  May 22 06:37:57.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-7329" for this suite. @ 05/22/23 06:37:57.603
• [6.689 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 05/22/23 06:37:57.607
  May 22 06:37:57.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/22/23 06:37:57.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:57.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:57.619
  STEP: creating @ 05/22/23 06:37:57.621
  STEP: getting @ 05/22/23 06:37:57.63
  STEP: listing in namespace @ 05/22/23 06:37:57.632
  STEP: patching @ 05/22/23 06:37:57.633
  STEP: deleting @ 05/22/23 06:37:57.643
  May 22 06:37:57.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-9945" for this suite. @ 05/22/23 06:37:57.652
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/22/23 06:37:57.655
  May 22 06:37:57.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename containers @ 05/22/23 06:37:57.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:37:57.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:37:57.665
  STEP: Creating a pod to test override command @ 05/22/23 06:37:57.667
  E0522 06:37:57.851403      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:58.851609      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:37:59.852528      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:00.852723      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:38:01.68
  May 22 06:38:01.682: INFO: Trying to get logs from node node2 pod client-containers-2368589a-b603-4d50-845c-6aac141c5a52 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:38:01.686
  May 22 06:38:01.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8396" for this suite. @ 05/22/23 06:38:01.697
• [4.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/22/23 06:38:01.701
  May 22 06:38:01.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/22/23 06:38:01.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:38:01.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:38:01.712
  STEP: Creating 50 configmaps @ 05/22/23 06:38:01.714
  E0522 06:38:01.852955      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 05/22/23 06:38:01.956
  May 22 06:38:02.057: INFO: Pod name wrapped-volume-race-c7053d13-f0ae-464e-8080-0a1a47d5274e: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/22/23 06:38:02.057
  E0522 06:38:02.853915      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:03.854130      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 05/22/23 06:38:04.124
  May 22 06:38:04.133: INFO: Pod name wrapped-volume-race-79d1a820-a437-4e95-9a1c-83f3023f5e97: Found 0 pods out of 5
  E0522 06:38:04.854230      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:05.854999      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:06.855345      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:07.855603      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:08.855759      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:38:09.140: INFO: Pod name wrapped-volume-race-79d1a820-a437-4e95-9a1c-83f3023f5e97: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/22/23 06:38:09.14
  STEP: Creating RC which spawns configmap-volume pods @ 05/22/23 06:38:09.153
  May 22 06:38:09.163: INFO: Pod name wrapped-volume-race-87901d11-3ac3-463b-9424-723ea20045ef: Found 0 pods out of 5
  E0522 06:38:09.855949      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:10.856286      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:11.856400      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:12.856528      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:13.856722      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:38:14.171: INFO: Pod name wrapped-volume-race-87901d11-3ac3-463b-9424-723ea20045ef: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/22/23 06:38:14.171
  May 22 06:38:14.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-87901d11-3ac3-463b-9424-723ea20045ef in namespace emptydir-wrapper-52, will wait for the garbage collector to delete the pods @ 05/22/23 06:38:14.186
  May 22 06:38:14.242: INFO: Deleting ReplicationController wrapped-volume-race-87901d11-3ac3-463b-9424-723ea20045ef took: 3.942997ms
  May 22 06:38:14.343: INFO: Terminating ReplicationController wrapped-volume-race-87901d11-3ac3-463b-9424-723ea20045ef pods took: 101.190717ms
  E0522 06:38:14.857416      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-79d1a820-a437-4e95-9a1c-83f3023f5e97 in namespace emptydir-wrapper-52, will wait for the garbage collector to delete the pods @ 05/22/23 06:38:15.544
  May 22 06:38:15.602: INFO: Deleting ReplicationController wrapped-volume-race-79d1a820-a437-4e95-9a1c-83f3023f5e97 took: 4.551522ms
  May 22 06:38:15.704: INFO: Terminating ReplicationController wrapped-volume-race-79d1a820-a437-4e95-9a1c-83f3023f5e97 pods took: 102.229139ms
  E0522 06:38:15.858788      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-c7053d13-f0ae-464e-8080-0a1a47d5274e in namespace emptydir-wrapper-52, will wait for the garbage collector to delete the pods @ 05/22/23 06:38:16.604
  May 22 06:38:16.661: INFO: Deleting ReplicationController wrapped-volume-race-c7053d13-f0ae-464e-8080-0a1a47d5274e took: 3.457781ms
  May 22 06:38:16.762: INFO: Terminating ReplicationController wrapped-volume-race-c7053d13-f0ae-464e-8080-0a1a47d5274e pods took: 100.821359ms
  E0522 06:38:16.859730      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 05/22/23 06:38:17.762
  E0522 06:38:17.860374      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-52" for this suite. @ 05/22/23 06:38:17.877
• [16.179 seconds]
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/22/23 06:38:17.88
  May 22 06:38:17.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-runtime @ 05/22/23 06:38:17.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:38:17.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:38:17.89
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/22/23 06:38:17.897
  E0522 06:38:18.861074      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:19.861993      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:20.862898      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:21.863346      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:22.864132      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:23.864931      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:24.865746      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:25.866784      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:26.867109      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:27.867223      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:28.867355      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:29.868403      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:30.868994      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:31.869068      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:32.869322      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:33.870132      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:34.870971      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:35.871369      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:36.871903      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:37.872607      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/22/23 06:38:37.956
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/22/23 06:38:37.958
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/22/23 06:38:37.962
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 05/22/23 06:38:37.963
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/22/23 06:38:37.973
  E0522 06:38:38.873456      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:39.874024      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:40.874198      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/22/23 06:38:40.982
  E0522 06:38:41.874782      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/22/23 06:38:41.986
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/22/23 06:38:41.989
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 05/22/23 06:38:41.989
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/22/23 06:38:41.999
  E0522 06:38:42.875760      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/22/23 06:38:43.004
  E0522 06:38:43.875920      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:44.876763      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/22/23 06:38:45.013
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/22/23 06:38:45.017
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 05/22/23 06:38:45.017
  May 22 06:38:45.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-692" for this suite. @ 05/22/23 06:38:45.031
• [27.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 05/22/23 06:38:45.036
  May 22 06:38:45.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:38:45.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:38:45.044
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:38:45.046
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:38:45.048
  E0522 06:38:45.877232      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:46.877748      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:47.877886      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:48.878119      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:38:49.061
  May 22 06:38:49.063: INFO: Trying to get logs from node node2 pod downwardapi-volume-dbb088e4-fe4c-478f-914e-e16b12d88fdf container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:38:49.067
  May 22 06:38:49.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1955" for this suite. @ 05/22/23 06:38:49.135
• [4.102 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 05/22/23 06:38:49.139
  May 22 06:38:49.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sched-preemption @ 05/22/23 06:38:49.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:38:49.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:38:49.147
  May 22 06:38:49.155: INFO: Waiting up to 1m0s for all nodes to be ready
  E0522 06:38:49.878238      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:50.878363      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:51.878467      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:52.879392      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:53.880008      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:54.880269      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:55.880455      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:56.881397      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:57.881881      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:58.882106      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:38:59.882184      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:00.882386      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:01.883456      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:02.883861      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:03.884588      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:04.884996      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:05.885306      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:06.885526      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:07.886048      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:08.886347      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:09.887163      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:10.887386      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:11.887919      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:12.888134      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:13.889097      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:14.889380      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:15.889648      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:16.889858      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:17.889981      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:18.890198      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:19.890490      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:20.890799      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:21.891505      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:22.891685      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:23.892259      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:24.892587      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:25.893052      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:26.893365      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:27.893543      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:28.893923      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:29.894392      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:30.894630      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:31.895758      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:32.896055      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:33.896096      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:34.896431      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:35.896928      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:36.897114      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:37.897326      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:38.897709      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:39.898740      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:40.898979      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:41.899970      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:42.900208      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:43.900339      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:44.900731      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:45.901511      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:46.901894      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:47.901943      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:48.902168      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:39:49.181: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/22/23 06:39:49.183
  May 22 06:39:49.196: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 22 06:39:49.199: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 22 06:39:49.214: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 22 06:39:49.220: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  May 22 06:39:49.235: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  May 22 06:39:49.240: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/22/23 06:39:49.24
  E0522 06:39:49.905196      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:50.905941      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/22/23 06:39:51.256
  E0522 06:39:51.906141      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:52.906422      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:53.906966      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:54.907305      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:55.907967      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:39:56.908227      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:39:57.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1468" for this suite. @ 05/22/23 06:39:57.306
• [68.170 seconds]
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 05/22/23 06:39:57.309
  May 22 06:39:57.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename daemonsets @ 05/22/23 06:39:57.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:39:57.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:39:57.318
  May 22 06:39:57.330: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/22/23 06:39:57.332
  May 22 06:39:57.334: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:39:57.334: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/22/23 06:39:57.334
  May 22 06:39:57.346: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:39:57.346: INFO: Node node3 is running 0 daemon pod, expected 1
  E0522 06:39:57.909075      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:39:58.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:39:58.349: INFO: Node node3 is running 0 daemon pod, expected 1
  E0522 06:39:58.909355      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:39:59.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 22 06:39:59.349: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/22/23 06:39:59.351
  May 22 06:39:59.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 22 06:39:59.362: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0522 06:39:59.909732      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:00.364: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:40:00.364: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/22/23 06:40:00.364
  May 22 06:40:00.370: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:40:00.370: INFO: Node node3 is running 0 daemon pod, expected 1
  E0522 06:40:00.910056      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:01.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:40:01.373: INFO: Node node3 is running 0 daemon pod, expected 1
  E0522 06:40:01.911007      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:02.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:40:02.372: INFO: Node node3 is running 0 daemon pod, expected 1
  E0522 06:40:02.911293      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:03.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 22 06:40:03.373: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/22/23 06:40:03.376
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5314, will wait for the garbage collector to delete the pods @ 05/22/23 06:40:03.376
  May 22 06:40:03.432: INFO: Deleting DaemonSet.extensions daemon-set took: 3.627201ms
  May 22 06:40:03.533: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.536303ms
  E0522 06:40:03.911428      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:04.911985      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:05.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:40:05.735: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 22 06:40:05.737: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"910200"},"items":null}

  May 22 06:40:05.739: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"910200"},"items":null}

  May 22 06:40:05.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5314" for this suite. @ 05/22/23 06:40:05.758
• [8.453 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 05/22/23 06:40:05.761
  May 22 06:40:05.761: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename emptydir @ 05/22/23 06:40:05.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:05.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:05.771
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/22/23 06:40:05.773
  E0522 06:40:05.912172      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:06.912236      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:07.912397      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:08.912612      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:40:09.786
  May 22 06:40:09.788: INFO: Trying to get logs from node node2 pod pod-fd5d82e2-45f9-40b0-a4ae-f3a72b88d03a container test-container: <nil>
  STEP: delete the pod @ 05/22/23 06:40:09.792
  May 22 06:40:09.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8480" for this suite. @ 05/22/23 06:40:09.802
• [4.044 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 05/22/23 06:40:09.806
  May 22 06:40:09.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename replication-controller @ 05/22/23 06:40:09.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:09.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:09.819
  STEP: Creating replication controller my-hostname-basic-597b1030-cee9-4c45-826f-68bcd4ba219f @ 05/22/23 06:40:09.821
  May 22 06:40:09.825: INFO: Pod name my-hostname-basic-597b1030-cee9-4c45-826f-68bcd4ba219f: Found 0 pods out of 1
  E0522 06:40:09.912638      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:10.912851      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:11.913118      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:12.913997      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:13.914191      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:14.828: INFO: Pod name my-hostname-basic-597b1030-cee9-4c45-826f-68bcd4ba219f: Found 1 pods out of 1
  May 22 06:40:14.828: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-597b1030-cee9-4c45-826f-68bcd4ba219f" are running
  May 22 06:40:14.830: INFO: Pod "my-hostname-basic-597b1030-cee9-4c45-826f-68bcd4ba219f-sb4nj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-22 06:40:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-22 06:40:10 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-22 06:40:10 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-22 06:40:09 +0000 UTC Reason: Message:}])
  May 22 06:40:14.830: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/22/23 06:40:14.83
  May 22 06:40:14.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9641" for this suite. @ 05/22/23 06:40:14.838
• [5.035 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 05/22/23 06:40:14.842
  May 22 06:40:14.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 06:40:14.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:14.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:14.853
  STEP: Counting existing ResourceQuota @ 05/22/23 06:40:14.855
  E0522 06:40:14.914232      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:15.915253      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:16.916134      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:17.916932      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:18.918045      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/22/23 06:40:19.858
  STEP: Ensuring resource quota status is calculated @ 05/22/23 06:40:19.861
  E0522 06:40:19.919163      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:20.919383      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:21.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4831" for this suite. @ 05/22/23 06:40:21.867
• [7.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 05/22/23 06:40:21.871
  May 22 06:40:21.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 06:40:21.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:21.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:21.881
  STEP: create deployment with httpd image @ 05/22/23 06:40:21.883
  May 22 06:40:21.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3854 create -f -'
  E0522 06:40:21.919923      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:22.863: INFO: stderr: ""
  May 22 06:40:22.863: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/22/23 06:40:22.863
  May 22 06:40:22.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3854 diff -f -'
  E0522 06:40:22.920729      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:23.096: INFO: rc: 1
  May 22 06:40:23.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3854 delete -f -'
  May 22 06:40:23.164: INFO: stderr: ""
  May 22 06:40:23.164: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  May 22 06:40:23.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3854" for this suite. @ 05/22/23 06:40:23.166
• [1.299 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 05/22/23 06:40:23.17
  May 22 06:40:23.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename resourcequota @ 05/22/23 06:40:23.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:23.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:23.18
  STEP: Counting existing ResourceQuota @ 05/22/23 06:40:23.182
  E0522 06:40:23.920824      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:24.921682      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:25.921725      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:26.921851      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:27.922281      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/22/23 06:40:28.184
  STEP: Ensuring resource quota status is calculated @ 05/22/23 06:40:28.186
  E0522 06:40:28.922634      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:29.923063      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 05/22/23 06:40:30.19
  STEP: Ensuring resource quota status captures replicaset creation @ 05/22/23 06:40:30.197
  E0522 06:40:30.923349      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:31.923537      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 05/22/23 06:40:32.2
  STEP: Ensuring resource quota status released usage @ 05/22/23 06:40:32.203
  E0522 06:40:32.923648      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:33.923888      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:34.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7295" for this suite. @ 05/22/23 06:40:34.208
• [11.041 seconds]
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/22/23 06:40:34.211
  May 22 06:40:34.211: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename proxy @ 05/22/23 06:40:34.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:34.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:34.222
  May 22 06:40:34.224: INFO: Creating pod...
  E0522 06:40:34.924199      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:35.924640      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:36.233: INFO: Creating service...
  May 22 06:40:36.241: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/pods/agnhost/proxy?method=DELETE
  May 22 06:40:36.244: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 22 06:40:36.244: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/pods/agnhost/proxy?method=OPTIONS
  May 22 06:40:36.246: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 22 06:40:36.246: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/pods/agnhost/proxy?method=PATCH
  May 22 06:40:36.248: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 22 06:40:36.248: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/pods/agnhost/proxy?method=POST
  May 22 06:40:36.249: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 22 06:40:36.249: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/pods/agnhost/proxy?method=PUT
  May 22 06:40:36.251: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 22 06:40:36.251: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/services/e2e-proxy-test-service/proxy?method=DELETE
  May 22 06:40:36.254: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 22 06:40:36.254: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/services/e2e-proxy-test-service/proxy?method=OPTIONS
  May 22 06:40:36.256: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 22 06:40:36.256: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/services/e2e-proxy-test-service/proxy?method=PATCH
  May 22 06:40:36.258: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 22 06:40:36.259: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/services/e2e-proxy-test-service/proxy?method=POST
  May 22 06:40:36.261: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 22 06:40:36.261: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/services/e2e-proxy-test-service/proxy?method=PUT
  May 22 06:40:36.263: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 22 06:40:36.263: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/pods/agnhost/proxy?method=GET
  May 22 06:40:36.264: INFO: http.Client request:GET StatusCode:301
  May 22 06:40:36.264: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/services/e2e-proxy-test-service/proxy?method=GET
  May 22 06:40:36.266: INFO: http.Client request:GET StatusCode:301
  May 22 06:40:36.266: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/pods/agnhost/proxy?method=HEAD
  May 22 06:40:36.267: INFO: http.Client request:HEAD StatusCode:301
  May 22 06:40:36.267: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9276/services/e2e-proxy-test-service/proxy?method=HEAD
  May 22 06:40:36.271: INFO: http.Client request:HEAD StatusCode:301
  May 22 06:40:36.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-9276" for this suite. @ 05/22/23 06:40:36.273
• [2.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/22/23 06:40:36.278
  May 22 06:40:36.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:40:36.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:36.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:36.287
  STEP: Creating configMap with name configmap-projected-all-test-volume-f74662c5-38bf-4515-af54-b9c2a80b4e70 @ 05/22/23 06:40:36.289
  STEP: Creating secret with name secret-projected-all-test-volume-28e8ffc6-203c-4027-9066-4d598e6f58f7 @ 05/22/23 06:40:36.292
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/22/23 06:40:36.294
  E0522 06:40:36.924752      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:37.925078      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:38.925906      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:39.926536      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:40:40.306
  May 22 06:40:40.308: INFO: Trying to get logs from node node2 pod projected-volume-eccf08f3-6697-4557-92f1-1cc2e1c463bf container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 06:40:40.312
  May 22 06:40:40.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3269" for this suite. @ 05/22/23 06:40:40.321
• [4.045 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 05/22/23 06:40:40.324
  May 22 06:40:40.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:40:40.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:40.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:40.336
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:40:40.338
  E0522 06:40:40.926604      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:41.926998      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:42.927117      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:43.927349      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:40:44.351
  May 22 06:40:44.353: INFO: Trying to get logs from node node2 pod downwardapi-volume-209f53f0-26e4-4cb8-b881-00cc63d51687 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:40:44.357
  May 22 06:40:44.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5355" for this suite. @ 05/22/23 06:40:44.366
• [4.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/22/23 06:40:44.371
  May 22 06:40:44.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/22/23 06:40:44.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:44.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:44.381
  STEP: Setting up the test @ 05/22/23 06:40:44.383
  STEP: Creating hostNetwork=false pod @ 05/22/23 06:40:44.383
  E0522 06:40:44.927459      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:45.927855      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 05/22/23 06:40:46.393
  E0522 06:40:46.928719      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:47.929012      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 05/22/23 06:40:48.402
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/22/23 06:40:48.402
  May 22 06:40:48.402: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2690 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:40:48.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:40:48.403: INFO: ExecWithOptions: Clientset creation
  May 22 06:40:48.403: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 22 06:40:48.462: INFO: Exec stderr: ""
  May 22 06:40:48.462: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2690 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:40:48.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:40:48.463: INFO: ExecWithOptions: Clientset creation
  May 22 06:40:48.463: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 22 06:40:48.524: INFO: Exec stderr: ""
  May 22 06:40:48.524: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2690 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:40:48.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:40:48.525: INFO: ExecWithOptions: Clientset creation
  May 22 06:40:48.525: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 22 06:40:48.567: INFO: Exec stderr: ""
  May 22 06:40:48.567: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2690 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:40:48.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:40:48.567: INFO: ExecWithOptions: Clientset creation
  May 22 06:40:48.567: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 22 06:40:48.621: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/22/23 06:40:48.621
  May 22 06:40:48.621: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2690 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:40:48.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:40:48.622: INFO: ExecWithOptions: Clientset creation
  May 22 06:40:48.622: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 22 06:40:48.660: INFO: Exec stderr: ""
  May 22 06:40:48.660: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2690 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:40:48.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:40:48.661: INFO: ExecWithOptions: Clientset creation
  May 22 06:40:48.661: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2690/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 22 06:40:48.722: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/22/23 06:40:48.722
  May 22 06:40:48.722: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2690 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:40:48.722: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:40:48.723: INFO: ExecWithOptions: Clientset creation
  May 22 06:40:48.723: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 22 06:40:48.794: INFO: Exec stderr: ""
  May 22 06:40:48.794: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2690 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:40:48.794: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:40:48.795: INFO: ExecWithOptions: Clientset creation
  May 22 06:40:48.795: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 22 06:40:48.856: INFO: Exec stderr: ""
  May 22 06:40:48.856: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2690 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:40:48.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:40:48.857: INFO: ExecWithOptions: Clientset creation
  May 22 06:40:48.857: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 22 06:40:48.897: INFO: Exec stderr: ""
  May 22 06:40:48.897: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2690 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:40:48.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:40:48.898: INFO: ExecWithOptions: Clientset creation
  May 22 06:40:48.898: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2690/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0522 06:40:48.930697      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:48.937: INFO: Exec stderr: ""
  May 22 06:40:48.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-2690" for this suite. @ 05/22/23 06:40:48.94
• [4.572 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/22/23 06:40:48.944
  May 22 06:40:48.944: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename deployment @ 05/22/23 06:40:48.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:48.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:48.953
  May 22 06:40:48.954: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  May 22 06:40:48.959: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0522 06:40:49.931117      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:50.931602      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:51.931824      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:52.932107      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:53.932334      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:53.961: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/22/23 06:40:53.961
  May 22 06:40:53.961: INFO: Creating deployment "test-rolling-update-deployment"
  May 22 06:40:53.964: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  May 22 06:40:53.967: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0522 06:40:54.932982      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:55.933105      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:40:55.973: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  May 22 06:40:55.974: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  May 22 06:40:55.980: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7170  3dad4e72-d57d-40be-9804-b7a5c8188a76 910737 1 2023-05-22 06:40:53 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-22 06:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005363748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-22 06:40:53 +0000 UTC,LastTransitionTime:2023-05-22 06:40:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-05-22 06:40:54 +0000 UTC,LastTransitionTime:2023-05-22 06:40:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 22 06:40:55.982: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-7170  cea95263-0cb3-41c5-99d7-ee0319452b6d 910723 1 2023-05-22 06:40:53 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3dad4e72-d57d-40be-9804-b7a5c8188a76 0xc004a55067 0xc004a55068}] [] [{kube-controller-manager Update apps/v1 2023-05-22 06:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dad4e72-d57d-40be-9804-b7a5c8188a76\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:40:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a55118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 22 06:40:55.982: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  May 22 06:40:55.982: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7170  5219f4f3-74c3-42af-b84a-53e3e0e0faed 910736 2 2023-05-22 06:40:48 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3dad4e72-d57d-40be-9804-b7a5c8188a76 0xc004a54f37 0xc004a54f38}] [] [{e2e.test Update apps/v1 2023-05-22 06:40:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3dad4e72-d57d-40be-9804-b7a5c8188a76\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-22 06:40:54 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004a54ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 22 06:40:55.984: INFO: Pod "test-rolling-update-deployment-656d657cd8-7nqkx" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-7nqkx test-rolling-update-deployment-656d657cd8- deployment-7170  c4846eee-3cba-4316-bb30-099688038bb0 910721 0 2023-05-22 06:40:53 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[cni.projectcalico.org/containerID:e67ee32b86d77118e11af69bc846eab2bd242514e40efdeec0db6000def51d81 cni.projectcalico.org/podIP:192.168.135.13/32 cni.projectcalico.org/podIPs:192.168.135.13/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 cea95263-0cb3-41c5-99d7-ee0319452b6d 0xc004a55587 0xc004a55588}] [] [{kube-controller-manager Update v1 2023-05-22 06:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cea95263-0cb3-41c5-99d7-ee0319452b6d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-22 06:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-22 06:40:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.135.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwgsn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwgsn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:40:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-22 06:40:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.33.123,PodIP:192.168.135.13,StartTime:2023-05-22 06:40:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-22 06:40:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://cb8fba49f9178f812d092f0c6d5c4b022a84103ca774da20e9324c63a42ab91f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.135.13,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 22 06:40:55.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7170" for this suite. @ 05/22/23 06:40:55.987
• [7.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 05/22/23 06:40:55.992
  May 22 06:40:55.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:40:55.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:56.002
  STEP: Creating configMap with name configmap-test-volume-817364c6-90a1-4a1a-95a8-3f0184657cb8 @ 05/22/23 06:40:56.004
  STEP: Creating a pod to test consume configMaps @ 05/22/23 06:40:56.007
  E0522 06:40:56.933381      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:57.933679      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:40:58.016
  May 22 06:40:58.018: INFO: Trying to get logs from node node2 pod pod-configmaps-8b6aacc0-c481-4623-9953-19210293ef29 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:40:58.022
  May 22 06:40:58.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7437" for this suite. @ 05/22/23 06:40:58.033
• [2.045 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 05/22/23 06:40:58.036
  May 22 06:40:58.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:40:58.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:40:58.044
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:40:58.045
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:40:58.047
  E0522 06:40:58.933885      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:40:59.934575      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:00.935198      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:01.936217      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:41:02.061
  May 22 06:41:02.063: INFO: Trying to get logs from node node2 pod downwardapi-volume-889fbe6f-eeb8-4fd7-9745-af18d705f352 container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:41:02.068
  May 22 06:41:02.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4917" for this suite. @ 05/22/23 06:41:02.077
• [4.044 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/22/23 06:41:02.081
  May 22 06:41:02.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename certificates @ 05/22/23 06:41:02.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:41:02.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:41:02.089
  STEP: getting /apis @ 05/22/23 06:41:02.644
  STEP: getting /apis/certificates.k8s.io @ 05/22/23 06:41:02.648
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/22/23 06:41:02.649
  STEP: creating @ 05/22/23 06:41:02.649
  STEP: getting @ 05/22/23 06:41:02.658
  STEP: listing @ 05/22/23 06:41:02.66
  STEP: watching @ 05/22/23 06:41:02.661
  May 22 06:41:02.661: INFO: starting watch
  STEP: patching @ 05/22/23 06:41:02.662
  STEP: updating @ 05/22/23 06:41:02.665
  May 22 06:41:02.668: INFO: waiting for watch events with expected annotations
  May 22 06:41:02.668: INFO: saw patched and updated annotations
  STEP: getting /approval @ 05/22/23 06:41:02.668
  STEP: patching /approval @ 05/22/23 06:41:02.669
  STEP: updating /approval @ 05/22/23 06:41:02.672
  STEP: getting /status @ 05/22/23 06:41:02.676
  STEP: patching /status @ 05/22/23 06:41:02.677
  STEP: updating /status @ 05/22/23 06:41:02.683
  STEP: deleting @ 05/22/23 06:41:02.687
  STEP: deleting a collection @ 05/22/23 06:41:02.692
  May 22 06:41:02.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-2321" for this suite. @ 05/22/23 06:41:02.701
• [0.624 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 05/22/23 06:41:02.705
  May 22 06:41:02.705: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename job @ 05/22/23 06:41:02.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:41:02.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:41:02.716
  STEP: Creating a job @ 05/22/23 06:41:02.718
  STEP: Ensuring job reaches completions @ 05/22/23 06:41:02.722
  E0522 06:41:02.936363      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:03.936858      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:04.937850      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:05.938071      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:06.938695      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:07.938989      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:08.939661      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:09.940045      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:10.940954      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:11.941157      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:41:12.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5188" for this suite. @ 05/22/23 06:41:12.728
• [10.026 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 05/22/23 06:41:12.731
  May 22 06:41:12.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename services @ 05/22/23 06:41:12.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:41:12.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:41:12.74
  May 22 06:41:12.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3165" for this suite. @ 05/22/23 06:41:12.745
• [0.017 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 05/22/23 06:41:12.748
  May 22 06:41:12.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:41:12.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:41:12.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:41:12.756
  STEP: Creating configMap with name configmap-test-volume-e0948aeb-659a-4423-b031-221c94a05103 @ 05/22/23 06:41:12.758
  STEP: Creating a pod to test consume configMaps @ 05/22/23 06:41:12.76
  E0522 06:41:12.942006      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:13.942794      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:14.943229      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:15.943472      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:41:16.772
  May 22 06:41:16.774: INFO: Trying to get logs from node node2 pod pod-configmaps-b942f243-9d86-42d4-9353-c9d6c69d19f6 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:41:16.778
  May 22 06:41:16.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7746" for this suite. @ 05/22/23 06:41:16.789
• [4.044 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 05/22/23 06:41:16.793
  May 22 06:41:16.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/22/23 06:41:16.793
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:41:16.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:41:16.803
  STEP: getting /apis @ 05/22/23 06:41:16.805
  STEP: getting /apis/storage.k8s.io @ 05/22/23 06:41:16.809
  STEP: getting /apis/storage.k8s.io/v1 @ 05/22/23 06:41:16.813
  STEP: creating @ 05/22/23 06:41:16.814
  STEP: watching @ 05/22/23 06:41:16.821
  May 22 06:41:16.821: INFO: starting watch
  STEP: getting @ 05/22/23 06:41:16.824
  STEP: listing in namespace @ 05/22/23 06:41:16.825
  STEP: listing across namespaces @ 05/22/23 06:41:16.827
  STEP: patching @ 05/22/23 06:41:16.828
  STEP: updating @ 05/22/23 06:41:16.832
  May 22 06:41:16.834: INFO: waiting for watch events with expected annotations in namespace
  May 22 06:41:16.834: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/22/23 06:41:16.834
  STEP: deleting a collection @ 05/22/23 06:41:16.84
  May 22 06:41:16.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-9321" for this suite. @ 05/22/23 06:41:16.852
• [0.063 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/22/23 06:41:16.856
  May 22 06:41:16.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename containers @ 05/22/23 06:41:16.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:41:16.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:41:16.865
  E0522 06:41:16.944041      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:17.944352      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:41:18.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7353" for this suite. @ 05/22/23 06:41:18.885
• [2.033 seconds]
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 05/22/23 06:41:18.889
  May 22 06:41:18.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/22/23 06:41:18.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:41:18.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:41:18.898
  May 22 06:41:18.900: INFO: Waiting up to 1m0s for all nodes to be ready
  E0522 06:41:18.944978      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:19.945563      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:20.945630      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:21.946032      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:22.946385      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:23.946808      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:24.947273      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:25.947527      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:26.947730      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:27.947942      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:28.948246      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:29.948756      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:30.949801      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:31.950420      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:32.951053      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:33.951866      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:34.952491      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:35.952697      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:36.953126      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:37.953359      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:38.953691      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:39.954463      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:40.954625      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:41.954825      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:42.955599      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:43.955774      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:44.956451      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:45.956763      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:46.956965      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:47.957197      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:48.957268      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:49.957931      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:50.958912      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:51.959213      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:52.960004      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:53.960205      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:54.961350      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:55.961569      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:56.962580      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:57.962789      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:58.963837      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:41:59.964469      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:00.965081      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:01.965579      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:02.966440      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:03.966681      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:04.967611      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:05.967839      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:06.968136      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:07.969183      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:08.970038      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:09.970805      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:10.971839      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:11.972040      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:12.972651      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:13.972977      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:14.973012      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:15.973205      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:16.973223      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:17.973485      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:42:18.922: INFO: Waiting for terminating namespaces to be deleted...
  May 22 06:42:18.924: INFO: Starting informer...
  STEP: Starting pods... @ 05/22/23 06:42:18.924
  E0522 06:42:18.974074      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:42:19.134: INFO: Pod1 is running on node2. Tainting Node
  E0522 06:42:19.974804      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:20.975550      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:42:21.347: INFO: Pod2 is running on node2. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/22/23 06:42:21.347
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/22/23 06:42:21.357
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/22/23 06:42:21.36
  E0522 06:42:21.975657      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:22.975871      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:23.975994      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:24.976110      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:25.976277      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:26.977270      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:42:27.038: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0522 06:42:27.977483      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:28.977818      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:29.977969      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:30.978191      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:31.978785      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:32.978863      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:33.979059      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:34.979308      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:35.979553      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:36.979820      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:37.980275      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:38.980458      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:39.980910      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:40.981086      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:41.981283      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:42.981471      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:43.981702      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:44.982149      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:45.982368      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:46.983184      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:42:47.077: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  May 22 06:42:47.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/22/23 06:42:47.089
  STEP: Destroying namespace "taint-multiple-pods-1646" for this suite. @ 05/22/23 06:42:47.091
• [88.205 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 05/22/23 06:42:47.094
  May 22 06:42:47.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:42:47.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:42:47.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:42:47.112
  STEP: Creating configMap with name projected-configmap-test-volume-e3177f2f-8b15-4df1-bacf-5252f1676680 @ 05/22/23 06:42:47.114
  STEP: Creating a pod to test consume configMaps @ 05/22/23 06:42:47.117
  E0522 06:42:47.983258      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:48.983937      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:49.984289      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:50.984556      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:42:51.134
  May 22 06:42:51.136: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-b582147b-a318-426a-a0f9-1a150de985b6 container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:42:51.154
  May 22 06:42:51.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2679" for this suite. @ 05/22/23 06:42:51.166
• [4.076 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 05/22/23 06:42:51.17
  May 22 06:42:51.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename field-validation @ 05/22/23 06:42:51.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:42:51.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:42:51.179
  May 22 06:42:51.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  E0522 06:42:51.985506      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:52.985872      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:42:53.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5971" for this suite. @ 05/22/23 06:42:53.732
• [2.565 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/22/23 06:42:53.736
  May 22 06:42:53.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename watch @ 05/22/23 06:42:53.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:42:53.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:42:53.746
  STEP: creating a watch on configmaps with label A @ 05/22/23 06:42:53.748
  STEP: creating a watch on configmaps with label B @ 05/22/23 06:42:53.749
  STEP: creating a watch on configmaps with label A or B @ 05/22/23 06:42:53.749
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/22/23 06:42:53.75
  May 22 06:42:53.752: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9848  901851f5-fa82-4b48-b6f1-9fb41fde2ea2 911658 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:42:53.752: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9848  901851f5-fa82-4b48-b6f1-9fb41fde2ea2 911658 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/22/23 06:42:53.752
  May 22 06:42:53.755: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9848  901851f5-fa82-4b48-b6f1-9fb41fde2ea2 911659 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:42:53.755: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9848  901851f5-fa82-4b48-b6f1-9fb41fde2ea2 911659 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/22/23 06:42:53.755
  May 22 06:42:53.759: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9848  901851f5-fa82-4b48-b6f1-9fb41fde2ea2 911660 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:42:53.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9848  901851f5-fa82-4b48-b6f1-9fb41fde2ea2 911660 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/22/23 06:42:53.76
  May 22 06:42:53.762: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9848  901851f5-fa82-4b48-b6f1-9fb41fde2ea2 911661 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:42:53.762: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9848  901851f5-fa82-4b48-b6f1-9fb41fde2ea2 911661 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/22/23 06:42:53.762
  May 22 06:42:53.764: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9848  0b1c7c76-7c32-4af6-bc18-42666640299d 911662 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:42:53.764: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9848  0b1c7c76-7c32-4af6-bc18-42666640299d 911662 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0522 06:42:53.986076      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:54.986492      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:55.986717      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:56.986926      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:57.987343      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:58.987772      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:42:59.988351      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:00.988552      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:01.988828      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:02.989183      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/22/23 06:43:03.765
  May 22 06:43:03.768: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9848  0b1c7c76-7c32-4af6-bc18-42666640299d 911741 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:43:03.768: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9848  0b1c7c76-7c32-4af6-bc18-42666640299d 911741 0 2023-05-22 06:42:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-22 06:42:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0522 06:43:03.989742      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:04.989943      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:05.990159      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:06.990439      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:07.990757      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:08.991119      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:09.991768      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:10.991947      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:11.992167      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:12.992391      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:43:13.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9848" for this suite. @ 05/22/23 06:43:13.773
• [20.043 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/22/23 06:43:13.779
  May 22 06:43:13.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:43:13.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:43:13.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:43:13.789
  STEP: Creating projection with secret that has name projected-secret-test-map-f9551888-343a-48b2-9b9d-db89c1ad36e1 @ 05/22/23 06:43:13.791
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:43:13.793
  E0522 06:43:13.992721      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:14.992965      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:15.993785      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:16.993982      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:43:17.805
  May 22 06:43:17.807: INFO: Trying to get logs from node node2 pod pod-projected-secrets-ad9394ee-b32d-4971-8928-42f5fb5266a5 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 06:43:17.813
  May 22 06:43:17.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9083" for this suite. @ 05/22/23 06:43:17.822
• [4.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/22/23 06:43:17.828
  May 22 06:43:17.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:43:17.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:43:17.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:43:17.888
  STEP: Creating projection with secret that has name projected-secret-test-1d031d68-bae5-4cb9-bde2-c3240b126405 @ 05/22/23 06:43:17.891
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:43:17.894
  E0522 06:43:17.994599      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:18.994761      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:19.995442      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:20.995574      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:43:21.907
  May 22 06:43:21.909: INFO: Trying to get logs from node node2 pod pod-projected-secrets-acc716cf-4cd0-47a9-b260-47456f638c0e container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 06:43:21.914
  May 22 06:43:21.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4252" for this suite. @ 05/22/23 06:43:21.924
• [4.104 seconds]
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 05/22/23 06:43:21.932
  May 22 06:43:21.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename subpath @ 05/22/23 06:43:21.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:43:21.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:43:21.942
  STEP: Setting up data @ 05/22/23 06:43:21.944
  STEP: Creating pod pod-subpath-test-downwardapi-b8nj @ 05/22/23 06:43:21.949
  STEP: Creating a pod to test atomic-volume-subpath @ 05/22/23 06:43:21.949
  E0522 06:43:21.996408      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:22.996924      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:23.997586      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:24.998080      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:25.998778      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:26.999215      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:28.000282      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:29.000545      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:30.001167      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:31.001351      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:32.001697      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:33.001899      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:34.002591      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:35.002958      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:36.003657      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:37.003830      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:38.004584      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:39.004773      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:40.005205      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:41.005414      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:42.006417      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:43.006819      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:44.007479      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:45.008198      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:43:45.997
  May 22 06:43:45.999: INFO: Trying to get logs from node node2 pod pod-subpath-test-downwardapi-b8nj container test-container-subpath-downwardapi-b8nj: <nil>
  STEP: delete the pod @ 05/22/23 06:43:46.003
  E0522 06:43:46.008847      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pod pod-subpath-test-downwardapi-b8nj @ 05/22/23 06:43:46.011
  May 22 06:43:46.011: INFO: Deleting pod "pod-subpath-test-downwardapi-b8nj" in namespace "subpath-7726"
  May 22 06:43:46.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7726" for this suite. @ 05/22/23 06:43:46.014
• [24.085 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 05/22/23 06:43:46.018
  May 22 06:43:46.018: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename statefulset @ 05/22/23 06:43:46.019
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:43:46.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:43:46.027
  STEP: Creating service test in namespace statefulset-2355 @ 05/22/23 06:43:46.028
  STEP: Creating a new StatefulSet @ 05/22/23 06:43:46.031
  May 22 06:43:46.036: INFO: Found 0 stateful pods, waiting for 3
  E0522 06:43:47.008981      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:48.009411      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:49.009550      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:50.010529      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:51.010688      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:52.011030      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:53.011238      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:54.011479      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:55.011977      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:56.012303      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:43:56.039: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 22 06:43:56.039: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 22 06:43:56.039: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/22/23 06:43:56.046
  May 22 06:43:56.063: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/22/23 06:43:56.063
  E0522 06:43:57.012720      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:58.012943      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:43:59.013369      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:00.013860      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:01.014054      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:02.014252      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:03.014619      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:04.014841      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:05.015227      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:06.015459      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/22/23 06:44:06.072
  STEP: Performing a canary update @ 05/22/23 06:44:06.072
  May 22 06:44:06.088: INFO: Updating stateful set ss2
  May 22 06:44:06.091: INFO: Waiting for Pod statefulset-2355/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0522 06:44:07.015831      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:08.016103      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:09.016354      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:10.017043      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:11.017227      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:12.017318      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:13.017530      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:14.018433      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:15.018940      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:16.019131      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/22/23 06:44:16.097
  May 22 06:44:16.115: INFO: Found 1 stateful pods, waiting for 3
  E0522 06:44:17.020192      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:18.020497      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:19.020834      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:20.021304      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:21.021461      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:22.021631      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:23.021808      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:24.022005      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:25.022263      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:26.022424      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:44:26.119: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 22 06:44:26.119: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 22 06:44:26.119: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/22/23 06:44:26.123
  May 22 06:44:26.140: INFO: Updating stateful set ss2
  May 22 06:44:26.145: INFO: Waiting for Pod statefulset-2355/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0522 06:44:27.022699      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:28.022853      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:29.023059      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:30.023629      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:31.023856      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:32.024046      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:33.024247      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:34.024467      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:35.024793      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:36.024993      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:44:36.167: INFO: Updating stateful set ss2
  May 22 06:44:36.170: INFO: Waiting for StatefulSet statefulset-2355/ss2 to complete update
  May 22 06:44:36.170: INFO: Waiting for Pod statefulset-2355/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0522 06:44:37.025083      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:38.025213      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:39.025350      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:40.025481      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:41.025591      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:42.025719      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:43.025842      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:44.026073      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:45.026154      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:46.026281      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:44:46.176: INFO: Deleting all statefulset in ns statefulset-2355
  May 22 06:44:46.178: INFO: Scaling statefulset ss2 to 0
  E0522 06:44:47.026712      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:48.026919      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:49.027115      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:50.027727      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:51.027890      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:52.028593      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:53.028856      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:54.029049      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:55.029273      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:56.029460      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:44:56.188: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 06:44:56.190: INFO: Deleting statefulset ss2
  May 22 06:44:56.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2355" for this suite. @ 05/22/23 06:44:56.198
• [70.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 05/22/23 06:44:56.203
  May 22 06:44:56.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:44:56.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:44:56.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:44:56.213
  STEP: Creating the pod @ 05/22/23 06:44:56.216
  E0522 06:44:57.029627      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:44:58.030016      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:44:58.743: INFO: Successfully updated pod "annotationupdatea747e2b8-3f26-4867-ae94-62ed92d7fee6"
  E0522 06:44:59.030046      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:00.030364      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:01.030520      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:02.030800      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:45:02.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7642" for this suite. @ 05/22/23 06:45:02.761
• [6.561 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/22/23 06:45:02.764
  May 22 06:45:02.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename conformance-tests @ 05/22/23 06:45:02.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:45:02.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:45:02.773
  STEP: Getting node addresses @ 05/22/23 06:45:02.775
  May 22 06:45:02.775: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  May 22 06:45:02.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-4306" for this suite. @ 05/22/23 06:45:02.781
• [0.020 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/22/23 06:45:02.785
  May 22 06:45:02.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 06:45:02.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:45:02.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:45:02.793
  STEP: Create set of pods @ 05/22/23 06:45:02.795
  May 22 06:45:02.800: INFO: created test-pod-1
  May 22 06:45:02.803: INFO: created test-pod-2
  May 22 06:45:02.806: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/22/23 06:45:02.806
  E0522 06:45:03.031523      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:04.031763      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 05/22/23 06:45:04.828
  May 22 06:45:04.830: INFO: Pod quantity 3 is different from expected quantity 0
  E0522 06:45:05.032337      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:45:05.833: INFO: Pod quantity 3 is different from expected quantity 0
  E0522 06:45:06.032401      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:45:06.833: INFO: Pod quantity 3 is different from expected quantity 0
  E0522 06:45:07.032422      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:45:07.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6424" for this suite. @ 05/22/23 06:45:07.834
• [5.052 seconds]
------------------------------
SSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 05/22/23 06:45:07.837
  May 22 06:45:07.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/22/23 06:45:07.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:45:07.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:45:07.848
  STEP: creating a target pod @ 05/22/23 06:45:07.85
  E0522 06:45:08.032555      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:09.033052      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 05/22/23 06:45:09.861
  E0522 06:45:10.033737      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:11.033961      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:12.034033      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:13.034251      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 05/22/23 06:45:13.876
  May 22 06:45:13.877: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4567 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 22 06:45:13.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  May 22 06:45:13.878: INFO: ExecWithOptions: Clientset creation
  May 22 06:45:13.878: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4567/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  May 22 06:45:13.955: INFO: Exec stderr: ""
  May 22 06:45:13.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-4567" for this suite. @ 05/22/23 06:45:13.963
• [6.129 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 05/22/23 06:45:13.967
  May 22 06:45:13.967: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename downward-api @ 05/22/23 06:45:13.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:45:13.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:45:13.975
  STEP: Creating a pod to test downward api env vars @ 05/22/23 06:45:13.977
  E0522 06:45:14.034802      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:15.035338      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:16.036224      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:17.036425      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:45:17.991
  May 22 06:45:17.993: INFO: Trying to get logs from node node2 pod downward-api-c80b7b39-d8e8-430e-b700-36a6102dbc0e container dapi-container: <nil>
  STEP: delete the pod @ 05/22/23 06:45:17.998
  May 22 06:45:18.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5683" for this suite. @ 05/22/23 06:45:18.008
• [4.044 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/22/23 06:45:18.012
  May 22 06:45:18.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sched-pred @ 05/22/23 06:45:18.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:45:18.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:45:18.022
  May 22 06:45:18.024: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 22 06:45:18.029: INFO: Waiting for terminating namespaces to be deleted...
  May 22 06:45:18.030: INFO: 
  Logging pods the apiserver thinks is on node node1 before test
  May 22 06:45:18.035: INFO: calico-apiserver-666fc8f69-6l8kk from calico-apiserver started at 2023-05-19 09:15:01 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.035: INFO: 	Container calico-apiserver ready: true, restart count 0
  May 22 06:45:18.035: INFO: calico-node-k76wg from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.035: INFO: 	Container calico-node ready: true, restart count 0
  May 22 06:45:18.035: INFO: csi-node-driver-vg8gt from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 06:45:18.035: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 06:45:18.035: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 06:45:18.035: INFO: csi-rbdplugin-hjnwl from default started at 2023-05-19 12:08:20 +0000 UTC (3 container statuses recorded)
  May 22 06:45:18.035: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:45:18.035: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 06:45:18.035: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:45:18.035: INFO: csi-rbdplugin-provisioner-6d5864d5f5-2scr8 from default started at 2023-05-19 12:08:07 +0000 UTC (7 container statuses recorded)
  May 22 06:45:18.035: INFO: 	Container csi-attacher ready: true, restart count 0
  May 22 06:45:18.035: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 22 06:45:18.035: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:45:18.035: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 22 06:45:18.035: INFO: 	Container csi-resizer ready: true, restart count 0
  May 22 06:45:18.035: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 06:45:18.035: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:45:18.035: INFO: coredns-5d78c9869d-nckgt from kube-system started at 2023-05-17 07:47:08 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.035: INFO: 	Container coredns ready: true, restart count 0
  May 22 06:45:18.035: INFO: coredns-5d78c9869d-wrhg2 from kube-system started at 2023-05-17 07:47:08 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.035: INFO: 	Container coredns ready: true, restart count 0
  May 22 06:45:18.035: INFO: etcd-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.035: INFO: 	Container etcd ready: true, restart count 0
  May 22 06:45:18.035: INFO: kube-apiserver-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.035: INFO: 	Container kube-apiserver ready: true, restart count 0
  May 22 06:45:18.035: INFO: kube-controller-manager-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.035: INFO: 	Container kube-controller-manager ready: true, restart count 1
  May 22 06:45:18.035: INFO: kube-proxy-qx9ss from kube-system started at 2023-05-17 07:19:40 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.036: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 06:45:18.036: INFO: kube-scheduler-node1 from kube-system started at 2023-05-19 11:57:18 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.036: INFO: 	Container kube-scheduler ready: true, restart count 1
  May 22 06:45:18.036: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-bdnbl from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 06:45:18.036: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 06:45:18.036: INFO: 	Container systemd-logs ready: true, restart count 0
  May 22 06:45:18.036: INFO: 
  Logging pods the apiserver thinks is on node node2 before test
  E0522 06:45:18.036411      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:45:18.041: INFO: calico-node-s8lxt from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.041: INFO: 	Container calico-node ready: true, restart count 0
  May 22 06:45:18.041: INFO: calico-typha-cf4d768f9-99xbv from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.041: INFO: 	Container calico-typha ready: true, restart count 0
  May 22 06:45:18.041: INFO: csi-node-driver-ffpxl from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 06:45:18.041: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 06:45:18.041: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 06:45:18.041: INFO: csi-rbdplugin-856rw from default started at 2023-05-22 06:42:47 +0000 UTC (3 container statuses recorded)
  May 22 06:45:18.041: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:45:18.041: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 06:45:18.041: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:45:18.041: INFO: csi-rbdplugin-provisioner-6d5864d5f5-mqnhh from default started at 2023-05-22 06:42:55 +0000 UTC (7 container statuses recorded)
  May 22 06:45:18.041: INFO: 	Container csi-attacher ready: true, restart count 0
  May 22 06:45:18.041: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 22 06:45:18.041: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:45:18.041: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 22 06:45:18.041: INFO: 	Container csi-resizer ready: true, restart count 0
  May 22 06:45:18.041: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 06:45:18.041: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:45:18.041: INFO: ephemeral-containers-target-pod from ephemeral-containers-test-4567 started at 2023-05-22 06:45:07 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.041: INFO: 	Container test-container-1 ready: true, restart count 0
  May 22 06:45:18.041: INFO: kube-proxy-k5zhn from kube-system started at 2023-05-17 07:50:57 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.041: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 06:45:18.041: INFO: sonobuoy from sonobuoy started at 2023-05-22 05:20:17 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.041: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 22 06:45:18.041: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-hm7kk from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 06:45:18.041: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 06:45:18.041: INFO: 	Container systemd-logs ready: true, restart count 0
  May 22 06:45:18.041: INFO: tigera-operator-549d4f9bdb-g29xs from tigera-operator started at 2023-05-19 08:43:58 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.041: INFO: 	Container tigera-operator ready: true, restart count 2
  May 22 06:45:18.041: INFO: 
  Logging pods the apiserver thinks is on node node3 before test
  May 22 06:45:18.046: INFO: calico-apiserver-666fc8f69-48crp from calico-apiserver started at 2023-05-19 12:51:48 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.046: INFO: 	Container calico-apiserver ready: true, restart count 1
  May 22 06:45:18.046: INFO: calico-kube-controllers-789dc4c76b-qqbbj from calico-system started at 2023-05-19 12:51:48 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.046: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 22 06:45:18.046: INFO: calico-node-g6cnn from calico-system started at 2023-05-19 08:44:08 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.046: INFO: 	Container calico-node ready: true, restart count 0
  May 22 06:45:18.046: INFO: calico-typha-cf4d768f9-fgk49 from calico-system started at 2023-05-19 08:44:10 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.047: INFO: 	Container calico-typha ready: true, restart count 0
  May 22 06:45:18.047: INFO: csi-node-driver-jjbvg from calico-system started at 2023-05-19 08:44:08 +0000 UTC (2 container statuses recorded)
  May 22 06:45:18.047: INFO: 	Container calico-csi ready: true, restart count 0
  May 22 06:45:18.047: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
  May 22 06:45:18.047: INFO: csi-rbdplugin-nkgs7 from default started at 2023-05-19 12:08:20 +0000 UTC (3 container statuses recorded)
  May 22 06:45:18.047: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:45:18.047: INFO: 	Container driver-registrar ready: true, restart count 0
  May 22 06:45:18.047: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:45:18.047: INFO: csi-rbdplugin-provisioner-6d5864d5f5-cn9zx from default started at 2023-05-19 12:08:07 +0000 UTC (7 container statuses recorded)
  May 22 06:45:18.047: INFO: 	Container csi-attacher ready: true, restart count 2
  May 22 06:45:18.047: INFO: 	Container csi-provisioner ready: true, restart count 1
  May 22 06:45:18.047: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 22 06:45:18.047: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 1
  May 22 06:45:18.047: INFO: 	Container csi-resizer ready: true, restart count 2
  May 22 06:45:18.047: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 22 06:45:18.047: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 22 06:45:18.047: INFO: kube-proxy-kgvhd from kube-system started at 2023-05-17 07:51:09 +0000 UTC (1 container statuses recorded)
  May 22 06:45:18.047: INFO: 	Container kube-proxy ready: true, restart count 0
  May 22 06:45:18.047: INFO: sonobuoy-e2e-job-3f7387b6fe3c48b5 from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 06:45:18.047: INFO: 	Container e2e ready: true, restart count 0
  May 22 06:45:18.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 06:45:18.047: INFO: sonobuoy-systemd-logs-daemon-set-fc141dde39ef4473-d6c6x from sonobuoy started at 2023-05-22 05:20:18 +0000 UTC (2 container statuses recorded)
  May 22 06:45:18.047: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 22 06:45:18.047: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/22/23 06:45:18.047
  E0522 06:45:19.037534      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:20.038045      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/22/23 06:45:20.057
  STEP: Trying to apply a random label on the found node. @ 05/22/23 06:45:20.066
  STEP: verifying the node has the label kubernetes.io/e2e-19692dcd-4b42-4b1c-a5d3-be139098d8cf 95 @ 05/22/23 06:45:20.073
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/22/23 06:45:20.076
  E0522 06:45:21.039022      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:22.039381      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.33.122 on the node which pod4 resides and expect not scheduled @ 05/22/23 06:45:22.086
  E0522 06:45:23.039467      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:24.039912      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:25.039924      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:26.040112      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:27.040228      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:28.040348      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:29.040653      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:30.041064      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:31.041527      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:32.041733      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:33.042712      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:34.043053      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:35.043015      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:36.043232      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:37.043882      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:38.044112      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:39.045137      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:40.045854      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:41.045922      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:42.046145      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:43.046763      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:44.047542      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:45.047542      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:46.047769      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:47.048635      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:48.048860      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:49.049009      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:50.049566      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:51.050492      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:52.050919      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:53.051461      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:54.051612      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:55.051774      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:56.052790      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:57.053600      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:58.054000      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:45:59.054139      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:00.055013      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:01.055138      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:02.055344      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:03.055906      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:04.056128      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:05.056772      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:06.056999      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:07.057530      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:08.057892      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:09.058025      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:10.058874      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:11.059391      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:12.060281      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:13.060858      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:14.061045      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:15.061531      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:16.061712      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:17.062669      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:18.062980      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:19.063096      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:20.063669      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:21.063849      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:22.064587      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:23.065267      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:24.065471      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:25.065718      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:26.065907      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:27.066014      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:28.066148      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:29.067189      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:30.067907      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:31.068536      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:32.068704      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:33.068737      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:34.069757      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:35.070744      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:36.071039      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:37.071857      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:38.072041      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:39.072466      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:40.073036      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:41.073871      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:42.074054      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:43.074694      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:44.074872      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:45.075463      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:46.075849      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:47.076464      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:48.076621      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:49.076676      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:50.077177      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:51.078215      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:52.078331      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:53.079034      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:54.079121      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:55.079134      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:56.079503      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:57.080612      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:58.080956      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:46:59.082081      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:00.082233      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:01.082840      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:02.083220      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:03.083127      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:04.083284      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:05.084102      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:06.084687      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:07.085525      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:08.085900      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:09.086054      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:10.086758      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:11.087465      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:12.087688      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:13.088140      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:14.088552      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:15.088656      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:16.088902      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:17.089631      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:18.089765      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:19.089847      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:20.090598      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:21.090732      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:22.091316      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:23.091634      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:24.091582      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:25.092275      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:26.092460      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:27.093233      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:28.093511      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:29.093855      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:30.093953      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:31.094619      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:32.094832      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:33.095837      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:34.096060      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:35.096765      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:36.096889      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:37.097602      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:38.097811      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:39.097941      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:40.098820      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:41.099446      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:42.099670      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:43.099844      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:44.100036      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:45.100318      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:46.100509      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:47.101168      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:48.101522      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:49.102223      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:50.102759      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:51.103733      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:52.103948      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:53.104663      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:54.104890      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:55.105845      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:56.106071      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:57.107063      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:58.107321      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:47:59.107702      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:00.108477      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:01.109323      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:02.110124      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:03.110636      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:04.110863      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:05.110894      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:06.111108      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:07.111448      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:08.111693      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:09.111947      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:10.112625      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:11.113217      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:12.113579      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:13.113797      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:14.114134      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:15.114960      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:16.115164      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:17.115974      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:18.116698      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:19.116775      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:20.117335      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:21.118106      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:22.118209      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:23.118844      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:24.119651      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:25.120653      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:26.120827      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:27.121893      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:28.122034      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:29.122177      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:30.122742      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:31.122857      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:32.123039      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:33.123191      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:34.123287      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:35.123989      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:36.124163      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:37.124307      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:38.124440      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:39.124716      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:40.125550      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:41.126279      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:42.126567      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:43.127330      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:44.127555      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:45.127694      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:46.127914      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:47.128033      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:48.128270      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:49.129339      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:50.129981      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:51.130577      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:52.130817      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:53.130956      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:54.131495      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:55.131739      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:56.132180      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:57.132533      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:58.132806      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:48:59.132934      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:00.133583      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:01.133989      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:02.134361      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:03.134385      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:04.134696      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:05.135541      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:06.135773      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:07.136240      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:08.136624      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:09.136888      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:10.137478      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:11.137651      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:12.137993      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:13.139045      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:14.139277      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:15.140334      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:16.140632      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:17.141164      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:18.141433      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:19.141713      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:20.142225      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:21.142582      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:22.143269      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:23.143031      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:24.143252      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:25.144102      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:26.144423      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:27.145069      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:28.145262      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:29.145698      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:30.146162      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:31.146504      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:32.146708      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:33.147361      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:34.147655      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:35.148587      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:36.148811      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:37.149730      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:38.149940      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:39.150178      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:40.150314      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:41.150787      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:42.151866      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:43.152160      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:44.152361      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:45.152479      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:46.153120      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:47.153605      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:48.153795      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:49.153898      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:50.154132      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:51.154560      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:52.154771      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:53.154914      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:54.155067      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:55.155866      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:56.156075      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:57.156219      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:58.156449      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:49:59.156598      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:00.157251      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:01.158112      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:02.158336      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:03.158899      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:04.159893      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:05.160919      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:06.161075      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:07.161234      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:08.161303      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:09.161627      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:10.161653      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:11.162239      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:12.162326      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:13.162897      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:14.163246      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:15.164192      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:16.164418      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:17.165025      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:18.165279      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:19.165699      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:20.166691      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:21.167316      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-19692dcd-4b42-4b1c-a5d3-be139098d8cf off the node node2 @ 05/22/23 06:50:22.092
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-19692dcd-4b42-4b1c-a5d3-be139098d8cf @ 05/22/23 06:50:22.102
  May 22 06:50:22.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2278" for this suite. @ 05/22/23 06:50:22.108
• [304.099 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/22/23 06:50:22.112
  May 22 06:50:22.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:50:22.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:50:22.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:50:22.123
  STEP: Creating secret with name secret-test-7467f29e-7265-414b-8a4d-a44e38cd7f05 @ 05/22/23 06:50:22.125
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:50:22.127
  E0522 06:50:22.167607      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:23.168111      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:24.168463      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:25.168584      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:50:26.138
  May 22 06:50:26.140: INFO: Trying to get logs from node node2 pod pod-secrets-3947ee4d-8d3d-4205-9c82-2faf3467050e container secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 06:50:26.156
  May 22 06:50:26.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0522 06:50:26.168587      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "secrets-1427" for this suite. @ 05/22/23 06:50:26.169
• [4.061 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 05/22/23 06:50:26.174
  May 22 06:50:26.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/22/23 06:50:26.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:50:26.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:50:26.185
  STEP: set up a multi version CRD @ 05/22/23 06:50:26.187
  May 22 06:50:26.188: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  E0522 06:50:27.168755      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:28.169289      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:29.169847      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 05/22/23 06:50:30.012
  STEP: check the unserved version gets removed @ 05/22/23 06:50:30.024
  E0522 06:50:30.170931      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:31.171847      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/22/23 06:50:31.436
  E0522 06:50:32.174280      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:33.174896      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:34.175546      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:50:34.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6481" for this suite. @ 05/22/23 06:50:34.434
• [8.264 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/22/23 06:50:34.438
  May 22 06:50:34.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename watch @ 05/22/23 06:50:34.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:50:34.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:50:34.448
  STEP: creating a watch on configmaps with a certain label @ 05/22/23 06:50:34.45
  STEP: creating a new configmap @ 05/22/23 06:50:34.451
  STEP: modifying the configmap once @ 05/22/23 06:50:34.453
  STEP: changing the label value of the configmap @ 05/22/23 06:50:34.457
  STEP: Expecting to observe a delete notification for the watched object @ 05/22/23 06:50:34.461
  May 22 06:50:34.461: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5113  e57d0e2a-0af1-4b4a-a161-43e201ca213f 914008 0 2023-05-22 06:50:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-22 06:50:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:50:34.461: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5113  e57d0e2a-0af1-4b4a-a161-43e201ca213f 914009 0 2023-05-22 06:50:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-22 06:50:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:50:34.461: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5113  e57d0e2a-0af1-4b4a-a161-43e201ca213f 914010 0 2023-05-22 06:50:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-22 06:50:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/22/23 06:50:34.462
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/22/23 06:50:34.465
  E0522 06:50:35.176116      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:36.176489      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:37.176920      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:38.177093      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:39.177264      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:40.177862      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:41.178320      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:42.178646      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:43.178863      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:44.179026      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 05/22/23 06:50:44.466
  STEP: modifying the configmap a third time @ 05/22/23 06:50:44.472
  STEP: deleting the configmap @ 05/22/23 06:50:44.476
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/22/23 06:50:44.478
  May 22 06:50:44.479: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5113  e57d0e2a-0af1-4b4a-a161-43e201ca213f 914049 0 2023-05-22 06:50:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-22 06:50:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:50:44.479: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5113  e57d0e2a-0af1-4b4a-a161-43e201ca213f 914050 0 2023-05-22 06:50:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-22 06:50:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:50:44.479: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5113  e57d0e2a-0af1-4b4a-a161-43e201ca213f 914051 0 2023-05-22 06:50:34 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-22 06:50:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 22 06:50:44.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5113" for this suite. @ 05/22/23 06:50:44.482
• [10.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 05/22/23 06:50:44.486
  May 22 06:50:44.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 06:50:44.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:50:44.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:50:44.5
  STEP: validating cluster-info @ 05/22/23 06:50:44.502
  May 22 06:50:44.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-9274 cluster-info'
  May 22 06:50:44.559: INFO: stderr: ""
  May 22 06:50:44.559: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  May 22 06:50:44.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9274" for this suite. @ 05/22/23 06:50:44.563
• [0.080 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 05/22/23 06:50:44.567
  May 22 06:50:44.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename var-expansion @ 05/22/23 06:50:44.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:50:44.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:50:44.576
  E0522 06:50:45.179992      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:46.181045      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:50:46.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 06:50:46.591: INFO: Deleting pod "var-expansion-e8f33c26-0581-4fe3-a3f4-defb18c693b8" in namespace "var-expansion-9440"
  May 22 06:50:46.594: INFO: Wait up to 5m0s for pod "var-expansion-e8f33c26-0581-4fe3-a3f4-defb18c693b8" to be fully deleted
  E0522 06:50:47.181570      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:48.181769      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:49.182367      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:50.182947      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-9440" for this suite. @ 05/22/23 06:50:50.603
• [6.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 05/22/23 06:50:50.606
  May 22 06:50:50.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl-logs @ 05/22/23 06:50:50.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:50:50.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:50:50.617
  STEP: creating an pod @ 05/22/23 06:50:50.619
  May 22 06:50:50.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-logs-7601 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  May 22 06:50:50.679: INFO: stderr: ""
  May 22 06:50:50.679: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/22/23 06:50:50.68
  May 22 06:50:50.680: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0522 06:50:51.183466      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:52.183758      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:50:52.684: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/22/23 06:50:52.684
  May 22 06:50:52.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-logs-7601 logs logs-generator logs-generator'
  May 22 06:50:52.764: INFO: stderr: ""
  May 22 06:50:52.764: INFO: stdout: "I0522 06:50:51.328292       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5v46 500\nI0522 06:50:51.528696       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/q7xf 574\nI0522 06:50:51.729102       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/vrm 349\nI0522 06:50:51.928356       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/mr6 456\nI0522 06:50:52.128713       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/jrwg 543\nI0522 06:50:52.329056       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/lsr 281\nI0522 06:50:52.528339       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/s7sd 546\nI0522 06:50:52.728709       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/b7z 441\n"
  E0522 06:50:53.184361      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:54.184554      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:50:54.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-logs-7601 logs logs-generator logs-generator'
  May 22 06:50:54.827: INFO: stderr: ""
  May 22 06:50:54.827: INFO: stdout: "I0522 06:50:51.328292       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5v46 500\nI0522 06:50:51.528696       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/q7xf 574\nI0522 06:50:51.729102       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/vrm 349\nI0522 06:50:51.928356       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/mr6 456\nI0522 06:50:52.128713       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/jrwg 543\nI0522 06:50:52.329056       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/lsr 281\nI0522 06:50:52.528339       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/s7sd 546\nI0522 06:50:52.728709       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/b7z 441\nI0522 06:50:52.928978       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/jdq 560\nI0522 06:50:53.129332       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/gqhm 397\nI0522 06:50:53.328649       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/bvl 385\nI0522 06:50:53.529011       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/xnv 451\nI0522 06:50:53.729455       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/msd8 584\nI0522 06:50:53.928687       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/x4r 518\nI0522 06:50:54.129031       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/hz9 214\nI0522 06:50:54.328312       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/k7d 569\nI0522 06:50:54.529964       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/n9z 242\nI0522 06:50:54.729322       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/29x 513\n"
  STEP: limiting log lines @ 05/22/23 06:50:54.827
  May 22 06:50:54.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-logs-7601 logs logs-generator logs-generator --tail=1'
  May 22 06:50:54.892: INFO: stderr: ""
  May 22 06:50:54.892: INFO: stdout: "I0522 06:50:54.729322       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/29x 513\n"
  May 22 06:50:54.892: INFO: got output "I0522 06:50:54.729322       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/29x 513\n"
  STEP: limiting log bytes @ 05/22/23 06:50:54.892
  May 22 06:50:54.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-logs-7601 logs logs-generator logs-generator --limit-bytes=1'
  May 22 06:50:54.953: INFO: stderr: ""
  May 22 06:50:54.953: INFO: stdout: "I"
  May 22 06:50:54.953: INFO: got output "I"
  STEP: exposing timestamps @ 05/22/23 06:50:54.953
  May 22 06:50:54.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-logs-7601 logs logs-generator logs-generator --tail=1 --timestamps'
  May 22 06:50:55.014: INFO: stderr: ""
  May 22 06:50:55.014: INFO: stdout: "2023-05-22T06:50:54.928715633Z I0522 06:50:54.928614       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/99nk 518\n"
  May 22 06:50:55.014: INFO: got output "2023-05-22T06:50:54.928715633Z I0522 06:50:54.928614       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/99nk 518\n"
  STEP: restricting to a time range @ 05/22/23 06:50:55.014
  E0522 06:50:55.185157      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:56.185395      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:57.185598      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:50:57.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-logs-7601 logs logs-generator logs-generator --since=1s'
  May 22 06:50:57.576: INFO: stderr: ""
  May 22 06:50:57.576: INFO: stdout: "I0522 06:50:56.728948       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/c6d 484\nI0522 06:50:56.929301       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/b6j 429\nI0522 06:50:57.128623       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/g4hh 559\nI0522 06:50:57.328973       1 logs_generator.go:76] 30 GET /api/v1/namespaces/kube-system/pods/w47s 220\nI0522 06:50:57.529298       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/ffm2 577\n"
  May 22 06:50:57.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-logs-7601 logs logs-generator logs-generator --since=24h'
  May 22 06:50:57.636: INFO: stderr: ""
  May 22 06:50:57.636: INFO: stdout: "I0522 06:50:51.328292       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/5v46 500\nI0522 06:50:51.528696       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/q7xf 574\nI0522 06:50:51.729102       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/vrm 349\nI0522 06:50:51.928356       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/mr6 456\nI0522 06:50:52.128713       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/jrwg 543\nI0522 06:50:52.329056       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/lsr 281\nI0522 06:50:52.528339       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/s7sd 546\nI0522 06:50:52.728709       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/b7z 441\nI0522 06:50:52.928978       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/jdq 560\nI0522 06:50:53.129332       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/gqhm 397\nI0522 06:50:53.328649       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/bvl 385\nI0522 06:50:53.529011       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/xnv 451\nI0522 06:50:53.729455       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/msd8 584\nI0522 06:50:53.928687       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/x4r 518\nI0522 06:50:54.129031       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/hz9 214\nI0522 06:50:54.328312       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/k7d 569\nI0522 06:50:54.529964       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/n9z 242\nI0522 06:50:54.729322       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/29x 513\nI0522 06:50:54.928614       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/99nk 518\nI0522 06:50:55.128943       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/8g69 472\nI0522 06:50:55.329248       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/mffc 495\nI0522 06:50:55.528510       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/29b6 448\nI0522 06:50:55.728845       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/8dh 331\nI0522 06:50:55.929321       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/vtm 277\nI0522 06:50:56.128781       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/wdj 592\nI0522 06:50:56.329271       1 logs_generator.go:76] 25 GET /api/v1/namespaces/default/pods/8q5n 519\nI0522 06:50:56.528654       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/nb2 207\nI0522 06:50:56.728948       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/c6d 484\nI0522 06:50:56.929301       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/b6j 429\nI0522 06:50:57.128623       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/g4hh 559\nI0522 06:50:57.328973       1 logs_generator.go:76] 30 GET /api/v1/namespaces/kube-system/pods/w47s 220\nI0522 06:50:57.529298       1 logs_generator.go:76] 31 POST /api/v1/namespaces/kube-system/pods/ffm2 577\n"
  May 22 06:50:57.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-logs-7601 delete pod logs-generator'
  May 22 06:50:58.156: INFO: stderr: ""
  May 22 06:50:58.156: INFO: stdout: "pod \"logs-generator\" deleted\n"
  May 22 06:50:58.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-7601" for this suite. @ 05/22/23 06:50:58.158
• [7.556 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 05/22/23 06:50:58.162
  May 22 06:50:58.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename dns @ 05/22/23 06:50:58.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:50:58.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:50:58.171
  STEP: Creating a test externalName service @ 05/22/23 06:50:58.172
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7081.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local; sleep 1; done
   @ 05/22/23 06:50:58.175
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7081.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local; sleep 1; done
   @ 05/22/23 06:50:58.175
  STEP: creating a pod to probe DNS @ 05/22/23 06:50:58.175
  STEP: submitting the pod to kubernetes @ 05/22/23 06:50:58.175
  E0522 06:50:58.186171      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:50:59.186540      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/22/23 06:51:00.184
  E0522 06:51:00.186670      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: looking for the results for each expected name from probers @ 05/22/23 06:51:00.186
  May 22 06:51:00.192: INFO: DNS probes using dns-test-23072d31-6fb7-452c-b61b-7425a79611f5 succeeded

  STEP: changing the externalName to bar.example.com @ 05/22/23 06:51:00.192
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7081.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local; sleep 1; done
   @ 05/22/23 06:51:00.196
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7081.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local; sleep 1; done
   @ 05/22/23 06:51:00.196
  STEP: creating a second pod to probe DNS @ 05/22/23 06:51:00.196
  STEP: submitting the pod to kubernetes @ 05/22/23 06:51:00.196
  E0522 06:51:01.187035      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:02.187276      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/22/23 06:51:02.209
  STEP: looking for the results for each expected name from probers @ 05/22/23 06:51:02.211
  May 22 06:51:02.213: INFO: File wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:02.215: INFO: File jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:02.215: INFO: Lookups using dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 failed for: [wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local]

  E0522 06:51:03.187949      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:04.188197      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:05.188897      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:06.189129      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:07.189410      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:51:07.219: INFO: File wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:07.221: INFO: File jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:07.221: INFO: Lookups using dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 failed for: [wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local]

  E0522 06:51:08.189665      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:09.189870      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:10.190456      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:11.190617      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:12.190781      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:51:12.219: INFO: File wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:12.221: INFO: File jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:12.221: INFO: Lookups using dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 failed for: [wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local]

  E0522 06:51:13.190812      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:14.191978      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:15.192549      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:16.192752      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:17.193071      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:51:17.219: INFO: File wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:17.221: INFO: File jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:17.221: INFO: Lookups using dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 failed for: [wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local]

  E0522 06:51:18.193284      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:19.193488      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:20.193640      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:21.193842      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:22.194217      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:51:22.219: INFO: File wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:22.221: INFO: File jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:22.221: INFO: Lookups using dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 failed for: [wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local]

  E0522 06:51:23.194726      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:24.194853      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:25.195317      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:26.195530      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:27.195717      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:51:27.218: INFO: File wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:27.220: INFO: File jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local from pod  dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 22 06:51:27.220: INFO: Lookups using dns-7081/dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 failed for: [wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local]

  E0522 06:51:28.195921      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:29.196080      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:30.196748      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:31.196932      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:32.197338      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:51:32.221: INFO: DNS probes using dns-test-7cf30e94-083d-42d3-a87a-8dfc45fbe767 succeeded

  STEP: changing the service to type=ClusterIP @ 05/22/23 06:51:32.221
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7081.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7081.svc.cluster.local; sleep 1; done
   @ 05/22/23 06:51:32.231
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7081.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7081.svc.cluster.local; sleep 1; done
   @ 05/22/23 06:51:32.231
  STEP: creating a third pod to probe DNS @ 05/22/23 06:51:32.232
  STEP: submitting the pod to kubernetes @ 05/22/23 06:51:32.234
  E0522 06:51:33.197684      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:34.197821      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/22/23 06:51:34.244
  STEP: looking for the results for each expected name from probers @ 05/22/23 06:51:34.245
  May 22 06:51:34.249: INFO: DNS probes using dns-test-887703a1-0f28-485e-a7b5-515d03814fd2 succeeded

  May 22 06:51:34.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 06:51:34.252
  STEP: deleting the pod @ 05/22/23 06:51:34.257
  STEP: deleting the pod @ 05/22/23 06:51:34.264
  STEP: deleting the test externalName service @ 05/22/23 06:51:34.27
  STEP: Destroying namespace "dns-7081" for this suite. @ 05/22/23 06:51:34.28
• [36.124 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 05/22/23 06:51:34.287
  May 22 06:51:34.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:51:34.288
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:51:34.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:51:34.304
  STEP: creating a ConfigMap @ 05/22/23 06:51:34.306
  STEP: fetching the ConfigMap @ 05/22/23 06:51:34.309
  STEP: patching the ConfigMap @ 05/22/23 06:51:34.31
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/22/23 06:51:34.313
  STEP: deleting the ConfigMap by collection with a label selector @ 05/22/23 06:51:34.316
  STEP: listing all ConfigMaps in test namespace @ 05/22/23 06:51:34.318
  May 22 06:51:34.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6837" for this suite. @ 05/22/23 06:51:34.322
• [0.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 05/22/23 06:51:34.326
  May 22 06:51:34.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename namespaces @ 05/22/23 06:51:34.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:51:34.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:51:34.335
  STEP: Creating a test namespace @ 05/22/23 06:51:34.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:51:34.342
  STEP: Creating a service in the namespace @ 05/22/23 06:51:34.344
  STEP: Deleting the namespace @ 05/22/23 06:51:34.351
  STEP: Waiting for the namespace to be removed. @ 05/22/23 06:51:34.356
  E0522 06:51:35.198243      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:36.198181      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:37.198579      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:38.199097      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:39.199613      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:40.199745      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/22/23 06:51:40.359
  STEP: Verifying there is no service in the namespace @ 05/22/23 06:51:40.365
  May 22 06:51:40.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1832" for this suite. @ 05/22/23 06:51:40.369
  STEP: Destroying namespace "nsdeletetest-4452" for this suite. @ 05/22/23 06:51:40.372
  May 22 06:51:40.373: INFO: Namespace nsdeletetest-4452 was already deleted
  STEP: Destroying namespace "nsdeletetest-9078" for this suite. @ 05/22/23 06:51:40.373
• [6.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 05/22/23 06:51:40.377
  May 22 06:51:40.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename kubectl @ 05/22/23 06:51:40.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:51:40.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:51:40.385
  STEP: creating all guestbook components @ 05/22/23 06:51:40.387
  May 22 06:51:40.387: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  May 22 06:51:40.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 create -f -'
  E0522 06:51:41.199770      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:51:41.290: INFO: stderr: ""
  May 22 06:51:41.290: INFO: stdout: "service/agnhost-replica created\n"
  May 22 06:51:41.290: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  May 22 06:51:41.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 create -f -'
  May 22 06:51:41.583: INFO: stderr: ""
  May 22 06:51:41.583: INFO: stdout: "service/agnhost-primary created\n"
  May 22 06:51:41.583: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  May 22 06:51:41.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 create -f -'
  May 22 06:51:41.860: INFO: stderr: ""
  May 22 06:51:41.860: INFO: stdout: "service/frontend created\n"
  May 22 06:51:41.860: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  May 22 06:51:41.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 create -f -'
  May 22 06:51:42.119: INFO: stderr: ""
  May 22 06:51:42.119: INFO: stdout: "deployment.apps/frontend created\n"
  May 22 06:51:42.120: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 22 06:51:42.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 create -f -'
  E0522 06:51:42.200520      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:51:42.383: INFO: stderr: ""
  May 22 06:51:42.383: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  May 22 06:51:42.383: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 22 06:51:42.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 create -f -'
  May 22 06:51:42.687: INFO: stderr: ""
  May 22 06:51:42.687: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/22/23 06:51:42.687
  May 22 06:51:42.687: INFO: Waiting for all frontend pods to be Running.
  E0522 06:51:43.201514      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:44.201631      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:45.202138      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:46.202248      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:47.202371      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:51:47.737: INFO: Waiting for frontend to serve content.
  May 22 06:51:47.743: INFO: Trying to add a new entry to the guestbook.
  May 22 06:51:47.750: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 05/22/23 06:51:47.755
  May 22 06:51:47.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 delete --grace-period=0 --force -f -'
  May 22 06:51:47.822: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 22 06:51:47.822: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/22/23 06:51:47.822
  May 22 06:51:47.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 delete --grace-period=0 --force -f -'
  May 22 06:51:47.887: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 22 06:51:47.887: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/22/23 06:51:47.887
  May 22 06:51:47.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 delete --grace-period=0 --force -f -'
  May 22 06:51:47.949: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 22 06:51:47.949: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/22/23 06:51:47.949
  May 22 06:51:47.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 delete --grace-period=0 --force -f -'
  May 22 06:51:48.006: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 22 06:51:48.006: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/22/23 06:51:48.007
  May 22 06:51:48.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 delete --grace-period=0 --force -f -'
  May 22 06:51:48.071: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 22 06:51:48.071: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/22/23 06:51:48.071
  May 22 06:51:48.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=kubectl-3919 delete --grace-period=0 --force -f -'
  May 22 06:51:48.138: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 22 06:51:48.138: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  May 22 06:51:48.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3919" for this suite. @ 05/22/23 06:51:48.142
• [7.772 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 05/22/23 06:51:48.149
  May 22 06:51:48.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename projected @ 05/22/23 06:51:48.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:51:48.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:51:48.163
  STEP: Creating a pod to test downward API volume plugin @ 05/22/23 06:51:48.165
  E0522 06:51:48.202970      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:49.203162      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:50.203188      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:51.203421      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:51:52.18
  May 22 06:51:52.182: INFO: Trying to get logs from node node2 pod downwardapi-volume-50a94cb5-c454-4e90-aa4c-5239f2a8722a container client-container: <nil>
  STEP: delete the pod @ 05/22/23 06:51:52.186
  May 22 06:51:52.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4804" for this suite. @ 05/22/23 06:51:52.195
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 05/22/23 06:51:52.199
  May 22 06:51:52.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename subpath @ 05/22/23 06:51:52.199
  E0522 06:51:52.203787      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:51:52.205
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:51:52.207
  STEP: Setting up data @ 05/22/23 06:51:52.209
  STEP: Creating pod pod-subpath-test-secret-lksv @ 05/22/23 06:51:52.213
  STEP: Creating a pod to test atomic-volume-subpath @ 05/22/23 06:51:52.213
  E0522 06:51:53.204467      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:54.204950      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:55.205126      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:56.205318      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:57.205496      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:58.205765      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:51:59.205903      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:00.206568      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:01.206670      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:02.207630      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:03.208503      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:04.208741      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:05.208882      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:06.209086      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:07.209780      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:08.209960      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:09.210024      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:10.210353      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:11.211113      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:12.211335      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:13.212380      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:14.212602      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:15.212990      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:16.213210      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:52:16.257
  May 22 06:52:16.259: INFO: Trying to get logs from node node2 pod pod-subpath-test-secret-lksv container test-container-subpath-secret-lksv: <nil>
  STEP: delete the pod @ 05/22/23 06:52:16.264
  STEP: Deleting pod pod-subpath-test-secret-lksv @ 05/22/23 06:52:16.277
  May 22 06:52:16.277: INFO: Deleting pod "pod-subpath-test-secret-lksv" in namespace "subpath-2119"
  May 22 06:52:16.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2119" for this suite. @ 05/22/23 06:52:16.285
• [24.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/22/23 06:52:16.331
  May 22 06:52:16.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename svc-latency @ 05/22/23 06:52:16.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:52:16.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:52:16.341
  May 22 06:52:16.343: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-3784 @ 05/22/23 06:52:16.343
  I0522 06:52:16.347758      25 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3784, replica count: 1
  E0522 06:52:17.213496      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0522 06:52:17.399812      25 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 22 06:52:17.509: INFO: Created: latency-svc-jwxck
  May 22 06:52:17.512: INFO: Got endpoints: latency-svc-jwxck [11.86974ms]
  May 22 06:52:17.520: INFO: Created: latency-svc-c4h29
  May 22 06:52:17.525: INFO: Got endpoints: latency-svc-c4h29 [12.920264ms]
  May 22 06:52:17.527: INFO: Created: latency-svc-jswkt
  May 22 06:52:17.529: INFO: Got endpoints: latency-svc-jswkt [16.752209ms]
  May 22 06:52:17.534: INFO: Created: latency-svc-k5r4r
  May 22 06:52:17.537: INFO: Got endpoints: latency-svc-k5r4r [24.281233ms]
  May 22 06:52:17.542: INFO: Created: latency-svc-qnwks
  May 22 06:52:17.547: INFO: Got endpoints: latency-svc-qnwks [35.21317ms]
  May 22 06:52:17.549: INFO: Created: latency-svc-d4s2m
  May 22 06:52:17.550: INFO: Got endpoints: latency-svc-d4s2m [38.284589ms]
  May 22 06:52:17.555: INFO: Created: latency-svc-k5j7m
  May 22 06:52:17.557: INFO: Got endpoints: latency-svc-k5j7m [45.186774ms]
  May 22 06:52:17.562: INFO: Created: latency-svc-w6pnw
  May 22 06:52:17.564: INFO: Got endpoints: latency-svc-w6pnw [51.837575ms]
  May 22 06:52:17.569: INFO: Created: latency-svc-2z56h
  May 22 06:52:17.572: INFO: Got endpoints: latency-svc-2z56h [59.363254ms]
  May 22 06:52:17.572: INFO: Created: latency-svc-d6xdq
  May 22 06:52:17.574: INFO: Got endpoints: latency-svc-d6xdq [61.950265ms]
  May 22 06:52:17.581: INFO: Created: latency-svc-g55l5
  May 22 06:52:17.584: INFO: Got endpoints: latency-svc-g55l5 [71.450122ms]
  May 22 06:52:17.586: INFO: Created: latency-svc-558mh
  May 22 06:52:17.589: INFO: Got endpoints: latency-svc-558mh [77.137899ms]
  May 22 06:52:17.591: INFO: Created: latency-svc-wx9vb
  May 22 06:52:17.593: INFO: Got endpoints: latency-svc-wx9vb [80.941015ms]
  May 22 06:52:17.598: INFO: Created: latency-svc-shd8q
  May 22 06:52:17.602: INFO: Got endpoints: latency-svc-shd8q [89.943424ms]
  May 22 06:52:17.605: INFO: Created: latency-svc-x7w5t
  May 22 06:52:17.607: INFO: Got endpoints: latency-svc-x7w5t [94.502643ms]
  May 22 06:52:17.611: INFO: Created: latency-svc-d7fk2
  May 22 06:52:17.614: INFO: Got endpoints: latency-svc-d7fk2 [101.952543ms]
  May 22 06:52:17.616: INFO: Created: latency-svc-2z488
  May 22 06:52:17.619: INFO: Got endpoints: latency-svc-2z488 [93.862743ms]
  May 22 06:52:17.622: INFO: Created: latency-svc-nv68c
  May 22 06:52:17.624: INFO: Got endpoints: latency-svc-nv68c [94.916352ms]
  May 22 06:52:17.629: INFO: Created: latency-svc-bwxc8
  May 22 06:52:17.637: INFO: Got endpoints: latency-svc-bwxc8 [100.056486ms]
  May 22 06:52:17.638: INFO: Created: latency-svc-9k799
  May 22 06:52:17.641: INFO: Got endpoints: latency-svc-9k799 [93.076181ms]
  May 22 06:52:17.642: INFO: Created: latency-svc-hc8qf
  May 22 06:52:17.645: INFO: Got endpoints: latency-svc-hc8qf [94.26014ms]
  May 22 06:52:17.662: INFO: Created: latency-svc-svqw8
  May 22 06:52:17.666: INFO: Got endpoints: latency-svc-svqw8 [108.326196ms]
  May 22 06:52:17.669: INFO: Created: latency-svc-99sj9
  May 22 06:52:17.670: INFO: Got endpoints: latency-svc-99sj9 [106.372144ms]
  May 22 06:52:17.674: INFO: Created: latency-svc-wnvdz
  May 22 06:52:17.678: INFO: Got endpoints: latency-svc-wnvdz [105.888328ms]
  May 22 06:52:17.680: INFO: Created: latency-svc-dhdsv
  May 22 06:52:17.683: INFO: Got endpoints: latency-svc-dhdsv [108.697169ms]
  May 22 06:52:17.684: INFO: Created: latency-svc-gdk2s
  May 22 06:52:17.691: INFO: Got endpoints: latency-svc-gdk2s [107.266521ms]
  May 22 06:52:17.694: INFO: Created: latency-svc-wh7tn
  May 22 06:52:17.697: INFO: Got endpoints: latency-svc-wh7tn [107.242223ms]
  May 22 06:52:17.699: INFO: Created: latency-svc-dtfjs
  May 22 06:52:17.700: INFO: Got endpoints: latency-svc-dtfjs [106.960216ms]
  May 22 06:52:17.704: INFO: Created: latency-svc-qfgdg
  May 22 06:52:17.709: INFO: Got endpoints: latency-svc-qfgdg [107.021432ms]
  May 22 06:52:17.737: INFO: Created: latency-svc-6rtfx
  May 22 06:52:17.746: INFO: Got endpoints: latency-svc-6rtfx [138.852057ms]
  May 22 06:52:17.751: INFO: Created: latency-svc-x7hvc
  May 22 06:52:17.751: INFO: Got endpoints: latency-svc-x7hvc [136.556337ms]
  May 22 06:52:17.752: INFO: Created: latency-svc-47vzt
  May 22 06:52:17.754: INFO: Got endpoints: latency-svc-47vzt [135.270426ms]
  May 22 06:52:17.758: INFO: Created: latency-svc-sjpww
  May 22 06:52:17.771: INFO: Got endpoints: latency-svc-sjpww [147.002966ms]
  May 22 06:52:17.772: INFO: Created: latency-svc-knptf
  May 22 06:52:17.777: INFO: Got endpoints: latency-svc-knptf [139.988371ms]
  May 22 06:52:17.778: INFO: Created: latency-svc-8pvcs
  May 22 06:52:17.780: INFO: Got endpoints: latency-svc-8pvcs [139.085851ms]
  May 22 06:52:17.784: INFO: Created: latency-svc-bbshz
  May 22 06:52:17.788: INFO: Created: latency-svc-ssgf5
  May 22 06:52:17.793: INFO: Created: latency-svc-hf4kj
  May 22 06:52:17.798: INFO: Created: latency-svc-fdlk8
  May 22 06:52:17.803: INFO: Created: latency-svc-t58hx
  May 22 06:52:17.807: INFO: Created: latency-svc-mxs59
  May 22 06:52:17.813: INFO: Got endpoints: latency-svc-bbshz [168.496181ms]
  May 22 06:52:17.819: INFO: Created: latency-svc-87nnd
  May 22 06:52:17.823: INFO: Created: latency-svc-7lrtr
  May 22 06:52:17.827: INFO: Created: latency-svc-69n9l
  May 22 06:52:17.832: INFO: Created: latency-svc-zqtbg
  May 22 06:52:17.838: INFO: Created: latency-svc-ljb6g
  May 22 06:52:17.841: INFO: Created: latency-svc-9d7pr
  May 22 06:52:17.846: INFO: Created: latency-svc-98v8f
  May 22 06:52:17.850: INFO: Created: latency-svc-wnft7
  May 22 06:52:17.855: INFO: Created: latency-svc-fg5cc
  May 22 06:52:17.860: INFO: Created: latency-svc-lvmx2
  May 22 06:52:17.861: INFO: Got endpoints: latency-svc-ssgf5 [195.161323ms]
  May 22 06:52:17.871: INFO: Created: latency-svc-r76sq
  May 22 06:52:17.911: INFO: Got endpoints: latency-svc-hf4kj [240.970312ms]
  May 22 06:52:17.920: INFO: Created: latency-svc-99sgb
  May 22 06:52:17.961: INFO: Got endpoints: latency-svc-fdlk8 [283.198367ms]
  May 22 06:52:17.971: INFO: Created: latency-svc-pw7rk
  May 22 06:52:18.011: INFO: Got endpoints: latency-svc-t58hx [328.052162ms]
  May 22 06:52:18.018: INFO: Created: latency-svc-4679r
  May 22 06:52:18.061: INFO: Got endpoints: latency-svc-mxs59 [370.063028ms]
  May 22 06:52:18.069: INFO: Created: latency-svc-pr8ww
  May 22 06:52:18.111: INFO: Got endpoints: latency-svc-87nnd [414.658885ms]
  May 22 06:52:18.120: INFO: Created: latency-svc-4765n
  May 22 06:52:18.161: INFO: Got endpoints: latency-svc-7lrtr [460.322437ms]
  May 22 06:52:18.169: INFO: Created: latency-svc-tg97n
  May 22 06:52:18.212: INFO: Got endpoints: latency-svc-69n9l [502.395996ms]
  E0522 06:52:18.214277      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:18.220: INFO: Created: latency-svc-vdvm8
  May 22 06:52:18.262: INFO: Got endpoints: latency-svc-zqtbg [516.296706ms]
  May 22 06:52:18.271: INFO: Created: latency-svc-r2db2
  May 22 06:52:18.311: INFO: Got endpoints: latency-svc-ljb6g [560.250988ms]
  May 22 06:52:18.319: INFO: Created: latency-svc-526fw
  May 22 06:52:18.362: INFO: Got endpoints: latency-svc-9d7pr [607.663406ms]
  May 22 06:52:18.371: INFO: Created: latency-svc-c5dgv
  May 22 06:52:18.411: INFO: Got endpoints: latency-svc-98v8f [639.708859ms]
  May 22 06:52:18.419: INFO: Created: latency-svc-crgcl
  May 22 06:52:18.461: INFO: Got endpoints: latency-svc-wnft7 [684.337512ms]
  May 22 06:52:18.470: INFO: Created: latency-svc-j4d2x
  May 22 06:52:18.511: INFO: Got endpoints: latency-svc-fg5cc [731.606779ms]
  May 22 06:52:18.520: INFO: Created: latency-svc-zw547
  May 22 06:52:18.562: INFO: Got endpoints: latency-svc-lvmx2 [748.327324ms]
  May 22 06:52:18.571: INFO: Created: latency-svc-n6wgg
  May 22 06:52:18.614: INFO: Got endpoints: latency-svc-r76sq [752.899482ms]
  May 22 06:52:18.626: INFO: Created: latency-svc-6dqq7
  May 22 06:52:18.662: INFO: Got endpoints: latency-svc-99sgb [750.879544ms]
  May 22 06:52:18.671: INFO: Created: latency-svc-stjjl
  May 22 06:52:18.712: INFO: Got endpoints: latency-svc-pw7rk [751.190418ms]
  May 22 06:52:18.723: INFO: Created: latency-svc-58m42
  May 22 06:52:18.761: INFO: Got endpoints: latency-svc-4679r [750.357563ms]
  May 22 06:52:18.770: INFO: Created: latency-svc-mjzms
  May 22 06:52:18.811: INFO: Got endpoints: latency-svc-pr8ww [749.907803ms]
  May 22 06:52:18.822: INFO: Created: latency-svc-fxd79
  May 22 06:52:18.862: INFO: Got endpoints: latency-svc-4765n [750.543101ms]
  May 22 06:52:18.876: INFO: Created: latency-svc-zk5r5
  May 22 06:52:18.911: INFO: Got endpoints: latency-svc-tg97n [750.31309ms]
  May 22 06:52:18.919: INFO: Created: latency-svc-px5kn
  May 22 06:52:18.961: INFO: Got endpoints: latency-svc-vdvm8 [749.657329ms]
  May 22 06:52:18.971: INFO: Created: latency-svc-k7k7s
  May 22 06:52:19.012: INFO: Got endpoints: latency-svc-r2db2 [749.40413ms]
  May 22 06:52:19.020: INFO: Created: latency-svc-8jb2d
  May 22 06:52:19.062: INFO: Got endpoints: latency-svc-526fw [750.38733ms]
  May 22 06:52:19.070: INFO: Created: latency-svc-cgj6k
  May 22 06:52:19.112: INFO: Got endpoints: latency-svc-c5dgv [749.693265ms]
  May 22 06:52:19.120: INFO: Created: latency-svc-vmmbq
  May 22 06:52:19.162: INFO: Got endpoints: latency-svc-crgcl [750.966215ms]
  May 22 06:52:19.169: INFO: Created: latency-svc-f7666
  May 22 06:52:19.210: INFO: Got endpoints: latency-svc-j4d2x [749.03179ms]
  E0522 06:52:19.214838      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:19.220: INFO: Created: latency-svc-nx7zx
  May 22 06:52:19.262: INFO: Got endpoints: latency-svc-zw547 [750.53431ms]
  May 22 06:52:19.271: INFO: Created: latency-svc-7shfv
  May 22 06:52:19.313: INFO: Got endpoints: latency-svc-n6wgg [751.532161ms]
  May 22 06:52:19.322: INFO: Created: latency-svc-bcqtc
  May 22 06:52:19.361: INFO: Got endpoints: latency-svc-6dqq7 [746.741867ms]
  May 22 06:52:19.369: INFO: Created: latency-svc-glnbx
  May 22 06:52:19.411: INFO: Got endpoints: latency-svc-stjjl [748.866182ms]
  May 22 06:52:19.420: INFO: Created: latency-svc-776hb
  May 22 06:52:19.461: INFO: Got endpoints: latency-svc-58m42 [748.454066ms]
  May 22 06:52:19.468: INFO: Created: latency-svc-tcrvh
  May 22 06:52:19.512: INFO: Got endpoints: latency-svc-mjzms [750.080294ms]
  May 22 06:52:19.521: INFO: Created: latency-svc-srmq9
  May 22 06:52:19.561: INFO: Got endpoints: latency-svc-fxd79 [749.88315ms]
  May 22 06:52:19.570: INFO: Created: latency-svc-zmfcj
  May 22 06:52:19.611: INFO: Got endpoints: latency-svc-zk5r5 [748.703446ms]
  May 22 06:52:19.618: INFO: Created: latency-svc-7lfm8
  May 22 06:52:19.661: INFO: Got endpoints: latency-svc-px5kn [750.330793ms]
  May 22 06:52:19.675: INFO: Created: latency-svc-nq8ng
  May 22 06:52:19.712: INFO: Got endpoints: latency-svc-k7k7s [750.650617ms]
  May 22 06:52:19.727: INFO: Created: latency-svc-x2rn8
  May 22 06:52:19.761: INFO: Got endpoints: latency-svc-8jb2d [749.798538ms]
  May 22 06:52:19.771: INFO: Created: latency-svc-7qt7t
  May 22 06:52:19.817: INFO: Got endpoints: latency-svc-cgj6k [755.564599ms]
  May 22 06:52:19.829: INFO: Created: latency-svc-zxj8k
  May 22 06:52:19.865: INFO: Got endpoints: latency-svc-vmmbq [752.881639ms]
  May 22 06:52:19.874: INFO: Created: latency-svc-t9zpz
  May 22 06:52:19.911: INFO: Got endpoints: latency-svc-f7666 [748.916475ms]
  May 22 06:52:19.923: INFO: Created: latency-svc-jtrcg
  May 22 06:52:19.960: INFO: Got endpoints: latency-svc-nx7zx [749.943072ms]
  May 22 06:52:19.969: INFO: Created: latency-svc-d5g9s
  May 22 06:52:20.011: INFO: Got endpoints: latency-svc-7shfv [748.801622ms]
  May 22 06:52:20.020: INFO: Created: latency-svc-g7shm
  May 22 06:52:20.062: INFO: Got endpoints: latency-svc-bcqtc [748.552836ms]
  May 22 06:52:20.071: INFO: Created: latency-svc-55cls
  May 22 06:52:20.111: INFO: Got endpoints: latency-svc-glnbx [750.525754ms]
  May 22 06:52:20.119: INFO: Created: latency-svc-jsqh5
  May 22 06:52:20.163: INFO: Got endpoints: latency-svc-776hb [751.338353ms]
  May 22 06:52:20.173: INFO: Created: latency-svc-8d2tl
  May 22 06:52:20.212: INFO: Got endpoints: latency-svc-tcrvh [750.821ms]
  E0522 06:52:20.215258      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:20.221: INFO: Created: latency-svc-r7wvh
  May 22 06:52:20.262: INFO: Got endpoints: latency-svc-srmq9 [750.13512ms]
  May 22 06:52:20.270: INFO: Created: latency-svc-996nm
  May 22 06:52:20.312: INFO: Got endpoints: latency-svc-zmfcj [750.910251ms]
  May 22 06:52:20.321: INFO: Created: latency-svc-xz5qx
  May 22 06:52:20.361: INFO: Got endpoints: latency-svc-7lfm8 [750.005349ms]
  May 22 06:52:20.369: INFO: Created: latency-svc-5lbw7
  May 22 06:52:20.412: INFO: Got endpoints: latency-svc-nq8ng [750.328258ms]
  May 22 06:52:20.421: INFO: Created: latency-svc-422cn
  May 22 06:52:20.461: INFO: Got endpoints: latency-svc-x2rn8 [749.040358ms]
  May 22 06:52:20.470: INFO: Created: latency-svc-vbdpg
  May 22 06:52:20.511: INFO: Got endpoints: latency-svc-7qt7t [749.952811ms]
  May 22 06:52:20.521: INFO: Created: latency-svc-5rmbw
  May 22 06:52:20.561: INFO: Got endpoints: latency-svc-zxj8k [743.638787ms]
  May 22 06:52:20.569: INFO: Created: latency-svc-mzk4t
  May 22 06:52:20.612: INFO: Got endpoints: latency-svc-t9zpz [747.732783ms]
  May 22 06:52:20.621: INFO: Created: latency-svc-wx5ck
  May 22 06:52:20.661: INFO: Got endpoints: latency-svc-jtrcg [749.770501ms]
  May 22 06:52:20.669: INFO: Created: latency-svc-p6qk4
  May 22 06:52:20.713: INFO: Got endpoints: latency-svc-d5g9s [752.444465ms]
  May 22 06:52:20.720: INFO: Created: latency-svc-n5lbv
  May 22 06:52:20.761: INFO: Got endpoints: latency-svc-g7shm [750.185832ms]
  May 22 06:52:20.770: INFO: Created: latency-svc-mb9lh
  May 22 06:52:20.811: INFO: Got endpoints: latency-svc-55cls [749.068104ms]
  May 22 06:52:20.821: INFO: Created: latency-svc-9cn7j
  May 22 06:52:20.862: INFO: Got endpoints: latency-svc-jsqh5 [750.377864ms]
  May 22 06:52:20.870: INFO: Created: latency-svc-9xcn4
  May 22 06:52:20.912: INFO: Got endpoints: latency-svc-8d2tl [749.459761ms]
  May 22 06:52:20.920: INFO: Created: latency-svc-hnrq7
  May 22 06:52:20.962: INFO: Got endpoints: latency-svc-r7wvh [749.872689ms]
  May 22 06:52:20.970: INFO: Created: latency-svc-q927j
  May 22 06:52:21.011: INFO: Got endpoints: latency-svc-996nm [749.179459ms]
  May 22 06:52:21.020: INFO: Created: latency-svc-k68js
  May 22 06:52:21.062: INFO: Got endpoints: latency-svc-xz5qx [750.065083ms]
  May 22 06:52:21.070: INFO: Created: latency-svc-2jzq7
  May 22 06:52:21.112: INFO: Got endpoints: latency-svc-5lbw7 [751.048796ms]
  May 22 06:52:21.120: INFO: Created: latency-svc-q5qrq
  May 22 06:52:21.163: INFO: Got endpoints: latency-svc-422cn [750.79684ms]
  May 22 06:52:21.171: INFO: Created: latency-svc-vp2xs
  May 22 06:52:21.212: INFO: Got endpoints: latency-svc-vbdpg [750.552697ms]
  E0522 06:52:21.216304      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:21.224: INFO: Created: latency-svc-rmqc7
  May 22 06:52:21.262: INFO: Got endpoints: latency-svc-5rmbw [750.084524ms]
  May 22 06:52:21.270: INFO: Created: latency-svc-pxqd7
  May 22 06:52:21.313: INFO: Got endpoints: latency-svc-mzk4t [751.735988ms]
  May 22 06:52:21.328: INFO: Created: latency-svc-wmscx
  May 22 06:52:21.361: INFO: Got endpoints: latency-svc-wx5ck [748.414744ms]
  May 22 06:52:21.369: INFO: Created: latency-svc-5nw25
  May 22 06:52:21.412: INFO: Got endpoints: latency-svc-p6qk4 [750.716353ms]
  May 22 06:52:21.422: INFO: Created: latency-svc-zvjcv
  May 22 06:52:21.462: INFO: Got endpoints: latency-svc-n5lbv [748.861342ms]
  May 22 06:52:21.470: INFO: Created: latency-svc-7vv6l
  May 22 06:52:21.512: INFO: Got endpoints: latency-svc-mb9lh [751.122431ms]
  May 22 06:52:21.521: INFO: Created: latency-svc-vkkkl
  May 22 06:52:21.562: INFO: Got endpoints: latency-svc-9cn7j [750.690686ms]
  May 22 06:52:21.571: INFO: Created: latency-svc-psssq
  May 22 06:52:21.611: INFO: Got endpoints: latency-svc-9xcn4 [749.556766ms]
  May 22 06:52:21.623: INFO: Created: latency-svc-vpl7s
  May 22 06:52:21.660: INFO: Got endpoints: latency-svc-hnrq7 [748.139811ms]
  May 22 06:52:21.669: INFO: Created: latency-svc-c9vmq
  May 22 06:52:21.710: INFO: Got endpoints: latency-svc-q927j [748.521116ms]
  May 22 06:52:21.719: INFO: Created: latency-svc-pt7kv
  May 22 06:52:21.762: INFO: Got endpoints: latency-svc-k68js [750.784663ms]
  May 22 06:52:21.771: INFO: Created: latency-svc-d79rt
  May 22 06:52:21.810: INFO: Got endpoints: latency-svc-2jzq7 [748.338867ms]
  May 22 06:52:21.821: INFO: Created: latency-svc-gwnhg
  May 22 06:52:21.863: INFO: Got endpoints: latency-svc-q5qrq [750.655363ms]
  May 22 06:52:21.872: INFO: Created: latency-svc-flxd5
  May 22 06:52:21.911: INFO: Got endpoints: latency-svc-vp2xs [748.401575ms]
  May 22 06:52:21.946: INFO: Created: latency-svc-lx8l9
  May 22 06:52:21.960: INFO: Got endpoints: latency-svc-rmqc7 [748.437245ms]
  May 22 06:52:21.968: INFO: Created: latency-svc-clgp2
  May 22 06:52:22.012: INFO: Got endpoints: latency-svc-pxqd7 [750.434262ms]
  May 22 06:52:22.020: INFO: Created: latency-svc-h6lsd
  May 22 06:52:22.062: INFO: Got endpoints: latency-svc-wmscx [749.121028ms]
  May 22 06:52:22.070: INFO: Created: latency-svc-4b7bj
  May 22 06:52:22.113: INFO: Got endpoints: latency-svc-5nw25 [751.673362ms]
  May 22 06:52:22.120: INFO: Created: latency-svc-4q8dw
  May 22 06:52:22.161: INFO: Got endpoints: latency-svc-zvjcv [749.176286ms]
  May 22 06:52:22.169: INFO: Created: latency-svc-dnpcj
  May 22 06:52:22.211: INFO: Got endpoints: latency-svc-7vv6l [749.468753ms]
  E0522 06:52:22.216870      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:22.218: INFO: Created: latency-svc-c59t4
  May 22 06:52:22.262: INFO: Got endpoints: latency-svc-vkkkl [749.864006ms]
  May 22 06:52:22.270: INFO: Created: latency-svc-rjjn9
  May 22 06:52:22.312: INFO: Got endpoints: latency-svc-psssq [750.22602ms]
  May 22 06:52:22.320: INFO: Created: latency-svc-l4s82
  May 22 06:52:22.361: INFO: Got endpoints: latency-svc-vpl7s [749.640889ms]
  May 22 06:52:22.371: INFO: Created: latency-svc-ftld4
  May 22 06:52:22.411: INFO: Got endpoints: latency-svc-c9vmq [750.16837ms]
  May 22 06:52:22.419: INFO: Created: latency-svc-6dncp
  May 22 06:52:22.462: INFO: Got endpoints: latency-svc-pt7kv [751.555308ms]
  May 22 06:52:22.469: INFO: Created: latency-svc-lwg5k
  May 22 06:52:22.511: INFO: Got endpoints: latency-svc-d79rt [749.275011ms]
  May 22 06:52:22.519: INFO: Created: latency-svc-2znds
  May 22 06:52:22.562: INFO: Got endpoints: latency-svc-gwnhg [751.457289ms]
  May 22 06:52:22.570: INFO: Created: latency-svc-s9vkt
  May 22 06:52:22.611: INFO: Got endpoints: latency-svc-flxd5 [748.122327ms]
  May 22 06:52:22.621: INFO: Created: latency-svc-44kpm
  May 22 06:52:22.661: INFO: Got endpoints: latency-svc-lx8l9 [749.870216ms]
  May 22 06:52:22.669: INFO: Created: latency-svc-qpzk6
  May 22 06:52:22.712: INFO: Got endpoints: latency-svc-clgp2 [751.362988ms]
  May 22 06:52:22.720: INFO: Created: latency-svc-rft2s
  May 22 06:52:22.766: INFO: Got endpoints: latency-svc-h6lsd [753.585801ms]
  May 22 06:52:22.774: INFO: Created: latency-svc-gcvc8
  May 22 06:52:22.811: INFO: Got endpoints: latency-svc-4b7bj [748.564207ms]
  May 22 06:52:22.820: INFO: Created: latency-svc-w22fb
  May 22 06:52:22.861: INFO: Got endpoints: latency-svc-4q8dw [748.618793ms]
  May 22 06:52:22.871: INFO: Created: latency-svc-tx572
  May 22 06:52:22.911: INFO: Got endpoints: latency-svc-dnpcj [750.530292ms]
  May 22 06:52:22.919: INFO: Created: latency-svc-x7mdn
  May 22 06:52:22.961: INFO: Got endpoints: latency-svc-c59t4 [749.767343ms]
  May 22 06:52:22.969: INFO: Created: latency-svc-bg5h6
  May 22 06:52:23.012: INFO: Got endpoints: latency-svc-rjjn9 [749.711897ms]
  May 22 06:52:23.020: INFO: Created: latency-svc-4295n
  May 22 06:52:23.061: INFO: Got endpoints: latency-svc-l4s82 [749.053371ms]
  May 22 06:52:23.069: INFO: Created: latency-svc-rvwx7
  May 22 06:52:23.112: INFO: Got endpoints: latency-svc-ftld4 [750.946744ms]
  May 22 06:52:23.121: INFO: Created: latency-svc-f5skr
  May 22 06:52:23.161: INFO: Got endpoints: latency-svc-6dncp [750.419454ms]
  May 22 06:52:23.169: INFO: Created: latency-svc-pgkhw
  May 22 06:52:23.212: INFO: Got endpoints: latency-svc-lwg5k [749.866475ms]
  E0522 06:52:23.217177      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:23.220: INFO: Created: latency-svc-hzhcw
  May 22 06:52:23.262: INFO: Got endpoints: latency-svc-2znds [750.54721ms]
  May 22 06:52:23.269: INFO: Created: latency-svc-wkdml
  May 22 06:52:23.312: INFO: Got endpoints: latency-svc-s9vkt [750.381441ms]
  May 22 06:52:23.321: INFO: Created: latency-svc-j8f5g
  May 22 06:52:23.361: INFO: Got endpoints: latency-svc-44kpm [749.616604ms]
  May 22 06:52:23.369: INFO: Created: latency-svc-wfsrz
  May 22 06:52:23.413: INFO: Got endpoints: latency-svc-qpzk6 [751.551298ms]
  May 22 06:52:23.420: INFO: Created: latency-svc-62cdd
  May 22 06:52:23.461: INFO: Got endpoints: latency-svc-rft2s [749.509221ms]
  May 22 06:52:23.470: INFO: Created: latency-svc-kd6s6
  May 22 06:52:23.511: INFO: Got endpoints: latency-svc-gcvc8 [745.439583ms]
  May 22 06:52:23.523: INFO: Created: latency-svc-pvmlz
  May 22 06:52:23.562: INFO: Got endpoints: latency-svc-w22fb [750.903442ms]
  May 22 06:52:23.570: INFO: Created: latency-svc-xvn2p
  May 22 06:52:23.611: INFO: Got endpoints: latency-svc-tx572 [749.624702ms]
  May 22 06:52:23.619: INFO: Created: latency-svc-lfjw7
  May 22 06:52:23.661: INFO: Got endpoints: latency-svc-x7mdn [749.599527ms]
  May 22 06:52:23.669: INFO: Created: latency-svc-wtnbf
  May 22 06:52:23.711: INFO: Got endpoints: latency-svc-bg5h6 [749.367122ms]
  May 22 06:52:23.718: INFO: Created: latency-svc-lj4l4
  May 22 06:52:23.761: INFO: Got endpoints: latency-svc-4295n [749.149204ms]
  May 22 06:52:23.769: INFO: Created: latency-svc-sm2mr
  May 22 06:52:23.812: INFO: Got endpoints: latency-svc-rvwx7 [750.418217ms]
  May 22 06:52:23.822: INFO: Created: latency-svc-64kqf
  May 22 06:52:23.862: INFO: Got endpoints: latency-svc-f5skr [750.156584ms]
  May 22 06:52:23.870: INFO: Created: latency-svc-9bcm2
  May 22 06:52:23.912: INFO: Got endpoints: latency-svc-pgkhw [750.494506ms]
  May 22 06:52:23.919: INFO: Created: latency-svc-2xk64
  May 22 06:52:23.961: INFO: Got endpoints: latency-svc-hzhcw [749.577185ms]
  May 22 06:52:23.969: INFO: Created: latency-svc-j7pqx
  May 22 06:52:24.012: INFO: Got endpoints: latency-svc-wkdml [749.984375ms]
  May 22 06:52:24.021: INFO: Created: latency-svc-8zqtj
  May 22 06:52:24.061: INFO: Got endpoints: latency-svc-j8f5g [748.563184ms]
  May 22 06:52:24.070: INFO: Created: latency-svc-v6c6d
  May 22 06:52:24.112: INFO: Got endpoints: latency-svc-wfsrz [750.656647ms]
  May 22 06:52:24.121: INFO: Created: latency-svc-cjw84
  May 22 06:52:24.161: INFO: Got endpoints: latency-svc-62cdd [748.545755ms]
  May 22 06:52:24.169: INFO: Created: latency-svc-l7nnd
  May 22 06:52:24.210: INFO: Got endpoints: latency-svc-kd6s6 [748.971849ms]
  E0522 06:52:24.217949      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:24.219: INFO: Created: latency-svc-mpd2j
  May 22 06:52:24.262: INFO: Got endpoints: latency-svc-pvmlz [750.444678ms]
  May 22 06:52:24.271: INFO: Created: latency-svc-645fc
  May 22 06:52:24.311: INFO: Got endpoints: latency-svc-xvn2p [749.321713ms]
  May 22 06:52:24.319: INFO: Created: latency-svc-2zgx4
  May 22 06:52:24.362: INFO: Got endpoints: latency-svc-lfjw7 [750.485138ms]
  May 22 06:52:24.369: INFO: Created: latency-svc-bjh54
  May 22 06:52:24.412: INFO: Got endpoints: latency-svc-wtnbf [750.748588ms]
  May 22 06:52:24.419: INFO: Created: latency-svc-gb6w5
  May 22 06:52:24.461: INFO: Got endpoints: latency-svc-lj4l4 [750.36971ms]
  May 22 06:52:24.469: INFO: Created: latency-svc-jl4lx
  May 22 06:52:24.514: INFO: Got endpoints: latency-svc-sm2mr [753.057698ms]
  May 22 06:52:24.522: INFO: Created: latency-svc-ltwbb
  May 22 06:52:24.563: INFO: Got endpoints: latency-svc-64kqf [750.889754ms]
  May 22 06:52:24.571: INFO: Created: latency-svc-bmzhc
  May 22 06:52:24.612: INFO: Got endpoints: latency-svc-9bcm2 [749.500891ms]
  May 22 06:52:24.619: INFO: Created: latency-svc-f84z7
  May 22 06:52:24.661: INFO: Got endpoints: latency-svc-2xk64 [749.889315ms]
  May 22 06:52:24.669: INFO: Created: latency-svc-cg4kb
  May 22 06:52:24.711: INFO: Got endpoints: latency-svc-j7pqx [749.918498ms]
  May 22 06:52:24.721: INFO: Created: latency-svc-kch2c
  May 22 06:52:24.762: INFO: Got endpoints: latency-svc-8zqtj [749.370294ms]
  May 22 06:52:24.769: INFO: Created: latency-svc-jn4qm
  May 22 06:52:24.811: INFO: Got endpoints: latency-svc-v6c6d [749.757562ms]
  May 22 06:52:24.818: INFO: Created: latency-svc-dlcpj
  May 22 06:52:24.862: INFO: Got endpoints: latency-svc-cjw84 [749.919771ms]
  May 22 06:52:24.870: INFO: Created: latency-svc-lqvjs
  May 22 06:52:24.911: INFO: Got endpoints: latency-svc-l7nnd [749.447927ms]
  May 22 06:52:24.919: INFO: Created: latency-svc-r7gqz
  May 22 06:52:24.962: INFO: Got endpoints: latency-svc-mpd2j [751.655124ms]
  May 22 06:52:24.970: INFO: Created: latency-svc-zrgh6
  May 22 06:52:25.011: INFO: Got endpoints: latency-svc-645fc [749.356267ms]
  May 22 06:52:25.019: INFO: Created: latency-svc-qnqwb
  May 22 06:52:25.062: INFO: Got endpoints: latency-svc-2zgx4 [750.533762ms]
  May 22 06:52:25.069: INFO: Created: latency-svc-vchtb
  May 22 06:52:25.111: INFO: Got endpoints: latency-svc-bjh54 [749.795193ms]
  May 22 06:52:25.120: INFO: Created: latency-svc-qrtzd
  May 22 06:52:25.162: INFO: Got endpoints: latency-svc-gb6w5 [749.621886ms]
  May 22 06:52:25.169: INFO: Created: latency-svc-s7sdz
  May 22 06:52:25.211: INFO: Got endpoints: latency-svc-jl4lx [750.369114ms]
  E0522 06:52:25.218255      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:25.219: INFO: Created: latency-svc-7gq9b
  May 22 06:52:25.262: INFO: Got endpoints: latency-svc-ltwbb [748.062827ms]
  May 22 06:52:25.271: INFO: Created: latency-svc-9hcdd
  May 22 06:52:25.312: INFO: Got endpoints: latency-svc-bmzhc [749.09161ms]
  May 22 06:52:25.320: INFO: Created: latency-svc-vxmhk
  May 22 06:52:25.361: INFO: Got endpoints: latency-svc-f84z7 [749.343545ms]
  May 22 06:52:25.413: INFO: Got endpoints: latency-svc-cg4kb [750.943954ms]
  May 22 06:52:25.461: INFO: Got endpoints: latency-svc-kch2c [749.88915ms]
  May 22 06:52:25.511: INFO: Got endpoints: latency-svc-jn4qm [749.632593ms]
  May 22 06:52:25.562: INFO: Got endpoints: latency-svc-dlcpj [750.860939ms]
  May 22 06:52:25.612: INFO: Got endpoints: latency-svc-lqvjs [750.158422ms]
  May 22 06:52:25.661: INFO: Got endpoints: latency-svc-r7gqz [749.911103ms]
  May 22 06:52:25.711: INFO: Got endpoints: latency-svc-zrgh6 [748.889501ms]
  May 22 06:52:25.762: INFO: Got endpoints: latency-svc-qnqwb [750.808302ms]
  May 22 06:52:25.816: INFO: Got endpoints: latency-svc-vchtb [754.582878ms]
  May 22 06:52:25.861: INFO: Got endpoints: latency-svc-qrtzd [749.501921ms]
  May 22 06:52:25.911: INFO: Got endpoints: latency-svc-s7sdz [749.308156ms]
  May 22 06:52:25.965: INFO: Got endpoints: latency-svc-7gq9b [753.185781ms]
  May 22 06:52:26.011: INFO: Got endpoints: latency-svc-9hcdd [748.62379ms]
  May 22 06:52:26.061: INFO: Got endpoints: latency-svc-vxmhk [748.707402ms]
  May 22 06:52:26.061: INFO: Latencies: [12.920264ms 16.752209ms 24.281233ms 35.21317ms 38.284589ms 45.186774ms 51.837575ms 59.363254ms 61.950265ms 71.450122ms 77.137899ms 80.941015ms 89.943424ms 93.076181ms 93.862743ms 94.26014ms 94.502643ms 94.916352ms 100.056486ms 101.952543ms 105.888328ms 106.372144ms 106.960216ms 107.021432ms 107.242223ms 107.266521ms 108.326196ms 108.697169ms 135.270426ms 136.556337ms 138.852057ms 139.085851ms 139.988371ms 147.002966ms 168.496181ms 195.161323ms 240.970312ms 283.198367ms 328.052162ms 370.063028ms 414.658885ms 460.322437ms 502.395996ms 516.296706ms 560.250988ms 607.663406ms 639.708859ms 684.337512ms 731.606779ms 743.638787ms 745.439583ms 746.741867ms 747.732783ms 748.062827ms 748.122327ms 748.139811ms 748.327324ms 748.338867ms 748.401575ms 748.414744ms 748.437245ms 748.454066ms 748.521116ms 748.545755ms 748.552836ms 748.563184ms 748.564207ms 748.618793ms 748.62379ms 748.703446ms 748.707402ms 748.801622ms 748.861342ms 748.866182ms 748.889501ms 748.916475ms 748.971849ms 749.03179ms 749.040358ms 749.053371ms 749.068104ms 749.09161ms 749.121028ms 749.149204ms 749.176286ms 749.179459ms 749.275011ms 749.308156ms 749.321713ms 749.343545ms 749.356267ms 749.367122ms 749.370294ms 749.40413ms 749.447927ms 749.459761ms 749.468753ms 749.500891ms 749.501921ms 749.509221ms 749.556766ms 749.577185ms 749.599527ms 749.616604ms 749.621886ms 749.624702ms 749.632593ms 749.640889ms 749.657329ms 749.693265ms 749.711897ms 749.757562ms 749.767343ms 749.770501ms 749.795193ms 749.798538ms 749.864006ms 749.866475ms 749.870216ms 749.872689ms 749.88315ms 749.88915ms 749.889315ms 749.907803ms 749.911103ms 749.918498ms 749.919771ms 749.943072ms 749.952811ms 749.984375ms 750.005349ms 750.065083ms 750.080294ms 750.084524ms 750.13512ms 750.156584ms 750.158422ms 750.16837ms 750.185832ms 750.22602ms 750.31309ms 750.328258ms 750.330793ms 750.357563ms 750.369114ms 750.36971ms 750.377864ms 750.381441ms 750.38733ms 750.418217ms 750.419454ms 750.434262ms 750.444678ms 750.485138ms 750.494506ms 750.525754ms 750.530292ms 750.533762ms 750.53431ms 750.543101ms 750.54721ms 750.552697ms 750.650617ms 750.655363ms 750.656647ms 750.690686ms 750.716353ms 750.748588ms 750.784663ms 750.79684ms 750.808302ms 750.821ms 750.860939ms 750.879544ms 750.889754ms 750.903442ms 750.910251ms 750.943954ms 750.946744ms 750.966215ms 751.048796ms 751.122431ms 751.190418ms 751.338353ms 751.362988ms 751.457289ms 751.532161ms 751.551298ms 751.555308ms 751.655124ms 751.673362ms 751.735988ms 752.444465ms 752.881639ms 752.899482ms 753.057698ms 753.185781ms 753.585801ms 754.582878ms 755.564599ms]
  May 22 06:52:26.061: INFO: 50 %ile: 749.556766ms
  May 22 06:52:26.061: INFO: 90 %ile: 751.048796ms
  May 22 06:52:26.061: INFO: 99 %ile: 754.582878ms
  May 22 06:52:26.062: INFO: Total sample count: 200
  May 22 06:52:26.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-3784" for this suite. @ 05/22/23 06:52:26.066
• [9.739 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 05/22/23 06:52:26.073
  May 22 06:52:26.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename webhook @ 05/22/23 06:52:26.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:52:26.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:52:26.083
  STEP: Setting up server cert @ 05/22/23 06:52:26.095
  E0522 06:52:26.218922      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/22/23 06:52:26.595
  STEP: Deploying the webhook pod @ 05/22/23 06:52:26.601
  STEP: Wait for the deployment to be ready @ 05/22/23 06:52:26.608
  May 22 06:52:26.614: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0522 06:52:27.219856      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:28.220184      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/22/23 06:52:28.62
  STEP: Verifying the service has paired with the endpoint @ 05/22/23 06:52:28.628
  E0522 06:52:29.221022      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:29.628: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/22/23 06:52:29.63
  STEP: create a pod @ 05/22/23 06:52:29.641
  E0522 06:52:30.221146      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:31.221347      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/22/23 06:52:31.649
  May 22 06:52:31.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1009545739 --namespace=webhook-9139 attach --namespace=webhook-9139 to-be-attached-pod -i -c=container1'
  May 22 06:52:31.720: INFO: rc: 1
  May 22 06:52:31.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9139" for this suite. @ 05/22/23 06:52:31.755
  STEP: Destroying namespace "webhook-markers-2139" for this suite. @ 05/22/23 06:52:31.758
• [5.689 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 05/22/23 06:52:31.763
  May 22 06:52:31.763: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename configmap @ 05/22/23 06:52:31.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:52:31.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:52:31.773
  STEP: Creating configMap with name configmap-test-volume-2a568341-a4b8-41d6-af37-4bf73363ce38 @ 05/22/23 06:52:31.775
  STEP: Creating a pod to test consume configMaps @ 05/22/23 06:52:31.778
  E0522 06:52:32.221405      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:33.222489      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:34.222780      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:35.223239      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:52:35.792
  May 22 06:52:35.794: INFO: Trying to get logs from node node2 pod pod-configmaps-a400b8a4-a703-4bc2-ba80-9eac9290eebc container agnhost-container: <nil>
  STEP: delete the pod @ 05/22/23 06:52:35.798
  May 22 06:52:35.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9214" for this suite. @ 05/22/23 06:52:35.809
• [4.049 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 05/22/23 06:52:35.811
  May 22 06:52:35.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename job @ 05/22/23 06:52:35.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:52:35.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:52:35.821
  STEP: Creating a suspended job @ 05/22/23 06:52:35.825
  STEP: Patching the Job @ 05/22/23 06:52:35.827
  STEP: Watching for Job to be patched @ 05/22/23 06:52:35.834
  May 22 06:52:35.835: INFO: Event ADDED observed for Job e2e-xvbnd in namespace job-7283 with labels: map[e2e-job-label:e2e-xvbnd] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 22 06:52:35.835: INFO: Event MODIFIED observed for Job e2e-xvbnd in namespace job-7283 with labels: map[e2e-job-label:e2e-xvbnd] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 22 06:52:35.835: INFO: Event MODIFIED found for Job e2e-xvbnd in namespace job-7283 with labels: map[e2e-job-label:e2e-xvbnd e2e-xvbnd:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 05/22/23 06:52:35.835
  STEP: Watching for Job to be updated @ 05/22/23 06:52:35.84
  May 22 06:52:35.841: INFO: Event MODIFIED found for Job e2e-xvbnd in namespace job-7283 with labels: map[e2e-job-label:e2e-xvbnd e2e-xvbnd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 22 06:52:35.841: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/22/23 06:52:35.841
  May 22 06:52:35.843: INFO: Job: e2e-xvbnd as labels: map[e2e-job-label:e2e-xvbnd e2e-xvbnd:patched]
  STEP: Waiting for job to complete @ 05/22/23 06:52:35.843
  E0522 06:52:36.223450      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:37.224358      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:38.224862      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:39.225302      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:40.226301      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:41.226546      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:42.227180      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:43.227421      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 05/22/23 06:52:43.847
  STEP: Watching for Job to be deleted @ 05/22/23 06:52:43.85
  May 22 06:52:43.852: INFO: Event MODIFIED observed for Job e2e-xvbnd in namespace job-7283 with labels: map[e2e-job-label:e2e-xvbnd e2e-xvbnd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 22 06:52:43.852: INFO: Event MODIFIED observed for Job e2e-xvbnd in namespace job-7283 with labels: map[e2e-job-label:e2e-xvbnd e2e-xvbnd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 22 06:52:43.852: INFO: Event MODIFIED observed for Job e2e-xvbnd in namespace job-7283 with labels: map[e2e-job-label:e2e-xvbnd e2e-xvbnd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 22 06:52:43.852: INFO: Event MODIFIED observed for Job e2e-xvbnd in namespace job-7283 with labels: map[e2e-job-label:e2e-xvbnd e2e-xvbnd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 22 06:52:43.852: INFO: Event MODIFIED observed for Job e2e-xvbnd in namespace job-7283 with labels: map[e2e-job-label:e2e-xvbnd e2e-xvbnd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 22 06:52:43.852: INFO: Event DELETED found for Job e2e-xvbnd in namespace job-7283 with labels: map[e2e-job-label:e2e-xvbnd e2e-xvbnd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 05/22/23 06:52:43.852
  May 22 06:52:43.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7283" for this suite. @ 05/22/23 06:52:43.865
• [8.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/22/23 06:52:43.87
  May 22 06:52:43.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename secrets @ 05/22/23 06:52:43.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:52:43.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:52:43.88
  STEP: Creating secret with name secret-test-map-a8d94b50-ccd1-47a2-9584-7b1bc24661e0 @ 05/22/23 06:52:43.881
  STEP: Creating a pod to test consume secrets @ 05/22/23 06:52:43.884
  E0522 06:52:44.228529      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:45.228924      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:46.229564      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:47.229678      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/22/23 06:52:47.896
  May 22 06:52:47.898: INFO: Trying to get logs from node node2 pod pod-secrets-a3e6700a-0da4-4e15-b856-889e9537129c container secret-volume-test: <nil>
  STEP: delete the pod @ 05/22/23 06:52:47.902
  May 22 06:52:47.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6895" for this suite. @ 05/22/23 06:52:47.918
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 05/22/23 06:52:47.923
  May 22 06:52:47.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename security-context-test @ 05/22/23 06:52:47.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:52:47.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:52:47.933
  E0522 06:52:48.229797      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:49.230431      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:50.231068      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:51.231259      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:51.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9083" for this suite. @ 05/22/23 06:52:51.951
• [4.030 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 05/22/23 06:52:51.954
  May 22 06:52:51.954: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename container-probe @ 05/22/23 06:52:51.955
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:52:51.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:52:51.964
  STEP: Creating pod busybox-75c65ee0-8e30-4a2f-9baf-2caa4226bc9b in namespace container-probe-5384 @ 05/22/23 06:52:51.965
  E0522 06:52:52.231926      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:53.232540      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:52:53.974: INFO: Started pod busybox-75c65ee0-8e30-4a2f-9baf-2caa4226bc9b in namespace container-probe-5384
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/22/23 06:52:53.974
  May 22 06:52:53.976: INFO: Initial restart count of pod busybox-75c65ee0-8e30-4a2f-9baf-2caa4226bc9b is 0
  E0522 06:52:54.233410      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:55.233646      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:56.233927      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:57.234153      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:58.234870      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:52:59.237053      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:00.237323      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:01.237563      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:02.238575      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:03.238769      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:04.239643      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:05.239825      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:06.240922      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:07.241125      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:08.241778      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:09.242808      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:10.243607      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:11.243935      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:12.244330      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:13.244568      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:14.245604      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:15.245822      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:16.246383      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:17.246608      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:18.247318      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:19.247849      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:20.248648      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:21.248991      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:22.250056      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:23.250902      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:24.250916      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:25.251160      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:26.251484      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:27.251792      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:28.252585      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:29.253148      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:30.253555      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:31.253777      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:32.254370      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:33.254564      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:34.255337      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:35.255478      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:36.256038      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:37.256164      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:38.256657      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:39.257362      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:40.258183      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:41.258350      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:42.258897      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:43.259113      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:53:44.058: INFO: Restart count of pod container-probe-5384/busybox-75c65ee0-8e30-4a2f-9baf-2caa4226bc9b is now 1 (50.082232382s elapsed)
  May 22 06:53:44.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/22/23 06:53:44.061
  STEP: Destroying namespace "container-probe-5384" for this suite. @ 05/22/23 06:53:44.067
• [52.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 05/22/23 06:53:44.074
  May 22 06:53:44.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename runtimeclass @ 05/22/23 06:53:44.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:53:44.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:53:44.088
  E0522 06:53:44.259822      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:45.260087      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:53:46.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4352" for this suite. @ 05/22/23 06:53:46.11
• [2.039 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 05/22/23 06:53:46.114
  May 22 06:53:46.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sched-preemption @ 05/22/23 06:53:46.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:53:46.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:53:46.124
  May 22 06:53:46.132: INFO: Waiting up to 1m0s for all nodes to be ready
  E0522 06:53:46.260623      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:47.261013      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:48.261387      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:49.262799      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:50.263795      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:51.264017      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:52.264568      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:53.264838      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:54.265489      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:55.265758      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:56.266541      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:57.266746      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:58.267592      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:53:59.267844      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:00.268271      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:01.268485      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:02.269297      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:03.269500      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:04.270089      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:05.270541      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:06.271073      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:07.271847      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:08.272906      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:09.273422      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:10.273445      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:11.273770      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:12.274215      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:13.274416      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:14.275167      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:15.275364      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:16.276250      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:17.276461      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:18.276596      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:19.277268      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:20.277766      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:21.277936      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:22.278037      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:23.278329      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:24.278552      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:25.278801      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:26.279495      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:27.280132      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:28.280860      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:29.281827      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:30.281881      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:31.282037      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:32.282614      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:33.283019      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:34.283961      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:35.284178      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:36.284718      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:37.284864      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:38.285591      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:39.286268      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:40.287150      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:41.287449      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:42.287865      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:43.288078      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:44.288513      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:45.289260      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:54:46.155: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/22/23 06:54:46.156
  May 22 06:54:46.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/22/23 06:54:46.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:54:46.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:54:46.165
  May 22 06:54:46.174: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  May 22 06:54:46.175: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  May 22 06:54:46.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 22 06:54:46.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-2332" for this suite. @ 05/22/23 06:54:46.222
  STEP: Destroying namespace "sched-preemption-2178" for this suite. @ 05/22/23 06:54:46.225
• [60.115 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 05/22/23 06:54:46.229
  May 22 06:54:46.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename statefulset @ 05/22/23 06:54:46.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:54:46.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:54:46.238
  STEP: Creating service test in namespace statefulset-4797 @ 05/22/23 06:54:46.24
  STEP: Looking for a node to schedule stateful set and pod @ 05/22/23 06:54:46.242
  STEP: Creating pod with conflicting port in namespace statefulset-4797 @ 05/22/23 06:54:46.244
  STEP: Waiting until pod test-pod will start running in namespace statefulset-4797 @ 05/22/23 06:54:46.248
  E0522 06:54:46.290024      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:47.290485      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-4797 @ 05/22/23 06:54:48.253
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4797 @ 05/22/23 06:54:48.256
  May 22 06:54:48.263: INFO: Observed stateful pod in namespace: statefulset-4797, name: ss-0, uid: 6d8b5bb9-2eea-4459-b871-420dd128cb25, status phase: Pending. Waiting for statefulset controller to delete.
  May 22 06:54:48.270: INFO: Observed stateful pod in namespace: statefulset-4797, name: ss-0, uid: 6d8b5bb9-2eea-4459-b871-420dd128cb25, status phase: Failed. Waiting for statefulset controller to delete.
  May 22 06:54:48.275: INFO: Observed stateful pod in namespace: statefulset-4797, name: ss-0, uid: 6d8b5bb9-2eea-4459-b871-420dd128cb25, status phase: Failed. Waiting for statefulset controller to delete.
  May 22 06:54:48.277: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4797
  STEP: Removing pod with conflicting port in namespace statefulset-4797 @ 05/22/23 06:54:48.277
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4797 and will be in running state @ 05/22/23 06:54:48.283
  E0522 06:54:48.291111      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:49.291621      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:54:50.289: INFO: Deleting all statefulset in ns statefulset-4797
  May 22 06:54:50.290: INFO: Scaling statefulset ss to 0
  E0522 06:54:50.292024      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:51.292830      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:52.293682      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:53.293804      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:54.294683      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:55.294894      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:56.295100      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:57.295211      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:58.295333      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:54:59.295675      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:55:00.295955      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:55:00.302: INFO: Waiting for statefulset status.replicas updated to 0
  May 22 06:55:00.303: INFO: Deleting statefulset ss
  May 22 06:55:00.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4797" for this suite. @ 05/22/23 06:55:00.313
• [14.086 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/22/23 06:55:00.316
  May 22 06:55:00.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename pods @ 05/22/23 06:55:00.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:55:00.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:55:00.325
  STEP: creating pod @ 05/22/23 06:55:00.327
  E0522 06:55:01.296849      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:55:02.297160      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:55:02.340: INFO: Pod pod-hostip-16477c8a-f3ad-4066-a0c1-c089be5b9f59 has hostIP: 192.168.33.122
  May 22 06:55:02.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5396" for this suite. @ 05/22/23 06:55:02.344
• [2.031 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 05/22/23 06:55:02.347
  May 22 06:55:02.347: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename disruption @ 05/22/23 06:55:02.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:55:02.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:55:02.357
  STEP: Waiting for the pdb to be processed @ 05/22/23 06:55:02.361
  E0522 06:55:03.297729      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:55:04.297880      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 05/22/23 06:55:04.376
  May 22 06:55:04.379: INFO: running pods: 0 < 3
  E0522 06:55:05.298011      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:55:06.298272      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:55:06.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7710" for this suite. @ 05/22/23 06:55:06.386
• [4.042 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 05/22/23 06:55:06.39
  May 22 06:55:06.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1009545739
  STEP: Building a namespace api object, basename daemonsets @ 05/22/23 06:55:06.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/22/23 06:55:06.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/22/23 06:55:06.399
  May 22 06:55:06.413: INFO: Create a RollingUpdate DaemonSet
  May 22 06:55:06.416: INFO: Check that daemon pods launch on every node of the cluster
  May 22 06:55:06.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:55:06.420: INFO: Node node1 is running 0 daemon pod, expected 1
  E0522 06:55:07.298312      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:55:07.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:55:07.425: INFO: Node node1 is running 0 daemon pod, expected 1
  E0522 06:55:08.299164      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:55:08.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 22 06:55:08.426: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  May 22 06:55:08.426: INFO: Update the DaemonSet to trigger a rollout
  May 22 06:55:08.432: INFO: Updating DaemonSet daemon-set
  E0522 06:55:09.299294      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:55:10.299308      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:55:11.300303      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:55:11.442: INFO: Roll back the DaemonSet before rollout is complete
  May 22 06:55:11.449: INFO: Updating DaemonSet daemon-set
  May 22 06:55:11.449: INFO: Make sure DaemonSet rollback is complete
  May 22 06:55:11.452: INFO: Wrong image for pod: daemon-set-9b459. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  May 22 06:55:11.452: INFO: Pod daemon-set-9b459 is not available
  E0522 06:55:12.300467      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:55:13.300652      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:55:14.301516      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:55:15.301646      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:55:15.458: INFO: Pod daemon-set-zhtmq is not available
  STEP: Deleting DaemonSet "daemon-set" @ 05/22/23 06:55:15.464
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7085, will wait for the garbage collector to delete the pods @ 05/22/23 06:55:15.464
  May 22 06:55:15.519: INFO: Deleting DaemonSet.extensions daemon-set took: 3.774965ms
  May 22 06:55:15.620: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.966593ms
  E0522 06:55:16.302320      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0522 06:55:17.303301      25 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 22 06:55:17.723: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 22 06:55:17.723: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 22 06:55:17.725: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"917950"},"items":null}

  May 22 06:55:17.726: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"917950"},"items":null}

  May 22 06:55:17.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7085" for this suite. @ 05/22/23 06:55:17.735
• [11.347 seconds]
------------------------------
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  May 22 06:55:17.739: INFO: Running AfterSuite actions on node 1
  May 22 06:55:17.739: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.058 seconds]
------------------------------

Ran 378 of 7207 Specs in 5698.324 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h34m58.690468532s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

