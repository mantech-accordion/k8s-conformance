  I0507 11:54:36.970403      20 e2e.go:117] Starting e2e run "a7c2dced-f026-4955-bba1-4e1a58282135" on Ginkgo node 1
  May  7 11:54:36.992: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1683460476 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  May  7 11:54:37.123: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:54:37.124: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  May  7 11:54:37.142: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  May  7 11:54:37.144: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
  May  7 11:54:37.144: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
  May  7 11:54:37.144: INFO: e2e test version: v1.27.1
  May  7 11:54:37.145: INFO: kube-apiserver version: v1.27.1
  May  7 11:54:37.145: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:54:37.146: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.024 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 05/07/23 11:54:37.354
  May  7 11:54:37.354: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename disruption @ 05/07/23 11:54:37.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:54:37.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:54:37.369
  STEP: Waiting for the pdb to be processed @ 05/07/23 11:54:37.372
  STEP: Updating PodDisruptionBudget status @ 05/07/23 11:54:39.376
  STEP: Waiting for all pods to be running @ 05/07/23 11:54:39.379
  May  7 11:54:39.381: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 05/07/23 11:54:41.383
  STEP: Waiting for the pdb to be processed @ 05/07/23 11:54:41.39
  STEP: Patching PodDisruptionBudget status @ 05/07/23 11:54:41.393
  STEP: Waiting for the pdb to be processed @ 05/07/23 11:54:41.397
  May  7 11:54:41.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8974" for this suite. @ 05/07/23 11:54:41.401
• [4.049 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 05/07/23 11:54:41.403
  May  7 11:54:41.403: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename security-context-test @ 05/07/23 11:54:41.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:54:41.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:54:41.413
  May  7 11:54:45.442: INFO: Got logs for pod "busybox-privileged-false-1145cd18-eb37-4568-ab01-7c6331cf109b": "ip: RTNETLINK answers: Operation not permitted\n"
  May  7 11:54:45.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9539" for this suite. @ 05/07/23 11:54:45.443
• [4.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 05/07/23 11:54:45.447
  May  7 11:54:45.447: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 11:54:45.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:54:45.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:54:45.458
  STEP: creating an Endpoint @ 05/07/23 11:54:45.46
  STEP: waiting for available Endpoint @ 05/07/23 11:54:45.462
  STEP: listing all Endpoints @ 05/07/23 11:54:45.463
  STEP: updating the Endpoint @ 05/07/23 11:54:45.464
  STEP: fetching the Endpoint @ 05/07/23 11:54:45.466
  STEP: patching the Endpoint @ 05/07/23 11:54:45.467
  STEP: fetching the Endpoint @ 05/07/23 11:54:45.471
  STEP: deleting the Endpoint by Collection @ 05/07/23 11:54:45.472
  STEP: waiting for Endpoint deletion @ 05/07/23 11:54:45.474
  STEP: fetching the Endpoint @ 05/07/23 11:54:45.475
  May  7 11:54:45.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8269" for this suite. @ 05/07/23 11:54:45.478
• [0.033 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 05/07/23 11:54:45.481
  May  7 11:54:45.481: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 11:54:45.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:54:45.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:54:45.491
  STEP: Creating configMap configmap-8842/configmap-test-8a1af696-0faf-4550-8b14-b478784a8056 @ 05/07/23 11:54:45.492
  STEP: Creating a pod to test consume configMaps @ 05/07/23 11:54:45.494
  STEP: Saw pod success @ 05/07/23 11:54:49.504
  May  7 11:54:49.505: INFO: Trying to get logs from node 10.255.0.202 pod pod-configmaps-fe79d687-a948-4d87-ad59-23fb3e3fa320 container env-test: <nil>
  STEP: delete the pod @ 05/07/23 11:54:49.515
  May  7 11:54:49.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8842" for this suite. @ 05/07/23 11:54:49.522
• [4.044 seconds]
------------------------------
S
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 05/07/23 11:54:49.525
  May  7 11:54:49.525: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 11:54:49.526
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:54:49.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:54:49.536
  STEP: Creating secret with name secret-test-5c8b66b6-fce6-48f4-adfe-0bb1450ccbf8 @ 05/07/23 11:54:49.537
  STEP: Creating a pod to test consume secrets @ 05/07/23 11:54:49.538
  STEP: Saw pod success @ 05/07/23 11:54:53.557
  May  7 11:54:53.558: INFO: Trying to get logs from node 10.255.0.202 pod pod-secrets-0445b7f4-ee33-406d-86d8-8df39dce993f container secret-env-test: <nil>
  STEP: delete the pod @ 05/07/23 11:54:53.561
  May  7 11:54:53.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5178" for this suite. @ 05/07/23 11:54:53.569
• [4.046 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 05/07/23 11:54:53.572
  May  7 11:54:53.572: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/07/23 11:54:53.573
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:54:53.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:54:53.588
  May  7 11:54:53.589: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/07/23 11:54:54.782
  May  7 11:54:54.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-237 --namespace=crd-publish-openapi-237 create -f -'
  May  7 11:54:55.153: INFO: stderr: ""
  May  7 11:54:55.153: INFO: stdout: "e2e-test-crd-publish-openapi-2240-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May  7 11:54:55.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-237 --namespace=crd-publish-openapi-237 delete e2e-test-crd-publish-openapi-2240-crds test-cr'
  May  7 11:54:55.225: INFO: stderr: ""
  May  7 11:54:55.225: INFO: stdout: "e2e-test-crd-publish-openapi-2240-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  May  7 11:54:55.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-237 --namespace=crd-publish-openapi-237 apply -f -'
  May  7 11:54:55.388: INFO: stderr: ""
  May  7 11:54:55.388: INFO: stdout: "e2e-test-crd-publish-openapi-2240-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May  7 11:54:55.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-237 --namespace=crd-publish-openapi-237 delete e2e-test-crd-publish-openapi-2240-crds test-cr'
  May  7 11:54:55.448: INFO: stderr: ""
  May  7 11:54:55.448: INFO: stdout: "e2e-test-crd-publish-openapi-2240-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/07/23 11:54:55.448
  May  7 11:54:55.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-237 explain e2e-test-crd-publish-openapi-2240-crds'
  May  7 11:54:55.577: INFO: stderr: ""
  May  7 11:54:55.577: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-2240-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  May  7 11:54:56.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-237" for this suite. @ 05/07/23 11:54:56.754
• [3.189 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 05/07/23 11:54:56.762
  May  7 11:54:56.762: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename statefulset @ 05/07/23 11:54:56.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:54:56.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:54:56.772
  STEP: Creating service test in namespace statefulset-5317 @ 05/07/23 11:54:56.773
  STEP: Creating a new StatefulSet @ 05/07/23 11:54:56.776
  May  7 11:54:56.792: INFO: Found 0 stateful pods, waiting for 3
  May  7 11:55:06.798: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May  7 11:55:06.798: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May  7 11:55:06.798: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  May  7 11:55:06.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5317 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  7 11:55:06.910: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  7 11:55:06.910: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  7 11:55:06.910: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/07/23 11:55:16.92
  May  7 11:55:16.935: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/07/23 11:55:16.935
  STEP: Updating Pods in reverse ordinal order @ 05/07/23 11:55:26.943
  May  7 11:55:26.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5317 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  7 11:55:27.060: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  7 11:55:27.060: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  7 11:55:27.060: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 05/07/23 11:55:37.069
  May  7 11:55:37.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5317 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  7 11:55:37.182: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  7 11:55:37.182: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  7 11:55:37.182: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  7 11:55:47.203: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 05/07/23 11:55:57.211
  May  7 11:55:57.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5317 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  7 11:55:57.330: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  7 11:55:57.330: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  7 11:55:57.330: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  7 11:56:07.339: INFO: Deleting all statefulset in ns statefulset-5317
  May  7 11:56:07.340: INFO: Scaling statefulset ss2 to 0
  May  7 11:56:17.349: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 11:56:17.350: INFO: Deleting statefulset ss2
  May  7 11:56:17.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5317" for this suite. @ 05/07/23 11:56:17.36
• [80.601 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 05/07/23 11:56:17.363
  May  7 11:56:17.363: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 11:56:17.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:56:17.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:56:17.375
  STEP: Counting existing ResourceQuota @ 05/07/23 11:56:17.377
  STEP: Creating a ResourceQuota @ 05/07/23 11:56:22.381
  STEP: Ensuring resource quota status is calculated @ 05/07/23 11:56:22.387
  May  7 11:56:24.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7309" for this suite. @ 05/07/23 11:56:24.391
• [7.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 05/07/23 11:56:24.395
  May  7 11:56:24.395: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 11:56:24.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:56:24.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:56:24.404
  STEP: Counting existing ResourceQuota @ 05/07/23 11:56:24.406
  STEP: Creating a ResourceQuota @ 05/07/23 11:56:29.414
  STEP: Ensuring resource quota status is calculated @ 05/07/23 11:56:29.422
  STEP: Creating a Pod that fits quota @ 05/07/23 11:56:31.424
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/07/23 11:56:31.433
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/07/23 11:56:33.435
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/07/23 11:56:33.436
  STEP: Ensuring a pod cannot update its resource requirements @ 05/07/23 11:56:33.437
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/07/23 11:56:33.439
  STEP: Deleting the pod @ 05/07/23 11:56:35.442
  STEP: Ensuring resource quota status released the pod usage @ 05/07/23 11:56:35.453
  May  7 11:56:37.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1720" for this suite. @ 05/07/23 11:56:37.457
• [13.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/07/23 11:56:37.46
  May  7 11:56:37.460: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/07/23 11:56:37.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:56:37.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:56:37.473
  STEP: Setting up the test @ 05/07/23 11:56:37.474
  STEP: Creating hostNetwork=false pod @ 05/07/23 11:56:37.474
  STEP: Creating hostNetwork=true pod @ 05/07/23 11:56:39.483
  STEP: Running the test @ 05/07/23 11:56:41.491
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/07/23 11:56:41.491
  May  7 11:56:41.491: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:41.491: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:41.491: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:41.491: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  7 11:56:41.548: INFO: Exec stderr: ""
  May  7 11:56:41.548: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:41.548: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:41.548: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:41.549: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  7 11:56:41.614: INFO: Exec stderr: ""
  May  7 11:56:41.614: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:41.614: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:41.614: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:41.614: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  7 11:56:41.660: INFO: Exec stderr: ""
  May  7 11:56:41.660: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:41.661: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:41.661: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:41.661: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  7 11:56:41.689: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/07/23 11:56:41.689
  May  7 11:56:41.689: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:41.689: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:41.689: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:41.689: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May  7 11:56:41.712: INFO: Exec stderr: ""
  May  7 11:56:41.712: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:41.712: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:41.712: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:41.712: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May  7 11:56:41.752: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/07/23 11:56:41.752
  May  7 11:56:41.752: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:41.752: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:41.752: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:41.752: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  7 11:56:41.813: INFO: Exec stderr: ""
  May  7 11:56:41.813: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:41.813: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:41.814: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:41.814: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May  7 11:56:41.887: INFO: Exec stderr: ""
  May  7 11:56:41.887: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:41.887: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:41.887: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:41.887: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  7 11:56:41.911: INFO: Exec stderr: ""
  May  7 11:56:41.911: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4158 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:41.911: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:41.912: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:41.912: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-4158/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May  7 11:56:41.957: INFO: Exec stderr: ""
  May  7 11:56:41.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-4158" for this suite. @ 05/07/23 11:56:41.959
• [4.502 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 05/07/23 11:56:41.962
  May  7 11:56:41.962: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pod-network-test @ 05/07/23 11:56:41.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:56:41.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:56:41.973
  STEP: Performing setup for networking test in namespace pod-network-test-470 @ 05/07/23 11:56:41.975
  STEP: creating a selector @ 05/07/23 11:56:41.975
  STEP: Creating the service pods in kubernetes @ 05/07/23 11:56:41.975
  May  7 11:56:41.975: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/07/23 11:56:54.034
  May  7 11:56:56.048: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May  7 11:56:56.048: INFO: Going to poll 172.20.3.97 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May  7 11:56:56.049: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.3.97 8081 | grep -v '^\s*$'] Namespace:pod-network-test-470 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:56.049: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:56.049: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:56.049: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-470/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.3.97+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  7 11:56:57.127: INFO: Found all 1 expected endpoints: [netserver-0]
  May  7 11:56:57.127: INFO: Going to poll 172.20.231.243 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May  7 11:56:57.129: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.231.243 8081 | grep -v '^\s*$'] Namespace:pod-network-test-470 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:57.129: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:57.129: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:57.129: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-470/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.231.243+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  7 11:56:58.175: INFO: Found all 1 expected endpoints: [netserver-1]
  May  7 11:56:58.175: INFO: Going to poll 172.20.191.62 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May  7 11:56:58.177: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.191.62 8081 | grep -v '^\s*$'] Namespace:pod-network-test-470 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:56:58.177: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:56:58.177: INFO: ExecWithOptions: Clientset creation
  May  7 11:56:58.177: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-470/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.191.62+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  7 11:56:59.221: INFO: Found all 1 expected endpoints: [netserver-2]
  May  7 11:56:59.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-470" for this suite. @ 05/07/23 11:56:59.223
• [17.267 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 05/07/23 11:56:59.23
  May  7 11:56:59.230: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replicaset @ 05/07/23 11:56:59.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:56:59.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:56:59.243
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/07/23 11:56:59.245
  STEP: When a replicaset with a matching selector is created @ 05/07/23 11:57:01.253
  STEP: Then the orphan pod is adopted @ 05/07/23 11:57:01.256
  STEP: When the matched label of one of its pods change @ 05/07/23 11:57:02.259
  May  7 11:57:02.260: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/07/23 11:57:02.265
  May  7 11:57:03.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6463" for this suite. @ 05/07/23 11:57:03.27
• [4.044 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/07/23 11:57:03.274
  May  7 11:57:03.274: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sched-pred @ 05/07/23 11:57:03.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:03.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:03.284
  May  7 11:57:03.285: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  7 11:57:03.288: INFO: Waiting for terminating namespaces to be deleted...
  May  7 11:57:03.289: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.201 before test
  May  7 11:57:03.292: INFO: calico-node-w75tc from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.292: INFO: 	Container calico-node ready: true, restart count 0
  May  7 11:57:03.292: INFO: coredns-6557d7db9c-cgp7l from kube-system started at 2023-05-07 09:07:01 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.292: INFO: 	Container coredns ready: true, restart count 0
  May  7 11:57:03.292: INFO: dashboard-metrics-scraper-5c876f54bd-z7s9t from kube-system started at 2023-05-07 09:07:07 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.292: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May  7 11:57:03.292: INFO: metrics-server-57fbbb5957-fw6s6 from kube-system started at 2023-05-07 09:10:30 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.292: INFO: 	Container metrics-server ready: true, restart count 0
  May  7 11:57:03.292: INFO: node-local-dns-hwr8c from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.292: INFO: 	Container node-cache ready: true, restart count 0
  May  7 11:57:03.292: INFO: netserver-0 from pod-network-test-470 started at 2023-05-07 11:56:41 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.292: INFO: 	Container webserver ready: true, restart count 0
  May  7 11:57:03.292: INFO: pod-adoption-release-9mtkc from replicaset-6463 started at 2023-05-07 11:57:02 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.292: INFO: 	Container pod-adoption-release ready: false, restart count 0
  May  7 11:57:03.292: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-w58n4 from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 11:57:03.292: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 11:57:03.292: INFO: 	Container systemd-logs ready: true, restart count 0
  May  7 11:57:03.292: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.202 before test
  May  7 11:57:03.295: INFO: calico-kube-controllers-7ccf856ff8-tzlpm from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.295: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May  7 11:57:03.295: INFO: calico-node-j77rx from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.295: INFO: 	Container calico-node ready: true, restart count 0
  May  7 11:57:03.295: INFO: kubernetes-dashboard-89b5448d6-flhtr from kube-system started at 2023-05-07 09:07:07 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.295: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May  7 11:57:03.295: INFO: node-local-dns-4lrj5 from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.295: INFO: 	Container node-cache ready: true, restart count 0
  May  7 11:57:03.295: INFO: host-test-container-pod from pod-network-test-470 started at 2023-05-07 11:56:54 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.295: INFO: 	Container agnhost-container ready: true, restart count 0
  May  7 11:57:03.295: INFO: netserver-1 from pod-network-test-470 started at 2023-05-07 11:56:42 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.295: INFO: 	Container webserver ready: true, restart count 0
  May  7 11:57:03.295: INFO: test-container-pod from pod-network-test-470 started at 2023-05-07 11:56:54 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.295: INFO: 	Container webserver ready: true, restart count 0
  May  7 11:57:03.295: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-7v67t from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 11:57:03.295: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 11:57:03.295: INFO: 	Container systemd-logs ready: true, restart count 0
  May  7 11:57:03.295: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.203 before test
  May  7 11:57:03.297: INFO: calico-node-2m8cq from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.297: INFO: 	Container calico-node ready: true, restart count 0
  May  7 11:57:03.297: INFO: node-local-dns-9mnsl from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.297: INFO: 	Container node-cache ready: true, restart count 0
  May  7 11:57:03.297: INFO: netserver-2 from pod-network-test-470 started at 2023-05-07 11:56:42 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.297: INFO: 	Container webserver ready: true, restart count 0
  May  7 11:57:03.297: INFO: pod-adoption-release from replicaset-6463 started at 2023-05-07 11:56:59 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.297: INFO: 	Container pod-adoption-release ready: true, restart count 0
  May  7 11:57:03.297: INFO: sonobuoy from sonobuoy started at 2023-05-07 11:54:26 +0000 UTC (1 container statuses recorded)
  May  7 11:57:03.297: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  7 11:57:03.297: INFO: sonobuoy-e2e-job-8208a86a1e4a4756 from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 11:57:03.297: INFO: 	Container e2e ready: true, restart count 0
  May  7 11:57:03.297: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 11:57:03.297: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-mhtmw from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 11:57:03.297: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 11:57:03.297: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/07/23 11:57:03.297
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/07/23 11:57:05.306
  STEP: Trying to apply a random label on the found node. @ 05/07/23 11:57:05.312
  STEP: verifying the node has the label kubernetes.io/e2e-de67f8f5-3115-4636-89c7-79fecedc6ea9 42 @ 05/07/23 11:57:05.317
  STEP: Trying to relaunch the pod, now with labels. @ 05/07/23 11:57:05.32
  STEP: removing the label kubernetes.io/e2e-de67f8f5-3115-4636-89c7-79fecedc6ea9 off the node 10.255.0.203 @ 05/07/23 11:57:07.339
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-de67f8f5-3115-4636-89c7-79fecedc6ea9 @ 05/07/23 11:57:07.352
  May  7 11:57:07.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-485" for this suite. @ 05/07/23 11:57:07.359
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 05/07/23 11:57:07.363
  May  7 11:57:07.363: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename controllerrevisions @ 05/07/23 11:57:07.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:07.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:07.392
  STEP: Creating DaemonSet "e2e-2w2ch-daemon-set" @ 05/07/23 11:57:07.406
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/07/23 11:57:07.41
  May  7 11:57:07.422: INFO: Number of nodes with available pods controlled by daemonset e2e-2w2ch-daemon-set: 0
  May  7 11:57:07.422: INFO: Node 10.255.0.201 is running 0 daemon pod, expected 1
  May  7 11:57:08.426: INFO: Number of nodes with available pods controlled by daemonset e2e-2w2ch-daemon-set: 1
  May  7 11:57:08.426: INFO: Node 10.255.0.201 is running 0 daemon pod, expected 1
  May  7 11:57:09.425: INFO: Number of nodes with available pods controlled by daemonset e2e-2w2ch-daemon-set: 3
  May  7 11:57:09.425: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-2w2ch-daemon-set
  STEP: Confirm DaemonSet "e2e-2w2ch-daemon-set" successfully created with "daemonset-name=e2e-2w2ch-daemon-set" label @ 05/07/23 11:57:09.426
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-2w2ch-daemon-set" @ 05/07/23 11:57:09.428
  May  7 11:57:09.430: INFO: Located ControllerRevision: "e2e-2w2ch-daemon-set-ddb459bb5"
  STEP: Patching ControllerRevision "e2e-2w2ch-daemon-set-ddb459bb5" @ 05/07/23 11:57:09.431
  May  7 11:57:09.436: INFO: e2e-2w2ch-daemon-set-ddb459bb5 has been patched
  STEP: Create a new ControllerRevision @ 05/07/23 11:57:09.436
  May  7 11:57:09.439: INFO: Created ControllerRevision: e2e-2w2ch-daemon-set-75c6c4f8b5
  STEP: Confirm that there are two ControllerRevisions @ 05/07/23 11:57:09.439
  May  7 11:57:09.439: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  7 11:57:09.440: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-2w2ch-daemon-set-ddb459bb5" @ 05/07/23 11:57:09.44
  STEP: Confirm that there is only one ControllerRevision @ 05/07/23 11:57:09.442
  May  7 11:57:09.442: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  7 11:57:09.443: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-2w2ch-daemon-set-75c6c4f8b5" @ 05/07/23 11:57:09.444
  May  7 11:57:09.447: INFO: e2e-2w2ch-daemon-set-75c6c4f8b5 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/07/23 11:57:09.447
  W0507 11:57:09.451144      20 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/07/23 11:57:09.451
  May  7 11:57:09.451: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  7 11:57:10.452: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  7 11:57:10.454: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-2w2ch-daemon-set-75c6c4f8b5=updated" @ 05/07/23 11:57:10.454
  STEP: Confirm that there is only one ControllerRevision @ 05/07/23 11:57:10.457
  May  7 11:57:10.457: INFO: Requesting list of ControllerRevisions to confirm quantity
  May  7 11:57:10.458: INFO: Found 1 ControllerRevisions
  May  7 11:57:10.459: INFO: ControllerRevision "e2e-2w2ch-daemon-set-6596d5f856" has revision 3
  STEP: Deleting DaemonSet "e2e-2w2ch-daemon-set" @ 05/07/23 11:57:10.46
  STEP: deleting DaemonSet.extensions e2e-2w2ch-daemon-set in namespace controllerrevisions-6608, will wait for the garbage collector to delete the pods @ 05/07/23 11:57:10.46
  May  7 11:57:10.520: INFO: Deleting DaemonSet.extensions e2e-2w2ch-daemon-set took: 7.737839ms
  May  7 11:57:10.620: INFO: Terminating DaemonSet.extensions e2e-2w2ch-daemon-set pods took: 100.668848ms
  May  7 11:57:11.325: INFO: Number of nodes with available pods controlled by daemonset e2e-2w2ch-daemon-set: 0
  May  7 11:57:11.325: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-2w2ch-daemon-set
  May  7 11:57:11.327: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47798"},"items":null}

  May  7 11:57:11.328: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47798"},"items":null}

  May  7 11:57:11.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-6608" for this suite. @ 05/07/23 11:57:11.334
• [3.973 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 05/07/23 11:57:11.337
  May  7 11:57:11.337: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/07/23 11:57:11.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:11.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:11.356
  STEP: fetching the /apis discovery document @ 05/07/23 11:57:11.357
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/07/23 11:57:11.358
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/07/23 11:57:11.358
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/07/23 11:57:11.358
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/07/23 11:57:11.359
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/07/23 11:57:11.359
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/07/23 11:57:11.359
  May  7 11:57:11.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4541" for this suite. @ 05/07/23 11:57:11.362
• [0.028 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 05/07/23 11:57:11.366
  May  7 11:57:11.366: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/07/23 11:57:11.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:11.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:11.377
  May  7 11:57:11.378: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:57:12.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8157" for this suite. @ 05/07/23 11:57:12.392
• [1.029 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 05/07/23 11:57:12.395
  May  7 11:57:12.395: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 11:57:12.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:12.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:12.456
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5229 @ 05/07/23 11:57:12.458
  STEP: changing the ExternalName service to type=NodePort @ 05/07/23 11:57:12.463
  STEP: creating replication controller externalname-service in namespace services-5229 @ 05/07/23 11:57:12.473
  I0507 11:57:12.485972      20 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5229, replica count: 2
  I0507 11:57:15.536927      20 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  7 11:57:15.536: INFO: Creating new exec pod
  May  7 11:57:18.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-5229 exec execpod22vv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  7 11:57:18.679: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  7 11:57:18.679: INFO: stdout: "externalname-service-thl4r"
  May  7 11:57:18.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-5229 exec execpod22vv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.200.170 80'
  May  7 11:57:18.771: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.200.170 80\nConnection to 10.68.200.170 80 port [tcp/http] succeeded!\n"
  May  7 11:57:18.771: INFO: stdout: "externalname-service-psk5b"
  May  7 11:57:18.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-5229 exec execpod22vv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.0.202 31218'
  May  7 11:57:18.859: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.0.202 31218\nConnection to 10.255.0.202 31218 port [tcp/*] succeeded!\n"
  May  7 11:57:18.859: INFO: stdout: "externalname-service-thl4r"
  May  7 11:57:18.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-5229 exec execpod22vv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.0.201 31218'
  May  7 11:57:18.948: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.0.201 31218\nConnection to 10.255.0.201 31218 port [tcp/*] succeeded!\n"
  May  7 11:57:18.948: INFO: stdout: "externalname-service-thl4r"
  May  7 11:57:18.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 11:57:18.949: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-5229" for this suite. @ 05/07/23 11:57:18.961
• [6.571 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 05/07/23 11:57:18.967
  May  7 11:57:18.967: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/07/23 11:57:18.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:18.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:18.986
  STEP: getting /apis @ 05/07/23 11:57:18.988
  STEP: getting /apis/storage.k8s.io @ 05/07/23 11:57:18.991
  STEP: getting /apis/storage.k8s.io/v1 @ 05/07/23 11:57:18.991
  STEP: creating @ 05/07/23 11:57:18.992
  STEP: watching @ 05/07/23 11:57:19
  May  7 11:57:19.000: INFO: starting watch
  STEP: getting @ 05/07/23 11:57:19.007
  STEP: listing in namespace @ 05/07/23 11:57:19.008
  STEP: listing across namespaces @ 05/07/23 11:57:19.009
  STEP: patching @ 05/07/23 11:57:19.011
  STEP: updating @ 05/07/23 11:57:19.015
  May  7 11:57:19.017: INFO: waiting for watch events with expected annotations in namespace
  May  7 11:57:19.018: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/07/23 11:57:19.018
  STEP: deleting a collection @ 05/07/23 11:57:19.022
  May  7 11:57:19.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-695" for this suite. @ 05/07/23 11:57:19.03
• [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 05/07/23 11:57:19.034
  May  7 11:57:19.034: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename svcaccounts @ 05/07/23 11:57:19.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:19.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:19.05
  May  7 11:57:19.053: INFO: Got root ca configmap in namespace "svcaccounts-1715"
  May  7 11:57:19.056: INFO: Deleted root ca configmap in namespace "svcaccounts-1715"
  STEP: waiting for a new root ca configmap created @ 05/07/23 11:57:19.557
  May  7 11:57:19.558: INFO: Recreated root ca configmap in namespace "svcaccounts-1715"
  May  7 11:57:19.561: INFO: Updated root ca configmap in namespace "svcaccounts-1715"
  STEP: waiting for the root ca configmap reconciled @ 05/07/23 11:57:20.061
  May  7 11:57:20.063: INFO: Reconciled root ca configmap in namespace "svcaccounts-1715"
  May  7 11:57:20.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1715" for this suite. @ 05/07/23 11:57:20.064
• [1.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/07/23 11:57:20.067
  May  7 11:57:20.067: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename watch @ 05/07/23 11:57:20.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:20.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:20.08
  STEP: creating a new configmap @ 05/07/23 11:57:20.084
  STEP: modifying the configmap once @ 05/07/23 11:57:20.086
  STEP: modifying the configmap a second time @ 05/07/23 11:57:20.089
  STEP: deleting the configmap @ 05/07/23 11:57:20.092
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/07/23 11:57:20.094
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/07/23 11:57:20.094
  May  7 11:57:20.095: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2615  c244a9b5-2235-405e-9e25-0ca34251c0ee 47987 0 2023-05-07 11:57:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-07 11:57:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 11:57:20.095: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2615  c244a9b5-2235-405e-9e25-0ca34251c0ee 47988 0 2023-05-07 11:57:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-07 11:57:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 11:57:20.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2615" for this suite. @ 05/07/23 11:57:20.096
• [0.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 05/07/23 11:57:20.099
  May  7 11:57:20.099: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 11:57:20.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:20.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:20.107
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/07/23 11:57:20.109
  STEP: Saw pod success @ 05/07/23 11:57:24.121
  May  7 11:57:24.127: INFO: Trying to get logs from node 10.255.0.202 pod pod-03cb2515-6b26-43e8-a29a-9e4464053249 container test-container: <nil>
  STEP: delete the pod @ 05/07/23 11:57:24.144
  May  7 11:57:24.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8448" for this suite. @ 05/07/23 11:57:24.159
• [4.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 05/07/23 11:57:24.166
  May  7 11:57:24.166: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename job @ 05/07/23 11:57:24.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:24.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:24.177
  STEP: Creating a job @ 05/07/23 11:57:24.181
  STEP: Ensuring active pods == parallelism @ 05/07/23 11:57:24.186
  STEP: Orphaning one of the Job's Pods @ 05/07/23 11:57:26.188
  May  7 11:57:26.698: INFO: Successfully updated pod "adopt-release-jncx5"
  STEP: Checking that the Job readopts the Pod @ 05/07/23 11:57:26.698
  STEP: Removing the labels from the Job's Pod @ 05/07/23 11:57:28.702
  May  7 11:57:29.211: INFO: Successfully updated pod "adopt-release-jncx5"
  STEP: Checking that the Job releases the Pod @ 05/07/23 11:57:29.211
  May  7 11:57:31.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8156" for this suite. @ 05/07/23 11:57:31.217
• [7.054 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/07/23 11:57:31.22
  May  7 11:57:31.220: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 11:57:31.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:31.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:31.228
  STEP: Creating projection with secret that has name projected-secret-test-map-16c73265-2b38-433b-b48a-0c05be6a9929 @ 05/07/23 11:57:31.23
  STEP: Creating a pod to test consume secrets @ 05/07/23 11:57:31.232
  STEP: Saw pod success @ 05/07/23 11:57:35.251
  May  7 11:57:35.252: INFO: Trying to get logs from node 10.255.0.201 pod pod-projected-secrets-6df36c0f-f0f4-4f54-836c-41eee2c4c67b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 11:57:35.262
  May  7 11:57:35.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3609" for this suite. @ 05/07/23 11:57:35.271
• [4.053 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 05/07/23 11:57:35.273
  May  7 11:57:35.273: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename svcaccounts @ 05/07/23 11:57:35.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:57:35.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:57:35.282
  May  7 11:57:35.290: INFO: created pod
  STEP: Saw pod success @ 05/07/23 11:57:39.297
  May  7 11:58:09.297: INFO: polling logs
  May  7 11:58:09.301: INFO: Pod logs: 
  I0507 11:57:35.893167       1 log.go:198] OK: Got token
  I0507 11:57:35.893266       1 log.go:198] validating with in-cluster discovery
  I0507 11:57:35.893497       1 log.go:198] OK: got issuer https://kubernetes.default.svc
  I0507 11:57:35.893552       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1744:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683461255, NotBefore:1683460655, IssuedAt:1683460655, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1744", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9dac5735-10db-4734-986c-1af9842ebc0a"}}}
  I0507 11:57:35.899430       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
  I0507 11:57:35.902917       1 log.go:198] OK: Validated signature on JWT
  I0507 11:57:35.903013       1 log.go:198] OK: Got valid claims from token!
  I0507 11:57:35.903036       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1744:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683461255, NotBefore:1683460655, IssuedAt:1683460655, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1744", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9dac5735-10db-4734-986c-1af9842ebc0a"}}}

  May  7 11:58:09.301: INFO: completed pod
  May  7 11:58:09.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1744" for this suite. @ 05/07/23 11:58:09.305
• [34.035 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/07/23 11:58:09.309
  May  7 11:58:09.309: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 11:58:09.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:58:09.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:58:09.32
  STEP: Creating secret with name secret-test-8780803b-5075-4281-9a7e-bfe1939de21a @ 05/07/23 11:58:09.321
  STEP: Creating a pod to test consume secrets @ 05/07/23 11:58:09.323
  STEP: Saw pod success @ 05/07/23 11:58:13.332
  May  7 11:58:13.334: INFO: Trying to get logs from node 10.255.0.202 pod pod-secrets-78746940-d5d9-47f1-90e7-f7fab311449e container secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 11:58:13.336
  May  7 11:58:13.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6028" for this suite. @ 05/07/23 11:58:13.344
• [4.037 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 05/07/23 11:58:13.346
  May  7 11:58:13.346: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename statefulset @ 05/07/23 11:58:13.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:58:13.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:58:13.355
  STEP: Creating service test in namespace statefulset-1237 @ 05/07/23 11:58:13.357
  STEP: Creating statefulset ss in namespace statefulset-1237 @ 05/07/23 11:58:13.359
  May  7 11:58:13.372: INFO: Found 0 stateful pods, waiting for 1
  May  7 11:58:23.374: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/07/23 11:58:23.377
  STEP: updating a scale subresource @ 05/07/23 11:58:23.378
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/07/23 11:58:23.38
  STEP: Patch a scale subresource @ 05/07/23 11:58:23.382
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/07/23 11:58:23.387
  May  7 11:58:23.389: INFO: Deleting all statefulset in ns statefulset-1237
  May  7 11:58:23.391: INFO: Scaling statefulset ss to 0
  May  7 11:58:33.406: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 11:58:33.408: INFO: Deleting statefulset ss
  May  7 11:58:33.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1237" for this suite. @ 05/07/23 11:58:33.416
• [20.073 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/07/23 11:58:33.419
  May  7 11:58:33.419: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 11:58:33.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:58:33.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:58:33.429
  STEP: creating pod @ 05/07/23 11:58:33.431
  May  7 11:58:35.442: INFO: Pod pod-hostip-32f3d1dd-21e0-4057-b7ac-75489eeaf4cd has hostIP: 10.255.0.202
  May  7 11:58:35.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8859" for this suite. @ 05/07/23 11:58:35.444
• [2.027 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 05/07/23 11:58:35.447
  May  7 11:58:35.447: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/07/23 11:58:35.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:58:35.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:58:35.456
  May  7 11:58:35.459: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/07/23 11:58:36.663
  May  7 11:58:36.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 --namespace=crd-publish-openapi-7807 create -f -'
  May  7 11:58:37.154: INFO: stderr: ""
  May  7 11:58:37.154: INFO: stdout: "e2e-test-crd-publish-openapi-3849-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May  7 11:58:37.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 --namespace=crd-publish-openapi-7807 delete e2e-test-crd-publish-openapi-3849-crds test-foo'
  May  7 11:58:37.199: INFO: stderr: ""
  May  7 11:58:37.199: INFO: stdout: "e2e-test-crd-publish-openapi-3849-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  May  7 11:58:37.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 --namespace=crd-publish-openapi-7807 apply -f -'
  May  7 11:58:37.333: INFO: stderr: ""
  May  7 11:58:37.333: INFO: stdout: "e2e-test-crd-publish-openapi-3849-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May  7 11:58:37.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 --namespace=crd-publish-openapi-7807 delete e2e-test-crd-publish-openapi-3849-crds test-foo'
  May  7 11:58:37.378: INFO: stderr: ""
  May  7 11:58:37.378: INFO: stdout: "e2e-test-crd-publish-openapi-3849-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/07/23 11:58:37.378
  May  7 11:58:37.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 --namespace=crd-publish-openapi-7807 create -f -'
  May  7 11:58:37.508: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/07/23 11:58:37.508
  May  7 11:58:37.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 --namespace=crd-publish-openapi-7807 create -f -'
  May  7 11:58:37.666: INFO: rc: 1
  May  7 11:58:37.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 --namespace=crd-publish-openapi-7807 apply -f -'
  May  7 11:58:37.805: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/07/23 11:58:37.805
  May  7 11:58:37.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 --namespace=crd-publish-openapi-7807 create -f -'
  May  7 11:58:37.942: INFO: rc: 1
  May  7 11:58:37.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 --namespace=crd-publish-openapi-7807 apply -f -'
  May  7 11:58:38.085: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/07/23 11:58:38.085
  May  7 11:58:38.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 explain e2e-test-crd-publish-openapi-3849-crds'
  May  7 11:58:38.222: INFO: stderr: ""
  May  7 11:58:38.222: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3849-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/07/23 11:58:38.223
  May  7 11:58:38.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 explain e2e-test-crd-publish-openapi-3849-crds.metadata'
  May  7 11:58:38.359: INFO: stderr: ""
  May  7 11:58:38.359: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3849-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  May  7 11:58:38.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 explain e2e-test-crd-publish-openapi-3849-crds.spec'
  May  7 11:58:38.494: INFO: stderr: ""
  May  7 11:58:38.494: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3849-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  May  7 11:58:38.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 explain e2e-test-crd-publish-openapi-3849-crds.spec.bars'
  May  7 11:58:38.622: INFO: stderr: ""
  May  7 11:58:38.622: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-3849-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/07/23 11:58:38.622
  May  7 11:58:38.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-7807 explain e2e-test-crd-publish-openapi-3849-crds.spec.bars2'
  May  7 11:58:38.749: INFO: rc: 1
  May  7 11:58:39.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7807" for this suite. @ 05/07/23 11:58:39.946
• [4.502 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 05/07/23 11:58:39.95
  May  7 11:58:39.950: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 11:58:39.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:58:39.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:58:39.959
  STEP: creating Agnhost RC @ 05/07/23 11:58:39.961
  May  7 11:58:39.961: INFO: namespace kubectl-9068
  May  7 11:58:39.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9068 create -f -'
  May  7 11:58:40.467: INFO: stderr: ""
  May  7 11:58:40.467: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/07/23 11:58:40.467
  May  7 11:58:41.469: INFO: Selector matched 1 pods for map[app:agnhost]
  May  7 11:58:41.469: INFO: Found 1 / 1
  May  7 11:58:41.469: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May  7 11:58:41.471: INFO: Selector matched 1 pods for map[app:agnhost]
  May  7 11:58:41.471: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  7 11:58:41.471: INFO: wait on agnhost-primary startup in kubectl-9068 
  May  7 11:58:41.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9068 logs agnhost-primary-gj9j6 agnhost-primary'
  May  7 11:58:41.518: INFO: stderr: ""
  May  7 11:58:41.518: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 05/07/23 11:58:41.518
  May  7 11:58:41.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9068 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  May  7 11:58:41.569: INFO: stderr: ""
  May  7 11:58:41.569: INFO: stdout: "service/rm2 exposed\n"
  May  7 11:58:41.576: INFO: Service rm2 in namespace kubectl-9068 found.
  STEP: exposing service @ 05/07/23 11:58:43.579
  May  7 11:58:43.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9068 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  May  7 11:58:43.635: INFO: stderr: ""
  May  7 11:58:43.635: INFO: stdout: "service/rm3 exposed\n"
  May  7 11:58:43.639: INFO: Service rm3 in namespace kubectl-9068 found.
  May  7 11:58:45.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9068" for this suite. @ 05/07/23 11:58:45.648
• [5.701 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 05/07/23 11:58:45.651
  May  7 11:58:45.651: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 11:58:45.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:58:45.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:58:45.662
  STEP: Creating Pod @ 05/07/23 11:58:45.665
  STEP: Reading file content from the nginx-container @ 05/07/23 11:58:47.673
  May  7 11:58:47.673: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3320 PodName:pod-sharedvolume-7d1ce782-de10-4ba6-b6f2-159b269d8c55 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 11:58:47.673: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 11:58:47.673: INFO: ExecWithOptions: Clientset creation
  May  7 11:58:47.673: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/emptydir-3320/pods/pod-sharedvolume-7d1ce782-de10-4ba6-b6f2-159b269d8c55/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  May  7 11:58:47.732: INFO: Exec stderr: ""
  May  7 11:58:47.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3320" for this suite. @ 05/07/23 11:58:47.734
• [2.085 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 05/07/23 11:58:47.737
  May  7 11:58:47.737: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 11:58:47.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:58:47.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:58:47.747
  STEP: creating a replication controller @ 05/07/23 11:58:47.748
  May  7 11:58:47.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 create -f -'
  May  7 11:58:47.929: INFO: stderr: ""
  May  7 11:58:47.929: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/07/23 11:58:47.929
  May  7 11:58:47.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  7 11:58:47.976: INFO: stderr: ""
  May  7 11:58:47.976: INFO: stdout: "update-demo-nautilus-cf7gl update-demo-nautilus-m6z7k "
  May  7 11:58:47.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-cf7gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  7 11:58:48.017: INFO: stderr: ""
  May  7 11:58:48.017: INFO: stdout: ""
  May  7 11:58:48.017: INFO: update-demo-nautilus-cf7gl is created but not running
  May  7 11:58:53.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  7 11:58:53.064: INFO: stderr: ""
  May  7 11:58:53.064: INFO: stdout: "update-demo-nautilus-cf7gl update-demo-nautilus-m6z7k "
  May  7 11:58:53.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-cf7gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  7 11:58:53.107: INFO: stderr: ""
  May  7 11:58:53.107: INFO: stdout: ""
  May  7 11:58:53.107: INFO: update-demo-nautilus-cf7gl is created but not running
  May  7 11:58:58.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  7 11:58:58.158: INFO: stderr: ""
  May  7 11:58:58.158: INFO: stdout: "update-demo-nautilus-cf7gl update-demo-nautilus-m6z7k "
  May  7 11:58:58.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-cf7gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  7 11:58:58.207: INFO: stderr: ""
  May  7 11:58:58.207: INFO: stdout: "true"
  May  7 11:58:58.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-cf7gl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  7 11:58:58.250: INFO: stderr: ""
  May  7 11:58:58.250: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  7 11:58:58.250: INFO: validating pod update-demo-nautilus-cf7gl
  May  7 11:58:58.255: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  7 11:58:58.255: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  7 11:58:58.255: INFO: update-demo-nautilus-cf7gl is verified up and running
  May  7 11:58:58.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-m6z7k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  7 11:58:58.300: INFO: stderr: ""
  May  7 11:58:58.300: INFO: stdout: "true"
  May  7 11:58:58.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-m6z7k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  7 11:58:58.346: INFO: stderr: ""
  May  7 11:58:58.346: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  7 11:58:58.346: INFO: validating pod update-demo-nautilus-m6z7k
  May  7 11:58:58.350: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  7 11:58:58.350: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  7 11:58:58.350: INFO: update-demo-nautilus-m6z7k is verified up and running
  STEP: scaling down the replication controller @ 05/07/23 11:58:58.35
  May  7 11:58:58.351: INFO: scanned /root for discovery docs: <nil>
  May  7 11:58:58.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  May  7 11:58:59.406: INFO: stderr: ""
  May  7 11:58:59.406: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/07/23 11:58:59.406
  May  7 11:58:59.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  7 11:58:59.453: INFO: stderr: ""
  May  7 11:58:59.453: INFO: stdout: "update-demo-nautilus-m6z7k "
  May  7 11:58:59.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-m6z7k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  7 11:58:59.496: INFO: stderr: ""
  May  7 11:58:59.496: INFO: stdout: "true"
  May  7 11:58:59.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-m6z7k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  7 11:58:59.539: INFO: stderr: ""
  May  7 11:58:59.539: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  7 11:58:59.539: INFO: validating pod update-demo-nautilus-m6z7k
  May  7 11:58:59.546: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  7 11:58:59.546: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  7 11:58:59.546: INFO: update-demo-nautilus-m6z7k is verified up and running
  STEP: scaling up the replication controller @ 05/07/23 11:58:59.546
  May  7 11:58:59.547: INFO: scanned /root for discovery docs: <nil>
  May  7 11:58:59.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  May  7 11:59:00.603: INFO: stderr: ""
  May  7 11:59:00.603: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/07/23 11:59:00.603
  May  7 11:59:00.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  7 11:59:00.648: INFO: stderr: ""
  May  7 11:59:00.648: INFO: stdout: "update-demo-nautilus-kxr6s update-demo-nautilus-m6z7k "
  May  7 11:59:00.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-kxr6s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  7 11:59:00.691: INFO: stderr: ""
  May  7 11:59:00.691: INFO: stdout: "true"
  May  7 11:59:00.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-kxr6s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  7 11:59:00.733: INFO: stderr: ""
  May  7 11:59:00.733: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  7 11:59:00.733: INFO: validating pod update-demo-nautilus-kxr6s
  May  7 11:59:00.739: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  7 11:59:00.739: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  7 11:59:00.739: INFO: update-demo-nautilus-kxr6s is verified up and running
  May  7 11:59:00.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-m6z7k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  7 11:59:00.785: INFO: stderr: ""
  May  7 11:59:00.785: INFO: stdout: "true"
  May  7 11:59:00.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods update-demo-nautilus-m6z7k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  7 11:59:00.827: INFO: stderr: ""
  May  7 11:59:00.827: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  7 11:59:00.827: INFO: validating pod update-demo-nautilus-m6z7k
  May  7 11:59:00.831: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  7 11:59:00.831: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  7 11:59:00.831: INFO: update-demo-nautilus-m6z7k is verified up and running
  STEP: using delete to clean up resources @ 05/07/23 11:59:00.831
  May  7 11:59:00.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 delete --grace-period=0 --force -f -'
  May  7 11:59:00.874: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  7 11:59:00.874: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May  7 11:59:00.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get rc,svc -l name=update-demo --no-headers'
  May  7 11:59:00.924: INFO: stderr: "No resources found in kubectl-2531 namespace.\n"
  May  7 11:59:00.924: INFO: stdout: ""
  May  7 11:59:00.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-2531 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May  7 11:59:00.970: INFO: stderr: ""
  May  7 11:59:00.970: INFO: stdout: ""
  May  7 11:59:00.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2531" for this suite. @ 05/07/23 11:59:00.972
• [13.238 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 05/07/23 11:59:00.975
  May  7 11:59:00.975: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 11:59:00.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:59:00.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:59:00.991
  STEP: creating service endpoint-test2 in namespace services-5129 @ 05/07/23 11:59:00.993
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5129 to expose endpoints map[] @ 05/07/23 11:59:01
  May  7 11:59:01.013: INFO: successfully validated that service endpoint-test2 in namespace services-5129 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-5129 @ 05/07/23 11:59:01.013
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5129 to expose endpoints map[pod1:[80]] @ 05/07/23 11:59:03.029
  May  7 11:59:03.033: INFO: successfully validated that service endpoint-test2 in namespace services-5129 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/07/23 11:59:03.033
  May  7 11:59:03.033: INFO: Creating new exec pod
  May  7 11:59:06.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-5129 exec execpodrxktb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  7 11:59:06.161: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May  7 11:59:06.161: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 11:59:06.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-5129 exec execpodrxktb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.177.17 80'
  May  7 11:59:06.274: INFO: stderr: "+ nc -v -t -w 2 10.68.177.17 80\nConnection to 10.68.177.17 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  May  7 11:59:06.274: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-5129 @ 05/07/23 11:59:06.274
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5129 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/07/23 11:59:08.283
  May  7 11:59:08.288: INFO: successfully validated that service endpoint-test2 in namespace services-5129 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/07/23 11:59:08.288
  May  7 11:59:09.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-5129 exec execpodrxktb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  7 11:59:09.393: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May  7 11:59:09.394: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 11:59:09.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-5129 exec execpodrxktb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.177.17 80'
  May  7 11:59:09.506: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.177.17 80\nConnection to 10.68.177.17 80 port [tcp/http] succeeded!\n"
  May  7 11:59:09.506: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-5129 @ 05/07/23 11:59:09.506
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5129 to expose endpoints map[pod2:[80]] @ 05/07/23 11:59:09.521
  May  7 11:59:09.530: INFO: successfully validated that service endpoint-test2 in namespace services-5129 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/07/23 11:59:09.53
  May  7 11:59:10.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-5129 exec execpodrxktb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May  7 11:59:10.630: INFO: stderr: "+ + nc -v -t -w 2 endpoint-test2 80\necho hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May  7 11:59:10.630: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 11:59:10.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-5129 exec execpodrxktb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.177.17 80'
  May  7 11:59:10.727: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.177.17 80\nConnection to 10.68.177.17 80 port [tcp/http] succeeded!\n"
  May  7 11:59:10.727: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-5129 @ 05/07/23 11:59:10.727
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5129 to expose endpoints map[] @ 05/07/23 11:59:10.736
  May  7 11:59:11.748: INFO: successfully validated that service endpoint-test2 in namespace services-5129 exposes endpoints map[]
  May  7 11:59:11.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5129" for this suite. @ 05/07/23 11:59:11.767
• [10.797 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 05/07/23 11:59:11.772
  May  7 11:59:11.772: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename dns @ 05/07/23 11:59:11.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:59:11.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:59:11.785
  STEP: Creating a test headless service @ 05/07/23 11:59:11.787
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3765.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3765.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/07/23 11:59:11.791
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3765.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3765.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/07/23 11:59:11.791
  STEP: creating a pod to probe DNS @ 05/07/23 11:59:11.791
  STEP: submitting the pod to kubernetes @ 05/07/23 11:59:11.791
  STEP: retrieving the pod @ 05/07/23 11:59:13.8
  STEP: looking for the results for each expected name from probers @ 05/07/23 11:59:13.802
  May  7 11:59:13.840: INFO: DNS probes using dns-3765/dns-test-6e46ebb6-ffd6-423b-aa46-115d7b2779f8 succeeded

  May  7 11:59:13.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 11:59:13.842
  STEP: deleting the test headless service @ 05/07/23 11:59:13.853
  STEP: Destroying namespace "dns-3765" for this suite. @ 05/07/23 11:59:13.86
• [2.092 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 05/07/23 11:59:13.864
  May  7 11:59:13.864: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename subpath @ 05/07/23 11:59:13.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:59:13.871
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:59:13.874
  STEP: Setting up data @ 05/07/23 11:59:13.876
  STEP: Creating pod pod-subpath-test-projected-4khm @ 05/07/23 11:59:13.879
  STEP: Creating a pod to test atomic-volume-subpath @ 05/07/23 11:59:13.879
  STEP: Saw pod success @ 05/07/23 11:59:37.911
  May  7 11:59:37.912: INFO: Trying to get logs from node 10.255.0.203 pod pod-subpath-test-projected-4khm container test-container-subpath-projected-4khm: <nil>
  STEP: delete the pod @ 05/07/23 11:59:37.922
  STEP: Deleting pod pod-subpath-test-projected-4khm @ 05/07/23 11:59:37.932
  May  7 11:59:37.932: INFO: Deleting pod "pod-subpath-test-projected-4khm" in namespace "subpath-4893"
  May  7 11:59:37.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4893" for this suite. @ 05/07/23 11:59:37.939
• [24.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 05/07/23 11:59:37.943
  May  7 11:59:37.943: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replication-controller @ 05/07/23 11:59:37.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:59:37.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:59:37.96
  STEP: Creating ReplicationController "e2e-rc-k7mk9" @ 05/07/23 11:59:37.961
  May  7 11:59:37.964: INFO: Get Replication Controller "e2e-rc-k7mk9" to confirm replicas
  May  7 11:59:38.970: INFO: Get Replication Controller "e2e-rc-k7mk9" to confirm replicas
  May  7 11:59:38.972: INFO: Found 1 replicas for "e2e-rc-k7mk9" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-k7mk9" @ 05/07/23 11:59:38.972
  STEP: Updating a scale subresource @ 05/07/23 11:59:38.973
  STEP: Verifying replicas where modified for replication controller "e2e-rc-k7mk9" @ 05/07/23 11:59:38.975
  May  7 11:59:38.975: INFO: Get Replication Controller "e2e-rc-k7mk9" to confirm replicas
  May  7 11:59:39.978: INFO: Get Replication Controller "e2e-rc-k7mk9" to confirm replicas
  May  7 11:59:39.980: INFO: Found 2 replicas for "e2e-rc-k7mk9" replication controller
  May  7 11:59:39.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3292" for this suite. @ 05/07/23 11:59:39.982
• [2.041 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 05/07/23 11:59:39.984
  May  7 11:59:39.984: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 11:59:39.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:59:39.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:59:39.995
  May  7 11:59:39.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2670" for this suite. @ 05/07/23 11:59:39.999
• [0.018 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 05/07/23 11:59:40.002
  May  7 11:59:40.002: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename namespaces @ 05/07/23 11:59:40.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:59:40.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:59:40.012
  STEP: Updating Namespace "namespaces-7550" @ 05/07/23 11:59:40.013
  May  7 11:59:40.016: INFO: Namespace "namespaces-7550" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"a7c2dced-f026-4955-bba1-4e1a58282135", "kubernetes.io/metadata.name":"namespaces-7550", "namespaces-7550":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  May  7 11:59:40.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7550" for this suite. @ 05/07/23 11:59:40.018
• [0.018 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 05/07/23 11:59:40.021
  May  7 11:59:40.021: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-probe @ 05/07/23 11:59:40.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 11:59:40.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 11:59:40.031
  STEP: Creating pod test-grpc-ef04860c-d245-4179-a109-7f2e41d1b7c8 in namespace container-probe-206 @ 05/07/23 11:59:40.034
  May  7 11:59:42.043: INFO: Started pod test-grpc-ef04860c-d245-4179-a109-7f2e41d1b7c8 in namespace container-probe-206
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/07/23 11:59:42.043
  May  7 11:59:42.044: INFO: Initial restart count of pod test-grpc-ef04860c-d245-4179-a109-7f2e41d1b7c8 is 0
  May  7 12:00:46.128: INFO: Restart count of pod container-probe-206/test-grpc-ef04860c-d245-4179-a109-7f2e41d1b7c8 is now 1 (1m4.084515471s elapsed)
  May  7 12:00:46.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:00:46.13
  STEP: Destroying namespace "container-probe-206" for this suite. @ 05/07/23 12:00:46.137
• [66.118 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 05/07/23 12:00:46.139
  May  7 12:00:46.139: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replication-controller @ 05/07/23 12:00:46.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:00:46.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:00:46.147
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/07/23 12:00:46.15
  STEP: When a replication controller with a matching selector is created @ 05/07/23 12:00:48.159
  STEP: Then the orphan pod is adopted @ 05/07/23 12:00:48.162
  May  7 12:00:49.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5250" for this suite. @ 05/07/23 12:00:49.166
• [3.030 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 05/07/23 12:00:49.169
  May  7 12:00:49.169: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:00:49.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:00:49.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:00:49.179
  STEP: Setting up server cert @ 05/07/23 12:00:49.19
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:00:49.472
  STEP: Deploying the webhook pod @ 05/07/23 12:00:49.476
  STEP: Wait for the deployment to be ready @ 05/07/23 12:00:49.481
  May  7 12:00:49.485: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  May  7 12:00:51.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 0, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 0, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 0, 49, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 0, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/07/23 12:00:53.493
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:00:53.498
  May  7 12:00:54.499: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/07/23 12:00:54.5
  STEP: create a pod that should be denied by the webhook @ 05/07/23 12:00:54.509
  STEP: create a pod that causes the webhook to hang @ 05/07/23 12:00:54.515
  STEP: create a configmap that should be denied by the webhook @ 05/07/23 12:01:04.519
  STEP: create a configmap that should be admitted by the webhook @ 05/07/23 12:01:04.525
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/07/23 12:01:04.53
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/07/23 12:01:04.533
  STEP: create a namespace that bypass the webhook @ 05/07/23 12:01:04.534
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/07/23 12:01:04.542
  May  7 12:01:04.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3401" for this suite. @ 05/07/23 12:01:04.589
  STEP: Destroying namespace "webhook-markers-7157" for this suite. @ 05/07/23 12:01:04.608
  STEP: Destroying namespace "exempted-namespace-2845" for this suite. @ 05/07/23 12:01:04.614
• [15.449 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 05/07/23 12:01:04.618
  May  7 12:01:04.618: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replicaset @ 05/07/23 12:01:04.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:01:04.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:01:04.627
  May  7 12:01:04.641: INFO: Pod name sample-pod: Found 0 pods out of 1
  May  7 12:01:09.644: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/07/23 12:01:09.644
  STEP: Scaling up "test-rs" replicaset  @ 05/07/23 12:01:09.644
  May  7 12:01:09.653: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/07/23 12:01:09.653
  W0507 12:01:09.669329      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May  7 12:01:09.672: INFO: observed ReplicaSet test-rs in namespace replicaset-6931 with ReadyReplicas 1, AvailableReplicas 1
  May  7 12:01:09.673: INFO: observed ReplicaSet test-rs in namespace replicaset-6931 with ReadyReplicas 1, AvailableReplicas 1
  May  7 12:01:09.688: INFO: observed ReplicaSet test-rs in namespace replicaset-6931 with ReadyReplicas 1, AvailableReplicas 1
  May  7 12:01:09.716: INFO: observed ReplicaSet test-rs in namespace replicaset-6931 with ReadyReplicas 1, AvailableReplicas 1
  May  7 12:01:10.942: INFO: observed ReplicaSet test-rs in namespace replicaset-6931 with ReadyReplicas 2, AvailableReplicas 2
  May  7 12:01:11.152: INFO: observed Replicaset test-rs in namespace replicaset-6931 with ReadyReplicas 3 found true
  May  7 12:01:11.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6931" for this suite. @ 05/07/23 12:01:11.154
• [6.539 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 05/07/23 12:01:11.158
  May  7 12:01:11.158: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/07/23 12:01:11.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:01:11.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:01:11.169
  STEP: set up a multi version CRD @ 05/07/23 12:01:11.17
  May  7 12:01:11.170: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: mark a version not serverd @ 05/07/23 12:01:14.77
  STEP: check the unserved version gets removed @ 05/07/23 12:01:14.794
  STEP: check the other version is not changed @ 05/07/23 12:01:15.449
  May  7 12:01:17.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5184" for this suite. @ 05/07/23 12:01:17.899
• [6.743 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 05/07/23 12:01:17.902
  May  7 12:01:17.902: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:01:17.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:01:17.91
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:01:17.912
  STEP: Creating configMap with name projected-configmap-test-volume-map-1f210fc9-1956-4f1d-9a64-c3b08eadf03a @ 05/07/23 12:01:17.914
  STEP: Creating a pod to test consume configMaps @ 05/07/23 12:01:17.915
  STEP: Saw pod success @ 05/07/23 12:01:19.923
  May  7 12:01:19.924: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-configmaps-4d871af0-b004-4ef1-a71e-ba019a157cdf container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 12:01:19.934
  May  7 12:01:19.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5604" for this suite. @ 05/07/23 12:01:19.942
• [2.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 05/07/23 12:01:19.946
  May  7 12:01:19.946: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 12:01:19.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:01:19.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:01:19.957
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-3709 @ 05/07/23 12:01:19.958
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/07/23 12:01:19.966
  STEP: creating service externalsvc in namespace services-3709 @ 05/07/23 12:01:19.966
  STEP: creating replication controller externalsvc in namespace services-3709 @ 05/07/23 12:01:19.974
  I0507 12:01:19.981342      20 runners.go:194] Created replication controller with name: externalsvc, namespace: services-3709, replica count: 2
  I0507 12:01:23.032351      20 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/07/23 12:01:23.048
  May  7 12:01:23.072: INFO: Creating new exec pod
  May  7 12:01:25.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-3709 exec execpod78wl9 -- /bin/sh -x -c nslookup nodeport-service.services-3709.svc.cluster.local'
  May  7 12:01:25.200: INFO: stderr: "+ nslookup nodeport-service.services-3709.svc.cluster.local\n"
  May  7 12:01:25.200: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-3709.svc.cluster.local\tcanonical name = externalsvc.services-3709.svc.cluster.local.\nName:\texternalsvc.services-3709.svc.cluster.local\nAddress: 10.68.80.129\n\n"
  May  7 12:01:25.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-3709, will wait for the garbage collector to delete the pods @ 05/07/23 12:01:25.205
  May  7 12:01:25.259: INFO: Deleting ReplicationController externalsvc took: 2.301715ms
  May  7 12:01:25.359: INFO: Terminating ReplicationController externalsvc pods took: 100.243522ms
  May  7 12:01:27.272: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-3709" for this suite. @ 05/07/23 12:01:27.287
• [7.347 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 05/07/23 12:01:27.293
  May  7 12:01:27.293: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 12:01:27.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:01:27.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:01:27.308
  STEP: create deployment with httpd image @ 05/07/23 12:01:27.31
  May  7 12:01:27.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9517 create -f -'
  May  7 12:01:27.501: INFO: stderr: ""
  May  7 12:01:27.501: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/07/23 12:01:27.501
  May  7 12:01:27.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9517 diff -f -'
  May  7 12:01:27.650: INFO: rc: 1
  May  7 12:01:27.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9517 delete -f -'
  May  7 12:01:27.695: INFO: stderr: ""
  May  7 12:01:27.695: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  May  7 12:01:27.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9517" for this suite. @ 05/07/23 12:01:27.697
• [0.409 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 05/07/23 12:01:27.703
  May  7 12:01:27.703: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:01:27.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:01:27.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:01:27.714
  STEP: Creating configMap with name projected-configmap-test-volume-map-d0519d6c-ce4a-4de6-a6b9-6cbb96cbd25b @ 05/07/23 12:01:27.716
  STEP: Creating a pod to test consume configMaps @ 05/07/23 12:01:27.718
  STEP: Saw pod success @ 05/07/23 12:01:31.729
  May  7 12:01:31.730: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-configmaps-4ab2e8ce-69cc-4e32-88fc-2c3810fa6004 container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 12:01:31.733
  May  7 12:01:31.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8740" for this suite. @ 05/07/23 12:01:31.741
• [4.042 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 05/07/23 12:01:31.745
  May  7 12:01:31.745: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename runtimeclass @ 05/07/23 12:01:31.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:01:31.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:01:31.755
  STEP: Deleting RuntimeClass runtimeclass-1490-delete-me @ 05/07/23 12:01:31.758
  STEP: Waiting for the RuntimeClass to disappear @ 05/07/23 12:01:31.76
  May  7 12:01:31.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1490" for this suite. @ 05/07/23 12:01:31.765
• [0.022 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/07/23 12:01:31.768
  May  7 12:01:31.768: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename deployment @ 05/07/23 12:01:31.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:01:31.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:01:31.785
  May  7 12:01:31.786: INFO: Creating simple deployment test-new-deployment
  May  7 12:01:31.792: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 05/07/23 12:01:33.798
  STEP: updating a scale subresource @ 05/07/23 12:01:33.799
  STEP: verifying the deployment Spec.Replicas was modified @ 05/07/23 12:01:33.804
  STEP: Patch a scale subresource @ 05/07/23 12:01:33.806
  May  7 12:01:33.822: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-4334  f1086e37-6ff9-4561-a0ed-f6f73a8b0eb7 49881 3 2023-05-07 12:01:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-07 12:01:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:01:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038b4848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-07 12:01:32 +0000 UTC,LastTransitionTime:2023-05-07 12:01:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-05-07 12:01:32 +0000 UTC,LastTransitionTime:2023-05-07 12:01:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  7 12:01:33.830: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-4334  d71a7523-30ba-4138-a234-9fd1bf13fb75 49886 3 2023-05-07 12:01:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment f1086e37-6ff9-4561-a0ed-f6f73a8b0eb7 0xc003969807 0xc003969808}] [] [{kube-controller-manager Update apps/v1 2023-05-07 12:01:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-07 12:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f1086e37-6ff9-4561-a0ed-f6f73a8b0eb7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003969a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  7 12:01:33.838: INFO: Pod "test-new-deployment-67bd4bf6dc-6q9m9" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-6q9m9 test-new-deployment-67bd4bf6dc- deployment-4334  9264537a-6159-414f-b116-51c84608a750 49884 0 2023-05-07 12:01:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc d71a7523-30ba-4138-a234-9fd1bf13fb75 0xc003969e37 0xc003969e38}] [] [{kube-controller-manager Update v1 2023-05-07 12:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d71a7523-30ba-4138-a234-9fd1bf13fb75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8kbz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8kbz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:01:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:01:33.838: INFO: Pod "test-new-deployment-67bd4bf6dc-km54z" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-km54z test-new-deployment-67bd4bf6dc- deployment-4334  95921fb6-075a-4811-ade4-12334bc482ff 49869 0 2023-05-07 12:01:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc d71a7523-30ba-4138-a234-9fd1bf13fb75 0xc003969fa0 0xc003969fa1}] [] [{kube-controller-manager Update v1 2023-05-07 12:01:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d71a7523-30ba-4138-a234-9fd1bf13fb75\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:01:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.231.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vsffz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vsffz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:01:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:01:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:01:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:01:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:172.20.231.209,StartTime:2023-05-07 12:01:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:01:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://308e963460023c5e784de953a1126c9afca2fe06cbdf9af68b6a5bbacbf67f39,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.231.209,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:01:33.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4334" for this suite. @ 05/07/23 12:01:33.843
• [2.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 05/07/23 12:01:33.849
  May  7 12:01:33.850: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/07/23 12:01:33.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:01:33.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:01:33.863
  May  7 12:01:33.864: INFO: Waiting up to 1m0s for all nodes to be ready
  May  7 12:02:33.874: INFO: Waiting for terminating namespaces to be deleted...
  May  7 12:02:33.875: INFO: Starting informer...
  STEP: Starting pods... @ 05/07/23 12:02:33.875
  May  7 12:02:34.085: INFO: Pod1 is running on 10.255.0.202. Tainting Node
  May  7 12:02:36.294: INFO: Pod2 is running on 10.255.0.202. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/07/23 12:02:36.294
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/07/23 12:02:36.301
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/07/23 12:02:36.303
  May  7 12:02:41.805: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  May  7 12:03:01.834: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  May  7 12:03:01.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/07/23 12:03:01.842
  STEP: Destroying namespace "taint-multiple-pods-8839" for this suite. @ 05/07/23 12:03:01.844
• [87.998 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 05/07/23 12:03:01.848
  May  7 12:03:01.848: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename subpath @ 05/07/23 12:03:01.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:03:01.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:03:01.909
  STEP: Setting up data @ 05/07/23 12:03:01.91
  STEP: Creating pod pod-subpath-test-secret-942w @ 05/07/23 12:03:01.913
  STEP: Creating a pod to test atomic-volume-subpath @ 05/07/23 12:03:01.913
  STEP: Saw pod success @ 05/07/23 12:03:25.946
  May  7 12:03:25.947: INFO: Trying to get logs from node 10.255.0.202 pod pod-subpath-test-secret-942w container test-container-subpath-secret-942w: <nil>
  STEP: delete the pod @ 05/07/23 12:03:25.964
  STEP: Deleting pod pod-subpath-test-secret-942w @ 05/07/23 12:03:25.974
  May  7 12:03:25.974: INFO: Deleting pod "pod-subpath-test-secret-942w" in namespace "subpath-5417"
  May  7 12:03:25.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5417" for this suite. @ 05/07/23 12:03:25.977
• [24.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 05/07/23 12:03:25.982
  May  7 12:03:25.982: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename daemonsets @ 05/07/23 12:03:25.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:03:25.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:03:25.997
  STEP: Creating simple DaemonSet "daemon-set" @ 05/07/23 12:03:26.01
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/07/23 12:03:26.012
  May  7 12:03:26.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:03:26.016: INFO: Node 10.255.0.201 is running 0 daemon pod, expected 1
  May  7 12:03:27.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  7 12:03:27.020: INFO: Node 10.255.0.201 is running 0 daemon pod, expected 1
  May  7 12:03:28.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May  7 12:03:28.020: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/07/23 12:03:28.021
  STEP: DeleteCollection of the DaemonSets @ 05/07/23 12:03:28.022
  STEP: Verify that ReplicaSets have been deleted @ 05/07/23 12:03:28.025
  May  7 12:03:28.030: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50328"},"items":null}

  May  7 12:03:28.033: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50328"},"items":[{"metadata":{"name":"daemon-set-46zt2","generateName":"daemon-set-","namespace":"daemonsets-6669","uid":"16cc3f15-8291-45ae-ab79-c1e52fe265c7","resourceVersion":"50323","creationTimestamp":"2023-05-07T12:03:26Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9f470e35-c065-47fd-9546-a8180fc0ac62","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-07T12:03:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f470e35-c065-47fd-9546-a8180fc0ac62\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-07T12:03:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.3.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-fbns7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-fbns7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.255.0.201","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.255.0.201"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:26Z"}],"hostIP":"10.255.0.201","podIP":"172.20.3.82","podIPs":[{"ip":"172.20.3.82"}],"startTime":"2023-05-07T12:03:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-07T12:03:26Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://86992afd9d859587148eb6b816b8179a6b997266ee02baad544565eea12741c8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-89fn7","generateName":"daemon-set-","namespace":"daemonsets-6669","uid":"5f70f6ac-73f5-40b7-9588-3058b220f153","resourceVersion":"50325","creationTimestamp":"2023-05-07T12:03:26Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9f470e35-c065-47fd-9546-a8180fc0ac62","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-07T12:03:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f470e35-c065-47fd-9546-a8180fc0ac62\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-07T12:03:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.191.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-z8v9c","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-z8v9c","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.255.0.203","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.255.0.203"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:26Z"}],"hostIP":"10.255.0.203","podIP":"172.20.191.29","podIPs":[{"ip":"172.20.191.29"}],"startTime":"2023-05-07T12:03:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-07T12:03:26Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://34b554178ab5e3f300085750cafeba379251c6c5fa51d83f528306fe0078d816","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-g7wfn","generateName":"daemon-set-","namespace":"daemonsets-6669","uid":"60b9a479-c3f9-4eb5-b218-a544e0701851","resourceVersion":"50321","creationTimestamp":"2023-05-07T12:03:26Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9f470e35-c065-47fd-9546-a8180fc0ac62","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-07T12:03:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9f470e35-c065-47fd-9546-a8180fc0ac62\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-07T12:03:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.231.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wm8th","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wm8th","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.255.0.202","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.255.0.202"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-07T12:03:26Z"}],"hostIP":"10.255.0.202","podIP":"172.20.231.198","podIPs":[{"ip":"172.20.231.198"}],"startTime":"2023-05-07T12:03:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-07T12:03:26Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://d537f7a66fe3f4fb37a0500c16f57d45822d18d22b50e542f86656ea9a3460f8","started":true}],"qosClass":"BestEffort"}}]}

  May  7 12:03:28.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6669" for this suite. @ 05/07/23 12:03:28.051
• [2.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 05/07/23 12:03:28.055
  May  7 12:03:28.055: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/07/23 12:03:28.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:03:28.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:03:28.064
  May  7 12:03:28.067: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/07/23 12:03:29.255
  May  7 12:03:29.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-9351 --namespace=crd-publish-openapi-9351 create -f -'
  May  7 12:03:31.729: INFO: stderr: ""
  May  7 12:03:31.729: INFO: stdout: "e2e-test-crd-publish-openapi-3759-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May  7 12:03:31.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-9351 --namespace=crd-publish-openapi-9351 delete e2e-test-crd-publish-openapi-3759-crds test-cr'
  May  7 12:03:31.778: INFO: stderr: ""
  May  7 12:03:31.778: INFO: stdout: "e2e-test-crd-publish-openapi-3759-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  May  7 12:03:31.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-9351 --namespace=crd-publish-openapi-9351 apply -f -'
  May  7 12:03:31.920: INFO: stderr: ""
  May  7 12:03:31.920: INFO: stdout: "e2e-test-crd-publish-openapi-3759-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May  7 12:03:31.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-9351 --namespace=crd-publish-openapi-9351 delete e2e-test-crd-publish-openapi-3759-crds test-cr'
  May  7 12:03:31.965: INFO: stderr: ""
  May  7 12:03:31.965: INFO: stdout: "e2e-test-crd-publish-openapi-3759-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/07/23 12:03:31.966
  May  7 12:03:31.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-9351 explain e2e-test-crd-publish-openapi-3759-crds'
  May  7 12:03:32.110: INFO: stderr: ""
  May  7 12:03:32.110: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-3759-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  May  7 12:03:33.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9351" for this suite. @ 05/07/23 12:03:33.347
• [5.295 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 05/07/23 12:03:33.351
  May  7 12:03:33.351: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 12:03:33.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:03:33.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:03:33.363
  STEP: Creating a pod to test downward api env vars @ 05/07/23 12:03:33.365
  STEP: Saw pod success @ 05/07/23 12:03:37.378
  May  7 12:03:37.379: INFO: Trying to get logs from node 10.255.0.202 pod downward-api-23102ff7-1f16-4de2-afc4-c269b1fbef72 container dapi-container: <nil>
  STEP: delete the pod @ 05/07/23 12:03:37.383
  May  7 12:03:37.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4632" for this suite. @ 05/07/23 12:03:37.392
• [4.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 05/07/23 12:03:37.395
  May  7 12:03:37.395: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:03:37.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:03:37.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:03:37.406
  STEP: Setting up server cert @ 05/07/23 12:03:37.418
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:03:37.532
  STEP: Deploying the webhook pod @ 05/07/23 12:03:37.538
  STEP: Wait for the deployment to be ready @ 05/07/23 12:03:37.546
  May  7 12:03:37.548: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/07/23 12:03:39.554
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:03:39.559
  May  7 12:03:40.559: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/07/23 12:03:40.589
  STEP: Creating a configMap that should be mutated @ 05/07/23 12:03:40.596
  STEP: Deleting the collection of validation webhooks @ 05/07/23 12:03:40.608
  STEP: Creating a configMap that should not be mutated @ 05/07/23 12:03:40.626
  May  7 12:03:40.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8376" for this suite. @ 05/07/23 12:03:40.663
  STEP: Destroying namespace "webhook-markers-7317" for this suite. @ 05/07/23 12:03:40.672
• [3.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 05/07/23 12:03:40.679
  May  7 12:03:40.679: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:03:40.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:03:40.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:03:40.693
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 12:03:40.696
  STEP: Saw pod success @ 05/07/23 12:03:44.71
  May  7 12:03:44.711: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-793421bc-c236-4bf6-b74d-216a063c33fc container client-container: <nil>
  STEP: delete the pod @ 05/07/23 12:03:44.714
  May  7 12:03:44.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7125" for this suite. @ 05/07/23 12:03:44.726
• [4.050 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 05/07/23 12:03:44.73
  May  7 12:03:44.730: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename disruption @ 05/07/23 12:03:44.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:03:44.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:03:44.738
  STEP: Waiting for the pdb to be processed @ 05/07/23 12:03:44.743
  STEP: Waiting for all pods to be running @ 05/07/23 12:03:46.781
  May  7 12:03:46.784: INFO: running pods: 0 < 3
  May  7 12:03:48.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9132" for this suite. @ 05/07/23 12:03:48.789
• [4.062 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 05/07/23 12:03:48.793
  May  7 12:03:48.793: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename daemonsets @ 05/07/23 12:03:48.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:03:48.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:03:48.807
  STEP: Creating simple DaemonSet "daemon-set" @ 05/07/23 12:03:48.816
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/07/23 12:03:48.818
  May  7 12:03:48.827: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:03:48.827: INFO: Node 10.255.0.201 is running 0 daemon pod, expected 1
  May  7 12:03:49.830: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:03:49.830: INFO: Node 10.255.0.201 is running 0 daemon pod, expected 1
  May  7 12:03:50.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  7 12:03:50.831: INFO: Node 10.255.0.202 is running 0 daemon pod, expected 1
  May  7 12:03:51.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May  7 12:03:51.832: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 05/07/23 12:03:51.833
  May  7 12:03:51.834: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/07/23 12:03:51.834
  May  7 12:03:51.838: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/07/23 12:03:51.838
  May  7 12:03:51.839: INFO: Observed &DaemonSet event: ADDED
  May  7 12:03:51.839: INFO: Observed &DaemonSet event: MODIFIED
  May  7 12:03:51.839: INFO: Observed &DaemonSet event: MODIFIED
  May  7 12:03:51.839: INFO: Observed &DaemonSet event: MODIFIED
  May  7 12:03:51.840: INFO: Observed &DaemonSet event: MODIFIED
  May  7 12:03:51.840: INFO: Found daemon set daemon-set in namespace daemonsets-3531 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  7 12:03:51.840: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/07/23 12:03:51.84
  STEP: watching for the daemon set status to be patched @ 05/07/23 12:03:51.843
  May  7 12:03:51.845: INFO: Observed &DaemonSet event: ADDED
  May  7 12:03:51.845: INFO: Observed &DaemonSet event: MODIFIED
  May  7 12:03:51.845: INFO: Observed &DaemonSet event: MODIFIED
  May  7 12:03:51.845: INFO: Observed &DaemonSet event: MODIFIED
  May  7 12:03:51.845: INFO: Observed &DaemonSet event: MODIFIED
  May  7 12:03:51.845: INFO: Observed daemon set daemon-set in namespace daemonsets-3531 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  7 12:03:51.845: INFO: Observed &DaemonSet event: MODIFIED
  May  7 12:03:51.845: INFO: Found daemon set daemon-set in namespace daemonsets-3531 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  May  7 12:03:51.845: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/07/23 12:03:51.847
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3531, will wait for the garbage collector to delete the pods @ 05/07/23 12:03:51.847
  May  7 12:03:51.901: INFO: Deleting DaemonSet.extensions daemon-set took: 2.129785ms
  May  7 12:03:52.002: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.998009ms
  May  7 12:03:53.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:03:53.403: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  7 12:03:53.404: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50753"},"items":null}

  May  7 12:03:53.405: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50753"},"items":null}

  May  7 12:03:53.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3531" for this suite. @ 05/07/23 12:03:53.412
• [4.623 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/07/23 12:03:53.417
  May  7 12:03:53.417: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename deployment @ 05/07/23 12:03:53.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:03:53.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:03:53.431
  May  7 12:03:53.443: INFO: Pod name rollover-pod: Found 0 pods out of 1
  May  7 12:03:58.446: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/07/23 12:03:58.446
  May  7 12:03:58.446: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  May  7 12:04:00.448: INFO: Creating deployment "test-rollover-deployment"
  May  7 12:04:00.452: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  May  7 12:04:02.457: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  May  7 12:04:02.459: INFO: Ensure that both replica sets have 1 created replica
  May  7 12:04:02.462: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  May  7 12:04:02.466: INFO: Updating deployment test-rollover-deployment
  May  7 12:04:02.466: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  May  7 12:04:04.470: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  May  7 12:04:04.472: INFO: Make sure deployment "test-rollover-deployment" is complete
  May  7 12:04:04.475: INFO: all replica sets need to contain the pod-template-hash label
  May  7 12:04:04.475: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 4, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:04:06.479: INFO: all replica sets need to contain the pod-template-hash label
  May  7 12:04:06.479: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 4, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:04:08.478: INFO: all replica sets need to contain the pod-template-hash label
  May  7 12:04:08.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 4, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:04:10.478: INFO: all replica sets need to contain the pod-template-hash label
  May  7 12:04:10.479: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 4, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:04:12.479: INFO: all replica sets need to contain the pod-template-hash label
  May  7 12:04:12.479: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 4, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 4, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:04:14.478: INFO: 
  May  7 12:04:14.478: INFO: Ensure that both old replica sets have no replicas
  May  7 12:04:14.481: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2404  397925ee-1b75-4949-ac32-9760fc319a6f 50947 2 2023-05-07 12:04:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-07 12:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00425eaf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-07 12:04:00 +0000 UTC,LastTransitionTime:2023-05-07 12:04:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-05-07 12:04:13 +0000 UTC,LastTransitionTime:2023-05-07 12:04:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  7 12:04:14.490: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-2404  fba06590-6d64-4dde-b5ef-e6f94816f918 50937 2 2023-05-07 12:04:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 397925ee-1b75-4949-ac32-9760fc319a6f 0xc00442c4b7 0xc00442c4b8}] [] [{kube-controller-manager Update apps/v1 2023-05-07 12:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"397925ee-1b75-4949-ac32-9760fc319a6f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:04:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00442c578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  7 12:04:14.490: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  May  7 12:04:14.490: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2404  94736b3c-cda1-46f9-a8b8-ef7ac5464722 50946 2 2023-05-07 12:03:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 397925ee-1b75-4949-ac32-9760fc319a6f 0xc00442c377 0xc00442c378}] [] [{e2e.test Update apps/v1 2023-05-07 12:03:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"397925ee-1b75-4949-ac32-9760fc319a6f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:04:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00442c448 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  7 12:04:14.490: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-2404  f628b8ce-5744-40fa-9ef1-81300b80f2e8 50899 2 2023-05-07 12:04:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 397925ee-1b75-4949-ac32-9760fc319a6f 0xc00442c5e7 0xc00442c5e8}] [] [{kube-controller-manager Update apps/v1 2023-05-07 12:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"397925ee-1b75-4949-ac32-9760fc319a6f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:04:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00442c6a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  7 12:04:14.491: INFO: Pod "test-rollover-deployment-57777854c9-v8xsb" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-v8xsb test-rollover-deployment-57777854c9- deployment-2404  27a6143a-c469-45eb-a3e4-6b72eafe39f9 50918 0 2023-05-07 12:04:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 fba06590-6d64-4dde-b5ef-e6f94816f918 0xc00425ee87 0xc00425ee88}] [] [{kube-controller-manager Update v1 2023-05-07 12:04:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fba06590-6d64-4dde-b5ef-e6f94816f918\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:04:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.231.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5h96q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5h96q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:04:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:04:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:04:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:04:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:172.20.231.216,StartTime:2023-05-07 12:04:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:04:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://c7ca7e5f75cbb533e52af6486d20d7a99479466addd5e35bf377d5a061781094,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.231.216,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:04:14.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2404" for this suite. @ 05/07/23 12:04:14.493
• [21.078 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 05/07/23 12:04:14.496
  May  7 12:04:14.496: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename gc @ 05/07/23 12:04:14.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:04:14.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:04:14.51
  STEP: create the deployment @ 05/07/23 12:04:14.513
  W0507 12:04:14.516700      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/07/23 12:04:14.516
  STEP: delete the deployment @ 05/07/23 12:04:14.519
  STEP: wait for all rs to be garbage collected @ 05/07/23 12:04:14.527
  STEP: expected 0 rs, got 1 rs @ 05/07/23 12:04:14.529
  STEP: expected 0 pods, got 1 pods @ 05/07/23 12:04:14.534
  STEP: Gathering metrics @ 05/07/23 12:04:15.046
  W0507 12:04:15.049460      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  7 12:04:15.049: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  7 12:04:15.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-808" for this suite. @ 05/07/23 12:04:15.051
• [0.558 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 05/07/23 12:04:15.054
  May  7 12:04:15.054: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/07/23 12:04:15.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:04:15.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:04:15.065
  May  7 12:04:15.067: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:04:21.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7386" for this suite. @ 05/07/23 12:04:21.191
• [6.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 05/07/23 12:04:21.196
  May  7 12:04:21.196: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-webhook @ 05/07/23 12:04:21.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:04:21.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:04:21.257
  STEP: Setting up server cert @ 05/07/23 12:04:21.258
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/07/23 12:04:21.563
  STEP: Deploying the custom resource conversion webhook pod @ 05/07/23 12:04:21.568
  STEP: Wait for the deployment to be ready @ 05/07/23 12:04:21.573
  May  7 12:04:21.577: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/07/23 12:04:23.583
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:04:23.588
  May  7 12:04:24.588: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May  7 12:04:24.589: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Creating a v1 custom resource @ 05/07/23 12:04:27.13
  STEP: Create a v2 custom resource @ 05/07/23 12:04:27.137
  STEP: List CRs in v1 @ 05/07/23 12:04:27.162
  STEP: List CRs in v2 @ 05/07/23 12:04:27.166
  May  7 12:04:27.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-5480" for this suite. @ 05/07/23 12:04:27.711
• [6.521 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 05/07/23 12:04:27.717
  May  7 12:04:27.717: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replication-controller @ 05/07/23 12:04:27.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:04:27.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:04:27.75
  STEP: Creating replication controller my-hostname-basic-52639d5c-7132-494b-b06c-811501fdb2bb @ 05/07/23 12:04:27.755
  May  7 12:04:27.770: INFO: Pod name my-hostname-basic-52639d5c-7132-494b-b06c-811501fdb2bb: Found 0 pods out of 1
  May  7 12:04:32.772: INFO: Pod name my-hostname-basic-52639d5c-7132-494b-b06c-811501fdb2bb: Found 1 pods out of 1
  May  7 12:04:32.772: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-52639d5c-7132-494b-b06c-811501fdb2bb" are running
  May  7 12:04:32.773: INFO: Pod "my-hostname-basic-52639d5c-7132-494b-b06c-811501fdb2bb-pzs86" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-07 12:04:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-07 12:04:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-07 12:04:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-07 12:04:27 +0000 UTC Reason: Message:}])
  May  7 12:04:32.773: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/07/23 12:04:32.773
  May  7 12:04:32.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3037" for this suite. @ 05/07/23 12:04:32.781
• [5.067 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 05/07/23 12:04:32.785
  May  7 12:04:32.785: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename ingress @ 05/07/23 12:04:32.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:04:32.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:04:32.798
  STEP: getting /apis @ 05/07/23 12:04:32.799
  STEP: getting /apis/networking.k8s.io @ 05/07/23 12:04:32.802
  STEP: getting /apis/networking.k8s.iov1 @ 05/07/23 12:04:32.803
  STEP: creating @ 05/07/23 12:04:32.804
  STEP: getting @ 05/07/23 12:04:32.813
  STEP: listing @ 05/07/23 12:04:32.814
  STEP: watching @ 05/07/23 12:04:32.816
  May  7 12:04:32.816: INFO: starting watch
  STEP: cluster-wide listing @ 05/07/23 12:04:32.817
  STEP: cluster-wide watching @ 05/07/23 12:04:32.818
  May  7 12:04:32.818: INFO: starting watch
  STEP: patching @ 05/07/23 12:04:32.819
  STEP: updating @ 05/07/23 12:04:32.823
  May  7 12:04:32.829: INFO: waiting for watch events with expected annotations
  May  7 12:04:32.829: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/07/23 12:04:32.829
  STEP: updating /status @ 05/07/23 12:04:32.832
  STEP: get /status @ 05/07/23 12:04:32.836
  STEP: deleting @ 05/07/23 12:04:32.837
  STEP: deleting a collection @ 05/07/23 12:04:32.844
  May  7 12:04:32.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-9900" for this suite. @ 05/07/23 12:04:32.85
• [0.068 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 05/07/23 12:04:32.853
  May  7 12:04:32.853: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:04:32.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:04:32.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:04:32.865
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 12:04:32.866
  STEP: Saw pod success @ 05/07/23 12:04:36.877
  May  7 12:04:36.878: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-238fdb11-da6a-4d0f-89e4-e57ce97fca35 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 12:04:36.88
  May  7 12:04:36.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2223" for this suite. @ 05/07/23 12:04:36.888
• [4.038 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 05/07/23 12:04:36.892
  May  7 12:04:36.892: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 12:04:36.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:04:36.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:04:36.907
  STEP: Creating configMap with name configmap-test-volume-cd318c15-0b88-4bbe-b9de-54899f758bc8 @ 05/07/23 12:04:36.909
  STEP: Creating a pod to test consume configMaps @ 05/07/23 12:04:36.911
  STEP: Saw pod success @ 05/07/23 12:04:40.922
  May  7 12:04:40.923: INFO: Trying to get logs from node 10.255.0.202 pod pod-configmaps-e0181919-f230-48bd-b2b4-63913afce162 container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 12:04:40.926
  May  7 12:04:40.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-740" for this suite. @ 05/07/23 12:04:40.935
• [4.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 05/07/23 12:04:40.938
  May  7 12:04:40.938: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename statefulset @ 05/07/23 12:04:40.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:04:40.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:04:40.949
  STEP: Creating service test in namespace statefulset-7978 @ 05/07/23 12:04:40.95
  STEP: Creating statefulset ss in namespace statefulset-7978 @ 05/07/23 12:04:40.962
  May  7 12:04:40.973: INFO: Found 0 stateful pods, waiting for 1
  May  7 12:04:50.978: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/07/23 12:04:50.98
  STEP: Getting /status @ 05/07/23 12:04:50.984
  May  7 12:04:50.985: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/07/23 12:04:50.985
  May  7 12:04:50.990: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/07/23 12:04:50.99
  May  7 12:04:50.990: INFO: Observed &StatefulSet event: ADDED
  May  7 12:04:50.990: INFO: Found Statefulset ss in namespace statefulset-7978 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  7 12:04:50.990: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/07/23 12:04:50.99
  May  7 12:04:50.991: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May  7 12:04:50.994: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/07/23 12:04:50.994
  May  7 12:04:50.995: INFO: Observed &StatefulSet event: ADDED
  May  7 12:04:50.995: INFO: Deleting all statefulset in ns statefulset-7978
  May  7 12:04:50.996: INFO: Scaling statefulset ss to 0
  May  7 12:05:01.015: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 12:05:01.016: INFO: Deleting statefulset ss
  May  7 12:05:01.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7978" for this suite. @ 05/07/23 12:05:01.025
• [20.090 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 05/07/23 12:05:01.028
  May  7 12:05:01.028: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 12:05:01.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:01.038
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:01.041
  STEP: Creating resourceQuota "e2e-rq-status-9kwnk" @ 05/07/23 12:05:01.043
  May  7 12:05:01.046: INFO: Resource quota "e2e-rq-status-9kwnk" reports spec: hard cpu limit of 500m
  May  7 12:05:01.046: INFO: Resource quota "e2e-rq-status-9kwnk" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-9kwnk" /status @ 05/07/23 12:05:01.046
  STEP: Confirm /status for "e2e-rq-status-9kwnk" resourceQuota via watch @ 05/07/23 12:05:01.05
  May  7 12:05:01.051: INFO: observed resourceQuota "e2e-rq-status-9kwnk" in namespace "resourcequota-8406" with hard status: v1.ResourceList(nil)
  May  7 12:05:01.051: INFO: Found resourceQuota "e2e-rq-status-9kwnk" in namespace "resourcequota-8406" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May  7 12:05:01.051: INFO: ResourceQuota "e2e-rq-status-9kwnk" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/07/23 12:05:01.052
  May  7 12:05:01.054: INFO: Resource quota "e2e-rq-status-9kwnk" reports spec: hard cpu limit of 1
  May  7 12:05:01.054: INFO: Resource quota "e2e-rq-status-9kwnk" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-9kwnk" /status @ 05/07/23 12:05:01.054
  STEP: Confirm /status for "e2e-rq-status-9kwnk" resourceQuota via watch @ 05/07/23 12:05:01.057
  May  7 12:05:01.057: INFO: observed resourceQuota "e2e-rq-status-9kwnk" in namespace "resourcequota-8406" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May  7 12:05:01.057: INFO: Found resourceQuota "e2e-rq-status-9kwnk" in namespace "resourcequota-8406" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  May  7 12:05:01.058: INFO: ResourceQuota "e2e-rq-status-9kwnk" /status was patched
  STEP: Get "e2e-rq-status-9kwnk" /status @ 05/07/23 12:05:01.058
  May  7 12:05:01.059: INFO: Resourcequota "e2e-rq-status-9kwnk" reports status: hard cpu of 1
  May  7 12:05:01.059: INFO: Resourcequota "e2e-rq-status-9kwnk" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-9kwnk" /status before checking Spec is unchanged @ 05/07/23 12:05:01.06
  May  7 12:05:01.063: INFO: Resourcequota "e2e-rq-status-9kwnk" reports status: hard cpu of 2
  May  7 12:05:01.063: INFO: Resourcequota "e2e-rq-status-9kwnk" reports status: hard memory of 2Gi
  May  7 12:05:01.063: INFO: Found resourceQuota "e2e-rq-status-9kwnk" in namespace "resourcequota-8406" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  May  7 12:05:06.068: INFO: ResourceQuota "e2e-rq-status-9kwnk" Spec was unchanged and /status reset
  May  7 12:05:06.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8406" for this suite. @ 05/07/23 12:05:06.07
• [5.046 seconds]
------------------------------
SSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 05/07/23 12:05:06.075
  May  7 12:05:06.075: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename limitrange @ 05/07/23 12:05:06.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:06.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:06.089
  STEP: Creating LimitRange "e2e-limitrange-q7kp9" in namespace "limitrange-4556" @ 05/07/23 12:05:06.091
  STEP: Creating another limitRange in another namespace @ 05/07/23 12:05:06.094
  May  7 12:05:06.106: INFO: Namespace "e2e-limitrange-q7kp9-1851" created
  May  7 12:05:06.106: INFO: Creating LimitRange "e2e-limitrange-q7kp9" in namespace "e2e-limitrange-q7kp9-1851"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-q7kp9" @ 05/07/23 12:05:06.11
  May  7 12:05:06.112: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-q7kp9" in "limitrange-4556" namespace @ 05/07/23 12:05:06.112
  May  7 12:05:06.115: INFO: LimitRange "e2e-limitrange-q7kp9" has been patched
  STEP: Delete LimitRange "e2e-limitrange-q7kp9" by Collection with labelSelector: "e2e-limitrange-q7kp9=patched" @ 05/07/23 12:05:06.115
  STEP: Confirm that the limitRange "e2e-limitrange-q7kp9" has been deleted @ 05/07/23 12:05:06.118
  May  7 12:05:06.118: INFO: Requesting list of LimitRange to confirm quantity
  May  7 12:05:06.120: INFO: Found 0 LimitRange with label "e2e-limitrange-q7kp9=patched"
  May  7 12:05:06.120: INFO: LimitRange "e2e-limitrange-q7kp9" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-q7kp9" @ 05/07/23 12:05:06.12
  May  7 12:05:06.122: INFO: Found 1 limitRange
  May  7 12:05:06.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-4556" for this suite. @ 05/07/23 12:05:06.125
  STEP: Destroying namespace "e2e-limitrange-q7kp9-1851" for this suite. @ 05/07/23 12:05:06.129
• [0.059 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 05/07/23 12:05:06.133
  May  7 12:05:06.133: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replication-controller @ 05/07/23 12:05:06.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:06.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:06.193
  May  7 12:05:06.195: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/07/23 12:05:07.2
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/07/23 12:05:07.203
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/07/23 12:05:08.206
  May  7 12:05:08.210: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/07/23 12:05:08.21
  May  7 12:05:09.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-954" for this suite. @ 05/07/23 12:05:09.215
• [3.085 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 05/07/23 12:05:09.219
  May  7 12:05:09.219: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/07/23 12:05:09.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:09.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:09.239
  STEP: create the container to handle the HTTPGet hook request. @ 05/07/23 12:05:09.242
  STEP: create the pod with lifecycle hook @ 05/07/23 12:05:11.254
  STEP: delete the pod with lifecycle hook @ 05/07/23 12:05:13.267
  STEP: check prestop hook @ 05/07/23 12:05:15.274
  May  7 12:05:15.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8859" for this suite. @ 05/07/23 12:05:15.286
• [6.070 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 05/07/23 12:05:15.289
  May  7 12:05:15.289: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename dns @ 05/07/23 12:05:15.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:15.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:15.299
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8372.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8372.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/07/23 12:05:15.301
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8372.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8372.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/07/23 12:05:15.301
  STEP: creating a pod to probe /etc/hosts @ 05/07/23 12:05:15.301
  STEP: submitting the pod to kubernetes @ 05/07/23 12:05:15.301
  STEP: retrieving the pod @ 05/07/23 12:05:17.31
  STEP: looking for the results for each expected name from probers @ 05/07/23 12:05:17.312
  May  7 12:05:17.349: INFO: DNS probes using dns-8372/dns-test-10b4937b-3121-46c2-9fc9-4fd6722cef4f succeeded

  May  7 12:05:17.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:05:17.351
  STEP: Destroying namespace "dns-8372" for this suite. @ 05/07/23 12:05:17.361
• [2.083 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 05/07/23 12:05:17.372
  May  7 12:05:17.372: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 12:05:17.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:17.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:17.437
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 12:05:17.438
  STEP: Saw pod success @ 05/07/23 12:05:19.446
  May  7 12:05:19.447: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-a3c13afe-6ec5-43e6-a9ae-3e9c9476c062 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 12:05:19.45
  May  7 12:05:19.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2701" for this suite. @ 05/07/23 12:05:19.458
• [2.088 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 05/07/23 12:05:19.46
  May  7 12:05:19.460: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename var-expansion @ 05/07/23 12:05:19.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:19.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:19.471
  STEP: Creating a pod to test substitution in container's args @ 05/07/23 12:05:19.473
  STEP: Saw pod success @ 05/07/23 12:05:21.481
  May  7 12:05:21.483: INFO: Trying to get logs from node 10.255.0.202 pod var-expansion-8b95c0f6-cb47-4728-bb62-b8a7e7a7512f container dapi-container: <nil>
  STEP: delete the pod @ 05/07/23 12:05:21.492
  May  7 12:05:21.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3138" for this suite. @ 05/07/23 12:05:21.501
• [2.044 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/07/23 12:05:21.504
  May  7 12:05:21.504: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename certificates @ 05/07/23 12:05:21.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:21.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:21.516
  STEP: getting /apis @ 05/07/23 12:05:21.975
  STEP: getting /apis/certificates.k8s.io @ 05/07/23 12:05:21.978
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/07/23 12:05:21.984
  STEP: creating @ 05/07/23 12:05:21.984
  STEP: getting @ 05/07/23 12:05:21.996
  STEP: listing @ 05/07/23 12:05:21.997
  STEP: watching @ 05/07/23 12:05:21.998
  May  7 12:05:21.998: INFO: starting watch
  STEP: patching @ 05/07/23 12:05:21.999
  STEP: updating @ 05/07/23 12:05:22.001
  May  7 12:05:22.007: INFO: waiting for watch events with expected annotations
  May  7 12:05:22.007: INFO: saw patched and updated annotations
  STEP: getting /approval @ 05/07/23 12:05:22.007
  STEP: patching /approval @ 05/07/23 12:05:22.01
  STEP: updating /approval @ 05/07/23 12:05:22.012
  STEP: getting /status @ 05/07/23 12:05:22.016
  STEP: patching /status @ 05/07/23 12:05:22.017
  STEP: updating /status @ 05/07/23 12:05:22.021
  STEP: deleting @ 05/07/23 12:05:22.024
  STEP: deleting a collection @ 05/07/23 12:05:22.031
  May  7 12:05:22.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-1907" for this suite. @ 05/07/23 12:05:22.041
• [0.540 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 05/07/23 12:05:22.045
  May  7 12:05:22.045: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:05:22.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:22.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:22.058
  STEP: Creating configMap with name projected-configmap-test-volume-80ec1ed3-fc33-4a86-8fcb-a1bd61e3f092 @ 05/07/23 12:05:22.063
  STEP: Creating a pod to test consume configMaps @ 05/07/23 12:05:22.065
  STEP: Saw pod success @ 05/07/23 12:05:26.085
  May  7 12:05:26.086: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-configmaps-7fbba37e-6202-49f1-8a29-1319c94db664 container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 12:05:26.089
  May  7 12:05:26.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2251" for this suite. @ 05/07/23 12:05:26.098
• [4.057 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 05/07/23 12:05:26.102
  May  7 12:05:26.102: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 12:05:26.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:26.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:26.114
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 12:05:26.115
  STEP: Saw pod success @ 05/07/23 12:05:30.127
  May  7 12:05:30.129: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-e89dfc16-9e02-4a73-b87e-5b20cab7aa15 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 12:05:30.131
  May  7 12:05:30.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9008" for this suite. @ 05/07/23 12:05:30.14
• [4.041 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 05/07/23 12:05:30.143
  May  7 12:05:30.143: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/07/23 12:05:30.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:30.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:30.155
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/07/23 12:05:30.156
  May  7 12:05:30.157: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/07/23 12:05:35.145
  May  7 12:05:35.145: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:05:36.328: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:05:41.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3966" for this suite. @ 05/07/23 12:05:41.199
• [11.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/07/23 12:05:41.202
  May  7 12:05:41.202: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubelet-test @ 05/07/23 12:05:41.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:41.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:41.214
  May  7 12:05:43.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1699" for this suite. @ 05/07/23 12:05:43.23
• [2.030 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 05/07/23 12:05:43.233
  May  7 12:05:43.233: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/07/23 12:05:43.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:43.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:43.242
  May  7 12:05:43.244: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/07/23 12:05:44.416
  May  7 12:05:44.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-2493 --namespace=crd-publish-openapi-2493 create -f -'
  May  7 12:05:46.902: INFO: stderr: ""
  May  7 12:05:46.902: INFO: stdout: "e2e-test-crd-publish-openapi-3711-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May  7 12:05:46.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-2493 --namespace=crd-publish-openapi-2493 delete e2e-test-crd-publish-openapi-3711-crds test-cr'
  May  7 12:05:46.947: INFO: stderr: ""
  May  7 12:05:46.947: INFO: stdout: "e2e-test-crd-publish-openapi-3711-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  May  7 12:05:46.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-2493 --namespace=crd-publish-openapi-2493 apply -f -'
  May  7 12:05:47.083: INFO: stderr: ""
  May  7 12:05:47.083: INFO: stdout: "e2e-test-crd-publish-openapi-3711-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May  7 12:05:47.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-2493 --namespace=crd-publish-openapi-2493 delete e2e-test-crd-publish-openapi-3711-crds test-cr'
  May  7 12:05:47.129: INFO: stderr: ""
  May  7 12:05:47.129: INFO: stdout: "e2e-test-crd-publish-openapi-3711-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/07/23 12:05:47.129
  May  7 12:05:47.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=crd-publish-openapi-2493 explain e2e-test-crd-publish-openapi-3711-crds'
  May  7 12:05:47.266: INFO: stderr: ""
  May  7 12:05:47.266: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-3711-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  May  7 12:05:48.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2493" for this suite. @ 05/07/23 12:05:48.476
• [5.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 05/07/23 12:05:48.48
  May  7 12:05:48.480: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:05:48.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:48.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:48.489
  STEP: Setting up server cert @ 05/07/23 12:05:48.552
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:05:48.744
  STEP: Deploying the webhook pod @ 05/07/23 12:05:48.748
  STEP: Wait for the deployment to be ready @ 05/07/23 12:05:48.754
  May  7 12:05:48.758: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/07/23 12:05:50.764
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:05:50.769
  May  7 12:05:51.769: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  7 12:05:51.771: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1199-crds.webhook.example.com via the AdmissionRegistration API @ 05/07/23 12:05:52.286
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/07/23 12:05:52.295
  May  7 12:05:54.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6753" for this suite. @ 05/07/23 12:05:54.877
  STEP: Destroying namespace "webhook-markers-5796" for this suite. @ 05/07/23 12:05:54.88
• [6.402 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/07/23 12:05:54.883
  May  7 12:05:54.883: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sched-pred @ 05/07/23 12:05:54.883
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:05:54.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:05:54.895
  May  7 12:05:54.896: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  7 12:05:54.900: INFO: Waiting for terminating namespaces to be deleted...
  May  7 12:05:54.901: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.201 before test
  May  7 12:05:54.904: INFO: calico-kube-controllers-7ccf856ff8-2v596 from kube-system started at 2023-05-07 12:02:36 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.904: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May  7 12:05:54.904: INFO: calico-node-w75tc from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.904: INFO: 	Container calico-node ready: true, restart count 0
  May  7 12:05:54.904: INFO: coredns-6557d7db9c-cgp7l from kube-system started at 2023-05-07 09:07:01 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.904: INFO: 	Container coredns ready: true, restart count 0
  May  7 12:05:54.904: INFO: dashboard-metrics-scraper-5c876f54bd-z7s9t from kube-system started at 2023-05-07 09:07:07 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.904: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May  7 12:05:54.904: INFO: metrics-server-57fbbb5957-fw6s6 from kube-system started at 2023-05-07 09:10:30 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.904: INFO: 	Container metrics-server ready: true, restart count 0
  May  7 12:05:54.904: INFO: node-local-dns-hwr8c from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.904: INFO: 	Container node-cache ready: true, restart count 0
  May  7 12:05:54.904: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-w58n4 from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 12:05:54.904: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 12:05:54.904: INFO: 	Container systemd-logs ready: true, restart count 0
  May  7 12:05:54.904: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.202 before test
  May  7 12:05:54.906: INFO: calico-node-j77rx from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.906: INFO: 	Container calico-node ready: true, restart count 0
  May  7 12:05:54.906: INFO: node-local-dns-4lrj5 from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.906: INFO: 	Container node-cache ready: true, restart count 0
  May  7 12:05:54.906: INFO: busybox-readonly-fs4f894189-f243-4a2e-b1ce-026c6eaea057 from kubelet-test-1699 started at 2023-05-07 12:05:41 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.906: INFO: 	Container busybox-readonly-fs4f894189-f243-4a2e-b1ce-026c6eaea057 ready: true, restart count 0
  May  7 12:05:54.906: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-7v67t from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 12:05:54.906: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 12:05:54.906: INFO: 	Container systemd-logs ready: true, restart count 0
  May  7 12:05:54.906: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.203 before test
  May  7 12:05:54.909: INFO: calico-node-2m8cq from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.909: INFO: 	Container calico-node ready: true, restart count 0
  May  7 12:05:54.909: INFO: kubernetes-dashboard-89b5448d6-2d4rk from kube-system started at 2023-05-07 12:02:36 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.909: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May  7 12:05:54.909: INFO: node-local-dns-9mnsl from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.909: INFO: 	Container node-cache ready: true, restart count 0
  May  7 12:05:54.909: INFO: sonobuoy from sonobuoy started at 2023-05-07 11:54:26 +0000 UTC (1 container statuses recorded)
  May  7 12:05:54.909: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  7 12:05:54.909: INFO: sonobuoy-e2e-job-8208a86a1e4a4756 from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 12:05:54.909: INFO: 	Container e2e ready: true, restart count 0
  May  7 12:05:54.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 12:05:54.909: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-mhtmw from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 12:05:54.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 12:05:54.909: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/07/23 12:05:54.909
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/07/23 12:05:58.921
  STEP: Trying to apply a random label on the found node. @ 05/07/23 12:05:58.932
  STEP: verifying the node has the label kubernetes.io/e2e-b59fe612-a86d-4c34-a476-958be15ac0c8 95 @ 05/07/23 12:05:58.936
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/07/23 12:05:58.939
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.255.0.202 on the node which pod4 resides and expect not scheduled @ 05/07/23 12:06:00.951
  STEP: removing the label kubernetes.io/e2e-b59fe612-a86d-4c34-a476-958be15ac0c8 off the node 10.255.0.202 @ 05/07/23 12:11:00.956
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-b59fe612-a86d-4c34-a476-958be15ac0c8 @ 05/07/23 12:11:00.962
  May  7 12:11:00.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7998" for this suite. @ 05/07/23 12:11:00.968
• [306.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 05/07/23 12:11:00.973
  May  7 12:11:00.973: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sched-preemption @ 05/07/23 12:11:00.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:11:00.982
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:11:00.985
  May  7 12:11:00.992: INFO: Waiting up to 1m0s for all nodes to be ready
  May  7 12:12:01.002: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/07/23 12:12:01.003
  May  7 12:12:01.013: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May  7 12:12:01.020: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May  7 12:12:01.045: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May  7 12:12:01.055: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  May  7 12:12:01.069: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  May  7 12:12:01.077: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/07/23 12:12:01.077
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/07/23 12:12:03.09
  May  7 12:12:07.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-3941" for this suite. @ 05/07/23 12:12:07.147
• [66.176 seconds]
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/07/23 12:12:07.149
  May  7 12:12:07.149: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename events @ 05/07/23 12:12:07.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:12:07.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:12:07.161
  STEP: Create set of events @ 05/07/23 12:12:07.162
  STEP: get a list of Events with a label in the current namespace @ 05/07/23 12:12:07.168
  STEP: delete a list of events @ 05/07/23 12:12:07.169
  May  7 12:12:07.169: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/07/23 12:12:07.175
  May  7 12:12:07.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6971" for this suite. @ 05/07/23 12:12:07.178
• [0.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 05/07/23 12:12:07.181
  May  7 12:12:07.181: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 12:12:07.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:12:07.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:12:07.192
  STEP: Creating configMap that has name configmap-test-emptyKey-27a0c9bb-bcb8-4fcb-a6a6-83e403be8881 @ 05/07/23 12:12:07.193
  May  7 12:12:07.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-454" for this suite. @ 05/07/23 12:12:07.195
• [0.017 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 05/07/23 12:12:07.198
  May  7 12:12:07.198: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:12:07.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:12:07.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:12:07.21
  STEP: Creating configMap with name projected-configmap-test-volume-e43030c1-e855-4e86-8a59-dc8260245b89 @ 05/07/23 12:12:07.211
  STEP: Creating a pod to test consume configMaps @ 05/07/23 12:12:07.214
  STEP: Saw pod success @ 05/07/23 12:12:11.224
  May  7 12:12:11.225: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-configmaps-7ab10af9-45b3-4a4d-ada7-eb8399d8b1ed container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 12:12:11.235
  May  7 12:12:11.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2545" for this suite. @ 05/07/23 12:12:11.244
• [4.049 seconds]
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/07/23 12:12:11.246
  May  7 12:12:11.246: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename hostport @ 05/07/23 12:12:11.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:12:11.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:12:11.261
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/07/23 12:12:11.265
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.255.0.201 on the node which pod1 resides and expect scheduled @ 05/07/23 12:12:13.277
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.255.0.201 but use UDP protocol on the node which pod2 resides @ 05/07/23 12:12:15.289
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/07/23 12:12:19.304
  May  7 12:12:19.304: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.255.0.201 http://127.0.0.1:54323/hostname] Namespace:hostport-4712 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:12:19.304: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:12:19.305: INFO: ExecWithOptions: Clientset creation
  May  7 12:12:19.305: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/hostport-4712/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.255.0.201+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.255.0.201, port: 54323 @ 05/07/23 12:12:19.377
  May  7 12:12:19.377: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.255.0.201:54323/hostname] Namespace:hostport-4712 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:12:19.377: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:12:19.377: INFO: ExecWithOptions: Clientset creation
  May  7 12:12:19.377: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/hostport-4712/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.255.0.201%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.255.0.201, port: 54323 UDP @ 05/07/23 12:12:19.432
  May  7 12:12:19.432: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.255.0.201 54323] Namespace:hostport-4712 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:12:19.432: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:12:19.432: INFO: ExecWithOptions: Clientset creation
  May  7 12:12:19.433: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/hostport-4712/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.255.0.201+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  May  7 12:12:24.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-4712" for this suite. @ 05/07/23 12:12:24.482
• [13.238 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 05/07/23 12:12:24.485
  May  7 12:12:24.485: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 12:12:24.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:12:24.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:12:24.497
  STEP: Creating a ResourceQuota @ 05/07/23 12:12:24.499
  STEP: Getting a ResourceQuota @ 05/07/23 12:12:24.501
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/07/23 12:12:24.504
  STEP: Patching the ResourceQuota @ 05/07/23 12:12:24.505
  STEP: Deleting a Collection of ResourceQuotas @ 05/07/23 12:12:24.508
  STEP: Verifying the deleted ResourceQuota @ 05/07/23 12:12:24.512
  May  7 12:12:24.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2038" for this suite. @ 05/07/23 12:12:24.515
• [0.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 05/07/23 12:12:24.519
  May  7 12:12:24.519: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-probe @ 05/07/23 12:12:24.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:12:24.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:12:24.531
  STEP: Creating pod liveness-d32ffe7e-f75f-40ae-bb03-91d54dadf30a in namespace container-probe-3570 @ 05/07/23 12:12:24.532
  May  7 12:12:26.542: INFO: Started pod liveness-d32ffe7e-f75f-40ae-bb03-91d54dadf30a in namespace container-probe-3570
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/07/23 12:12:26.542
  May  7 12:12:26.544: INFO: Initial restart count of pod liveness-d32ffe7e-f75f-40ae-bb03-91d54dadf30a is 0
  May  7 12:12:46.568: INFO: Restart count of pod container-probe-3570/liveness-d32ffe7e-f75f-40ae-bb03-91d54dadf30a is now 1 (20.024054834s elapsed)
  May  7 12:13:06.597: INFO: Restart count of pod container-probe-3570/liveness-d32ffe7e-f75f-40ae-bb03-91d54dadf30a is now 2 (40.053720585s elapsed)
  May  7 12:13:26.621: INFO: Restart count of pod container-probe-3570/liveness-d32ffe7e-f75f-40ae-bb03-91d54dadf30a is now 3 (1m0.077227453s elapsed)
  May  7 12:13:46.642: INFO: Restart count of pod container-probe-3570/liveness-d32ffe7e-f75f-40ae-bb03-91d54dadf30a is now 4 (1m20.098532323s elapsed)
  May  7 12:14:48.717: INFO: Restart count of pod container-probe-3570/liveness-d32ffe7e-f75f-40ae-bb03-91d54dadf30a is now 5 (2m22.173283712s elapsed)
  May  7 12:14:48.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:14:48.719
  STEP: Destroying namespace "container-probe-3570" for this suite. @ 05/07/23 12:14:48.726
• [144.209 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/07/23 12:14:48.729
  May  7 12:14:48.729: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubelet-test @ 05/07/23 12:14:48.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:14:48.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:14:48.741
  May  7 12:14:50.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4968" for this suite. @ 05/07/23 12:14:50.767
• [2.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 05/07/23 12:14:50.77
  May  7 12:14:50.770: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:14:50.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:14:50.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:14:50.782
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 12:14:50.784
  STEP: Saw pod success @ 05/07/23 12:14:54.795
  May  7 12:14:54.798: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-00b794ca-8914-4c34-b0c6-360322d1ea7b container client-container: <nil>
  STEP: delete the pod @ 05/07/23 12:14:54.801
  May  7 12:14:54.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5004" for this suite. @ 05/07/23 12:14:54.811
• [4.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 05/07/23 12:14:54.814
  May  7 12:14:54.814: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sysctl @ 05/07/23 12:14:54.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:14:54.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:14:54.825
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/07/23 12:14:54.827
  May  7 12:14:54.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-856" for this suite. @ 05/07/23 12:14:54.83
• [0.018 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 05/07/23 12:14:54.832
  May  7 12:14:54.832: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pod-network-test @ 05/07/23 12:14:54.833
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:14:54.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:14:54.893
  STEP: Performing setup for networking test in namespace pod-network-test-684 @ 05/07/23 12:14:54.894
  STEP: creating a selector @ 05/07/23 12:14:54.894
  STEP: Creating the service pods in kubernetes @ 05/07/23 12:14:54.894
  May  7 12:14:54.894: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/07/23 12:15:06.94
  May  7 12:15:08.948: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May  7 12:15:08.948: INFO: Breadth first check of 172.20.3.109 on host 10.255.0.201...
  May  7 12:15:08.949: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.231.250:9080/dial?request=hostname&protocol=http&host=172.20.3.109&port=8083&tries=1'] Namespace:pod-network-test-684 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:15:08.949: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:15:08.950: INFO: ExecWithOptions: Clientset creation
  May  7 12:15:08.950: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-684/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.231.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.3.109%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  7 12:15:09.011: INFO: Waiting for responses: map[]
  May  7 12:15:09.011: INFO: reached 172.20.3.109 after 0/1 tries
  May  7 12:15:09.011: INFO: Breadth first check of 172.20.231.202 on host 10.255.0.202...
  May  7 12:15:09.013: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.231.250:9080/dial?request=hostname&protocol=http&host=172.20.231.202&port=8083&tries=1'] Namespace:pod-network-test-684 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:15:09.013: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:15:09.013: INFO: ExecWithOptions: Clientset creation
  May  7 12:15:09.013: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-684/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.231.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.231.202%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  7 12:15:09.070: INFO: Waiting for responses: map[]
  May  7 12:15:09.070: INFO: reached 172.20.231.202 after 0/1 tries
  May  7 12:15:09.070: INFO: Breadth first check of 172.20.191.33 on host 10.255.0.203...
  May  7 12:15:09.071: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.231.250:9080/dial?request=hostname&protocol=http&host=172.20.191.33&port=8083&tries=1'] Namespace:pod-network-test-684 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:15:09.071: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:15:09.072: INFO: ExecWithOptions: Clientset creation
  May  7 12:15:09.072: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-684/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.231.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.191.33%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  7 12:15:09.120: INFO: Waiting for responses: map[]
  May  7 12:15:09.120: INFO: reached 172.20.191.33 after 0/1 tries
  May  7 12:15:09.120: INFO: Going to retry 0 out of 3 pods....
  May  7 12:15:09.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-684" for this suite. @ 05/07/23 12:15:09.122
• [14.292 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/07/23 12:15:09.125
  May  7 12:15:09.125: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 12:15:09.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:15:09.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:15:09.136
  STEP: Creating secret with name secret-test-8ac1e704-f5c9-4341-8934-f2ba7b68a0d0 @ 05/07/23 12:15:09.138
  STEP: Creating a pod to test consume secrets @ 05/07/23 12:15:09.14
  STEP: Saw pod success @ 05/07/23 12:15:13.15
  May  7 12:15:13.151: INFO: Trying to get logs from node 10.255.0.202 pod pod-secrets-5d8ee64d-9993-47e0-8919-96b1faeb7fae container secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 12:15:13.154
  May  7 12:15:13.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1284" for this suite. @ 05/07/23 12:15:13.162
• [4.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 05/07/23 12:15:13.165
  May  7 12:15:13.165: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename deployment @ 05/07/23 12:15:13.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:15:13.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:15:13.176
  STEP: creating a Deployment @ 05/07/23 12:15:13.18
  May  7 12:15:13.180: INFO: Creating simple deployment test-deployment-kgrmr
  May  7 12:15:13.184: INFO: new replicaset for deployment "test-deployment-kgrmr" is yet to be created
  STEP: Getting /status @ 05/07/23 12:15:15.192
  May  7 12:15:15.193: INFO: Deployment test-deployment-kgrmr has Conditions: [{Available True 2023-05-07 12:15:14 +0000 UTC 2023-05-07 12:15:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-07 12:15:14 +0000 UTC 2023-05-07 12:15:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kgrmr-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/07/23 12:15:15.194
  May  7 12:15:15.199: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 15, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 15, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 15, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 15, 13, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-kgrmr-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/07/23 12:15:15.199
  May  7 12:15:15.200: INFO: Observed &Deployment event: ADDED
  May  7 12:15:15.200: INFO: Observed Deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-07 12:15:13 +0000 UTC 2023-05-07 12:15:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kgrmr-5994cf9475"}
  May  7 12:15:15.200: INFO: Observed &Deployment event: MODIFIED
  May  7 12:15:15.200: INFO: Observed Deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-07 12:15:13 +0000 UTC 2023-05-07 12:15:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kgrmr-5994cf9475"}
  May  7 12:15:15.200: INFO: Observed Deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-07 12:15:13 +0000 UTC 2023-05-07 12:15:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  7 12:15:15.200: INFO: Observed &Deployment event: MODIFIED
  May  7 12:15:15.200: INFO: Observed Deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-07 12:15:13 +0000 UTC 2023-05-07 12:15:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  7 12:15:15.200: INFO: Observed Deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-07 12:15:13 +0000 UTC 2023-05-07 12:15:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-kgrmr-5994cf9475" is progressing.}
  May  7 12:15:15.200: INFO: Observed &Deployment event: MODIFIED
  May  7 12:15:15.200: INFO: Observed Deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-07 12:15:14 +0000 UTC 2023-05-07 12:15:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  7 12:15:15.200: INFO: Observed Deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-07 12:15:14 +0000 UTC 2023-05-07 12:15:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kgrmr-5994cf9475" has successfully progressed.}
  May  7 12:15:15.200: INFO: Observed &Deployment event: MODIFIED
  May  7 12:15:15.200: INFO: Observed Deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-07 12:15:14 +0000 UTC 2023-05-07 12:15:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  7 12:15:15.200: INFO: Observed Deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-07 12:15:14 +0000 UTC 2023-05-07 12:15:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kgrmr-5994cf9475" has successfully progressed.}
  May  7 12:15:15.200: INFO: Found Deployment test-deployment-kgrmr in namespace deployment-2852 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  7 12:15:15.200: INFO: Deployment test-deployment-kgrmr has an updated status
  STEP: patching the Statefulset Status @ 05/07/23 12:15:15.2
  May  7 12:15:15.201: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May  7 12:15:15.209: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/07/23 12:15:15.209
  May  7 12:15:15.210: INFO: Observed &Deployment event: ADDED
  May  7 12:15:15.210: INFO: Observed deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-07 12:15:13 +0000 UTC 2023-05-07 12:15:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kgrmr-5994cf9475"}
  May  7 12:15:15.210: INFO: Observed &Deployment event: MODIFIED
  May  7 12:15:15.211: INFO: Observed deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-07 12:15:13 +0000 UTC 2023-05-07 12:15:13 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-kgrmr-5994cf9475"}
  May  7 12:15:15.211: INFO: Observed deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-07 12:15:13 +0000 UTC 2023-05-07 12:15:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  7 12:15:15.211: INFO: Observed &Deployment event: MODIFIED
  May  7 12:15:15.211: INFO: Observed deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-07 12:15:13 +0000 UTC 2023-05-07 12:15:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May  7 12:15:15.211: INFO: Observed deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-07 12:15:13 +0000 UTC 2023-05-07 12:15:13 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-kgrmr-5994cf9475" is progressing.}
  May  7 12:15:15.211: INFO: Observed &Deployment event: MODIFIED
  May  7 12:15:15.211: INFO: Observed deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-07 12:15:14 +0000 UTC 2023-05-07 12:15:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  7 12:15:15.211: INFO: Observed deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-07 12:15:14 +0000 UTC 2023-05-07 12:15:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kgrmr-5994cf9475" has successfully progressed.}
  May  7 12:15:15.212: INFO: Observed &Deployment event: MODIFIED
  May  7 12:15:15.212: INFO: Observed deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-07 12:15:14 +0000 UTC 2023-05-07 12:15:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May  7 12:15:15.212: INFO: Observed deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-07 12:15:14 +0000 UTC 2023-05-07 12:15:13 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-kgrmr-5994cf9475" has successfully progressed.}
  May  7 12:15:15.212: INFO: Observed deployment test-deployment-kgrmr in namespace deployment-2852 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  7 12:15:15.212: INFO: Observed &Deployment event: MODIFIED
  May  7 12:15:15.212: INFO: Found deployment test-deployment-kgrmr in namespace deployment-2852 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  May  7 12:15:15.212: INFO: Deployment test-deployment-kgrmr has a patched status
  May  7 12:15:15.215: INFO: Deployment "test-deployment-kgrmr":
  &Deployment{ObjectMeta:{test-deployment-kgrmr  deployment-2852  6073880e-ef8c-4cf6-b59e-b4d33ea9a315 53705 1 2023-05-07 12:15:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-07 12:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-07 12:15:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-07 12:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037919b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-07 12:15:15 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-kgrmr-5994cf9475" has successfully progressed.,LastUpdateTime:2023-05-07 12:15:15 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  7 12:15:15.219: INFO: New ReplicaSet "test-deployment-kgrmr-5994cf9475" of Deployment "test-deployment-kgrmr":
  &ReplicaSet{ObjectMeta:{test-deployment-kgrmr-5994cf9475  deployment-2852  a523ecb5-fc35-4825-b872-2134d421c786 53652 1 2023-05-07 12:15:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-kgrmr 6073880e-ef8c-4cf6-b59e-b4d33ea9a315 0xc003791df0 0xc003791df1}] [] [{kube-controller-manager Update apps/v1 2023-05-07 12:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6073880e-ef8c-4cf6-b59e-b4d33ea9a315\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:15:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003791e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  7 12:15:15.222: INFO: Pod "test-deployment-kgrmr-5994cf9475-hr6wb" is available:
  &Pod{ObjectMeta:{test-deployment-kgrmr-5994cf9475-hr6wb test-deployment-kgrmr-5994cf9475- deployment-2852  bb8e2dc6-9a80-4300-846c-76a46cd6115e 53651 0 2023-05-07 12:15:13 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-kgrmr-5994cf9475 a523ecb5-fc35-4825-b872-2134d421c786 0xc0038c4250 0xc0038c4251}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a523ecb5-fc35-4825-b872-2134d421c786\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.231.243\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kp7h6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kp7h6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:172.20.231.243,StartTime:2023-05-07 12:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:15:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cdb90404ec259eb3d3222574f05f0cd367cc1d8a5a8cee475a5ec0ffa2e11f7a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.231.243,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:15.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2852" for this suite. @ 05/07/23 12:15:15.224
• [2.064 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/07/23 12:15:15.229
  May  7 12:15:15.229: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename deployment @ 05/07/23 12:15:15.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:15:15.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:15:15.244
  May  7 12:15:15.245: INFO: Creating deployment "webserver-deployment"
  May  7 12:15:15.247: INFO: Waiting for observed generation 1
  May  7 12:15:17.253: INFO: Waiting for all required pods to come up
  May  7 12:15:17.255: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/07/23 12:15:17.255
  May  7 12:15:19.259: INFO: Waiting for deployment "webserver-deployment" to complete
  May  7 12:15:19.264: INFO: Updating deployment "webserver-deployment" with a non-existent image
  May  7 12:15:19.267: INFO: Updating deployment webserver-deployment
  May  7 12:15:19.267: INFO: Waiting for observed generation 2
  May  7 12:15:21.271: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  May  7 12:15:21.272: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  May  7 12:15:21.274: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May  7 12:15:21.277: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  May  7 12:15:21.277: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  May  7 12:15:21.278: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May  7 12:15:21.280: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  May  7 12:15:21.280: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  May  7 12:15:21.284: INFO: Updating deployment webserver-deployment
  May  7 12:15:21.284: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  May  7 12:15:21.288: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  May  7 12:15:21.293: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  May  7 12:15:21.311: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-3646  f6b7320e-df26-4927-b657-4a3d4edb6a6d 53995 3 2023-05-07 12:15:15 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003934118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-05-07 12:15:19 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-07 12:15:21 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  May  7 12:15:21.344: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-3646  aa85d579-2463-4cc1-8fd0-c04316661dfb 53991 3 2023-05-07 12:15:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f6b7320e-df26-4927-b657-4a3d4edb6a6d 0xc0035e4ba7 0xc0035e4ba8}] [] [{kube-controller-manager Update apps/v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6b7320e-df26-4927-b657-4a3d4edb6a6d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035e4c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  7 12:15:21.344: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  May  7 12:15:21.344: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-3646  1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 53988 3 2023-05-07 12:15:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f6b7320e-df26-4927-b657-4a3d4edb6a6d 0xc0035e4ab7 0xc0035e4ab8}] [] [{kube-controller-manager Update apps/v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6b7320e-df26-4927-b657-4a3d4edb6a6d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035e4b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  May  7 12:15:21.370: INFO: Pod "webserver-deployment-67bd4bf6dc-4v552" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4v552 webserver-deployment-67bd4bf6dc- deployment-3646  a30e807c-f6fa-4847-b436-310a4579412e 54026 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc003935177 0xc003935178}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q8tvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q8tvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.370: INFO: Pod "webserver-deployment-67bd4bf6dc-886k7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-886k7 webserver-deployment-67bd4bf6dc- deployment-3646  e2b48421-31e6-45bf-8954-2438b8f1c1d2 53858 0 2023-05-07 12:15:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc0039356c0 0xc0039356c1}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.191.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pbbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pbbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.203,PodIP:172.20.191.43,StartTime:2023-05-07 12:15:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:15:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://87b068c52f01934ce5b2ced605b22715f8d2eace2b47a07126ec234875943028,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.191.43,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.370: INFO: Pod "webserver-deployment-67bd4bf6dc-cmc47" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cmc47 webserver-deployment-67bd4bf6dc- deployment-3646  fbbb6ac6-9e8c-4525-ac21-36de7290e98d 53816 0 2023-05-07 12:15:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc003935d07 0xc003935d08}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.3.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wqc95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wqc95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.201,PodIP:172.20.3.84,StartTime:2023-05-07 12:15:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:15:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://74b3a7afe3de12e6c56b422918bf944e4d76b94ddf2b60b7fae00b45f85f4aa9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.3.84,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.370: INFO: Pod "webserver-deployment-67bd4bf6dc-czqsq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-czqsq webserver-deployment-67bd4bf6dc- deployment-3646  ab733965-4cfb-4391-9013-93dab74a5a6e 54015 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3c0a0 0xc004b3c0a1}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m5pg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m5pg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.371: INFO: Pod "webserver-deployment-67bd4bf6dc-dtd5r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dtd5r webserver-deployment-67bd4bf6dc- deployment-3646  35af4abc-611e-4a90-9305-da4d8f686610 54025 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3c1d7 0xc004b3c1d8}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s869d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s869d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.371: INFO: Pod "webserver-deployment-67bd4bf6dc-g72ll" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-g72ll webserver-deployment-67bd4bf6dc- deployment-3646  6c4c019d-c8ba-42cc-adef-0a84acd50185 53855 0 2023-05-07 12:15:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3c340 0xc004b3c341}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.191.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mkbqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mkbqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.203,PodIP:172.20.191.35,StartTime:2023-05-07 12:15:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:15:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ca1065e37d123e4715b817ba36c2402c387969bc1fcd006aee07460483e9a4eb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.191.35,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.371: INFO: Pod "webserver-deployment-67bd4bf6dc-gbbnq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gbbnq webserver-deployment-67bd4bf6dc- deployment-3646  67cd1497-7180-4606-86f6-06770bb14ebd 54022 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3c527 0xc004b3c528}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6l6qt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6l6qt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.371: INFO: Pod "webserver-deployment-67bd4bf6dc-gscl7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gscl7 webserver-deployment-67bd4bf6dc- deployment-3646  3be3f9c0-4b15-4224-9b28-92e62556da61 54017 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3c690 0xc004b3c691}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dh9rp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dh9rp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.371: INFO: Pod "webserver-deployment-67bd4bf6dc-hbtsm" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hbtsm webserver-deployment-67bd4bf6dc- deployment-3646  73b9dde9-fcdd-4db3-822e-9216faf934b0 53861 0 2023-05-07 12:15:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3c7c7 0xc004b3c7c8}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.191.48\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ppk5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ppk5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.203,PodIP:172.20.191.48,StartTime:2023-05-07 12:15:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:15:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bd8836b6cf4ee26f1b96a92ea5b824717a34641632f64cc2f402ea764befa2fc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.191.48,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.371: INFO: Pod "webserver-deployment-67bd4bf6dc-htdf9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-htdf9 webserver-deployment-67bd4bf6dc- deployment-3646  adbba3e6-2e48-4930-9efc-6de2342b60cd 54023 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3c9b7 0xc004b3c9b8}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wwm45,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wwm45,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.371: INFO: Pod "webserver-deployment-67bd4bf6dc-mtxzg" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-mtxzg webserver-deployment-67bd4bf6dc- deployment-3646  858642b5-42ac-4d9a-a3f1-f515d9685c2f 53841 0 2023-05-07 12:15:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3cb20 0xc004b3cb21}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.231.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgm75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgm75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:172.20.231.255,StartTime:2023-05-07 12:15:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:15:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://65ab3d6f1b67f234066f8c00e8ec3ce700b40ff57fed3af7af4bcdafd413379e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.231.255,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.372: INFO: Pod "webserver-deployment-67bd4bf6dc-nbxtj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nbxtj webserver-deployment-67bd4bf6dc- deployment-3646  5eb5fd92-9f6d-492f-ae11-9d7ef4128122 54016 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3cd17 0xc004b3cd18}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8xxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8xxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.372: INFO: Pod "webserver-deployment-67bd4bf6dc-ngswt" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ngswt webserver-deployment-67bd4bf6dc- deployment-3646  0013d230-fabb-48ef-a49c-83369b094d4e 53867 0 2023-05-07 12:15:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3ce57 0xc004b3ce58}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.3.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfjvd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfjvd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.201,PodIP:172.20.3.95,StartTime:2023-05-07 12:15:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:15:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://66818f1057b2b21f894245cd39afd8b431c088105ccdebcc4bf3b8b6785c5547,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.3.95,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.372: INFO: Pod "webserver-deployment-67bd4bf6dc-qttj2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-qttj2 webserver-deployment-67bd4bf6dc- deployment-3646  8e74691b-c1e2-4ce7-bcbf-e771cf4a6078 54008 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3d040 0xc004b3d041}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljspq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljspq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.372: INFO: Pod "webserver-deployment-67bd4bf6dc-th868" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-th868 webserver-deployment-67bd4bf6dc- deployment-3646  d1234657-e1e4-486d-897f-8a190033cf5a 53846 0 2023-05-07 12:15:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3d1a0 0xc004b3d1a1}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.231.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8fzp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8fzp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:172.20.231.253,StartTime:2023-05-07 12:15:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:15:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e0f74e878e8a8877f0a017e940337fdcc38776f7ee4d041f31008eaded3163b6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.231.253,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.372: INFO: Pod "webserver-deployment-67bd4bf6dc-tx728" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tx728 webserver-deployment-67bd4bf6dc- deployment-3646  632c81ef-1c8d-443b-92a0-7c163a0cff6f 54003 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3d397 0xc004b3d398}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2zf9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2zf9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:,StartTime:2023-05-07 12:15:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.372: INFO: Pod "webserver-deployment-67bd4bf6dc-w8q7h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w8q7h webserver-deployment-67bd4bf6dc- deployment-3646  ef392499-1e56-4885-b512-252058d48403 54011 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3d577 0xc004b3d578}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9p65h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9p65h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.373: INFO: Pod "webserver-deployment-67bd4bf6dc-xhdqs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xhdqs webserver-deployment-67bd4bf6dc- deployment-3646  a95e1b02-9fa6-482c-9a0c-e718f78bf12b 54014 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3d700 0xc004b3d701}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gsfhb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gsfhb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.373: INFO: Pod "webserver-deployment-67bd4bf6dc-znj2k" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-znj2k webserver-deployment-67bd4bf6dc- deployment-3646  719f7669-44d7-4692-be22-9275c8c4d43c 53865 0 2023-05-07 12:15:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3d847 0xc004b3d848}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.3.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hz9xl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hz9xl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.201,PodIP:172.20.3.102,StartTime:2023-05-07 12:15:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:15:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2f9beabe6a82f6404415f282b4433111a25b0461db96aa97c44a338a1e24efc0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.3.102,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.373: INFO: Pod "webserver-deployment-67bd4bf6dc-znmvx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-znmvx webserver-deployment-67bd4bf6dc- deployment-3646  2450ee9a-39df-418a-bef8-d72b2afc90f8 54018 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 1c28c866-b61f-48e5-8fdd-1c4d5c74d22b 0xc004b3da37 0xc004b3da38}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c28c866-b61f-48e5-8fdd-1c4d5c74d22b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r5xwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r5xwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.373: INFO: Pod "webserver-deployment-7b75d79cf5-475t7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-475t7 webserver-deployment-7b75d79cf5- deployment-3646  aaa7dfa3-81fd-4205-be40-d0aab70b0ca7 54020 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc004b3db77 0xc004b3db78}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgg2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgg2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.373: INFO: Pod "webserver-deployment-7b75d79cf5-5hkxb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5hkxb webserver-deployment-7b75d79cf5- deployment-3646  3eeb9a15-7578-4484-80e9-a43191f6f750 53920 0 2023-05-07 12:15:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc004b3dcc7 0xc004b3dcc8}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qg7kf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qg7kf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.201,PodIP:,StartTime:2023-05-07 12:15:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.373: INFO: Pod "webserver-deployment-7b75d79cf5-8vx94" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8vx94 webserver-deployment-7b75d79cf5- deployment-3646  33518974-ad05-40d8-b261-3f1acdbd3790 53917 0 2023-05-07 12:15:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc004b3dec7 0xc004b3dec8}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6jxp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6jxp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.203,PodIP:,StartTime:2023-05-07 12:15:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.373: INFO: Pod "webserver-deployment-7b75d79cf5-d6x5h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-d6x5h webserver-deployment-7b75d79cf5- deployment-3646  0b7831db-4859-4e18-b3b4-1f6f805aefa4 54009 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc00390a2a7 0xc00390a2a8}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t76h5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t76h5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.201,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.374: INFO: Pod "webserver-deployment-7b75d79cf5-ggftz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-ggftz webserver-deployment-7b75d79cf5- deployment-3646  3a1f6fc2-bcd7-483f-86d3-c023556486b3 54010 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc00390a910 0xc00390a911}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7m4lw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7m4lw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.374: INFO: Pod "webserver-deployment-7b75d79cf5-jsz4t" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jsz4t webserver-deployment-7b75d79cf5- deployment-3646  5c9bf614-2bcf-4667-9a5a-82cb7daad574 54012 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc00390ace7 0xc00390ace8}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7f8pv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7f8pv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.374: INFO: Pod "webserver-deployment-7b75d79cf5-kmg54" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-kmg54 webserver-deployment-7b75d79cf5- deployment-3646  587da969-b24c-4ea5-9dda-99efbe898353 54013 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc00390b157 0xc00390b158}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nk87p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nk87p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.374: INFO: Pod "webserver-deployment-7b75d79cf5-lsvlj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lsvlj webserver-deployment-7b75d79cf5- deployment-3646  0f02fc3a-ed59-4dc3-abf6-8adb60cf9e63 53935 0 2023-05-07 12:15:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc00390b5b0 0xc00390b5b1}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2mdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2mdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.203,PodIP:,StartTime:2023-05-07 12:15:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.374: INFO: Pod "webserver-deployment-7b75d79cf5-ltmgh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-ltmgh webserver-deployment-7b75d79cf5- deployment-3646  b13158b9-727b-4897-884c-34f56d6b00d3 54019 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc00390bcd7 0xc00390bcd8}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nskp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nskp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.374: INFO: Pod "webserver-deployment-7b75d79cf5-plqj2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-plqj2 webserver-deployment-7b75d79cf5- deployment-3646  408f11bb-f6ac-4a09-af3f-311ebfe2238a 53931 0 2023-05-07 12:15:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc0038260c7 0xc0038260c8}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pdf6m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdf6m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:,StartTime:2023-05-07 12:15:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.374: INFO: Pod "webserver-deployment-7b75d79cf5-t5jkp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-t5jkp webserver-deployment-7b75d79cf5- deployment-3646  1db147f1-e6cf-487c-9186-b58e45f92ebc 54024 0 2023-05-07 12:15:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc003826dc7 0xc003826dc8}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbfwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbfwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.203,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.375: INFO: Pod "webserver-deployment-7b75d79cf5-wfqnn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wfqnn webserver-deployment-7b75d79cf5- deployment-3646  8583a3be-e220-474b-b261-b7869f4f4e68 53904 0 2023-05-07 12:15:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 aa85d579-2463-4cc1-8fd0-c04316661dfb 0xc003827580 0xc003827581}] [] [{kube-controller-manager Update v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa85d579-2463-4cc1-8fd0-c04316661dfb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:15:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2gl8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2gl8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:15:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:,StartTime:2023-05-07 12:15:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:15:21.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3646" for this suite. @ 05/07/23 12:15:21.421
• [6.235 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 05/07/23 12:15:21.466
  May  7 12:15:21.466: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:15:21.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:15:21.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:15:21.509
  STEP: Setting up server cert @ 05/07/23 12:15:21.594
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:15:21.975
  STEP: Deploying the webhook pod @ 05/07/23 12:15:21.978
  STEP: Wait for the deployment to be ready @ 05/07/23 12:15:21.983
  May  7 12:15:21.988: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/07/23 12:15:23.992
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:15:23.997
  May  7 12:15:24.997: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/07/23 12:15:24.999
  STEP: create a pod @ 05/07/23 12:15:25.008
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/07/23 12:15:27.036
  May  7 12:15:27.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=webhook-6437 attach --namespace=webhook-6437 to-be-attached-pod -i -c=container1'
  May  7 12:15:27.333: INFO: rc: 1
  May  7 12:15:27.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6437" for this suite. @ 05/07/23 12:15:27.491
  STEP: Destroying namespace "webhook-markers-5360" for this suite. @ 05/07/23 12:15:27.506
• [6.050 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 05/07/23 12:15:27.516
  May  7 12:15:27.516: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename svcaccounts @ 05/07/23 12:15:27.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:15:27.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:15:27.566
  STEP: reading a file in the container @ 05/07/23 12:15:29.613
  May  7 12:15:29.613: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2664 pod-service-account-5f3034fd-7c25-4600-9995-abc78528cf9b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 05/07/23 12:15:29.731
  May  7 12:15:29.731: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2664 pod-service-account-5f3034fd-7c25-4600-9995-abc78528cf9b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 05/07/23 12:15:29.829
  May  7 12:15:29.829: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2664 pod-service-account-5f3034fd-7c25-4600-9995-abc78528cf9b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  May  7 12:15:29.914: INFO: Got root ca configmap in namespace "svcaccounts-2664"
  May  7 12:15:29.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2664" for this suite. @ 05/07/23 12:15:29.917
• [2.403 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/07/23 12:15:29.919
  May  7 12:15:29.919: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename svc-latency @ 05/07/23 12:15:29.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:15:29.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:15:29.932
  May  7 12:15:29.934: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-8316 @ 05/07/23 12:15:29.934
  I0507 12:15:29.938431      20 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8316, replica count: 1
  I0507 12:15:30.989259      20 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0507 12:15:31.989502      20 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  7 12:15:32.101: INFO: Created: latency-svc-pzpgn
  May  7 12:15:32.112: INFO: Got endpoints: latency-svc-pzpgn [21.653693ms]
  May  7 12:15:32.158: INFO: Created: latency-svc-q9fw6
  May  7 12:15:32.162: INFO: Created: latency-svc-cg2pm
  May  7 12:15:32.166: INFO: Got endpoints: latency-svc-q9fw6 [54.549716ms]
  May  7 12:15:32.176: INFO: Created: latency-svc-r6vpv
  May  7 12:15:32.196: INFO: Got endpoints: latency-svc-r6vpv [83.657456ms]
  May  7 12:15:32.196: INFO: Got endpoints: latency-svc-cg2pm [83.323588ms]
  May  7 12:15:32.232: INFO: Created: latency-svc-mm9mr
  May  7 12:15:32.232: INFO: Created: latency-svc-f59b7
  May  7 12:15:32.237: INFO: Created: latency-svc-v978k
  May  7 12:15:32.238: INFO: Created: latency-svc-74hkt
  May  7 12:15:32.240: INFO: Created: latency-svc-prx9b
  May  7 12:15:32.242: INFO: Created: latency-svc-6s9bw
  May  7 12:15:32.244: INFO: Created: latency-svc-bdcwx
  May  7 12:15:32.244: INFO: Created: latency-svc-lxjfj
  May  7 12:15:32.245: INFO: Created: latency-svc-wfc2s
  May  7 12:15:32.245: INFO: Created: latency-svc-nlprq
  May  7 12:15:32.245: INFO: Created: latency-svc-25ps4
  May  7 12:15:32.245: INFO: Created: latency-svc-5cvrd
  May  7 12:15:32.246: INFO: Created: latency-svc-spv9l
  May  7 12:15:32.246: INFO: Created: latency-svc-bz4g9
  May  7 12:15:32.246: INFO: Created: latency-svc-h9d66
  May  7 12:15:32.251: INFO: Got endpoints: latency-svc-mm9mr [55.072153ms]
  May  7 12:15:32.251: INFO: Got endpoints: latency-svc-f59b7 [55.302404ms]
  May  7 12:15:32.282: INFO: Got endpoints: latency-svc-lxjfj [169.790923ms]
  May  7 12:15:32.282: INFO: Got endpoints: latency-svc-v978k [170.17405ms]
  May  7 12:15:32.282: INFO: Got endpoints: latency-svc-bdcwx [169.932914ms]
  May  7 12:15:32.290: INFO: Got endpoints: latency-svc-74hkt [177.005629ms]
  May  7 12:15:32.335: INFO: Created: latency-svc-5g6kk
  May  7 12:15:32.341: INFO: Got endpoints: latency-svc-h9d66 [229.074762ms]
  May  7 12:15:32.341: INFO: Got endpoints: latency-svc-prx9b [175.173706ms]
  May  7 12:15:32.346: INFO: Got endpoints: latency-svc-nlprq [233.562169ms]
  May  7 12:15:32.360: INFO: Got endpoints: latency-svc-5cvrd [248.533602ms]
  May  7 12:15:32.360: INFO: Got endpoints: latency-svc-bz4g9 [248.52654ms]
  May  7 12:15:32.393: INFO: Created: latency-svc-k5rz5
  May  7 12:15:32.429: INFO: Created: latency-svc-svzn6
  May  7 12:15:32.430: INFO: Created: latency-svc-cw4nw
  May  7 12:15:32.430: INFO: Got endpoints: latency-svc-wfc2s [318.326876ms]
  May  7 12:15:32.430: INFO: Got endpoints: latency-svc-25ps4 [318.322076ms]
  May  7 12:15:32.431: INFO: Got endpoints: latency-svc-5g6kk [179.42145ms]
  May  7 12:15:32.447: INFO: Created: latency-svc-ghw9q
  May  7 12:15:32.449: INFO: Got endpoints: latency-svc-6s9bw [336.857497ms]
  May  7 12:15:32.449: INFO: Got endpoints: latency-svc-k5rz5 [198.20216ms]
  May  7 12:15:32.449: INFO: Got endpoints: latency-svc-spv9l [337.058849ms]
  May  7 12:15:32.450: INFO: Got endpoints: latency-svc-svzn6 [168.015496ms]
  May  7 12:15:32.470: INFO: Created: latency-svc-5xt26
  May  7 12:15:32.475: INFO: Got endpoints: latency-svc-cw4nw [185.218276ms]
  May  7 12:15:32.493: INFO: Got endpoints: latency-svc-5xt26 [210.681965ms]
  May  7 12:15:32.497: INFO: Got endpoints: latency-svc-ghw9q [214.727113ms]
  May  7 12:15:32.581: INFO: Created: latency-svc-ns7hf
  May  7 12:15:32.603: INFO: Created: latency-svc-54pzx
  May  7 12:15:32.603: INFO: Created: latency-svc-sx9fb
  May  7 12:15:32.603: INFO: Created: latency-svc-7nvg2
  May  7 12:15:32.603: INFO: Created: latency-svc-2kfxc
  May  7 12:15:32.603: INFO: Created: latency-svc-fsbpt
  May  7 12:15:32.604: INFO: Created: latency-svc-v78v9
  May  7 12:15:32.604: INFO: Created: latency-svc-8gcp8
  May  7 12:15:32.605: INFO: Created: latency-svc-bg7cd
  May  7 12:15:32.606: INFO: Created: latency-svc-p7ptm
  May  7 12:15:32.606: INFO: Created: latency-svc-wjbjf
  May  7 12:15:32.606: INFO: Created: latency-svc-l68tf
  May  7 12:15:32.608: INFO: Created: latency-svc-mqf4t
  May  7 12:15:32.608: INFO: Created: latency-svc-qzgkw
  May  7 12:15:32.608: INFO: Created: latency-svc-xcp4r
  May  7 12:15:32.609: INFO: Got endpoints: latency-svc-8gcp8 [111.606758ms]
  May  7 12:15:32.614: INFO: Got endpoints: latency-svc-qzgkw [253.187891ms]
  May  7 12:15:32.624: INFO: Got endpoints: latency-svc-7nvg2 [173.782296ms]
  May  7 12:15:32.624: INFO: Got endpoints: latency-svc-ns7hf [131.018249ms]
  May  7 12:15:32.624: INFO: Got endpoints: latency-svc-2kfxc [175.021667ms]
  May  7 12:15:32.643: INFO: Got endpoints: latency-svc-mqf4t [212.958343ms]
  May  7 12:15:32.644: INFO: Got endpoints: latency-svc-54pzx [302.472466ms]
  May  7 12:15:32.644: INFO: Got endpoints: latency-svc-fsbpt [213.503865ms]
  May  7 12:15:32.644: INFO: Got endpoints: latency-svc-xcp4r [297.908045ms]
  May  7 12:15:32.644: INFO: Got endpoints: latency-svc-v78v9 [302.946837ms]
  May  7 12:15:32.646: INFO: Created: latency-svc-nlhf5
  May  7 12:15:32.646: INFO: Got endpoints: latency-svc-l68tf [196.881158ms]
  May  7 12:15:32.660: INFO: Created: latency-svc-rgmn8
  May  7 12:15:32.667: INFO: Created: latency-svc-q6wzv
  May  7 12:15:32.673: INFO: Got endpoints: latency-svc-nlhf5 [64.12955ms]
  May  7 12:15:32.673: INFO: Got endpoints: latency-svc-sx9fb [242.571619ms]
  May  7 12:15:32.673: INFO: Got endpoints: latency-svc-bg7cd [223.830278ms]
  May  7 12:15:32.673: INFO: Got endpoints: latency-svc-wjbjf [312.950546ms]
  May  7 12:15:32.673: INFO: Got endpoints: latency-svc-p7ptm [198.718163ms]
  May  7 12:15:32.685: INFO: Created: latency-svc-cs8md
  May  7 12:15:32.687: INFO: Created: latency-svc-6ncmz
  May  7 12:15:32.688: INFO: Created: latency-svc-dl792
  May  7 12:15:32.699: INFO: Created: latency-svc-nr5sk
  May  7 12:15:32.709: INFO: Created: latency-svc-ztssk
  May  7 12:15:32.722: INFO: Created: latency-svc-q7cf7
  May  7 12:15:32.723: INFO: Got endpoints: latency-svc-rgmn8 [109.136528ms]
  May  7 12:15:32.723: INFO: Created: latency-svc-9qbj9
  May  7 12:15:32.729: INFO: Created: latency-svc-phq2f
  May  7 12:15:32.736: INFO: Created: latency-svc-9sptf
  May  7 12:15:32.742: INFO: Created: latency-svc-l442v
  May  7 12:15:32.745: INFO: Created: latency-svc-xqh8t
  May  7 12:15:32.750: INFO: Created: latency-svc-qm85p
  May  7 12:15:32.756: INFO: Got endpoints: latency-svc-q6wzv [131.785786ms]
  May  7 12:15:32.764: INFO: Created: latency-svc-5hn95
  May  7 12:15:32.768: INFO: Created: latency-svc-f8tdt
  May  7 12:15:32.772: INFO: Created: latency-svc-fwg9n
  May  7 12:15:32.810: INFO: Got endpoints: latency-svc-cs8md [185.767491ms]
  May  7 12:15:32.826: INFO: Created: latency-svc-krjfb
  May  7 12:15:32.854: INFO: Got endpoints: latency-svc-dl792 [229.927582ms]
  May  7 12:15:32.861: INFO: Created: latency-svc-ngspt
  May  7 12:15:32.904: INFO: Got endpoints: latency-svc-6ncmz [260.476642ms]
  May  7 12:15:32.910: INFO: Created: latency-svc-gdlxd
  May  7 12:15:32.954: INFO: Got endpoints: latency-svc-nr5sk [310.212934ms]
  May  7 12:15:32.960: INFO: Created: latency-svc-hfp9v
  May  7 12:15:33.004: INFO: Got endpoints: latency-svc-ztssk [360.348651ms]
  May  7 12:15:33.010: INFO: Created: latency-svc-vkm7n
  May  7 12:15:33.064: INFO: Got endpoints: latency-svc-q7cf7 [419.473671ms]
  May  7 12:15:33.071: INFO: Created: latency-svc-96clc
  May  7 12:15:33.103: INFO: Got endpoints: latency-svc-9qbj9 [459.267673ms]
  May  7 12:15:33.110: INFO: Created: latency-svc-plqv9
  May  7 12:15:33.154: INFO: Got endpoints: latency-svc-phq2f [508.052833ms]
  May  7 12:15:33.169: INFO: Created: latency-svc-98xp5
  May  7 12:15:33.205: INFO: Got endpoints: latency-svc-9sptf [531.361605ms]
  May  7 12:15:33.210: INFO: Created: latency-svc-qs5xg
  May  7 12:15:33.254: INFO: Got endpoints: latency-svc-l442v [580.395964ms]
  May  7 12:15:33.260: INFO: Created: latency-svc-k79x7
  May  7 12:15:33.303: INFO: Got endpoints: latency-svc-xqh8t [630.148032ms]
  May  7 12:15:33.309: INFO: Created: latency-svc-47768
  May  7 12:15:33.353: INFO: Got endpoints: latency-svc-qm85p [680.073776ms]
  May  7 12:15:33.359: INFO: Created: latency-svc-dnnsj
  May  7 12:15:33.404: INFO: Got endpoints: latency-svc-5hn95 [730.698102ms]
  May  7 12:15:33.410: INFO: Created: latency-svc-7q7r9
  May  7 12:15:33.454: INFO: Got endpoints: latency-svc-f8tdt [730.667956ms]
  May  7 12:15:33.459: INFO: Created: latency-svc-swq8p
  May  7 12:15:33.505: INFO: Got endpoints: latency-svc-fwg9n [748.890774ms]
  May  7 12:15:33.511: INFO: Created: latency-svc-72mpl
  May  7 12:15:33.553: INFO: Got endpoints: latency-svc-krjfb [742.581361ms]
  May  7 12:15:33.558: INFO: Created: latency-svc-t28pp
  May  7 12:15:33.603: INFO: Got endpoints: latency-svc-ngspt [748.903928ms]
  May  7 12:15:33.609: INFO: Created: latency-svc-59kmp
  May  7 12:15:33.654: INFO: Got endpoints: latency-svc-gdlxd [749.706246ms]
  May  7 12:15:33.660: INFO: Created: latency-svc-56tbp
  May  7 12:15:33.704: INFO: Got endpoints: latency-svc-hfp9v [749.556899ms]
  May  7 12:15:33.709: INFO: Created: latency-svc-7qbfd
  May  7 12:15:33.758: INFO: Got endpoints: latency-svc-vkm7n [753.188059ms]
  May  7 12:15:33.764: INFO: Created: latency-svc-2r95c
  May  7 12:15:33.804: INFO: Got endpoints: latency-svc-96clc [739.988843ms]
  May  7 12:15:33.810: INFO: Created: latency-svc-w4rp7
  May  7 12:15:33.855: INFO: Got endpoints: latency-svc-plqv9 [751.182724ms]
  May  7 12:15:33.860: INFO: Created: latency-svc-sdnqr
  May  7 12:15:33.902: INFO: Got endpoints: latency-svc-98xp5 [748.170883ms]
  May  7 12:15:33.908: INFO: Created: latency-svc-wqtxt
  May  7 12:15:33.953: INFO: Got endpoints: latency-svc-qs5xg [748.746345ms]
  May  7 12:15:33.961: INFO: Created: latency-svc-9xc66
  May  7 12:15:34.005: INFO: Got endpoints: latency-svc-k79x7 [750.81695ms]
  May  7 12:15:34.015: INFO: Created: latency-svc-p8kw5
  May  7 12:15:34.055: INFO: Got endpoints: latency-svc-47768 [751.440968ms]
  May  7 12:15:34.061: INFO: Created: latency-svc-75ph2
  May  7 12:15:34.104: INFO: Got endpoints: latency-svc-dnnsj [750.674397ms]
  May  7 12:15:34.110: INFO: Created: latency-svc-pwng2
  May  7 12:15:34.152: INFO: Got endpoints: latency-svc-7q7r9 [748.347737ms]
  May  7 12:15:34.159: INFO: Created: latency-svc-msb4c
  May  7 12:15:34.205: INFO: Got endpoints: latency-svc-swq8p [751.175054ms]
  May  7 12:15:34.218: INFO: Created: latency-svc-9c8s2
  May  7 12:15:34.254: INFO: Got endpoints: latency-svc-72mpl [749.358421ms]
  May  7 12:15:34.259: INFO: Created: latency-svc-kjf9h
  May  7 12:15:34.304: INFO: Got endpoints: latency-svc-t28pp [751.453009ms]
  May  7 12:15:34.316: INFO: Created: latency-svc-pbhf2
  May  7 12:15:34.356: INFO: Got endpoints: latency-svc-59kmp [753.415471ms]
  May  7 12:15:34.362: INFO: Created: latency-svc-wzn6m
  May  7 12:15:34.405: INFO: Got endpoints: latency-svc-56tbp [751.066188ms]
  May  7 12:15:34.410: INFO: Created: latency-svc-7m86z
  May  7 12:15:34.453: INFO: Got endpoints: latency-svc-7qbfd [749.152645ms]
  May  7 12:15:34.459: INFO: Created: latency-svc-7phgk
  May  7 12:15:34.504: INFO: Got endpoints: latency-svc-2r95c [745.891474ms]
  May  7 12:15:34.509: INFO: Created: latency-svc-62bfj
  May  7 12:15:34.554: INFO: Got endpoints: latency-svc-w4rp7 [750.494077ms]
  May  7 12:15:34.562: INFO: Created: latency-svc-6tksk
  May  7 12:15:34.604: INFO: Got endpoints: latency-svc-sdnqr [749.49019ms]
  May  7 12:15:34.610: INFO: Created: latency-svc-kcp5z
  May  7 12:15:34.652: INFO: Got endpoints: latency-svc-wqtxt [749.597564ms]
  May  7 12:15:34.659: INFO: Created: latency-svc-4hxms
  May  7 12:15:34.703: INFO: Got endpoints: latency-svc-9xc66 [749.845302ms]
  May  7 12:15:34.709: INFO: Created: latency-svc-7nzv9
  May  7 12:15:34.754: INFO: Got endpoints: latency-svc-p8kw5 [748.900085ms]
  May  7 12:15:34.760: INFO: Created: latency-svc-bx9gc
  May  7 12:15:34.805: INFO: Got endpoints: latency-svc-75ph2 [750.416619ms]
  May  7 12:15:34.811: INFO: Created: latency-svc-mnmhj
  May  7 12:15:34.854: INFO: Got endpoints: latency-svc-pwng2 [750.476216ms]
  May  7 12:15:34.865: INFO: Created: latency-svc-9dlpq
  May  7 12:15:34.904: INFO: Got endpoints: latency-svc-msb4c [751.32254ms]
  May  7 12:15:34.909: INFO: Created: latency-svc-7p6d7
  May  7 12:15:34.959: INFO: Got endpoints: latency-svc-9c8s2 [754.506099ms]
  May  7 12:15:34.966: INFO: Created: latency-svc-bqslw
  May  7 12:15:35.009: INFO: Got endpoints: latency-svc-kjf9h [754.166768ms]
  May  7 12:15:35.017: INFO: Created: latency-svc-bftc2
  May  7 12:15:35.054: INFO: Got endpoints: latency-svc-pbhf2 [750.242436ms]
  May  7 12:15:35.062: INFO: Created: latency-svc-f2d8x
  May  7 12:15:35.105: INFO: Got endpoints: latency-svc-wzn6m [748.621129ms]
  May  7 12:15:35.110: INFO: Created: latency-svc-twsl6
  May  7 12:15:35.154: INFO: Got endpoints: latency-svc-7m86z [749.418137ms]
  May  7 12:15:35.161: INFO: Created: latency-svc-knmzm
  May  7 12:15:35.208: INFO: Got endpoints: latency-svc-7phgk [754.827363ms]
  May  7 12:15:35.219: INFO: Created: latency-svc-wmbjw
  May  7 12:15:35.253: INFO: Got endpoints: latency-svc-62bfj [749.890065ms]
  May  7 12:15:35.259: INFO: Created: latency-svc-mh5c9
  May  7 12:15:35.303: INFO: Got endpoints: latency-svc-6tksk [749.058884ms]
  May  7 12:15:35.310: INFO: Created: latency-svc-9tg6r
  May  7 12:15:35.355: INFO: Got endpoints: latency-svc-kcp5z [751.093997ms]
  May  7 12:15:35.361: INFO: Created: latency-svc-9j9qc
  May  7 12:15:35.405: INFO: Got endpoints: latency-svc-4hxms [752.693189ms]
  May  7 12:15:35.410: INFO: Created: latency-svc-lh4jt
  May  7 12:15:35.454: INFO: Got endpoints: latency-svc-7nzv9 [750.840715ms]
  May  7 12:15:35.461: INFO: Created: latency-svc-ghln4
  May  7 12:15:35.502: INFO: Got endpoints: latency-svc-bx9gc [748.369144ms]
  May  7 12:15:35.509: INFO: Created: latency-svc-r4vvb
  May  7 12:15:35.553: INFO: Got endpoints: latency-svc-mnmhj [748.308038ms]
  May  7 12:15:35.560: INFO: Created: latency-svc-l8nww
  May  7 12:15:35.604: INFO: Got endpoints: latency-svc-9dlpq [749.432719ms]
  May  7 12:15:35.610: INFO: Created: latency-svc-xqrdc
  May  7 12:15:35.654: INFO: Got endpoints: latency-svc-7p6d7 [749.820339ms]
  May  7 12:15:35.659: INFO: Created: latency-svc-kcmsk
  May  7 12:15:35.704: INFO: Got endpoints: latency-svc-bqslw [744.077924ms]
  May  7 12:15:35.709: INFO: Created: latency-svc-8g47t
  May  7 12:15:35.754: INFO: Got endpoints: latency-svc-bftc2 [745.856901ms]
  May  7 12:15:35.760: INFO: Created: latency-svc-qj8lh
  May  7 12:15:35.808: INFO: Got endpoints: latency-svc-f2d8x [753.296722ms]
  May  7 12:15:35.816: INFO: Created: latency-svc-kl9v9
  May  7 12:15:35.853: INFO: Got endpoints: latency-svc-twsl6 [748.007355ms]
  May  7 12:15:35.860: INFO: Created: latency-svc-2phjv
  May  7 12:15:35.904: INFO: Got endpoints: latency-svc-knmzm [749.572475ms]
  May  7 12:15:35.909: INFO: Created: latency-svc-dvg26
  May  7 12:15:35.953: INFO: Got endpoints: latency-svc-wmbjw [745.603386ms]
  May  7 12:15:35.961: INFO: Created: latency-svc-s22bv
  May  7 12:15:36.002: INFO: Got endpoints: latency-svc-mh5c9 [748.684089ms]
  May  7 12:15:36.010: INFO: Created: latency-svc-mx87c
  May  7 12:15:36.054: INFO: Got endpoints: latency-svc-9tg6r [750.934233ms]
  May  7 12:15:36.061: INFO: Created: latency-svc-htk6l
  May  7 12:15:36.103: INFO: Got endpoints: latency-svc-9j9qc [747.972706ms]
  May  7 12:15:36.109: INFO: Created: latency-svc-52vcc
  May  7 12:15:36.154: INFO: Got endpoints: latency-svc-lh4jt [749.052036ms]
  May  7 12:15:36.161: INFO: Created: latency-svc-8bvts
  May  7 12:15:36.204: INFO: Got endpoints: latency-svc-ghln4 [750.086404ms]
  May  7 12:15:36.212: INFO: Created: latency-svc-qbzv6
  May  7 12:15:36.255: INFO: Got endpoints: latency-svc-r4vvb [753.33115ms]
  May  7 12:15:36.267: INFO: Created: latency-svc-s2cq8
  May  7 12:15:36.303: INFO: Got endpoints: latency-svc-l8nww [749.463775ms]
  May  7 12:15:36.309: INFO: Created: latency-svc-7cwwd
  May  7 12:15:36.353: INFO: Got endpoints: latency-svc-xqrdc [748.882988ms]
  May  7 12:15:36.361: INFO: Created: latency-svc-d52fp
  May  7 12:15:36.407: INFO: Got endpoints: latency-svc-kcmsk [753.494414ms]
  May  7 12:15:36.413: INFO: Created: latency-svc-l9mcc
  May  7 12:15:36.453: INFO: Got endpoints: latency-svc-8g47t [749.581277ms]
  May  7 12:15:36.460: INFO: Created: latency-svc-wwdsk
  May  7 12:15:36.504: INFO: Got endpoints: latency-svc-qj8lh [749.127976ms]
  May  7 12:15:36.510: INFO: Created: latency-svc-crtrn
  May  7 12:15:36.556: INFO: Got endpoints: latency-svc-kl9v9 [747.740701ms]
  May  7 12:15:36.561: INFO: Created: latency-svc-fhm79
  May  7 12:15:36.604: INFO: Got endpoints: latency-svc-2phjv [750.351357ms]
  May  7 12:15:36.611: INFO: Created: latency-svc-p925m
  May  7 12:15:36.653: INFO: Got endpoints: latency-svc-dvg26 [749.077749ms]
  May  7 12:15:36.660: INFO: Created: latency-svc-vtwcj
  May  7 12:15:36.703: INFO: Got endpoints: latency-svc-s22bv [750.022501ms]
  May  7 12:15:36.709: INFO: Created: latency-svc-ltfq2
  May  7 12:15:36.755: INFO: Got endpoints: latency-svc-mx87c [753.179676ms]
  May  7 12:15:36.762: INFO: Created: latency-svc-l4m8m
  May  7 12:15:36.803: INFO: Got endpoints: latency-svc-htk6l [748.939645ms]
  May  7 12:15:36.818: INFO: Created: latency-svc-774fl
  May  7 12:15:36.854: INFO: Got endpoints: latency-svc-52vcc [750.824482ms]
  May  7 12:15:36.859: INFO: Created: latency-svc-kfl29
  May  7 12:15:36.903: INFO: Got endpoints: latency-svc-8bvts [748.866544ms]
  May  7 12:15:36.911: INFO: Created: latency-svc-ctrfg
  May  7 12:15:36.954: INFO: Got endpoints: latency-svc-qbzv6 [750.087511ms]
  May  7 12:15:36.960: INFO: Created: latency-svc-lz9z6
  May  7 12:15:37.005: INFO: Got endpoints: latency-svc-s2cq8 [749.668712ms]
  May  7 12:15:37.010: INFO: Created: latency-svc-htsn8
  May  7 12:15:37.055: INFO: Got endpoints: latency-svc-7cwwd [751.837574ms]
  May  7 12:15:37.060: INFO: Created: latency-svc-8gvml
  May  7 12:15:37.104: INFO: Got endpoints: latency-svc-d52fp [750.582743ms]
  May  7 12:15:37.111: INFO: Created: latency-svc-ckvmz
  May  7 12:15:37.155: INFO: Got endpoints: latency-svc-l9mcc [747.210618ms]
  May  7 12:15:37.164: INFO: Created: latency-svc-46dsj
  May  7 12:15:37.203: INFO: Got endpoints: latency-svc-wwdsk [749.591356ms]
  May  7 12:15:37.209: INFO: Created: latency-svc-wsz9d
  May  7 12:15:37.253: INFO: Got endpoints: latency-svc-crtrn [749.23513ms]
  May  7 12:15:37.261: INFO: Created: latency-svc-zmt6m
  May  7 12:15:37.303: INFO: Got endpoints: latency-svc-fhm79 [747.192104ms]
  May  7 12:15:37.309: INFO: Created: latency-svc-rzrhm
  May  7 12:15:37.356: INFO: Got endpoints: latency-svc-p925m [751.862724ms]
  May  7 12:15:37.367: INFO: Created: latency-svc-cs22z
  May  7 12:15:37.403: INFO: Got endpoints: latency-svc-vtwcj [749.978048ms]
  May  7 12:15:37.411: INFO: Created: latency-svc-npsqz
  May  7 12:15:37.454: INFO: Got endpoints: latency-svc-ltfq2 [750.623153ms]
  May  7 12:15:37.460: INFO: Created: latency-svc-g7h48
  May  7 12:15:37.506: INFO: Got endpoints: latency-svc-l4m8m [750.590296ms]
  May  7 12:15:37.512: INFO: Created: latency-svc-gpmcz
  May  7 12:15:37.556: INFO: Got endpoints: latency-svc-774fl [752.367554ms]
  May  7 12:15:37.566: INFO: Created: latency-svc-7h6wf
  May  7 12:15:37.604: INFO: Got endpoints: latency-svc-kfl29 [749.435746ms]
  May  7 12:15:37.610: INFO: Created: latency-svc-tw2xn
  May  7 12:15:37.654: INFO: Got endpoints: latency-svc-ctrfg [751.149296ms]
  May  7 12:15:37.660: INFO: Created: latency-svc-s9jdk
  May  7 12:15:37.703: INFO: Got endpoints: latency-svc-lz9z6 [748.933985ms]
  May  7 12:15:37.712: INFO: Created: latency-svc-679vj
  May  7 12:15:37.753: INFO: Got endpoints: latency-svc-htsn8 [747.99257ms]
  May  7 12:15:37.760: INFO: Created: latency-svc-mgw4w
  May  7 12:15:37.803: INFO: Got endpoints: latency-svc-8gvml [748.211157ms]
  May  7 12:15:37.810: INFO: Created: latency-svc-rwnd5
  May  7 12:15:37.853: INFO: Got endpoints: latency-svc-ckvmz [749.820201ms]
  May  7 12:15:37.859: INFO: Created: latency-svc-d9z2g
  May  7 12:15:37.907: INFO: Got endpoints: latency-svc-46dsj [752.252684ms]
  May  7 12:15:37.916: INFO: Created: latency-svc-j4d7l
  May  7 12:15:37.956: INFO: Got endpoints: latency-svc-wsz9d [753.359236ms]
  May  7 12:15:37.966: INFO: Created: latency-svc-rncgx
  May  7 12:15:38.008: INFO: Got endpoints: latency-svc-zmt6m [755.474733ms]
  May  7 12:15:38.016: INFO: Created: latency-svc-v8glp
  May  7 12:15:38.052: INFO: Got endpoints: latency-svc-rzrhm [749.675879ms]
  May  7 12:15:38.060: INFO: Created: latency-svc-grcx5
  May  7 12:15:38.104: INFO: Got endpoints: latency-svc-cs22z [748.667299ms]
  May  7 12:15:38.114: INFO: Created: latency-svc-n4467
  May  7 12:15:38.154: INFO: Got endpoints: latency-svc-npsqz [750.843582ms]
  May  7 12:15:38.160: INFO: Created: latency-svc-m8rpd
  May  7 12:15:38.204: INFO: Got endpoints: latency-svc-g7h48 [749.36694ms]
  May  7 12:15:38.210: INFO: Created: latency-svc-q6mwm
  May  7 12:15:38.257: INFO: Got endpoints: latency-svc-gpmcz [750.782081ms]
  May  7 12:15:38.263: INFO: Created: latency-svc-djlsx
  May  7 12:15:38.306: INFO: Got endpoints: latency-svc-7h6wf [750.400065ms]
  May  7 12:15:38.312: INFO: Created: latency-svc-xrcb6
  May  7 12:15:38.354: INFO: Got endpoints: latency-svc-tw2xn [750.614901ms]
  May  7 12:15:38.359: INFO: Created: latency-svc-sz5vx
  May  7 12:15:38.403: INFO: Got endpoints: latency-svc-s9jdk [748.607637ms]
  May  7 12:15:38.411: INFO: Created: latency-svc-pbg7j
  May  7 12:15:38.453: INFO: Got endpoints: latency-svc-679vj [750.206782ms]
  May  7 12:15:38.459: INFO: Created: latency-svc-wpdrs
  May  7 12:15:38.504: INFO: Got endpoints: latency-svc-mgw4w [750.445588ms]
  May  7 12:15:38.510: INFO: Created: latency-svc-g6zqf
  May  7 12:15:38.554: INFO: Got endpoints: latency-svc-rwnd5 [750.845541ms]
  May  7 12:15:38.559: INFO: Created: latency-svc-tx79h
  May  7 12:15:38.604: INFO: Got endpoints: latency-svc-d9z2g [750.256328ms]
  May  7 12:15:38.612: INFO: Created: latency-svc-hthzv
  May  7 12:15:38.655: INFO: Got endpoints: latency-svc-j4d7l [748.273526ms]
  May  7 12:15:38.661: INFO: Created: latency-svc-76vfq
  May  7 12:15:38.702: INFO: Got endpoints: latency-svc-rncgx [746.26049ms]
  May  7 12:15:38.708: INFO: Created: latency-svc-fm85v
  May  7 12:15:38.757: INFO: Got endpoints: latency-svc-v8glp [748.170099ms]
  May  7 12:15:38.762: INFO: Created: latency-svc-4wj9m
  May  7 12:15:38.804: INFO: Got endpoints: latency-svc-grcx5 [750.98532ms]
  May  7 12:15:38.810: INFO: Created: latency-svc-pvxw9
  May  7 12:15:38.855: INFO: Got endpoints: latency-svc-n4467 [750.863805ms]
  May  7 12:15:38.862: INFO: Created: latency-svc-pqkkf
  May  7 12:15:38.903: INFO: Got endpoints: latency-svc-m8rpd [748.890719ms]
  May  7 12:15:38.909: INFO: Created: latency-svc-tn6h9
  May  7 12:15:38.956: INFO: Got endpoints: latency-svc-q6mwm [752.587279ms]
  May  7 12:15:38.962: INFO: Created: latency-svc-jggrj
  May  7 12:15:39.005: INFO: Got endpoints: latency-svc-djlsx [748.45995ms]
  May  7 12:15:39.015: INFO: Created: latency-svc-7cqvg
  May  7 12:15:39.057: INFO: Got endpoints: latency-svc-xrcb6 [750.514535ms]
  May  7 12:15:39.063: INFO: Created: latency-svc-zt9dn
  May  7 12:15:39.102: INFO: Got endpoints: latency-svc-sz5vx [748.035904ms]
  May  7 12:15:39.108: INFO: Created: latency-svc-c29vn
  May  7 12:15:39.155: INFO: Got endpoints: latency-svc-pbg7j [752.587781ms]
  May  7 12:15:39.164: INFO: Created: latency-svc-hfdvx
  May  7 12:15:39.204: INFO: Got endpoints: latency-svc-wpdrs [750.192261ms]
  May  7 12:15:39.211: INFO: Created: latency-svc-mcjqt
  May  7 12:15:39.255: INFO: Got endpoints: latency-svc-g6zqf [751.685719ms]
  May  7 12:15:39.260: INFO: Created: latency-svc-jnxfn
  May  7 12:15:39.306: INFO: Got endpoints: latency-svc-tx79h [751.65274ms]
  May  7 12:15:39.311: INFO: Created: latency-svc-zfhtd
  May  7 12:15:39.354: INFO: Got endpoints: latency-svc-hthzv [749.957258ms]
  May  7 12:15:39.362: INFO: Created: latency-svc-czwvk
  May  7 12:15:39.402: INFO: Got endpoints: latency-svc-76vfq [747.02554ms]
  May  7 12:15:39.408: INFO: Created: latency-svc-fwbx7
  May  7 12:15:39.454: INFO: Got endpoints: latency-svc-fm85v [751.845401ms]
  May  7 12:15:39.461: INFO: Created: latency-svc-8slhn
  May  7 12:15:39.503: INFO: Got endpoints: latency-svc-4wj9m [746.791906ms]
  May  7 12:15:39.516: INFO: Created: latency-svc-krdmt
  May  7 12:15:39.553: INFO: Got endpoints: latency-svc-pvxw9 [748.974152ms]
  May  7 12:15:39.568: INFO: Created: latency-svc-x6mgp
  May  7 12:15:39.605: INFO: Got endpoints: latency-svc-pqkkf [750.000586ms]
  May  7 12:15:39.611: INFO: Created: latency-svc-zmhxg
  May  7 12:15:39.654: INFO: Got endpoints: latency-svc-tn6h9 [751.486381ms]
  May  7 12:15:39.660: INFO: Created: latency-svc-nw9tk
  May  7 12:15:39.703: INFO: Got endpoints: latency-svc-jggrj [746.753467ms]
  May  7 12:15:39.710: INFO: Created: latency-svc-v9z6m
  May  7 12:15:39.754: INFO: Got endpoints: latency-svc-7cqvg [749.02933ms]
  May  7 12:15:39.762: INFO: Created: latency-svc-rgn2f
  May  7 12:15:39.803: INFO: Got endpoints: latency-svc-zt9dn [746.414657ms]
  May  7 12:15:39.809: INFO: Created: latency-svc-r92zh
  May  7 12:15:39.853: INFO: Got endpoints: latency-svc-c29vn [750.49903ms]
  May  7 12:15:39.859: INFO: Created: latency-svc-k9hrw
  May  7 12:15:39.906: INFO: Got endpoints: latency-svc-hfdvx [750.174789ms]
  May  7 12:15:39.912: INFO: Created: latency-svc-bb47b
  May  7 12:15:39.954: INFO: Got endpoints: latency-svc-mcjqt [750.667644ms]
  May  7 12:15:40.004: INFO: Got endpoints: latency-svc-jnxfn [748.938115ms]
  May  7 12:15:40.054: INFO: Got endpoints: latency-svc-zfhtd [748.027794ms]
  May  7 12:15:40.105: INFO: Got endpoints: latency-svc-czwvk [751.550426ms]
  May  7 12:15:40.154: INFO: Got endpoints: latency-svc-fwbx7 [752.003786ms]
  May  7 12:15:40.205: INFO: Got endpoints: latency-svc-8slhn [750.407965ms]
  May  7 12:15:40.253: INFO: Got endpoints: latency-svc-krdmt [749.184308ms]
  May  7 12:15:40.305: INFO: Got endpoints: latency-svc-x6mgp [752.664105ms]
  May  7 12:15:40.354: INFO: Got endpoints: latency-svc-zmhxg [748.955206ms]
  May  7 12:15:40.406: INFO: Got endpoints: latency-svc-nw9tk [752.143722ms]
  May  7 12:15:40.454: INFO: Got endpoints: latency-svc-v9z6m [751.273968ms]
  May  7 12:15:40.504: INFO: Got endpoints: latency-svc-rgn2f [749.463852ms]
  May  7 12:15:40.553: INFO: Got endpoints: latency-svc-r92zh [750.082472ms]
  May  7 12:15:40.603: INFO: Got endpoints: latency-svc-k9hrw [750.292546ms]
  May  7 12:15:40.655: INFO: Got endpoints: latency-svc-bb47b [749.142956ms]
  May  7 12:15:40.655: INFO: Latencies: [54.549716ms 55.072153ms 55.302404ms 64.12955ms 83.323588ms 83.657456ms 109.136528ms 111.606758ms 131.018249ms 131.785786ms 168.015496ms 169.790923ms 169.932914ms 170.17405ms 173.782296ms 175.021667ms 175.173706ms 177.005629ms 179.42145ms 185.218276ms 185.767491ms 196.881158ms 198.20216ms 198.718163ms 210.681965ms 212.958343ms 213.503865ms 214.727113ms 223.830278ms 229.074762ms 229.927582ms 233.562169ms 242.571619ms 248.52654ms 248.533602ms 253.187891ms 260.476642ms 297.908045ms 302.472466ms 302.946837ms 310.212934ms 312.950546ms 318.322076ms 318.326876ms 336.857497ms 337.058849ms 360.348651ms 419.473671ms 459.267673ms 508.052833ms 531.361605ms 580.395964ms 630.148032ms 680.073776ms 730.667956ms 730.698102ms 739.988843ms 742.581361ms 744.077924ms 745.603386ms 745.856901ms 745.891474ms 746.26049ms 746.414657ms 746.753467ms 746.791906ms 747.02554ms 747.192104ms 747.210618ms 747.740701ms 747.972706ms 747.99257ms 748.007355ms 748.027794ms 748.035904ms 748.170099ms 748.170883ms 748.211157ms 748.273526ms 748.308038ms 748.347737ms 748.369144ms 748.45995ms 748.607637ms 748.621129ms 748.667299ms 748.684089ms 748.746345ms 748.866544ms 748.882988ms 748.890719ms 748.890774ms 748.900085ms 748.903928ms 748.933985ms 748.938115ms 748.939645ms 748.955206ms 748.974152ms 749.02933ms 749.052036ms 749.058884ms 749.077749ms 749.127976ms 749.142956ms 749.152645ms 749.184308ms 749.23513ms 749.358421ms 749.36694ms 749.418137ms 749.432719ms 749.435746ms 749.463775ms 749.463852ms 749.49019ms 749.556899ms 749.572475ms 749.581277ms 749.591356ms 749.597564ms 749.668712ms 749.675879ms 749.706246ms 749.820201ms 749.820339ms 749.845302ms 749.890065ms 749.957258ms 749.978048ms 750.000586ms 750.022501ms 750.082472ms 750.086404ms 750.087511ms 750.174789ms 750.192261ms 750.206782ms 750.242436ms 750.256328ms 750.292546ms 750.351357ms 750.400065ms 750.407965ms 750.416619ms 750.445588ms 750.476216ms 750.494077ms 750.49903ms 750.514535ms 750.582743ms 750.590296ms 750.614901ms 750.623153ms 750.667644ms 750.674397ms 750.782081ms 750.81695ms 750.824482ms 750.840715ms 750.843582ms 750.845541ms 750.863805ms 750.934233ms 750.98532ms 751.066188ms 751.093997ms 751.149296ms 751.175054ms 751.182724ms 751.273968ms 751.32254ms 751.440968ms 751.453009ms 751.486381ms 751.550426ms 751.65274ms 751.685719ms 751.837574ms 751.845401ms 751.862724ms 752.003786ms 752.143722ms 752.252684ms 752.367554ms 752.587279ms 752.587781ms 752.664105ms 752.693189ms 753.179676ms 753.188059ms 753.296722ms 753.33115ms 753.359236ms 753.415471ms 753.494414ms 754.166768ms 754.506099ms 754.827363ms 755.474733ms]
  May  7 12:15:40.655: INFO: 50 %ile: 749.052036ms
  May  7 12:15:40.655: INFO: 90 %ile: 751.862724ms
  May  7 12:15:40.655: INFO: 99 %ile: 754.827363ms
  May  7 12:15:40.655: INFO: Total sample count: 200
  May  7 12:15:40.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-8316" for this suite. @ 05/07/23 12:15:40.659
• [10.744 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/07/23 12:15:40.664
  May  7 12:15:40.664: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename containers @ 05/07/23 12:15:40.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:15:40.68
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:15:40.681
  May  7 12:15:44.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6184" for this suite. @ 05/07/23 12:15:44.702
• [4.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 05/07/23 12:15:44.708
  May  7 12:15:44.708: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replicaset @ 05/07/23 12:15:44.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:15:44.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:15:44.718
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/07/23 12:15:44.72
  May  7 12:15:44.729: INFO: Pod name sample-pod: Found 0 pods out of 1
  May  7 12:15:49.731: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/07/23 12:15:49.731
  STEP: getting scale subresource @ 05/07/23 12:15:49.731
  STEP: updating a scale subresource @ 05/07/23 12:15:49.736
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/07/23 12:15:49.741
  STEP: Patch a scale subresource @ 05/07/23 12:15:49.745
  May  7 12:15:49.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9521" for this suite. @ 05/07/23 12:15:49.777
• [5.077 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 05/07/23 12:15:49.786
  May  7 12:15:49.786: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename init-container @ 05/07/23 12:15:49.787
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:15:49.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:15:49.801
  STEP: creating the pod @ 05/07/23 12:15:49.804
  May  7 12:15:49.804: INFO: PodSpec: initContainers in spec.initContainers
  May  7 12:15:53.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6237" for this suite. @ 05/07/23 12:15:53.311
• [3.527 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 05/07/23 12:15:53.314
  May  7 12:15:53.314: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 12:15:53.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:15:53.325
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:15:53.332
  STEP: creating service in namespace services-4050 @ 05/07/23 12:15:53.334
  STEP: creating service affinity-nodeport in namespace services-4050 @ 05/07/23 12:15:53.334
  STEP: creating replication controller affinity-nodeport in namespace services-4050 @ 05/07/23 12:15:53.34
  I0507 12:15:53.346780      20 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-4050, replica count: 3
  I0507 12:15:56.397008      20 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  7 12:15:56.400: INFO: Creating new exec pod
  May  7 12:15:59.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-4050 exec execpod-affinitydn9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  May  7 12:15:59.537: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  May  7 12:15:59.537: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:15:59.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-4050 exec execpod-affinitydn9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.28.58 80'
  May  7 12:15:59.649: INFO: stderr: "+ + ncecho -v hostName\n -t -w 2 10.68.28.58 80\nConnection to 10.68.28.58 80 port [tcp/http] succeeded!\n"
  May  7 12:15:59.649: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:15:59.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-4050 exec execpod-affinitydn9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.0.203 31358'
  May  7 12:15:59.743: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.0.203 31358\nConnection to 10.255.0.203 31358 port [tcp/*] succeeded!\n"
  May  7 12:15:59.743: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:15:59.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-4050 exec execpod-affinitydn9cr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.0.201 31358'
  May  7 12:15:59.854: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.0.201 31358\nConnection to 10.255.0.201 31358 port [tcp/*] succeeded!\n"
  May  7 12:15:59.854: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:15:59.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-4050 exec execpod-affinitydn9cr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.0.201:31358/ ; done'
  May  7 12:16:00.019: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31358/\n"
  May  7 12:16:00.019: INFO: stdout: "\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf\naffinity-nodeport-vtgwf"
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Received response from host: affinity-nodeport-vtgwf
  May  7 12:16:00.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 12:16:00.021: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-4050, will wait for the garbage collector to delete the pods @ 05/07/23 12:16:00.032
  May  7 12:16:00.086: INFO: Deleting ReplicationController affinity-nodeport took: 2.246078ms
  May  7 12:16:00.186: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.302141ms
  STEP: Destroying namespace "services-4050" for this suite. @ 05/07/23 12:16:03.502
• [10.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/07/23 12:16:03.508
  May  7 12:16:03.508: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename watch @ 05/07/23 12:16:03.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:03.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:03.528
  STEP: creating a watch on configmaps with a certain label @ 05/07/23 12:16:03.529
  STEP: creating a new configmap @ 05/07/23 12:16:03.53
  STEP: modifying the configmap once @ 05/07/23 12:16:03.532
  STEP: changing the label value of the configmap @ 05/07/23 12:16:03.537
  STEP: Expecting to observe a delete notification for the watched object @ 05/07/23 12:16:03.541
  May  7 12:16:03.541: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7463  1dc9446b-2707-47b1-b2fa-18fc6ec0e53d 56551 0 2023-05-07 12:16:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-07 12:16:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 12:16:03.541: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7463  1dc9446b-2707-47b1-b2fa-18fc6ec0e53d 56552 0 2023-05-07 12:16:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-07 12:16:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 12:16:03.541: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7463  1dc9446b-2707-47b1-b2fa-18fc6ec0e53d 56553 0 2023-05-07 12:16:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-07 12:16:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/07/23 12:16:03.541
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/07/23 12:16:03.546
  STEP: changing the label value of the configmap back @ 05/07/23 12:16:13.547
  STEP: modifying the configmap a third time @ 05/07/23 12:16:13.552
  STEP: deleting the configmap @ 05/07/23 12:16:13.554
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/07/23 12:16:13.56
  May  7 12:16:13.560: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7463  1dc9446b-2707-47b1-b2fa-18fc6ec0e53d 56599 0 2023-05-07 12:16:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-07 12:16:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 12:16:13.560: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7463  1dc9446b-2707-47b1-b2fa-18fc6ec0e53d 56600 0 2023-05-07 12:16:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-07 12:16:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 12:16:13.560: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7463  1dc9446b-2707-47b1-b2fa-18fc6ec0e53d 56601 0 2023-05-07 12:16:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-07 12:16:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 12:16:13.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7463" for this suite. @ 05/07/23 12:16:13.562
• [10.057 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 05/07/23 12:16:13.565
  May  7 12:16:13.565: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename lease-test @ 05/07/23 12:16:13.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:13.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:13.578
  May  7 12:16:13.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-2852" for this suite. @ 05/07/23 12:16:13.6
• [0.037 seconds]
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 05/07/23 12:16:13.603
  May  7 12:16:13.603: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename dns @ 05/07/23 12:16:13.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:13.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:13.612
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/07/23 12:16:13.614
  May  7 12:16:13.617: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1990  4fe3976d-3d47-417f-8785-e53212cfa0ff 56621 0 2023-05-07 12:16:13 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-07 12:16:13 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtcbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtcbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/07/23 12:16:15.622
  May  7 12:16:15.622: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1990 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:16:15.622: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:16:15.622: INFO: ExecWithOptions: Clientset creation
  May  7 12:16:15.622: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/dns-1990/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/07/23 12:16:15.693
  May  7 12:16:15.694: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1990 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:16:15.694: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:16:15.694: INFO: ExecWithOptions: Clientset creation
  May  7 12:16:15.694: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/dns-1990/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  7 12:16:15.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 12:16:15.748: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-1990" for this suite. @ 05/07/23 12:16:15.754
• [2.154 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 05/07/23 12:16:15.757
  May  7 12:16:15.757: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename events @ 05/07/23 12:16:15.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:15.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:15.769
  STEP: creating a test event @ 05/07/23 12:16:15.77
  STEP: listing all events in all namespaces @ 05/07/23 12:16:15.772
  STEP: patching the test event @ 05/07/23 12:16:15.774
  STEP: fetching the test event @ 05/07/23 12:16:15.776
  STEP: updating the test event @ 05/07/23 12:16:15.777
  STEP: getting the test event @ 05/07/23 12:16:15.781
  STEP: deleting the test event @ 05/07/23 12:16:15.782
  STEP: listing all events in all namespaces @ 05/07/23 12:16:15.785
  May  7 12:16:15.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3246" for this suite. @ 05/07/23 12:16:15.788
• [0.033 seconds]
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 05/07/23 12:16:15.79
  May  7 12:16:15.790: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename init-container @ 05/07/23 12:16:15.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:15.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:15.802
  STEP: creating the pod @ 05/07/23 12:16:15.803
  May  7 12:16:15.803: INFO: PodSpec: initContainers in spec.initContainers
  May  7 12:16:19.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5590" for this suite. @ 05/07/23 12:16:19.186
• [3.398 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/07/23 12:16:19.189
  May  7 12:16:19.189: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 12:16:19.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:19.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:19.2
  STEP: creating a Pod with a static label @ 05/07/23 12:16:19.208
  STEP: watching for Pod to be ready @ 05/07/23 12:16:19.212
  May  7 12:16:19.213: INFO: observed Pod pod-test in namespace pods-9404 in phase Pending with labels: map[test-pod-static:true] & conditions []
  May  7 12:16:19.215: INFO: observed Pod pod-test in namespace pods-9404 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:16:19 +0000 UTC  }]
  May  7 12:16:19.225: INFO: observed Pod pod-test in namespace pods-9404 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:16:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:16:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:16:19 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:16:19 +0000 UTC  }]
  May  7 12:16:20.186: INFO: Found Pod pod-test in namespace pods-9404 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:16:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:16:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:16:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:16:19 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 05/07/23 12:16:20.188
  STEP: getting the Pod and ensuring that it's patched @ 05/07/23 12:16:20.193
  STEP: replacing the Pod's status Ready condition to False @ 05/07/23 12:16:20.196
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/07/23 12:16:20.2
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/07/23 12:16:20.2
  STEP: watching for the Pod to be deleted @ 05/07/23 12:16:20.204
  May  7 12:16:20.205: INFO: observed event type MODIFIED
  May  7 12:16:22.191: INFO: observed event type MODIFIED
  May  7 12:16:22.351: INFO: observed event type MODIFIED
  May  7 12:16:23.195: INFO: observed event type MODIFIED
  May  7 12:16:23.200: INFO: observed event type MODIFIED
  May  7 12:16:23.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9404" for this suite. @ 05/07/23 12:16:23.206
• [4.020 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 05/07/23 12:16:23.209
  May  7 12:16:23.209: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl-logs @ 05/07/23 12:16:23.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:23.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:23.221
  STEP: creating an pod @ 05/07/23 12:16:23.222
  May  7 12:16:23.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-logs-1864 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  May  7 12:16:23.274: INFO: stderr: ""
  May  7 12:16:23.274: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/07/23 12:16:23.274
  May  7 12:16:23.274: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  May  7 12:16:25.278: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/07/23 12:16:25.278
  May  7 12:16:25.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-logs-1864 logs logs-generator logs-generator'
  May  7 12:16:25.366: INFO: stderr: ""
  May  7 12:16:25.366: INFO: stdout: "I0507 12:16:23.886712       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/4cwl 535\nI0507 12:16:24.087062       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/sft 561\nI0507 12:16:24.287321       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/5c4 221\nI0507 12:16:24.487587       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/jr92 404\nI0507 12:16:24.686786       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/lp4 588\nI0507 12:16:24.887064       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/lvk 290\nI0507 12:16:25.087338       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/pj7z 357\nI0507 12:16:25.287616       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/qhf 248\n"
  STEP: limiting log lines @ 05/07/23 12:16:25.366
  May  7 12:16:25.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-logs-1864 logs logs-generator logs-generator --tail=1'
  May  7 12:16:25.463: INFO: stderr: ""
  May  7 12:16:25.463: INFO: stdout: "I0507 12:16:25.287616       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/qhf 248\n"
  May  7 12:16:25.463: INFO: got output "I0507 12:16:25.287616       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/qhf 248\n"
  STEP: limiting log bytes @ 05/07/23 12:16:25.463
  May  7 12:16:25.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-logs-1864 logs logs-generator logs-generator --limit-bytes=1'
  May  7 12:16:25.544: INFO: stderr: ""
  May  7 12:16:25.544: INFO: stdout: "I"
  May  7 12:16:25.544: INFO: got output "I"
  STEP: exposing timestamps @ 05/07/23 12:16:25.544
  May  7 12:16:25.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-logs-1864 logs logs-generator logs-generator --tail=1 --timestamps'
  May  7 12:16:25.617: INFO: stderr: ""
  May  7 12:16:25.617: INFO: stdout: "2023-05-07T20:16:25.486890961+08:00 I0507 12:16:25.486835       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/7bv 352\n"
  May  7 12:16:25.617: INFO: got output "2023-05-07T20:16:25.486890961+08:00 I0507 12:16:25.486835       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/7bv 352\n"
  STEP: restricting to a time range @ 05/07/23 12:16:25.617
  May  7 12:16:28.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-logs-1864 logs logs-generator logs-generator --since=1s'
  May  7 12:16:28.165: INFO: stderr: ""
  May  7 12:16:28.165: INFO: stdout: "I0507 12:16:27.287268       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/nqx5 407\nI0507 12:16:27.487541       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/56jr 306\nI0507 12:16:27.686766       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/5ndq 472\nI0507 12:16:27.887043       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/br9 440\nI0507 12:16:28.087322       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/f77t 527\n"
  May  7 12:16:28.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-logs-1864 logs logs-generator logs-generator --since=24h'
  May  7 12:16:28.210: INFO: stderr: ""
  May  7 12:16:28.210: INFO: stdout: "I0507 12:16:23.886712       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/4cwl 535\nI0507 12:16:24.087062       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/sft 561\nI0507 12:16:24.287321       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/5c4 221\nI0507 12:16:24.487587       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/jr92 404\nI0507 12:16:24.686786       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/lp4 588\nI0507 12:16:24.887064       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/lvk 290\nI0507 12:16:25.087338       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/pj7z 357\nI0507 12:16:25.287616       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/qhf 248\nI0507 12:16:25.486835       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/7bv 352\nI0507 12:16:25.687110       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/gk8h 262\nI0507 12:16:25.887387       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/dnr4 257\nI0507 12:16:26.087661       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/nmr 275\nI0507 12:16:26.286884       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/8z4 498\nI0507 12:16:26.487162       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/vcp 267\nI0507 12:16:26.687441       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/qpv 454\nI0507 12:16:26.887722       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/kvv2 335\nI0507 12:16:27.086996       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/5cs 487\nI0507 12:16:27.287268       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/nqx5 407\nI0507 12:16:27.487541       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/56jr 306\nI0507 12:16:27.686766       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/5ndq 472\nI0507 12:16:27.887043       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/br9 440\nI0507 12:16:28.087322       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/f77t 527\n"
  May  7 12:16:28.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-logs-1864 delete pod logs-generator'
  May  7 12:16:29.216: INFO: stderr: ""
  May  7 12:16:29.216: INFO: stdout: "pod \"logs-generator\" deleted\n"
  May  7 12:16:29.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-1864" for this suite. @ 05/07/23 12:16:29.217
• [6.010 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/07/23 12:16:29.22
  May  7 12:16:29.220: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 12:16:29.221
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:29.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:29.235
  STEP: Saw pod success @ 05/07/23 12:16:35.274
  May  7 12:16:35.275: INFO: Trying to get logs from node 10.255.0.202 pod client-envvars-e05557b1-a61d-490b-8822-b65f0da0c889 container env3cont: <nil>
  STEP: delete the pod @ 05/07/23 12:16:35.277
  May  7 12:16:35.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4425" for this suite. @ 05/07/23 12:16:35.286
• [6.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/07/23 12:16:35.289
  May  7 12:16:35.289: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 12:16:35.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:35.299
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:35.301
  May  7 12:16:35.302: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: creating the pod @ 05/07/23 12:16:35.303
  STEP: submitting the pod to kubernetes @ 05/07/23 12:16:35.303
  May  7 12:16:37.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9689" for this suite. @ 05/07/23 12:16:37.322
• [2.035 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 05/07/23 12:16:37.325
  May  7 12:16:37.325: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename ingressclass @ 05/07/23 12:16:37.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:37.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:37.338
  STEP: getting /apis @ 05/07/23 12:16:37.34
  STEP: getting /apis/networking.k8s.io @ 05/07/23 12:16:37.342
  STEP: getting /apis/networking.k8s.iov1 @ 05/07/23 12:16:37.342
  STEP: creating @ 05/07/23 12:16:37.343
  STEP: getting @ 05/07/23 12:16:37.348
  STEP: listing @ 05/07/23 12:16:37.349
  STEP: watching @ 05/07/23 12:16:37.35
  May  7 12:16:37.350: INFO: starting watch
  STEP: patching @ 05/07/23 12:16:37.35
  STEP: updating @ 05/07/23 12:16:37.353
  May  7 12:16:37.355: INFO: waiting for watch events with expected annotations
  May  7 12:16:37.355: INFO: saw patched and updated annotations
  STEP: deleting @ 05/07/23 12:16:37.355
  STEP: deleting a collection @ 05/07/23 12:16:37.359
  May  7 12:16:37.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-9939" for this suite. @ 05/07/23 12:16:37.367
• [0.044 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 05/07/23 12:16:37.37
  May  7 12:16:37.370: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:16:37.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:37.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:37.38
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 12:16:37.382
  STEP: Saw pod success @ 05/07/23 12:16:41.392
  May  7 12:16:41.393: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-38e83355-8103-4c20-9a80-4a30da49cd21 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 12:16:41.396
  May  7 12:16:41.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7690" for this suite. @ 05/07/23 12:16:41.404
• [4.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 05/07/23 12:16:41.407
  May  7 12:16:41.407: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename server-version @ 05/07/23 12:16:41.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:41.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:41.416
  STEP: Request ServerVersion @ 05/07/23 12:16:41.418
  STEP: Confirm major version @ 05/07/23 12:16:41.419
  May  7 12:16:41.419: INFO: Major version: 1
  STEP: Confirm minor version @ 05/07/23 12:16:41.419
  May  7 12:16:41.419: INFO: cleanMinorVersion: 27
  May  7 12:16:41.419: INFO: Minor version: 27
  May  7 12:16:41.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-6592" for this suite. @ 05/07/23 12:16:41.42
• [0.016 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 05/07/23 12:16:41.423
  May  7 12:16:41.423: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename gc @ 05/07/23 12:16:41.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:16:41.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:16:41.434
  STEP: create the rc @ 05/07/23 12:16:41.437
  W0507 12:16:41.440264      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/07/23 12:16:47.508
  STEP: wait for the rc to be deleted @ 05/07/23 12:16:47.57
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/07/23 12:16:52.573
  STEP: Gathering metrics @ 05/07/23 12:17:22.58
  W0507 12:17:22.583483      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  7 12:17:22.583: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  7 12:17:22.583: INFO: Deleting pod "simpletest.rc-26db6" in namespace "gc-2138"
  May  7 12:17:22.604: INFO: Deleting pod "simpletest.rc-29s7p" in namespace "gc-2138"
  May  7 12:17:22.620: INFO: Deleting pod "simpletest.rc-2chxw" in namespace "gc-2138"
  May  7 12:17:22.630: INFO: Deleting pod "simpletest.rc-2mj69" in namespace "gc-2138"
  May  7 12:17:22.644: INFO: Deleting pod "simpletest.rc-2x776" in namespace "gc-2138"
  May  7 12:17:22.668: INFO: Deleting pod "simpletest.rc-4dgv4" in namespace "gc-2138"
  May  7 12:17:22.686: INFO: Deleting pod "simpletest.rc-4gpdk" in namespace "gc-2138"
  May  7 12:17:22.713: INFO: Deleting pod "simpletest.rc-4lzj6" in namespace "gc-2138"
  May  7 12:17:22.735: INFO: Deleting pod "simpletest.rc-4pqgs" in namespace "gc-2138"
  May  7 12:17:22.762: INFO: Deleting pod "simpletest.rc-4tt6c" in namespace "gc-2138"
  May  7 12:17:22.787: INFO: Deleting pod "simpletest.rc-4v98r" in namespace "gc-2138"
  May  7 12:17:22.795: INFO: Deleting pod "simpletest.rc-56tpp" in namespace "gc-2138"
  May  7 12:17:22.815: INFO: Deleting pod "simpletest.rc-5k7x7" in namespace "gc-2138"
  May  7 12:17:22.828: INFO: Deleting pod "simpletest.rc-5r2sc" in namespace "gc-2138"
  May  7 12:17:22.837: INFO: Deleting pod "simpletest.rc-5z86c" in namespace "gc-2138"
  May  7 12:17:22.858: INFO: Deleting pod "simpletest.rc-69jqh" in namespace "gc-2138"
  May  7 12:17:22.880: INFO: Deleting pod "simpletest.rc-6nnp8" in namespace "gc-2138"
  May  7 12:17:22.892: INFO: Deleting pod "simpletest.rc-7px85" in namespace "gc-2138"
  May  7 12:17:22.906: INFO: Deleting pod "simpletest.rc-7wk8n" in namespace "gc-2138"
  May  7 12:17:22.917: INFO: Deleting pod "simpletest.rc-8c67b" in namespace "gc-2138"
  May  7 12:17:22.933: INFO: Deleting pod "simpletest.rc-8pvq2" in namespace "gc-2138"
  May  7 12:17:22.942: INFO: Deleting pod "simpletest.rc-8vjdw" in namespace "gc-2138"
  May  7 12:17:22.951: INFO: Deleting pod "simpletest.rc-8z996" in namespace "gc-2138"
  May  7 12:17:22.967: INFO: Deleting pod "simpletest.rc-926tq" in namespace "gc-2138"
  May  7 12:17:22.981: INFO: Deleting pod "simpletest.rc-948sb" in namespace "gc-2138"
  May  7 12:17:22.995: INFO: Deleting pod "simpletest.rc-9gs6l" in namespace "gc-2138"
  May  7 12:17:23.009: INFO: Deleting pod "simpletest.rc-9qj9q" in namespace "gc-2138"
  May  7 12:17:23.025: INFO: Deleting pod "simpletest.rc-9rb24" in namespace "gc-2138"
  May  7 12:17:23.060: INFO: Deleting pod "simpletest.rc-9rh4b" in namespace "gc-2138"
  May  7 12:17:23.073: INFO: Deleting pod "simpletest.rc-bj2lz" in namespace "gc-2138"
  May  7 12:17:23.092: INFO: Deleting pod "simpletest.rc-csgsz" in namespace "gc-2138"
  May  7 12:17:23.112: INFO: Deleting pod "simpletest.rc-d68mz" in namespace "gc-2138"
  May  7 12:17:23.136: INFO: Deleting pod "simpletest.rc-dcg4d" in namespace "gc-2138"
  May  7 12:17:23.181: INFO: Deleting pod "simpletest.rc-dmdpr" in namespace "gc-2138"
  May  7 12:17:23.231: INFO: Deleting pod "simpletest.rc-dn84r" in namespace "gc-2138"
  May  7 12:17:23.269: INFO: Deleting pod "simpletest.rc-dxgrm" in namespace "gc-2138"
  May  7 12:17:23.306: INFO: Deleting pod "simpletest.rc-f6kzz" in namespace "gc-2138"
  May  7 12:17:23.331: INFO: Deleting pod "simpletest.rc-fb572" in namespace "gc-2138"
  May  7 12:17:23.351: INFO: Deleting pod "simpletest.rc-ff4nz" in namespace "gc-2138"
  May  7 12:17:23.373: INFO: Deleting pod "simpletest.rc-fmns5" in namespace "gc-2138"
  May  7 12:17:23.391: INFO: Deleting pod "simpletest.rc-fmrmb" in namespace "gc-2138"
  May  7 12:17:23.404: INFO: Deleting pod "simpletest.rc-fz5hm" in namespace "gc-2138"
  May  7 12:17:23.425: INFO: Deleting pod "simpletest.rc-g4j76" in namespace "gc-2138"
  May  7 12:17:23.457: INFO: Deleting pod "simpletest.rc-grr5h" in namespace "gc-2138"
  May  7 12:17:23.472: INFO: Deleting pod "simpletest.rc-gx5nf" in namespace "gc-2138"
  May  7 12:17:23.489: INFO: Deleting pod "simpletest.rc-h5gm8" in namespace "gc-2138"
  May  7 12:17:23.498: INFO: Deleting pod "simpletest.rc-hbqc8" in namespace "gc-2138"
  May  7 12:17:23.517: INFO: Deleting pod "simpletest.rc-hg7sc" in namespace "gc-2138"
  May  7 12:17:23.542: INFO: Deleting pod "simpletest.rc-hzsbs" in namespace "gc-2138"
  May  7 12:17:23.554: INFO: Deleting pod "simpletest.rc-j4hg4" in namespace "gc-2138"
  May  7 12:17:23.575: INFO: Deleting pod "simpletest.rc-j6pgg" in namespace "gc-2138"
  May  7 12:17:23.586: INFO: Deleting pod "simpletest.rc-jj972" in namespace "gc-2138"
  May  7 12:17:23.604: INFO: Deleting pod "simpletest.rc-jmzxf" in namespace "gc-2138"
  May  7 12:17:23.620: INFO: Deleting pod "simpletest.rc-jn889" in namespace "gc-2138"
  May  7 12:17:23.635: INFO: Deleting pod "simpletest.rc-jtqrj" in namespace "gc-2138"
  May  7 12:17:23.653: INFO: Deleting pod "simpletest.rc-k66hp" in namespace "gc-2138"
  May  7 12:17:23.669: INFO: Deleting pod "simpletest.rc-l8x69" in namespace "gc-2138"
  May  7 12:17:23.702: INFO: Deleting pod "simpletest.rc-lgngm" in namespace "gc-2138"
  May  7 12:17:23.717: INFO: Deleting pod "simpletest.rc-lgsck" in namespace "gc-2138"
  May  7 12:17:23.732: INFO: Deleting pod "simpletest.rc-m45vb" in namespace "gc-2138"
  May  7 12:17:23.753: INFO: Deleting pod "simpletest.rc-n482w" in namespace "gc-2138"
  May  7 12:17:23.788: INFO: Deleting pod "simpletest.rc-p88fc" in namespace "gc-2138"
  May  7 12:17:23.814: INFO: Deleting pod "simpletest.rc-pg7tn" in namespace "gc-2138"
  May  7 12:17:23.840: INFO: Deleting pod "simpletest.rc-pp6r6" in namespace "gc-2138"
  May  7 12:17:23.866: INFO: Deleting pod "simpletest.rc-pp8cv" in namespace "gc-2138"
  May  7 12:17:23.888: INFO: Deleting pod "simpletest.rc-pslzc" in namespace "gc-2138"
  May  7 12:17:23.906: INFO: Deleting pod "simpletest.rc-pxqpv" in namespace "gc-2138"
  May  7 12:17:23.927: INFO: Deleting pod "simpletest.rc-pxzzr" in namespace "gc-2138"
  May  7 12:17:23.944: INFO: Deleting pod "simpletest.rc-q5bhv" in namespace "gc-2138"
  May  7 12:17:23.969: INFO: Deleting pod "simpletest.rc-q9cxg" in namespace "gc-2138"
  May  7 12:17:23.989: INFO: Deleting pod "simpletest.rc-qwdsx" in namespace "gc-2138"
  May  7 12:17:24.015: INFO: Deleting pod "simpletest.rc-r2t67" in namespace "gc-2138"
  May  7 12:17:24.073: INFO: Deleting pod "simpletest.rc-rgwvw" in namespace "gc-2138"
  May  7 12:17:24.089: INFO: Deleting pod "simpletest.rc-rn54p" in namespace "gc-2138"
  May  7 12:17:24.111: INFO: Deleting pod "simpletest.rc-rqkw9" in namespace "gc-2138"
  May  7 12:17:24.128: INFO: Deleting pod "simpletest.rc-slndr" in namespace "gc-2138"
  May  7 12:17:24.146: INFO: Deleting pod "simpletest.rc-sr8zh" in namespace "gc-2138"
  May  7 12:17:24.164: INFO: Deleting pod "simpletest.rc-ss9hg" in namespace "gc-2138"
  May  7 12:17:24.188: INFO: Deleting pod "simpletest.rc-sszjs" in namespace "gc-2138"
  May  7 12:17:24.210: INFO: Deleting pod "simpletest.rc-t7nks" in namespace "gc-2138"
  May  7 12:17:24.219: INFO: Deleting pod "simpletest.rc-tfz2v" in namespace "gc-2138"
  May  7 12:17:24.267: INFO: Deleting pod "simpletest.rc-tn9hr" in namespace "gc-2138"
  May  7 12:17:24.310: INFO: Deleting pod "simpletest.rc-v6sxc" in namespace "gc-2138"
  May  7 12:17:24.354: INFO: Deleting pod "simpletest.rc-vcqmk" in namespace "gc-2138"
  May  7 12:17:24.390: INFO: Deleting pod "simpletest.rc-vdw69" in namespace "gc-2138"
  May  7 12:17:24.437: INFO: Deleting pod "simpletest.rc-vl6nb" in namespace "gc-2138"
  May  7 12:17:24.491: INFO: Deleting pod "simpletest.rc-vx9kh" in namespace "gc-2138"
  May  7 12:17:24.555: INFO: Deleting pod "simpletest.rc-w4g9p" in namespace "gc-2138"
  May  7 12:17:24.590: INFO: Deleting pod "simpletest.rc-wclvk" in namespace "gc-2138"
  May  7 12:17:24.656: INFO: Deleting pod "simpletest.rc-whfcl" in namespace "gc-2138"
  May  7 12:17:24.685: INFO: Deleting pod "simpletest.rc-x82xf" in namespace "gc-2138"
  May  7 12:17:24.740: INFO: Deleting pod "simpletest.rc-xc4x9" in namespace "gc-2138"
  May  7 12:17:24.781: INFO: Deleting pod "simpletest.rc-xr6tl" in namespace "gc-2138"
  May  7 12:17:24.832: INFO: Deleting pod "simpletest.rc-xskr6" in namespace "gc-2138"
  May  7 12:17:24.892: INFO: Deleting pod "simpletest.rc-xvr5x" in namespace "gc-2138"
  May  7 12:17:25.050: INFO: Deleting pod "simpletest.rc-xzh8n" in namespace "gc-2138"
  May  7 12:17:25.109: INFO: Deleting pod "simpletest.rc-znhfc" in namespace "gc-2138"
  May  7 12:17:25.137: INFO: Deleting pod "simpletest.rc-zszsk" in namespace "gc-2138"
  May  7 12:17:25.199: INFO: Deleting pod "simpletest.rc-zv8cc" in namespace "gc-2138"
  May  7 12:17:25.220: INFO: Deleting pod "simpletest.rc-zzvmz" in namespace "gc-2138"
  May  7 12:17:25.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2138" for this suite. @ 05/07/23 12:17:25.266
• [43.876 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/07/23 12:17:25.3
  May  7 12:17:25.300: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 12:17:25.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:17:25.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:17:25.407
  STEP: creating the pod @ 05/07/23 12:17:25.415
  STEP: submitting the pod to kubernetes @ 05/07/23 12:17:25.415
  STEP: verifying QOS class is set on the pod @ 05/07/23 12:17:25.455
  May  7 12:17:25.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6382" for this suite. @ 05/07/23 12:17:25.487
• [0.208 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 05/07/23 12:17:25.509
  May  7 12:17:25.509: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 12:17:25.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:17:25.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:17:25.532
  STEP: Creating configMap with name configmap-test-volume-ab6904f5-200d-4ad1-8a1e-ecea0171621c @ 05/07/23 12:17:25.565
  STEP: Creating a pod to test consume configMaps @ 05/07/23 12:17:25.568
  STEP: Saw pod success @ 05/07/23 12:17:29.625
  May  7 12:17:29.627: INFO: Trying to get logs from node 10.255.0.202 pod pod-configmaps-673a2b8f-035d-4e33-bab5-f1916bf5f9b7 container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 12:17:29.639
  May  7 12:17:29.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6429" for this suite. @ 05/07/23 12:17:29.659
• [4.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 05/07/23 12:17:29.665
  May  7 12:17:29.665: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename cronjob @ 05/07/23 12:17:29.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:17:29.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:17:29.695
  STEP: Creating a cronjob @ 05/07/23 12:17:29.697
  STEP: creating @ 05/07/23 12:17:29.697
  STEP: getting @ 05/07/23 12:17:29.708
  STEP: listing @ 05/07/23 12:17:29.71
  STEP: watching @ 05/07/23 12:17:29.714
  May  7 12:17:29.714: INFO: starting watch
  STEP: cluster-wide listing @ 05/07/23 12:17:29.715
  STEP: cluster-wide watching @ 05/07/23 12:17:29.716
  May  7 12:17:29.716: INFO: starting watch
  STEP: patching @ 05/07/23 12:17:29.717
  STEP: updating @ 05/07/23 12:17:29.733
  May  7 12:17:29.739: INFO: waiting for watch events with expected annotations
  May  7 12:17:29.739: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/07/23 12:17:29.739
  STEP: updating /status @ 05/07/23 12:17:29.745
  STEP: get /status @ 05/07/23 12:17:29.749
  STEP: deleting @ 05/07/23 12:17:29.751
  STEP: deleting a collection @ 05/07/23 12:17:29.758
  May  7 12:17:29.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6581" for this suite. @ 05/07/23 12:17:29.763
• [0.102 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 05/07/23 12:17:29.767
  May  7 12:17:29.767: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 12:17:29.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:17:29.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:17:29.788
  STEP: creating service in namespace services-7686 @ 05/07/23 12:17:29.796
  STEP: creating service affinity-clusterip in namespace services-7686 @ 05/07/23 12:17:29.796
  STEP: creating replication controller affinity-clusterip in namespace services-7686 @ 05/07/23 12:17:29.823
  I0507 12:17:29.841026      20 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-7686, replica count: 3
  I0507 12:17:32.892077      20 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  7 12:17:32.895: INFO: Creating new exec pod
  May  7 12:17:35.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-7686 exec execpod-affinitylc2rx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  May  7 12:17:36.014: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  May  7 12:17:36.014: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:17:36.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-7686 exec execpod-affinitylc2rx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.161.147 80'
  May  7 12:17:36.120: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.161.147 80\nConnection to 10.68.161.147 80 port [tcp/http] succeeded!\n"
  May  7 12:17:36.120: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:17:36.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-7686 exec execpod-affinitylc2rx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.68.161.147:80/ ; done'
  May  7 12:17:36.255: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.161.147:80/\n"
  May  7 12:17:36.255: INFO: stdout: "\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8\naffinity-clusterip-wl6n8"
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Received response from host: affinity-clusterip-wl6n8
  May  7 12:17:36.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 12:17:36.257: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-7686, will wait for the garbage collector to delete the pods @ 05/07/23 12:17:36.264
  May  7 12:17:36.321: INFO: Deleting ReplicationController affinity-clusterip took: 4.132314ms
  May  7 12:17:36.421: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.270972ms
  STEP: Destroying namespace "services-7686" for this suite. @ 05/07/23 12:17:38.336
• [8.574 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 05/07/23 12:17:38.342
  May  7 12:17:38.342: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename endpointslice @ 05/07/23 12:17:38.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:17:38.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:17:38.367
  STEP: getting /apis @ 05/07/23 12:17:38.37
  STEP: getting /apis/discovery.k8s.io @ 05/07/23 12:17:38.375
  STEP: getting /apis/discovery.k8s.iov1 @ 05/07/23 12:17:38.376
  STEP: creating @ 05/07/23 12:17:38.376
  STEP: getting @ 05/07/23 12:17:38.386
  STEP: listing @ 05/07/23 12:17:38.387
  STEP: watching @ 05/07/23 12:17:38.389
  May  7 12:17:38.389: INFO: starting watch
  STEP: cluster-wide listing @ 05/07/23 12:17:38.389
  STEP: cluster-wide watching @ 05/07/23 12:17:38.39
  May  7 12:17:38.391: INFO: starting watch
  STEP: patching @ 05/07/23 12:17:38.391
  STEP: updating @ 05/07/23 12:17:38.393
  May  7 12:17:38.403: INFO: waiting for watch events with expected annotations
  May  7 12:17:38.403: INFO: saw patched and updated annotations
  STEP: deleting @ 05/07/23 12:17:38.403
  STEP: deleting a collection @ 05/07/23 12:17:38.415
  May  7 12:17:38.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9513" for this suite. @ 05/07/23 12:17:38.426
• [0.088 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 05/07/23 12:17:38.43
  May  7 12:17:38.430: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename var-expansion @ 05/07/23 12:17:38.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:17:38.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:17:38.442
  STEP: creating the pod with failed condition @ 05/07/23 12:17:38.444
  STEP: updating the pod @ 05/07/23 12:19:38.455
  May  7 12:19:38.961: INFO: Successfully updated pod "var-expansion-618a8dcc-d49e-45a6-add4-01b2d046880c"
  STEP: waiting for pod running @ 05/07/23 12:19:38.961
  STEP: deleting the pod gracefully @ 05/07/23 12:19:40.966
  May  7 12:19:40.966: INFO: Deleting pod "var-expansion-618a8dcc-d49e-45a6-add4-01b2d046880c" in namespace "var-expansion-2389"
  May  7 12:19:40.971: INFO: Wait up to 5m0s for pod "var-expansion-618a8dcc-d49e-45a6-add4-01b2d046880c" to be fully deleted
  May  7 12:20:13.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2389" for this suite. @ 05/07/23 12:20:13.011
• [154.585 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 05/07/23 12:20:13.015
  May  7 12:20:13.015: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 12:20:13.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:20:13.027
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:20:13.028
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/07/23 12:20:13.03
  STEP: Saw pod success @ 05/07/23 12:20:17.039
  May  7 12:20:17.040: INFO: Trying to get logs from node 10.255.0.202 pod pod-f02e9593-57ad-4405-8b4a-0742eb8d72b6 container test-container: <nil>
  STEP: delete the pod @ 05/07/23 12:20:17.049
  May  7 12:20:17.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9888" for this suite. @ 05/07/23 12:20:17.063
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 05/07/23 12:20:17.067
  May  7 12:20:17.067: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-probe @ 05/07/23 12:20:17.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:20:17.075
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:20:17.077
  STEP: Creating pod busybox-cb58ae39-cb52-46e1-98e7-9f49d87ce58a in namespace container-probe-9218 @ 05/07/23 12:20:17.079
  May  7 12:20:19.087: INFO: Started pod busybox-cb58ae39-cb52-46e1-98e7-9f49d87ce58a in namespace container-probe-9218
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/07/23 12:20:19.087
  May  7 12:20:19.089: INFO: Initial restart count of pod busybox-cb58ae39-cb52-46e1-98e7-9f49d87ce58a is 0
  May  7 12:21:09.138: INFO: Restart count of pod container-probe-9218/busybox-cb58ae39-cb52-46e1-98e7-9f49d87ce58a is now 1 (50.049006398s elapsed)
  May  7 12:21:09.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:21:09.139
  STEP: Destroying namespace "container-probe-9218" for this suite. @ 05/07/23 12:21:09.144
• [52.081 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 05/07/23 12:21:09.148
  May  7 12:21:09.148: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:21:09.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:09.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:09.159
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 12:21:09.16
  STEP: Saw pod success @ 05/07/23 12:21:13.172
  May  7 12:21:13.174: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-e6deb4f5-ee0a-4124-ae8a-7e0d3fd8b3d1 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 12:21:13.176
  May  7 12:21:13.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5049" for this suite. @ 05/07/23 12:21:13.194
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 05/07/23 12:21:13.198
  May  7 12:21:13.199: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:21:13.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:13.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:13.209
  STEP: Setting up server cert @ 05/07/23 12:21:13.224
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:21:13.755
  STEP: Deploying the webhook pod @ 05/07/23 12:21:13.759
  STEP: Wait for the deployment to be ready @ 05/07/23 12:21:13.765
  May  7 12:21:13.773: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/07/23 12:21:15.777
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:21:15.781
  May  7 12:21:16.781: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/07/23 12:21:16.783
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/07/23 12:21:16.783
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/07/23 12:21:16.792
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/07/23 12:21:17.796
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/07/23 12:21:17.796
  STEP: Having no error when timeout is longer than webhook latency @ 05/07/23 12:21:18.808
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/07/23 12:21:18.808
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/07/23 12:21:23.824
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/07/23 12:21:23.824
  May  7 12:21:28.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6301" for this suite. @ 05/07/23 12:21:28.877
  STEP: Destroying namespace "webhook-markers-783" for this suite. @ 05/07/23 12:21:28.881
• [15.697 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 05/07/23 12:21:28.896
  May  7 12:21:28.896: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/07/23 12:21:28.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:28.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:28.911
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/07/23 12:21:28.913
  May  7 12:21:28.913: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:21:30.110: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:21:34.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8530" for this suite. @ 05/07/23 12:21:34.99
• [6.096 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 05/07/23 12:21:34.993
  May  7 12:21:34.993: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 12:21:34.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:35.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:35.005
  STEP: creating service in namespace services-2314 @ 05/07/23 12:21:35.007
  STEP: creating service affinity-nodeport-transition in namespace services-2314 @ 05/07/23 12:21:35.007
  STEP: creating replication controller affinity-nodeport-transition in namespace services-2314 @ 05/07/23 12:21:35.012
  I0507 12:21:35.023255      20 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-2314, replica count: 3
  I0507 12:21:38.073680      20 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  7 12:21:38.078: INFO: Creating new exec pod
  May  7 12:21:41.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-2314 exec execpod-affinity8xgf4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  May  7 12:21:41.190: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  May  7 12:21:41.190: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:21:41.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-2314 exec execpod-affinity8xgf4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.255.85 80'
  May  7 12:21:41.287: INFO: stderr: "+ nc -v -t -w 2 10.68.255.85 80\n+ echo hostName\nConnection to 10.68.255.85 80 port [tcp/http] succeeded!\n"
  May  7 12:21:41.287: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:21:41.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-2314 exec execpod-affinity8xgf4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.0.203 31012'
  May  7 12:21:41.373: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.0.203 31012\nConnection to 10.255.0.203 31012 port [tcp/*] succeeded!\n"
  May  7 12:21:41.373: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:21:41.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-2314 exec execpod-affinity8xgf4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.0.202 31012'
  May  7 12:21:41.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.0.202 31012\nConnection to 10.255.0.202 31012 port [tcp/*] succeeded!\n"
  May  7 12:21:41.440: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:21:41.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-2314 exec execpod-affinity8xgf4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.0.201:31012/ ; done'
  May  7 12:21:41.604: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n"
  May  7 12:21:41.604: INFO: stdout: "\naffinity-nodeport-transition-wm4ts\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-5kpl4\naffinity-nodeport-transition-wm4ts\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-5kpl4\naffinity-nodeport-transition-wm4ts\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-5kpl4\naffinity-nodeport-transition-wm4ts\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-5kpl4\naffinity-nodeport-transition-wm4ts\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-5kpl4\naffinity-nodeport-transition-wm4ts"
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-wm4ts
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-5kpl4
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-wm4ts
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-5kpl4
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-wm4ts
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-5kpl4
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-wm4ts
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-5kpl4
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-wm4ts
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-5kpl4
  May  7 12:21:41.604: INFO: Received response from host: affinity-nodeport-transition-wm4ts
  May  7 12:21:41.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-2314 exec execpod-affinity8xgf4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.0.201:31012/ ; done'
  May  7 12:21:41.766: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.0.201:31012/\n"
  May  7 12:21:41.766: INFO: stdout: "\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g\naffinity-nodeport-transition-hnz7g"
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Received response from host: affinity-nodeport-transition-hnz7g
  May  7 12:21:41.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 12:21:41.768: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2314, will wait for the garbage collector to delete the pods @ 05/07/23 12:21:41.777
  May  7 12:21:41.832: INFO: Deleting ReplicationController affinity-nodeport-transition took: 2.226735ms
  May  7 12:21:41.932: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.589021ms
  STEP: Destroying namespace "services-2314" for this suite. @ 05/07/23 12:21:43.751
• [8.762 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 05/07/23 12:21:43.755
  May  7 12:21:43.755: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename disruption @ 05/07/23 12:21:43.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:43.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:43.776
  STEP: Creating a kubernetes client @ 05/07/23 12:21:43.778
  May  7 12:21:43.778: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename disruption-2 @ 05/07/23 12:21:43.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:43.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:43.797
  STEP: Waiting for the pdb to be processed @ 05/07/23 12:21:43.802
  STEP: Waiting for the pdb to be processed @ 05/07/23 12:21:45.808
  STEP: Waiting for the pdb to be processed @ 05/07/23 12:21:47.813
  STEP: listing a collection of PDBs across all namespaces @ 05/07/23 12:21:49.817
  STEP: listing a collection of PDBs in namespace disruption-7248 @ 05/07/23 12:21:49.819
  STEP: deleting a collection of PDBs @ 05/07/23 12:21:49.82
  STEP: Waiting for the PDB collection to be deleted @ 05/07/23 12:21:49.824
  May  7 12:21:49.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 12:21:49.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-5475" for this suite. @ 05/07/23 12:21:49.828
  STEP: Destroying namespace "disruption-7248" for this suite. @ 05/07/23 12:21:49.831
• [6.079 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/07/23 12:21:49.834
  May  7 12:21:49.834: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename watch @ 05/07/23 12:21:49.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:49.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:49.845
  STEP: creating a watch on configmaps @ 05/07/23 12:21:49.847
  STEP: creating a new configmap @ 05/07/23 12:21:49.848
  STEP: modifying the configmap once @ 05/07/23 12:21:49.85
  STEP: closing the watch once it receives two notifications @ 05/07/23 12:21:49.853
  May  7 12:21:49.853: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8707  1d1e2c3d-f035-4e1a-a5d7-4f9258f2518a 60543 0 2023-05-07 12:21:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-07 12:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 12:21:49.853: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8707  1d1e2c3d-f035-4e1a-a5d7-4f9258f2518a 60544 0 2023-05-07 12:21:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-07 12:21:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/07/23 12:21:49.853
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/07/23 12:21:49.856
  STEP: deleting the configmap @ 05/07/23 12:21:49.857
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/07/23 12:21:49.867
  May  7 12:21:49.867: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8707  1d1e2c3d-f035-4e1a-a5d7-4f9258f2518a 60545 0 2023-05-07 12:21:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-07 12:21:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 12:21:49.867: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8707  1d1e2c3d-f035-4e1a-a5d7-4f9258f2518a 60546 0 2023-05-07 12:21:49 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-07 12:21:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 12:21:49.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8707" for this suite. @ 05/07/23 12:21:49.869
• [0.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/07/23 12:21:49.873
  May  7 12:21:49.873: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-runtime @ 05/07/23 12:21:49.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:49.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:49.884
  STEP: create the container @ 05/07/23 12:21:49.885
  W0507 12:21:49.889626      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/07/23 12:21:49.889
  STEP: get the container status @ 05/07/23 12:21:52.897
  STEP: the container should be terminated @ 05/07/23 12:21:52.899
  STEP: the termination message should be set @ 05/07/23 12:21:52.899
  May  7 12:21:52.899: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/07/23 12:21:52.899
  May  7 12:21:52.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-3872" for this suite. @ 05/07/23 12:21:52.907
• [3.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 05/07/23 12:21:52.91
  May  7 12:21:52.910: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 12:21:52.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:52.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:52.922
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/07/23 12:21:52.923
  May  7 12:21:52.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-7408 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May  7 12:21:52.968: INFO: stderr: ""
  May  7 12:21:52.968: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/07/23 12:21:52.968
  May  7 12:21:52.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-7408 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  May  7 12:21:53.015: INFO: stderr: ""
  May  7 12:21:53.015: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/07/23 12:21:53.015
  May  7 12:21:53.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-7408 delete pods e2e-test-httpd-pod'
  May  7 12:21:55.449: INFO: stderr: ""
  May  7 12:21:55.449: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May  7 12:21:55.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7408" for this suite. @ 05/07/23 12:21:55.451
• [2.544 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/07/23 12:21:55.454
  May  7 12:21:55.454: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 12:21:55.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:55.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:55.465
  STEP: Creating secret with name secret-test-2b912827-bb42-4c33-8442-8d4fadc51e1c @ 05/07/23 12:21:55.478
  STEP: Creating a pod to test consume secrets @ 05/07/23 12:21:55.48
  STEP: Saw pod success @ 05/07/23 12:21:59.49
  May  7 12:21:59.492: INFO: Trying to get logs from node 10.255.0.202 pod pod-secrets-a03f4f2b-4528-443b-8daa-f1c482ed7f78 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 12:21:59.494
  May  7 12:21:59.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9251" for this suite. @ 05/07/23 12:21:59.502
  STEP: Destroying namespace "secret-namespace-9281" for this suite. @ 05/07/23 12:21:59.505
• [4.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 05/07/23 12:21:59.51
  May  7 12:21:59.510: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 12:21:59.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:59.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:59.52
  STEP: creating a ConfigMap @ 05/07/23 12:21:59.522
  STEP: fetching the ConfigMap @ 05/07/23 12:21:59.524
  STEP: patching the ConfigMap @ 05/07/23 12:21:59.525
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/07/23 12:21:59.527
  STEP: deleting the ConfigMap by collection with a label selector @ 05/07/23 12:21:59.528
  STEP: listing all ConfigMaps in test namespace @ 05/07/23 12:21:59.531
  May  7 12:21:59.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-101" for this suite. @ 05/07/23 12:21:59.533
• [0.026 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 05/07/23 12:21:59.536
  May  7 12:21:59.536: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename discovery @ 05/07/23 12:21:59.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:59.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:59.553
  STEP: Setting up server cert @ 05/07/23 12:21:59.562
  May  7 12:21:59.810: INFO: Checking APIGroup: apiregistration.k8s.io
  May  7 12:21:59.811: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  May  7 12:21:59.811: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  May  7 12:21:59.811: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  May  7 12:21:59.811: INFO: Checking APIGroup: apps
  May  7 12:21:59.811: INFO: PreferredVersion.GroupVersion: apps/v1
  May  7 12:21:59.811: INFO: Versions found [{apps/v1 v1}]
  May  7 12:21:59.811: INFO: apps/v1 matches apps/v1
  May  7 12:21:59.811: INFO: Checking APIGroup: events.k8s.io
  May  7 12:21:59.811: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  May  7 12:21:59.811: INFO: Versions found [{events.k8s.io/v1 v1}]
  May  7 12:21:59.811: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  May  7 12:21:59.811: INFO: Checking APIGroup: authentication.k8s.io
  May  7 12:21:59.812: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  May  7 12:21:59.812: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  May  7 12:21:59.812: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  May  7 12:21:59.812: INFO: Checking APIGroup: authorization.k8s.io
  May  7 12:21:59.812: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  May  7 12:21:59.812: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  May  7 12:21:59.812: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  May  7 12:21:59.812: INFO: Checking APIGroup: autoscaling
  May  7 12:21:59.813: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  May  7 12:21:59.813: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  May  7 12:21:59.813: INFO: autoscaling/v2 matches autoscaling/v2
  May  7 12:21:59.813: INFO: Checking APIGroup: batch
  May  7 12:21:59.813: INFO: PreferredVersion.GroupVersion: batch/v1
  May  7 12:21:59.813: INFO: Versions found [{batch/v1 v1}]
  May  7 12:21:59.813: INFO: batch/v1 matches batch/v1
  May  7 12:21:59.813: INFO: Checking APIGroup: certificates.k8s.io
  May  7 12:21:59.814: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  May  7 12:21:59.814: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  May  7 12:21:59.814: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  May  7 12:21:59.814: INFO: Checking APIGroup: networking.k8s.io
  May  7 12:21:59.820: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  May  7 12:21:59.820: INFO: Versions found [{networking.k8s.io/v1 v1}]
  May  7 12:21:59.820: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  May  7 12:21:59.820: INFO: Checking APIGroup: policy
  May  7 12:21:59.821: INFO: PreferredVersion.GroupVersion: policy/v1
  May  7 12:21:59.821: INFO: Versions found [{policy/v1 v1}]
  May  7 12:21:59.821: INFO: policy/v1 matches policy/v1
  May  7 12:21:59.821: INFO: Checking APIGroup: rbac.authorization.k8s.io
  May  7 12:21:59.821: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  May  7 12:21:59.821: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  May  7 12:21:59.821: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  May  7 12:21:59.821: INFO: Checking APIGroup: storage.k8s.io
  May  7 12:21:59.822: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  May  7 12:21:59.822: INFO: Versions found [{storage.k8s.io/v1 v1}]
  May  7 12:21:59.822: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  May  7 12:21:59.822: INFO: Checking APIGroup: admissionregistration.k8s.io
  May  7 12:21:59.822: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  May  7 12:21:59.822: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  May  7 12:21:59.822: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  May  7 12:21:59.822: INFO: Checking APIGroup: apiextensions.k8s.io
  May  7 12:21:59.822: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  May  7 12:21:59.822: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  May  7 12:21:59.823: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  May  7 12:21:59.823: INFO: Checking APIGroup: scheduling.k8s.io
  May  7 12:21:59.823: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  May  7 12:21:59.823: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  May  7 12:21:59.823: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  May  7 12:21:59.823: INFO: Checking APIGroup: coordination.k8s.io
  May  7 12:21:59.823: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  May  7 12:21:59.823: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  May  7 12:21:59.823: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  May  7 12:21:59.823: INFO: Checking APIGroup: node.k8s.io
  May  7 12:21:59.824: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  May  7 12:21:59.824: INFO: Versions found [{node.k8s.io/v1 v1}]
  May  7 12:21:59.824: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  May  7 12:21:59.824: INFO: Checking APIGroup: discovery.k8s.io
  May  7 12:21:59.824: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  May  7 12:21:59.824: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  May  7 12:21:59.824: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  May  7 12:21:59.824: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  May  7 12:21:59.825: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  May  7 12:21:59.825: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  May  7 12:21:59.825: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  May  7 12:21:59.825: INFO: Checking APIGroup: metrics.k8s.io
  May  7 12:21:59.825: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  May  7 12:21:59.825: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  May  7 12:21:59.825: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  May  7 12:21:59.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-5948" for this suite. @ 05/07/23 12:21:59.827
• [0.293 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 05/07/23 12:21:59.83
  May  7 12:21:59.830: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 12:21:59.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:59.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:59.841
  STEP: creating a Service @ 05/07/23 12:21:59.844
  STEP: watching for the Service to be added @ 05/07/23 12:21:59.849
  May  7 12:21:59.850: INFO: Found Service test-service-nlmch in namespace services-9185 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  May  7 12:21:59.851: INFO: Service test-service-nlmch created
  STEP: Getting /status @ 05/07/23 12:21:59.851
  May  7 12:21:59.854: INFO: Service test-service-nlmch has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/07/23 12:21:59.854
  STEP: watching for the Service to be patched @ 05/07/23 12:21:59.858
  May  7 12:21:59.859: INFO: observed Service test-service-nlmch in namespace services-9185 with annotations: map[] & LoadBalancer: {[]}
  May  7 12:21:59.859: INFO: Found Service test-service-nlmch in namespace services-9185 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  May  7 12:21:59.860: INFO: Service test-service-nlmch has service status patched
  STEP: updating the ServiceStatus @ 05/07/23 12:21:59.86
  May  7 12:21:59.865: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/07/23 12:21:59.865
  May  7 12:21:59.865: INFO: Observed Service test-service-nlmch in namespace services-9185 with annotations: map[] & Conditions: {[]}
  May  7 12:21:59.865: INFO: Observed event: &Service{ObjectMeta:{test-service-nlmch  services-9185  2cd2dbaf-deff-40ae-82cc-86e86c4b7b72 60698 0 2023-05-07 12:21:59 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-07 12:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-07 12:21:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.68.72.144,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.68.72.144],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  May  7 12:21:59.866: INFO: Found Service test-service-nlmch in namespace services-9185 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  7 12:21:59.866: INFO: Service test-service-nlmch has service status updated
  STEP: patching the service @ 05/07/23 12:21:59.866
  STEP: watching for the Service to be patched @ 05/07/23 12:21:59.87
  May  7 12:21:59.871: INFO: observed Service test-service-nlmch in namespace services-9185 with labels: map[test-service-static:true]
  May  7 12:21:59.871: INFO: observed Service test-service-nlmch in namespace services-9185 with labels: map[test-service-static:true]
  May  7 12:21:59.871: INFO: observed Service test-service-nlmch in namespace services-9185 with labels: map[test-service-static:true]
  May  7 12:21:59.871: INFO: Found Service test-service-nlmch in namespace services-9185 with labels: map[test-service:patched test-service-static:true]
  May  7 12:21:59.871: INFO: Service test-service-nlmch patched
  STEP: deleting the service @ 05/07/23 12:21:59.871
  STEP: watching for the Service to be deleted @ 05/07/23 12:21:59.879
  May  7 12:21:59.880: INFO: Observed event: ADDED
  May  7 12:21:59.880: INFO: Observed event: MODIFIED
  May  7 12:21:59.880: INFO: Observed event: MODIFIED
  May  7 12:21:59.880: INFO: Observed event: MODIFIED
  May  7 12:21:59.880: INFO: Found Service test-service-nlmch in namespace services-9185 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  May  7 12:21:59.880: INFO: Service test-service-nlmch deleted
  May  7 12:21:59.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9185" for this suite. @ 05/07/23 12:21:59.882
• [0.055 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/07/23 12:21:59.885
  May  7 12:21:59.885: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename proxy @ 05/07/23 12:21:59.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:21:59.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:21:59.898
  May  7 12:21:59.900: INFO: Creating pod...
  May  7 12:22:01.910: INFO: Creating service...
  May  7 12:22:01.915: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/pods/agnhost/proxy/some/path/with/DELETE
  May  7 12:22:01.924: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  7 12:22:01.924: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/pods/agnhost/proxy/some/path/with/GET
  May  7 12:22:01.930: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May  7 12:22:01.930: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/pods/agnhost/proxy/some/path/with/HEAD
  May  7 12:22:01.932: INFO: http.Client request:HEAD | StatusCode:200
  May  7 12:22:01.932: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/pods/agnhost/proxy/some/path/with/OPTIONS
  May  7 12:22:01.940: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  7 12:22:01.940: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/pods/agnhost/proxy/some/path/with/PATCH
  May  7 12:22:01.945: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  7 12:22:01.945: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/pods/agnhost/proxy/some/path/with/POST
  May  7 12:22:01.955: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  7 12:22:01.955: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/pods/agnhost/proxy/some/path/with/PUT
  May  7 12:22:01.965: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  7 12:22:01.965: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/services/test-service/proxy/some/path/with/DELETE
  May  7 12:22:01.968: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  7 12:22:01.968: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/services/test-service/proxy/some/path/with/GET
  May  7 12:22:01.970: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May  7 12:22:01.970: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/services/test-service/proxy/some/path/with/HEAD
  May  7 12:22:01.973: INFO: http.Client request:HEAD | StatusCode:200
  May  7 12:22:01.973: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/services/test-service/proxy/some/path/with/OPTIONS
  May  7 12:22:01.975: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  7 12:22:01.975: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/services/test-service/proxy/some/path/with/PATCH
  May  7 12:22:01.985: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  7 12:22:01.985: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/services/test-service/proxy/some/path/with/POST
  May  7 12:22:01.995: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  7 12:22:01.995: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-2630/services/test-service/proxy/some/path/with/PUT
  May  7 12:22:02.005: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  7 12:22:02.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2630" for this suite. @ 05/07/23 12:22:02.008
• [2.126 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 05/07/23 12:22:02.012
  May  7 12:22:02.012: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:22:02.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:22:02.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:22:02.023
  STEP: Setting up server cert @ 05/07/23 12:22:02.047
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:22:02.487
  STEP: Deploying the webhook pod @ 05/07/23 12:22:02.491
  STEP: Wait for the deployment to be ready @ 05/07/23 12:22:02.497
  May  7 12:22:02.500: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/07/23 12:22:04.505
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:22:04.51
  May  7 12:22:05.510: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/07/23 12:22:05.541
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/07/23 12:22:05.572
  STEP: Deleting the collection of validation webhooks @ 05/07/23 12:22:05.601
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/07/23 12:22:05.621
  May  7 12:22:05.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-345" for this suite. @ 05/07/23 12:22:05.658
  STEP: Destroying namespace "webhook-markers-6564" for this suite. @ 05/07/23 12:22:05.665
• [3.662 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 05/07/23 12:22:05.676
  May  7 12:22:05.676: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:22:05.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:22:05.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:22:05.691
  STEP: Creating configMap with name projected-configmap-test-volume-ad0f8fe2-6278-4484-8ab1-8e7261b19428 @ 05/07/23 12:22:05.693
  STEP: Creating a pod to test consume configMaps @ 05/07/23 12:22:05.697
  STEP: Saw pod success @ 05/07/23 12:22:09.713
  May  7 12:22:09.717: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-configmaps-ce4bb455-b6dd-462f-922b-2330fea8c513 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 12:22:09.722
  May  7 12:22:09.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6479" for this suite. @ 05/07/23 12:22:09.732
• [4.059 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 05/07/23 12:22:09.735
  May  7 12:22:09.735: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 12:22:09.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:22:09.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:22:09.746
  STEP: Creating configMap configmap-8932/configmap-test-c8ae92ab-9dde-4f9b-a19a-ffbe63237285 @ 05/07/23 12:22:09.748
  STEP: Creating a pod to test consume configMaps @ 05/07/23 12:22:09.749
  STEP: Saw pod success @ 05/07/23 12:22:11.757
  May  7 12:22:11.758: INFO: Trying to get logs from node 10.255.0.202 pod pod-configmaps-803eb841-31e7-49a6-97db-e89abf09c793 container env-test: <nil>
  STEP: delete the pod @ 05/07/23 12:22:11.761
  May  7 12:22:11.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8932" for this suite. @ 05/07/23 12:22:11.768
• [2.036 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 05/07/23 12:22:11.771
  May  7 12:22:11.771: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 12:22:11.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:22:11.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:22:11.782
  STEP: Creating a pod to test downward api env vars @ 05/07/23 12:22:11.785
  STEP: Saw pod success @ 05/07/23 12:22:13.792
  May  7 12:22:13.793: INFO: Trying to get logs from node 10.255.0.202 pod downward-api-08620315-26e8-4b9f-91ab-4e5d40d0057c container dapi-container: <nil>
  STEP: delete the pod @ 05/07/23 12:22:13.797
  May  7 12:22:13.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-907" for this suite. @ 05/07/23 12:22:13.808
• [2.043 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 05/07/23 12:22:13.814
  May  7 12:22:13.814: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename gc @ 05/07/23 12:22:13.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:22:13.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:22:13.825
  STEP: create the rc1 @ 05/07/23 12:22:13.828
  STEP: create the rc2 @ 05/07/23 12:22:13.831
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/07/23 12:22:19.866
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/07/23 12:22:20.809
  STEP: wait for the rc to be deleted @ 05/07/23 12:22:20.813
  May  7 12:22:25.834: INFO: 70 pods remaining
  May  7 12:22:25.834: INFO: 70 pods has nil DeletionTimestamp
  May  7 12:22:25.834: INFO: 
  STEP: Gathering metrics @ 05/07/23 12:22:30.821
  W0507 12:22:30.823934      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  7 12:22:30.823: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  7 12:22:30.824: INFO: Deleting pod "simpletest-rc-to-be-deleted-2dvcq" in namespace "gc-3882"
  May  7 12:22:30.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hdcb" in namespace "gc-3882"
  May  7 12:22:30.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jwnp" in namespace "gc-3882"
  May  7 12:22:30.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mddt" in namespace "gc-3882"
  May  7 12:22:30.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vdmz" in namespace "gc-3882"
  May  7 12:22:30.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vtjl" in namespace "gc-3882"
  May  7 12:22:30.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-2wtqq" in namespace "gc-3882"
  May  7 12:22:30.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v565" in namespace "gc-3882"
  May  7 12:22:30.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-57qkc" in namespace "gc-3882"
  May  7 12:22:30.923: INFO: Deleting pod "simpletest-rc-to-be-deleted-58xmf" in namespace "gc-3882"
  May  7 12:22:30.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gd2z" in namespace "gc-3882"
  May  7 12:22:30.976: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hpr9" in namespace "gc-3882"
  May  7 12:22:30.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-5plbn" in namespace "gc-3882"
  May  7 12:22:31.028: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c6ms" in namespace "gc-3882"
  May  7 12:22:31.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rjxg" in namespace "gc-3882"
  May  7 12:22:31.062: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ftp5" in namespace "gc-3882"
  May  7 12:22:31.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-89hfg" in namespace "gc-3882"
  May  7 12:22:31.089: INFO: Deleting pod "simpletest-rc-to-be-deleted-965ds" in namespace "gc-3882"
  May  7 12:22:31.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-9glkp" in namespace "gc-3882"
  May  7 12:22:31.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbrlp" in namespace "gc-3882"
  May  7 12:22:31.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdfgx" in namespace "gc-3882"
  May  7 12:22:31.193: INFO: Deleting pod "simpletest-rc-to-be-deleted-blzdc" in namespace "gc-3882"
  May  7 12:22:31.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvf4d" in namespace "gc-3882"
  May  7 12:22:31.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-c47db" in namespace "gc-3882"
  May  7 12:22:31.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5w7s" in namespace "gc-3882"
  May  7 12:22:31.300: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9fn2" in namespace "gc-3882"
  May  7 12:22:31.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9j9q" in namespace "gc-3882"
  May  7 12:22:31.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdtpz" in namespace "gc-3882"
  May  7 12:22:31.462: INFO: Deleting pod "simpletest-rc-to-be-deleted-ck4gg" in namespace "gc-3882"
  May  7 12:22:31.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9s66" in namespace "gc-3882"
  May  7 12:22:31.512: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh6g7" in namespace "gc-3882"
  May  7 12:22:31.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwd8c" in namespace "gc-3882"
  May  7 12:22:31.545: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7bzh" in namespace "gc-3882"
  May  7 12:22:31.555: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl8bn" in namespace "gc-3882"
  May  7 12:22:31.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqp7x" in namespace "gc-3882"
  May  7 12:22:31.603: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5jqt" in namespace "gc-3882"
  May  7 12:22:31.647: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4hks" in namespace "gc-3882"
  May  7 12:22:31.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgnw4" in namespace "gc-3882"
  May  7 12:22:31.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-hk27q" in namespace "gc-3882"
  May  7 12:22:31.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-j2xg2" in namespace "gc-3882"
  May  7 12:22:31.821: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9mt6" in namespace "gc-3882"
  May  7 12:22:31.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-jbsz5" in namespace "gc-3882"
  May  7 12:22:31.897: INFO: Deleting pod "simpletest-rc-to-be-deleted-jxr6w" in namespace "gc-3882"
  May  7 12:22:31.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-k57pb" in namespace "gc-3882"
  May  7 12:22:31.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-khkwf" in namespace "gc-3882"
  May  7 12:22:31.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-kj98t" in namespace "gc-3882"
  May  7 12:22:32.006: INFO: Deleting pod "simpletest-rc-to-be-deleted-kkd9j" in namespace "gc-3882"
  May  7 12:22:32.048: INFO: Deleting pod "simpletest-rc-to-be-deleted-kpzk6" in namespace "gc-3882"
  May  7 12:22:32.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-ks95k" in namespace "gc-3882"
  May  7 12:22:32.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-kxh5s" in namespace "gc-3882"
  May  7 12:22:32.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3882" for this suite. @ 05/07/23 12:22:32.201
• [18.433 seconds]
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 05/07/23 12:22:32.247
  May  7 12:22:32.247: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename var-expansion @ 05/07/23 12:22:32.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:22:32.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:22:32.375
  May  7 12:22:34.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 12:22:34.415: INFO: Deleting pod "var-expansion-ea7e818e-1d51-47b6-aa26-a902c9f59bbc" in namespace "var-expansion-8821"
  May  7 12:22:34.421: INFO: Wait up to 5m0s for pod "var-expansion-ea7e818e-1d51-47b6-aa26-a902c9f59bbc" to be fully deleted
  STEP: Destroying namespace "var-expansion-8821" for this suite. @ 05/07/23 12:22:36.432
• [4.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 05/07/23 12:22:36.436
  May  7 12:22:36.436: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-probe @ 05/07/23 12:22:36.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:22:36.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:22:36.445
  STEP: Creating pod liveness-6433cccb-c2ce-4fe9-98ae-5e06408e369d in namespace container-probe-67 @ 05/07/23 12:22:36.447
  May  7 12:22:38.459: INFO: Started pod liveness-6433cccb-c2ce-4fe9-98ae-5e06408e369d in namespace container-probe-67
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/07/23 12:22:38.459
  May  7 12:22:38.460: INFO: Initial restart count of pod liveness-6433cccb-c2ce-4fe9-98ae-5e06408e369d is 0
  May  7 12:26:38.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:26:38.746
  STEP: Destroying namespace "container-probe-67" for this suite. @ 05/07/23 12:26:38.753
• [242.321 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/07/23 12:26:38.758
  May  7 12:26:38.758: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 12:26:38.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:26:38.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:26:38.77
  STEP: creating the pod @ 05/07/23 12:26:38.772
  STEP: setting up watch @ 05/07/23 12:26:38.772
  STEP: submitting the pod to kubernetes @ 05/07/23 12:26:38.879
  STEP: verifying the pod is in kubernetes @ 05/07/23 12:26:38.884
  STEP: verifying pod creation was observed @ 05/07/23 12:26:38.887
  STEP: deleting the pod gracefully @ 05/07/23 12:26:40.892
  STEP: verifying pod deletion was observed @ 05/07/23 12:26:40.894
  May  7 12:26:41.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4011" for this suite. @ 05/07/23 12:26:41.625
• [2.869 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/07/23 12:26:41.628
  May  7 12:26:41.628: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename deployment @ 05/07/23 12:26:41.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:26:41.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:26:41.64
  May  7 12:26:41.641: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  May  7 12:26:41.646: INFO: Pod name sample-pod: Found 0 pods out of 1
  May  7 12:26:46.651: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/07/23 12:26:46.651
  May  7 12:26:46.651: INFO: Creating deployment "test-rolling-update-deployment"
  May  7 12:26:46.653: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  May  7 12:26:46.658: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  May  7 12:26:48.661: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  May  7 12:26:48.662: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  May  7 12:26:48.665: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8127  bd0f54fa-5ae4-4f8c-9fec-29274a0325a9 63776 1 2023-05-07 12:26:46 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-07 12:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c4998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-07 12:26:46 +0000 UTC,LastTransitionTime:2023-05-07 12:26:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-05-07 12:26:47 +0000 UTC,LastTransitionTime:2023-05-07 12:26:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May  7 12:26:48.666: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-8127  c1258ad1-a0ac-49b1-bfdc-7cfd81141c40 63766 1 2023-05-07 12:26:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment bd0f54fa-5ae4-4f8c-9fec-29274a0325a9 0xc00413e587 0xc00413e588}] [] [{kube-controller-manager Update apps/v1 2023-05-07 12:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd0f54fa-5ae4-4f8c-9fec-29274a0325a9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:26:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00413e648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  7 12:26:48.666: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  May  7 12:26:48.666: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8127  233beff1-02b3-47bb-8da7-fb3b2f0d4396 63775 2 2023-05-07 12:26:41 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment bd0f54fa-5ae4-4f8c-9fec-29274a0325a9 0xc00413e457 0xc00413e458}] [] [{e2e.test Update apps/v1 2023-05-07 12:26:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd0f54fa-5ae4-4f8c-9fec-29274a0325a9\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-07 12:26:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00413e518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  7 12:26:48.668: INFO: Pod "test-rolling-update-deployment-656d657cd8-nrnfm" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-nrnfm test-rolling-update-deployment-656d657cd8- deployment-8127  4563cc51-d17c-456a-af8e-941834c819c9 63765 0 2023-05-07 12:26:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 c1258ad1-a0ac-49b1-bfdc-7cfd81141c40 0xc0038c5507 0xc0038c5508}] [] [{kube-controller-manager Update v1 2023-05-07 12:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c1258ad1-a0ac-49b1-bfdc-7cfd81141c40\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 12:26:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.231.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mprvl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mprvl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:26:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:26:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:26:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 12:26:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:172.20.231.237,StartTime:2023-05-07 12:26:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 12:26:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://4ad20d9e907d8340237c301ae758eb3d90030c9a7d5f3a7f141ae2362ce03621,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.231.237,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 12:26:48.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8127" for this suite. @ 05/07/23 12:26:48.669
• [7.044 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 05/07/23 12:26:48.672
  May  7 12:26:48.672: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:26:48.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:26:48.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:26:48.684
  STEP: Setting up server cert @ 05/07/23 12:26:48.698
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:26:48.938
  STEP: Deploying the webhook pod @ 05/07/23 12:26:48.942
  STEP: Wait for the deployment to be ready @ 05/07/23 12:26:48.948
  May  7 12:26:48.952: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/07/23 12:26:50.958
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:26:50.962
  May  7 12:26:51.962: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/07/23 12:26:51.964
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/07/23 12:26:51.973
  May  7 12:26:51.973: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:26:51.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3041" for this suite. @ 05/07/23 12:26:52.035
  STEP: Destroying namespace "webhook-markers-1977" for this suite. @ 05/07/23 12:26:52.038
• [3.371 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 05/07/23 12:26:52.044
  May  7 12:26:52.044: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/07/23 12:26:52.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:26:52.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:26:52.083
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/07/23 12:26:52.085
  May  7 12:26:52.085: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:26:53.369: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:26:58.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7397" for this suite. @ 05/07/23 12:26:58.305
• [6.264 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/07/23 12:26:58.308
  May  7 12:26:58.308: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-watch @ 05/07/23 12:26:58.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:26:58.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:26:58.319
  May  7 12:26:58.321: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Creating first CR  @ 05/07/23 12:27:00.843
  May  7 12:27:00.846: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-07T12:27:00Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-07T12:27:00Z]] name:name1 resourceVersion:63936 uid:1e14a888-fc91-40fe-bc48-3ba9d18f1424] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 05/07/23 12:27:10.847
  May  7 12:27:10.850: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-07T12:27:10Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-07T12:27:10Z]] name:name2 resourceVersion:63958 uid:8201e819-f3f5-40fb-ba5f-73765fa57b5b] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 05/07/23 12:27:20.852
  May  7 12:27:20.856: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-07T12:27:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-07T12:27:20Z]] name:name1 resourceVersion:63973 uid:1e14a888-fc91-40fe-bc48-3ba9d18f1424] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 05/07/23 12:27:30.856
  May  7 12:27:30.860: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-07T12:27:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-07T12:27:30Z]] name:name2 resourceVersion:63989 uid:8201e819-f3f5-40fb-ba5f-73765fa57b5b] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 05/07/23 12:27:40.86
  May  7 12:27:40.864: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-07T12:27:00Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-07T12:27:20Z]] name:name1 resourceVersion:64007 uid:1e14a888-fc91-40fe-bc48-3ba9d18f1424] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 05/07/23 12:27:50.866
  May  7 12:27:50.870: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-07T12:27:10Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-07T12:27:30Z]] name:name2 resourceVersion:64023 uid:8201e819-f3f5-40fb-ba5f-73765fa57b5b] num:map[num1:9223372036854775807 num2:1000000]]}
  May  7 12:28:01.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-5504" for this suite. @ 05/07/23 12:28:01.381
• [63.075 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 05/07/23 12:28:01.383
  May  7 12:28:01.383: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename statefulset @ 05/07/23 12:28:01.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:28:01.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:28:01.394
  STEP: Creating service test in namespace statefulset-3375 @ 05/07/23 12:28:01.396
  STEP: Creating a new StatefulSet @ 05/07/23 12:28:01.399
  May  7 12:28:01.409: INFO: Found 0 stateful pods, waiting for 3
  May  7 12:28:11.415: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May  7 12:28:11.415: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May  7 12:28:11.415: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/07/23 12:28:11.419
  May  7 12:28:11.433: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/07/23 12:28:11.433
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/07/23 12:28:21.444
  STEP: Performing a canary update @ 05/07/23 12:28:21.444
  May  7 12:28:21.458: INFO: Updating stateful set ss2
  May  7 12:28:21.462: INFO: Waiting for Pod statefulset-3375/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/07/23 12:28:31.468
  May  7 12:28:31.496: INFO: Found 1 stateful pods, waiting for 3
  May  7 12:28:41.499: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May  7 12:28:41.499: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May  7 12:28:41.499: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/07/23 12:28:41.502
  May  7 12:28:41.516: INFO: Updating stateful set ss2
  May  7 12:28:41.519: INFO: Waiting for Pod statefulset-3375/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  May  7 12:28:51.539: INFO: Updating stateful set ss2
  May  7 12:28:51.542: INFO: Waiting for StatefulSet statefulset-3375/ss2 to complete update
  May  7 12:28:51.542: INFO: Waiting for Pod statefulset-3375/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  May  7 12:29:01.547: INFO: Deleting all statefulset in ns statefulset-3375
  May  7 12:29:01.548: INFO: Scaling statefulset ss2 to 0
  May  7 12:29:11.559: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 12:29:11.560: INFO: Deleting statefulset ss2
  May  7 12:29:11.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3375" for this suite. @ 05/07/23 12:29:11.569
• [70.190 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/07/23 12:29:11.573
  May  7 12:29:11.573: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 12:29:11.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:29:11.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:29:11.587
  STEP: Creating secret with name secret-test-map-b129211f-84f7-4d9c-af2f-800206ee9680 @ 05/07/23 12:29:11.59
  STEP: Creating a pod to test consume secrets @ 05/07/23 12:29:11.593
  STEP: Saw pod success @ 05/07/23 12:29:15.61
  May  7 12:29:15.611: INFO: Trying to get logs from node 10.255.0.202 pod pod-secrets-7894e54d-bd06-4652-8f48-22d4a238fa2f container secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 12:29:15.621
  May  7 12:29:15.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1676" for this suite. @ 05/07/23 12:29:15.629
• [4.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 05/07/23 12:29:15.634
  May  7 12:29:15.634: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename job @ 05/07/23 12:29:15.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:29:15.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:29:15.645
  STEP: Creating Indexed job @ 05/07/23 12:29:15.646
  STEP: Ensuring job reaches completions @ 05/07/23 12:29:15.649
  STEP: Ensuring pods with index for job exist @ 05/07/23 12:29:23.652
  May  7 12:29:23.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6483" for this suite. @ 05/07/23 12:29:23.656
• [8.025 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 05/07/23 12:29:23.658
  May  7 12:29:23.658: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename security-context-test @ 05/07/23 12:29:23.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:29:23.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:29:23.673
  May  7 12:29:27.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7141" for this suite. @ 05/07/23 12:29:27.689
• [4.033 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 05/07/23 12:29:27.692
  May  7 12:29:27.692: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename namespaces @ 05/07/23 12:29:27.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:29:27.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:29:27.704
  STEP: Creating namespace "e2e-ns-p5lr6" @ 05/07/23 12:29:27.705
  May  7 12:29:27.712: INFO: Namespace "e2e-ns-p5lr6-8058" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-p5lr6-8058" @ 05/07/23 12:29:27.712
  May  7 12:29:27.717: INFO: Namespace "e2e-ns-p5lr6-8058" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-p5lr6-8058" @ 05/07/23 12:29:27.717
  May  7 12:29:27.720: INFO: Namespace "e2e-ns-p5lr6-8058" has []v1.FinalizerName{"kubernetes"}
  May  7 12:29:27.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7244" for this suite. @ 05/07/23 12:29:27.722
  STEP: Destroying namespace "e2e-ns-p5lr6-8058" for this suite. @ 05/07/23 12:29:27.726
• [0.043 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 05/07/23 12:29:27.735
  May  7 12:29:27.735: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 12:29:27.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:29:27.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:29:27.753
  STEP: creating the pod @ 05/07/23 12:29:27.754
  May  7 12:29:27.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9318 create -f -'
  May  7 12:29:28.313: INFO: stderr: ""
  May  7 12:29:28.313: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/07/23 12:29:30.319
  May  7 12:29:30.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9318 label pods pause testing-label=testing-label-value'
  May  7 12:29:30.371: INFO: stderr: ""
  May  7 12:29:30.371: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/07/23 12:29:30.372
  May  7 12:29:30.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9318 get pod pause -L testing-label'
  May  7 12:29:30.414: INFO: stderr: ""
  May  7 12:29:30.414: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/07/23 12:29:30.414
  May  7 12:29:30.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9318 label pods pause testing-label-'
  May  7 12:29:30.465: INFO: stderr: ""
  May  7 12:29:30.465: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/07/23 12:29:30.465
  May  7 12:29:30.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9318 get pod pause -L testing-label'
  May  7 12:29:30.506: INFO: stderr: ""
  May  7 12:29:30.506: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 05/07/23 12:29:30.506
  May  7 12:29:30.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9318 delete --grace-period=0 --force -f -'
  May  7 12:29:30.550: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  7 12:29:30.550: INFO: stdout: "pod \"pause\" force deleted\n"
  May  7 12:29:30.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9318 get rc,svc -l name=pause --no-headers'
  May  7 12:29:30.594: INFO: stderr: "No resources found in kubectl-9318 namespace.\n"
  May  7 12:29:30.594: INFO: stdout: ""
  May  7 12:29:30.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-9318 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May  7 12:29:30.642: INFO: stderr: ""
  May  7 12:29:30.642: INFO: stdout: ""
  May  7 12:29:30.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9318" for this suite. @ 05/07/23 12:29:30.644
• [2.911 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/07/23 12:29:30.647
  May  7 12:29:30.647: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename cronjob @ 05/07/23 12:29:30.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:29:30.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:29:30.661
  STEP: Creating a cronjob @ 05/07/23 12:29:30.663
  STEP: Ensuring more than one job is running at a time @ 05/07/23 12:29:30.665
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/07/23 12:31:00.667
  STEP: Removing cronjob @ 05/07/23 12:31:00.669
  May  7 12:31:00.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7339" for this suite. @ 05/07/23 12:31:00.675
• [90.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/07/23 12:31:00.678
  May  7 12:31:00.678: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:31:00.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:31:00.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:31:00.698
  STEP: Creating projection with secret that has name projected-secret-test-map-9ef64d5e-5970-45af-a5b2-6b37b34ee95e @ 05/07/23 12:31:00.702
  STEP: Creating a pod to test consume secrets @ 05/07/23 12:31:00.707
  STEP: Saw pod success @ 05/07/23 12:31:04.725
  May  7 12:31:04.726: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-secrets-bbb1adfc-9dfa-42ed-bddb-5ed7c26043eb container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 12:31:04.735
  May  7 12:31:04.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7308" for this suite. @ 05/07/23 12:31:04.743
• [4.068 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/07/23 12:31:04.746
  May  7 12:31:04.746: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:31:04.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:31:04.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:31:04.757
  STEP: Creating secret with name s-test-opt-del-d2a19c22-a940-4b69-be73-6117d45d10d3 @ 05/07/23 12:31:04.76
  STEP: Creating secret with name s-test-opt-upd-4ef0c110-709b-40db-9887-f932dd240350 @ 05/07/23 12:31:04.763
  STEP: Creating the pod @ 05/07/23 12:31:04.764
  STEP: Deleting secret s-test-opt-del-d2a19c22-a940-4b69-be73-6117d45d10d3 @ 05/07/23 12:31:06.782
  STEP: Updating secret s-test-opt-upd-4ef0c110-709b-40db-9887-f932dd240350 @ 05/07/23 12:31:06.786
  STEP: Creating secret with name s-test-opt-create-73903f20-55a7-46d8-b7f3-ccb691094a8d @ 05/07/23 12:31:06.788
  STEP: waiting to observe update in volume @ 05/07/23 12:31:06.79
  May  7 12:32:33.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1269" for this suite. @ 05/07/23 12:32:33.054
• [88.312 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/07/23 12:32:33.059
  May  7 12:32:33.059: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/07/23 12:32:33.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:32:33.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:32:33.075
  May  7 12:32:35.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 05/07/23 12:32:35.09
  STEP: Cleaning up the configmap @ 05/07/23 12:32:35.093
  STEP: Cleaning up the pod @ 05/07/23 12:32:35.095
  STEP: Destroying namespace "emptydir-wrapper-6901" for this suite. @ 05/07/23 12:32:35.1
• [2.045 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 05/07/23 12:32:35.104
  May  7 12:32:35.104: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 12:32:35.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:32:35.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:32:35.116
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/07/23 12:32:35.117
  STEP: Saw pod success @ 05/07/23 12:32:39.126
  May  7 12:32:39.127: INFO: Trying to get logs from node 10.255.0.202 pod pod-7630dced-cc16-4c20-b009-bc7e9a75411f container test-container: <nil>
  STEP: delete the pod @ 05/07/23 12:32:39.13
  May  7 12:32:39.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7078" for this suite. @ 05/07/23 12:32:39.139
• [4.038 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 05/07/23 12:32:39.142
  May  7 12:32:39.142: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename job @ 05/07/23 12:32:39.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:32:39.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:32:39.156
  STEP: Creating a job @ 05/07/23 12:32:39.159
  STEP: Ensuring job reaches completions @ 05/07/23 12:32:39.163
  May  7 12:32:49.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1102" for this suite. @ 05/07/23 12:32:49.168
• [10.029 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/07/23 12:32:49.171
  May  7 12:32:49.171: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:32:49.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:32:49.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:32:49.185
  STEP: Creating projection with secret that has name projected-secret-test-f1cc1f6a-23df-424b-8ff6-20497c1cdae3 @ 05/07/23 12:32:49.186
  STEP: Creating a pod to test consume secrets @ 05/07/23 12:32:49.189
  STEP: Saw pod success @ 05/07/23 12:32:53.199
  May  7 12:32:53.200: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-secrets-731efa5e-c4f1-4e8c-8a7b-91f4956b8970 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 12:32:53.203
  May  7 12:32:53.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5015" for this suite. @ 05/07/23 12:32:53.211
• [4.042 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 05/07/23 12:32:53.214
  May  7 12:32:53.214: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 12:32:53.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:32:53.235
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:32:53.236
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 12:32:53.238
  STEP: Saw pod success @ 05/07/23 12:32:57.253
  May  7 12:32:57.254: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-6f905860-5e47-4cc5-ab3e-65f04f2773e6 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 12:32:57.257
  May  7 12:32:57.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4413" for this suite. @ 05/07/23 12:32:57.265
• [4.055 seconds]
------------------------------
S
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 05/07/23 12:32:57.268
  May  7 12:32:57.268: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename disruption @ 05/07/23 12:32:57.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:32:57.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:32:57.279
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/07/23 12:32:57.28
  STEP: Waiting for the pdb to be processed @ 05/07/23 12:32:57.283
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/07/23 12:32:57.29
  STEP: Waiting for all pods to be running @ 05/07/23 12:32:57.29
  May  7 12:32:57.293: INFO: pods: 0 < 3
  STEP: locating a running pod @ 05/07/23 12:32:59.295
  STEP: Updating the pdb to allow a pod to be evicted @ 05/07/23 12:32:59.299
  STEP: Waiting for the pdb to be processed @ 05/07/23 12:32:59.303
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/07/23 12:33:01.307
  STEP: Waiting for all pods to be running @ 05/07/23 12:33:01.307
  STEP: Waiting for the pdb to observed all healthy pods @ 05/07/23 12:33:01.308
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/07/23 12:33:01.322
  STEP: Waiting for the pdb to be processed @ 05/07/23 12:33:01.333
  STEP: Waiting for all pods to be running @ 05/07/23 12:33:03.34
  STEP: locating a running pod @ 05/07/23 12:33:03.343
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/07/23 12:33:03.348
  STEP: Waiting for the pdb to be deleted @ 05/07/23 12:33:03.351
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/07/23 12:33:03.352
  STEP: Waiting for all pods to be running @ 05/07/23 12:33:03.353
  May  7 12:33:03.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2876" for this suite. @ 05/07/23 12:33:03.369
• [6.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 05/07/23 12:33:03.377
  May  7 12:33:03.377: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:33:03.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:03.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:03.395
  STEP: Setting up server cert @ 05/07/23 12:33:03.416
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:33:03.726
  STEP: Deploying the webhook pod @ 05/07/23 12:33:03.729
  STEP: Wait for the deployment to be ready @ 05/07/23 12:33:03.734
  May  7 12:33:03.737: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/07/23 12:33:05.742
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:33:05.758
  May  7 12:33:06.758: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/07/23 12:33:06.76
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/07/23 12:33:06.761
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/07/23 12:33:06.761
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/07/23 12:33:06.761
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/07/23 12:33:06.761
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/07/23 12:33:06.761
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/07/23 12:33:06.762
  May  7 12:33:06.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5194" for this suite. @ 05/07/23 12:33:06.8
  STEP: Destroying namespace "webhook-markers-9629" for this suite. @ 05/07/23 12:33:06.81
• [3.442 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 05/07/23 12:33:06.82
  May  7 12:33:06.820: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename var-expansion @ 05/07/23 12:33:06.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:06.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:06.835
  STEP: Creating a pod to test env composition @ 05/07/23 12:33:06.836
  STEP: Saw pod success @ 05/07/23 12:33:10.852
  May  7 12:33:10.854: INFO: Trying to get logs from node 10.255.0.202 pod var-expansion-ead1908e-6b55-421f-9bb2-29a94b506dde container dapi-container: <nil>
  STEP: delete the pod @ 05/07/23 12:33:10.857
  May  7 12:33:10.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3818" for this suite. @ 05/07/23 12:33:10.865
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/07/23 12:33:10.869
  May  7 12:33:10.869: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 12:33:10.869
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:10.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:10.88
  STEP: Create a pod @ 05/07/23 12:33:10.881
  STEP: patching /status @ 05/07/23 12:33:12.893
  May  7 12:33:12.897: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  May  7 12:33:12.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7330" for this suite. @ 05/07/23 12:33:12.9
• [2.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/07/23 12:33:12.905
  May  7 12:33:12.905: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:33:12.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:12.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:12.917
  STEP: Creating projection with secret that has name projected-secret-test-1c3376e3-e817-42d1-96a0-e2c20acdc6b0 @ 05/07/23 12:33:12.918
  STEP: Creating a pod to test consume secrets @ 05/07/23 12:33:12.92
  STEP: Saw pod success @ 05/07/23 12:33:16.935
  May  7 12:33:16.936: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-secrets-12324b52-e9a2-4c35-8116-cbd8e0dbd956 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 12:33:16.939
  May  7 12:33:16.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5972" for this suite. @ 05/07/23 12:33:16.947
• [4.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 05/07/23 12:33:16.95
  May  7 12:33:16.950: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename daemonsets @ 05/07/23 12:33:16.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:16.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:16.962
  May  7 12:33:16.971: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/07/23 12:33:16.973
  May  7 12:33:16.980: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:33:16.980: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/07/23 12:33:16.98
  May  7 12:33:16.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:33:16.989: INFO: Node 10.255.0.203 is running 0 daemon pod, expected 1
  May  7 12:33:17.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  7 12:33:17.991: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/07/23 12:33:17.992
  May  7 12:33:18.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  7 12:33:18.008: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  May  7 12:33:19.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:33:19.010: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/07/23 12:33:19.01
  May  7 12:33:19.015: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:33:19.015: INFO: Node 10.255.0.203 is running 0 daemon pod, expected 1
  May  7 12:33:20.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:33:20.017: INFO: Node 10.255.0.203 is running 0 daemon pod, expected 1
  May  7 12:33:21.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  7 12:33:21.017: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/07/23 12:33:21.019
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3263, will wait for the garbage collector to delete the pods @ 05/07/23 12:33:21.019
  May  7 12:33:21.075: INFO: Deleting DaemonSet.extensions daemon-set took: 4.359092ms
  May  7 12:33:21.176: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.287458ms
  May  7 12:33:22.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:33:22.687: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  7 12:33:22.688: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66001"},"items":null}

  May  7 12:33:22.689: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66001"},"items":null}

  May  7 12:33:22.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3263" for this suite. @ 05/07/23 12:33:22.704
• [5.757 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 05/07/23 12:33:22.708
  May  7 12:33:22.708: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:33:22.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:22.717
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:22.72
  STEP: Setting up server cert @ 05/07/23 12:33:22.735
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:33:23.047
  STEP: Deploying the webhook pod @ 05/07/23 12:33:23.049
  STEP: Wait for the deployment to be ready @ 05/07/23 12:33:23.055
  May  7 12:33:23.057: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/07/23 12:33:25.062
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:33:25.068
  May  7 12:33:26.068: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  7 12:33:26.070: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/07/23 12:33:26.586
  STEP: Creating a custom resource that should be denied by the webhook @ 05/07/23 12:33:26.595
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/07/23 12:33:28.61
  STEP: Updating the custom resource with disallowed data should be denied @ 05/07/23 12:33:28.614
  STEP: Deleting the custom resource should be denied @ 05/07/23 12:33:28.619
  STEP: Remove the offending key and value from the custom resource data @ 05/07/23 12:33:28.621
  STEP: Deleting the updated custom resource should be successful @ 05/07/23 12:33:28.625
  May  7 12:33:28.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9716" for this suite. @ 05/07/23 12:33:29.184
  STEP: Destroying namespace "webhook-markers-2504" for this suite. @ 05/07/23 12:33:29.187
• [6.483 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 05/07/23 12:33:29.191
  May  7 12:33:29.191: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename daemonsets @ 05/07/23 12:33:29.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:29.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:29.207
  May  7 12:33:29.223: INFO: Create a RollingUpdate DaemonSet
  May  7 12:33:29.226: INFO: Check that daemon pods launch on every node of the cluster
  May  7 12:33:29.234: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:33:29.234: INFO: Node 10.255.0.201 is running 0 daemon pod, expected 1
  May  7 12:33:30.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May  7 12:33:30.238: INFO: Node 10.255.0.202 is running 0 daemon pod, expected 1
  May  7 12:33:31.237: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May  7 12:33:31.237: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  May  7 12:33:31.237: INFO: Update the DaemonSet to trigger a rollout
  May  7 12:33:31.242: INFO: Updating DaemonSet daemon-set
  May  7 12:33:33.250: INFO: Roll back the DaemonSet before rollout is complete
  May  7 12:33:33.254: INFO: Updating DaemonSet daemon-set
  May  7 12:33:33.254: INFO: Make sure DaemonSet rollback is complete
  May  7 12:33:33.256: INFO: Wrong image for pod: daemon-set-829pk. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  May  7 12:33:33.256: INFO: Pod daemon-set-829pk is not available
  May  7 12:33:37.261: INFO: Pod daemon-set-488nx is not available
  STEP: Deleting DaemonSet "daemon-set" @ 05/07/23 12:33:37.265
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4077, will wait for the garbage collector to delete the pods @ 05/07/23 12:33:37.265
  May  7 12:33:37.319: INFO: Deleting DaemonSet.extensions daemon-set took: 2.100176ms
  May  7 12:33:37.419: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.533479ms
  May  7 12:33:39.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 12:33:39.327: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  7 12:33:39.337: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66272"},"items":null}

  May  7 12:33:39.338: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66272"},"items":null}

  May  7 12:33:39.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4077" for this suite. @ 05/07/23 12:33:39.344
• [10.156 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 05/07/23 12:33:39.347
  May  7 12:33:39.347: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 12:33:39.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:39.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:39.359
  STEP: Creating configMap with name configmap-test-upd-1aa26f22-32b6-4ae8-b2ee-6cb80236e299 @ 05/07/23 12:33:39.362
  STEP: Creating the pod @ 05/07/23 12:33:39.364
  STEP: Waiting for pod with text data @ 05/07/23 12:33:41.379
  STEP: Waiting for pod with binary data @ 05/07/23 12:33:41.382
  May  7 12:33:41.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7638" for this suite. @ 05/07/23 12:33:41.386
• [2.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/07/23 12:33:41.39
  May  7 12:33:41.390: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 12:33:41.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:41.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:41.402
  STEP: Creating secret with name secret-test-d7582262-79f4-4aa0-b609-951907b81d91 @ 05/07/23 12:33:41.404
  STEP: Creating a pod to test consume secrets @ 05/07/23 12:33:41.405
  STEP: Saw pod success @ 05/07/23 12:33:45.415
  May  7 12:33:45.417: INFO: Trying to get logs from node 10.255.0.202 pod pod-secrets-4eded919-2dfb-421a-805d-cf11e757f4f9 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 12:33:45.419
  May  7 12:33:45.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-40" for this suite. @ 05/07/23 12:33:45.428
• [4.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 05/07/23 12:33:45.432
  May  7 12:33:45.432: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:33:45.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:45.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:45.44
  STEP: Setting up server cert @ 05/07/23 12:33:45.453
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:33:45.705
  STEP: Deploying the webhook pod @ 05/07/23 12:33:45.709
  STEP: Wait for the deployment to be ready @ 05/07/23 12:33:45.715
  May  7 12:33:45.720: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May  7 12:33:47.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 33, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 33, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 33, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/07/23 12:33:49.727
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:33:49.738
  May  7 12:33:50.738: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  7 12:33:50.740: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8609-crds.webhook.example.com via the AdmissionRegistration API @ 05/07/23 12:33:51.251
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/07/23 12:33:51.26
  May  7 12:33:53.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-215" for this suite. @ 05/07/23 12:33:53.853
  STEP: Destroying namespace "webhook-markers-2346" for this suite. @ 05/07/23 12:33:53.856
• [8.427 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 05/07/23 12:33:53.859
  May  7 12:33:53.859: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 12:33:53.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:33:53.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:33:53.871
  STEP: creating service multi-endpoint-test in namespace services-364 @ 05/07/23 12:33:53.873
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-364 to expose endpoints map[] @ 05/07/23 12:33:53.879
  May  7 12:33:53.886: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  May  7 12:33:54.890: INFO: successfully validated that service multi-endpoint-test in namespace services-364 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-364 @ 05/07/23 12:33:54.89
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-364 to expose endpoints map[pod1:[100]] @ 05/07/23 12:33:56.899
  May  7 12:33:56.902: INFO: successfully validated that service multi-endpoint-test in namespace services-364 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-364 @ 05/07/23 12:33:56.902
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-364 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/07/23 12:33:58.913
  May  7 12:33:58.921: INFO: successfully validated that service multi-endpoint-test in namespace services-364 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/07/23 12:33:58.921
  May  7 12:33:58.921: INFO: Creating new exec pod
  May  7 12:34:01.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-364 exec execpodgfzcc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  May  7 12:34:02.055: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  May  7 12:34:02.055: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:34:02.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-364 exec execpodgfzcc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.68.29 80'
  May  7 12:34:02.145: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.68.29 80\nConnection to 10.68.68.29 80 port [tcp/http] succeeded!\n"
  May  7 12:34:02.145: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:34:02.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-364 exec execpodgfzcc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  May  7 12:34:02.273: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  May  7 12:34:02.273: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 12:34:02.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-364 exec execpodgfzcc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.68.29 81'
  May  7 12:34:02.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.68.29 81\nConnection to 10.68.68.29 81 port [tcp/*] succeeded!\n"
  May  7 12:34:02.357: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-364 @ 05/07/23 12:34:02.357
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-364 to expose endpoints map[pod2:[101]] @ 05/07/23 12:34:02.366
  May  7 12:34:02.377: INFO: successfully validated that service multi-endpoint-test in namespace services-364 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-364 @ 05/07/23 12:34:02.377
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-364 to expose endpoints map[] @ 05/07/23 12:34:02.386
  May  7 12:34:03.407: INFO: successfully validated that service multi-endpoint-test in namespace services-364 exposes endpoints map[]
  May  7 12:34:03.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-364" for this suite. @ 05/07/23 12:34:03.422
• [9.565 seconds]
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 05/07/23 12:34:03.424
  May  7 12:34:03.424: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/07/23 12:34:03.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:34:03.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:34:03.437
  STEP: creating a target pod @ 05/07/23 12:34:03.438
  STEP: adding an ephemeral container @ 05/07/23 12:34:05.452
  STEP: checking pod container endpoints @ 05/07/23 12:34:07.46
  May  7 12:34:07.461: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6665 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:34:07.461: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:34:07.461: INFO: ExecWithOptions: Clientset creation
  May  7 12:34:07.461: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/ephemeral-containers-test-6665/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  May  7 12:34:07.525: INFO: Exec stderr: ""
  May  7 12:34:07.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-6665" for this suite. @ 05/07/23 12:34:07.529
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 05/07/23 12:34:07.533
  May  7 12:34:07.533: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 12:34:07.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:34:07.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:34:07.544
  STEP: Creating configMap with name configmap-test-upd-1c22cd05-8d03-43d6-8e95-872fb442eba6 @ 05/07/23 12:34:07.548
  STEP: Creating the pod @ 05/07/23 12:34:07.55
  STEP: Updating configmap configmap-test-upd-1c22cd05-8d03-43d6-8e95-872fb442eba6 @ 05/07/23 12:34:09.561
  STEP: waiting to observe update in volume @ 05/07/23 12:34:09.564
  May  7 12:35:15.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8151" for this suite. @ 05/07/23 12:35:15.734
• [68.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 05/07/23 12:35:15.737
  May  7 12:35:15.737: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename endpointslice @ 05/07/23 12:35:15.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:35:15.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:35:15.756
  STEP: referencing a single matching pod @ 05/07/23 12:35:20.837
  STEP: referencing matching pods with named port @ 05/07/23 12:35:25.84
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/07/23 12:35:30.846
  STEP: recreating EndpointSlices after they've been deleted @ 05/07/23 12:35:35.85
  May  7 12:35:35.860: INFO: EndpointSlice for Service endpointslice-5808/example-named-port not found
  May  7 12:35:45.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5808" for this suite. @ 05/07/23 12:35:45.869
• [30.135 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 05/07/23 12:35:45.872
  May  7 12:35:45.872: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename gc @ 05/07/23 12:35:45.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:35:45.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:35:45.884
  May  7 12:35:45.913: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b28cf251-356f-428b-b981-77c0fa9de877", Controller:(*bool)(0xc003b1f4b6), BlockOwnerDeletion:(*bool)(0xc003b1f4b7)}}
  May  7 12:35:45.918: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"837b32a8-8020-449d-8bb7-a7dd207cd99c", Controller:(*bool)(0xc003b1fee6), BlockOwnerDeletion:(*bool)(0xc003b1fee7)}}
  May  7 12:35:45.921: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"35a52667-8d3a-4725-842e-fe9234149404", Controller:(*bool)(0xc00442c1b6), BlockOwnerDeletion:(*bool)(0xc00442c1b7)}}
  May  7 12:35:50.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-905" for this suite. @ 05/07/23 12:35:50.946
• [5.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 05/07/23 12:35:50.954
  May  7 12:35:50.954: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 12:35:50.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:35:50.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:35:50.98
  STEP: Creating a pod to test downward api env vars @ 05/07/23 12:35:50.982
  STEP: Saw pod success @ 05/07/23 12:35:55.022
  May  7 12:35:55.023: INFO: Trying to get logs from node 10.255.0.202 pod downward-api-324c5f8a-792b-4429-ba85-cf8dafdca4c2 container dapi-container: <nil>
  STEP: delete the pod @ 05/07/23 12:35:55.026
  May  7 12:35:55.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6753" for this suite. @ 05/07/23 12:35:55.046
• [4.094 seconds]
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 05/07/23 12:35:55.048
  May  7 12:35:55.048: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename security-context @ 05/07/23 12:35:55.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:35:55.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:35:55.059
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/07/23 12:35:55.061
  STEP: Saw pod success @ 05/07/23 12:35:59.073
  May  7 12:35:59.074: INFO: Trying to get logs from node 10.255.0.202 pod security-context-216227d4-474c-4438-92d0-81ec7b8f4591 container test-container: <nil>
  STEP: delete the pod @ 05/07/23 12:35:59.077
  May  7 12:35:59.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-9005" for this suite. @ 05/07/23 12:35:59.086
• [4.040 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/07/23 12:35:59.09
  May  7 12:35:59.090: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 12:35:59.091
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:35:59.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:35:59.101
  STEP: Creating secret with name secret-test-530fd797-e027-42d8-99f8-b22a4efd9a21 @ 05/07/23 12:35:59.103
  STEP: Creating a pod to test consume secrets @ 05/07/23 12:35:59.105
  STEP: Saw pod success @ 05/07/23 12:36:03.117
  May  7 12:36:03.118: INFO: Trying to get logs from node 10.255.0.202 pod pod-secrets-22ae1c82-cebd-4acc-84da-3ca167eac467 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 12:36:03.121
  May  7 12:36:03.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2798" for this suite. @ 05/07/23 12:36:03.13
• [4.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/07/23 12:36:03.132
  May  7 12:36:03.132: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename tables @ 05/07/23 12:36:03.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:03.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:03.145
  May  7 12:36:03.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-8715" for this suite. @ 05/07/23 12:36:03.148
• [0.019 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 05/07/23 12:36:03.152
  May  7 12:36:03.152: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:36:03.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:03.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:03.162
  STEP: Setting up server cert @ 05/07/23 12:36:03.176
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:36:03.494
  STEP: Deploying the webhook pod @ 05/07/23 12:36:03.498
  STEP: Wait for the deployment to be ready @ 05/07/23 12:36:03.504
  May  7 12:36:03.507: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/07/23 12:36:05.512
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:36:05.517
  May  7 12:36:06.517: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May  7 12:36:06.519: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8037-crds.webhook.example.com via the AdmissionRegistration API @ 05/07/23 12:36:07.031
  STEP: Creating a custom resource while v1 is storage version @ 05/07/23 12:36:07.039
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/07/23 12:36:09.064
  STEP: Patching the custom resource while v2 is storage version @ 05/07/23 12:36:09.067
  May  7 12:36:09.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1869" for this suite. @ 05/07/23 12:36:09.667
  STEP: Destroying namespace "webhook-markers-6249" for this suite. @ 05/07/23 12:36:09.67
• [6.520 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 05/07/23 12:36:09.672
  May  7 12:36:09.672: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/07/23 12:36:09.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:09.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:09.683
  STEP: create the container to handle the HTTPGet hook request. @ 05/07/23 12:36:09.686
  STEP: create the pod with lifecycle hook @ 05/07/23 12:36:11.696
  STEP: check poststart hook @ 05/07/23 12:36:13.704
  STEP: delete the pod with lifecycle hook @ 05/07/23 12:36:13.714
  May  7 12:36:15.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4422" for this suite. @ 05/07/23 12:36:15.722
• [6.055 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/07/23 12:36:15.727
  May  7 12:36:15.727: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 12:36:15.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:15.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:15.738
  May  7 12:36:15.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5438" for this suite. @ 05/07/23 12:36:15.755
• [0.030 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 05/07/23 12:36:15.758
  May  7 12:36:15.758: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sysctl @ 05/07/23 12:36:15.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:15.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:15.768
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/07/23 12:36:15.77
  STEP: Watching for error events or started pod @ 05/07/23 12:36:15.772
  STEP: Waiting for pod completion @ 05/07/23 12:36:19.774
  STEP: Checking that the pod succeeded @ 05/07/23 12:36:19.776
  STEP: Getting logs from the pod @ 05/07/23 12:36:19.776
  STEP: Checking that the sysctl is actually updated @ 05/07/23 12:36:19.779
  May  7 12:36:19.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-6877" for this suite. @ 05/07/23 12:36:19.781
• [4.026 seconds]
------------------------------
SSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 05/07/23 12:36:19.784
  May  7 12:36:19.784: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename podtemplate @ 05/07/23 12:36:19.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:19.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:19.794
  STEP: Create set of pod templates @ 05/07/23 12:36:19.797
  May  7 12:36:19.799: INFO: created test-podtemplate-1
  May  7 12:36:19.801: INFO: created test-podtemplate-2
  May  7 12:36:19.803: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/07/23 12:36:19.803
  STEP: delete collection of pod templates @ 05/07/23 12:36:19.804
  May  7 12:36:19.804: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 05/07/23 12:36:19.81
  May  7 12:36:19.810: INFO: requesting list of pod templates to confirm quantity
  May  7 12:36:19.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-9128" for this suite. @ 05/07/23 12:36:19.813
• [0.031 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 05/07/23 12:36:19.815
  May  7 12:36:19.815: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:36:19.816
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:19.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:19.828
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 12:36:19.83
  STEP: Saw pod success @ 05/07/23 12:36:21.842
  May  7 12:36:21.843: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-75364a36-f204-4b7c-b32d-a6b7e1334d05 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 12:36:21.845
  May  7 12:36:21.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8783" for this suite. @ 05/07/23 12:36:21.856
• [2.044 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/07/23 12:36:21.859
  May  7 12:36:21.859: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubelet-test @ 05/07/23 12:36:21.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:21.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:21.869
  STEP: Waiting for pod completion @ 05/07/23 12:36:21.874
  May  7 12:36:23.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3270" for this suite. @ 05/07/23 12:36:23.882
• [2.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 05/07/23 12:36:23.894
  May  7 12:36:23.894: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 12:36:23.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:23.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:23.906
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7518 @ 05/07/23 12:36:23.908
  STEP: changing the ExternalName service to type=ClusterIP @ 05/07/23 12:36:23.91
  STEP: creating replication controller externalname-service in namespace services-7518 @ 05/07/23 12:36:23.915
  I0507 12:36:23.924757      20 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7518, replica count: 2
  I0507 12:36:26.975892      20 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  7 12:36:26.975: INFO: Creating new exec pod
  May  7 12:36:29.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-7518 exec execpods4qrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May  7 12:36:30.094: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May  7 12:36:30.094: INFO: stdout: "externalname-service-qtnsf"
  May  7 12:36:30.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-7518 exec execpods4qrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.98.37 80'
  May  7 12:36:30.177: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.98.37 80\nConnection to 10.68.98.37 80 port [tcp/http] succeeded!\n"
  May  7 12:36:30.177: INFO: stdout: ""
  May  7 12:36:31.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-7518 exec execpods4qrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.98.37 80'
  May  7 12:36:31.277: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.98.37 80\nConnection to 10.68.98.37 80 port [tcp/http] succeeded!\n"
  May  7 12:36:31.277: INFO: stdout: "externalname-service-qtnsf"
  May  7 12:36:31.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 12:36:31.279: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-7518" for this suite. @ 05/07/23 12:36:31.296
• [7.406 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 05/07/23 12:36:31.301
  May  7 12:36:31.301: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename subpath @ 05/07/23 12:36:31.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:31.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:31.321
  STEP: Setting up data @ 05/07/23 12:36:31.322
  STEP: Creating pod pod-subpath-test-configmap-k2jw @ 05/07/23 12:36:31.327
  STEP: Creating a pod to test atomic-volume-subpath @ 05/07/23 12:36:31.327
  STEP: Saw pod success @ 05/07/23 12:36:55.362
  May  7 12:36:55.364: INFO: Trying to get logs from node 10.255.0.202 pod pod-subpath-test-configmap-k2jw container test-container-subpath-configmap-k2jw: <nil>
  STEP: delete the pod @ 05/07/23 12:36:55.369
  STEP: Deleting pod pod-subpath-test-configmap-k2jw @ 05/07/23 12:36:55.376
  May  7 12:36:55.376: INFO: Deleting pod "pod-subpath-test-configmap-k2jw" in namespace "subpath-3934"
  May  7 12:36:55.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3934" for this suite. @ 05/07/23 12:36:55.378
• [24.080 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 05/07/23 12:36:55.381
  May  7 12:36:55.381: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 12:36:55.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:36:55.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:36:55.393
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7211 @ 05/07/23 12:36:55.395
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/07/23 12:36:55.4
  STEP: creating service externalsvc in namespace services-7211 @ 05/07/23 12:36:55.4
  STEP: creating replication controller externalsvc in namespace services-7211 @ 05/07/23 12:36:55.413
  I0507 12:36:55.417966      20 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7211, replica count: 2
  I0507 12:36:58.468851      20 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/07/23 12:36:58.47
  May  7 12:36:58.476: INFO: Creating new exec pod
  May  7 12:37:00.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-7211 exec execpoddmtfj -- /bin/sh -x -c nslookup clusterip-service.services-7211.svc.cluster.local'
  May  7 12:37:00.617: INFO: stderr: "+ nslookup clusterip-service.services-7211.svc.cluster.local\n"
  May  7 12:37:00.617: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-7211.svc.cluster.local\tcanonical name = externalsvc.services-7211.svc.cluster.local.\nName:\texternalsvc.services-7211.svc.cluster.local\nAddress: 10.68.228.95\n\n"
  May  7 12:37:00.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7211, will wait for the garbage collector to delete the pods @ 05/07/23 12:37:00.619
  May  7 12:37:00.673: INFO: Deleting ReplicationController externalsvc took: 2.409177ms
  May  7 12:37:00.774: INFO: Terminating ReplicationController externalsvc pods took: 100.681773ms
  May  7 12:37:02.986: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-7211" for this suite. @ 05/07/23 12:37:03
• [7.625 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/07/23 12:37:03.006
  May  7 12:37:03.006: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubelet-test @ 05/07/23 12:37:03.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:37:03.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:37:03.022
  May  7 12:37:07.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6976" for this suite. @ 05/07/23 12:37:07.039
• [4.036 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 05/07/23 12:37:07.042
  May  7 12:37:07.042: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-probe @ 05/07/23 12:37:07.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:37:07.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:37:07.055
  STEP: Creating pod test-webserver-16ddeaf6-a9a8-44a9-a8dd-26493d8a3743 in namespace container-probe-4501 @ 05/07/23 12:37:07.056
  May  7 12:37:09.065: INFO: Started pod test-webserver-16ddeaf6-a9a8-44a9-a8dd-26493d8a3743 in namespace container-probe-4501
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/07/23 12:37:09.065
  May  7 12:37:09.066: INFO: Initial restart count of pod test-webserver-16ddeaf6-a9a8-44a9-a8dd-26493d8a3743 is 0
  May  7 12:41:09.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:41:09.347
  STEP: Destroying namespace "container-probe-4501" for this suite. @ 05/07/23 12:41:09.356
• [242.316 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 05/07/23 12:41:09.359
  May  7 12:41:09.359: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:41:09.359
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:41:09.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:41:09.375
  STEP: Setting up server cert @ 05/07/23 12:41:09.392
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:41:09.62
  STEP: Deploying the webhook pod @ 05/07/23 12:41:09.623
  STEP: Wait for the deployment to be ready @ 05/07/23 12:41:09.628
  May  7 12:41:09.633: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/07/23 12:41:11.638
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:41:11.643
  May  7 12:41:12.643: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/07/23 12:41:12.644
  STEP: create a namespace for the webhook @ 05/07/23 12:41:12.653
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/07/23 12:41:12.66
  May  7 12:41:12.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-187" for this suite. @ 05/07/23 12:41:12.699
  STEP: Destroying namespace "webhook-markers-916" for this suite. @ 05/07/23 12:41:12.709
  STEP: Destroying namespace "fail-closed-namespace-9063" for this suite. @ 05/07/23 12:41:12.714
• [3.360 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 05/07/23 12:41:12.719
  May  7 12:41:12.719: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename podtemplate @ 05/07/23 12:41:12.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:41:12.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:41:12.74
  STEP: Create a pod template @ 05/07/23 12:41:12.742
  STEP: Replace a pod template @ 05/07/23 12:41:12.746
  May  7 12:41:12.749: INFO: Found updated podtemplate annotation: "true"

  May  7 12:41:12.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6911" for this suite. @ 05/07/23 12:41:12.751
• [0.034 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 05/07/23 12:41:12.754
  May  7 12:41:12.754: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename endpointslice @ 05/07/23 12:41:12.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:41:12.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:41:12.766
  May  7 12:41:12.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7479" for this suite. @ 05/07/23 12:41:12.806
• [0.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 05/07/23 12:41:12.81
  May  7 12:41:12.810: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename namespaces @ 05/07/23 12:41:12.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:41:12.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:41:12.822
  STEP: Creating a test namespace @ 05/07/23 12:41:12.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:41:12.839
  STEP: Creating a service in the namespace @ 05/07/23 12:41:12.842
  STEP: Deleting the namespace @ 05/07/23 12:41:12.846
  STEP: Waiting for the namespace to be removed. @ 05/07/23 12:41:12.851
  STEP: Recreating the namespace @ 05/07/23 12:41:18.853
  STEP: Verifying there is no service in the namespace @ 05/07/23 12:41:18.861
  May  7 12:41:18.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3978" for this suite. @ 05/07/23 12:41:18.865
  STEP: Destroying namespace "nsdeletetest-6358" for this suite. @ 05/07/23 12:41:18.868
  May  7 12:41:18.869: INFO: Namespace nsdeletetest-6358 was already deleted
  STEP: Destroying namespace "nsdeletetest-8151" for this suite. @ 05/07/23 12:41:18.869
• [6.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/07/23 12:41:18.872
  May  7 12:41:18.872: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename containers @ 05/07/23 12:41:18.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:41:18.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:41:18.882
  STEP: Creating a pod to test override arguments @ 05/07/23 12:41:18.884
  STEP: Saw pod success @ 05/07/23 12:41:22.894
  May  7 12:41:22.895: INFO: Trying to get logs from node 10.255.0.202 pod client-containers-c8fe3605-3835-4042-aff5-914d7969a055 container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 12:41:22.905
  May  7 12:41:22.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8630" for this suite. @ 05/07/23 12:41:22.913
• [4.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 05/07/23 12:41:22.917
  May  7 12:41:22.917: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 12:41:22.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:41:22.928
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:41:22.929
  STEP: Creating configMap with name cm-test-opt-del-06073320-1e51-427b-842d-ba91301745b6 @ 05/07/23 12:41:22.933
  STEP: Creating configMap with name cm-test-opt-upd-51b69478-2fa4-465e-bfbd-8c4be12720ae @ 05/07/23 12:41:22.935
  STEP: Creating the pod @ 05/07/23 12:41:22.936
  STEP: Deleting configmap cm-test-opt-del-06073320-1e51-427b-842d-ba91301745b6 @ 05/07/23 12:41:24.953
  STEP: Updating configmap cm-test-opt-upd-51b69478-2fa4-465e-bfbd-8c4be12720ae @ 05/07/23 12:41:24.955
  STEP: Creating configMap with name cm-test-opt-create-1a117d79-fcaa-4872-8256-93d543a40ae6 @ 05/07/23 12:41:24.957
  STEP: waiting to observe update in volume @ 05/07/23 12:41:24.959
  May  7 12:42:51.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4993" for this suite. @ 05/07/23 12:42:51.206
• [88.292 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 05/07/23 12:42:51.21
  May  7 12:42:51.210: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename limitrange @ 05/07/23 12:42:51.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:42:51.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:42:51.231
  STEP: Creating a LimitRange @ 05/07/23 12:42:51.232
  STEP: Setting up watch @ 05/07/23 12:42:51.233
  STEP: Submitting a LimitRange @ 05/07/23 12:42:51.334
  STEP: Verifying LimitRange creation was observed @ 05/07/23 12:42:51.337
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/07/23 12:42:51.337
  May  7 12:42:51.338: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May  7 12:42:51.338: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/07/23 12:42:51.338
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/07/23 12:42:51.34
  May  7 12:42:51.341: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May  7 12:42:51.342: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/07/23 12:42:51.342
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/07/23 12:42:51.346
  May  7 12:42:51.348: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  May  7 12:42:51.348: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/07/23 12:42:51.348
  STEP: Failing to create a Pod with more than max resources @ 05/07/23 12:42:51.349
  STEP: Updating a LimitRange @ 05/07/23 12:42:51.352
  STEP: Verifying LimitRange updating is effective @ 05/07/23 12:42:51.356
  STEP: Creating a Pod with less than former min resources @ 05/07/23 12:42:53.358
  STEP: Failing to create a Pod with more than max resources @ 05/07/23 12:42:53.361
  STEP: Deleting a LimitRange @ 05/07/23 12:42:53.364
  STEP: Verifying the LimitRange was deleted @ 05/07/23 12:42:53.369
  May  7 12:42:58.371: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/07/23 12:42:58.371
  May  7 12:42:58.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-3255" for this suite. @ 05/07/23 12:42:58.381
• [7.175 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 05/07/23 12:42:58.385
  May  7 12:42:58.385: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename statefulset @ 05/07/23 12:42:58.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:42:58.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:42:58.396
  STEP: Creating service test in namespace statefulset-2172 @ 05/07/23 12:42:58.398
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/07/23 12:42:58.401
  STEP: Creating stateful set ss in namespace statefulset-2172 @ 05/07/23 12:42:58.411
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2172 @ 05/07/23 12:42:58.415
  May  7 12:42:58.416: INFO: Found 0 stateful pods, waiting for 1
  May  7 12:43:08.419: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/07/23 12:43:08.419
  May  7 12:43:08.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-2172 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  7 12:43:08.523: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  7 12:43:08.523: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  7 12:43:08.523: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  7 12:43:08.524: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May  7 12:43:18.526: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  7 12:43:18.526: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 12:43:18.533: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999876s
  May  7 12:43:19.535: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997874263s
  May  7 12:43:20.538: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99531136s
  May  7 12:43:21.539: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.993571753s
  May  7 12:43:22.542: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991112347s
  May  7 12:43:23.544: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.989117889s
  May  7 12:43:24.546: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.98730567s
  May  7 12:43:25.549: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.984565471s
  May  7 12:43:26.551: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.981864131s
  May  7 12:43:27.553: INFO: Verifying statefulset ss doesn't scale past 1 for another 979.939821ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2172 @ 05/07/23 12:43:28.553
  May  7 12:43:28.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-2172 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  7 12:43:28.656: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  7 12:43:28.656: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  7 12:43:28.656: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  7 12:43:28.660: INFO: Found 1 stateful pods, waiting for 3
  May  7 12:43:38.663: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May  7 12:43:38.663: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May  7 12:43:38.663: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/07/23 12:43:38.663
  STEP: Scale down will halt with unhealthy stateful pod @ 05/07/23 12:43:38.663
  May  7 12:43:38.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-2172 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  7 12:43:38.765: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  7 12:43:38.765: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  7 12:43:38.765: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  7 12:43:38.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-2172 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  7 12:43:38.872: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  7 12:43:38.872: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  7 12:43:38.872: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  7 12:43:38.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-2172 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  7 12:43:38.991: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  7 12:43:38.991: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  7 12:43:38.991: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  7 12:43:38.991: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 12:43:38.992: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  May  7 12:43:48.997: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  7 12:43:48.997: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May  7 12:43:48.997: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May  7 12:43:49.003: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999863s
  May  7 12:43:50.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997796346s
  May  7 12:43:51.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993537697s
  May  7 12:43:52.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990550487s
  May  7 12:43:53.015: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987750336s
  May  7 12:43:54.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98518232s
  May  7 12:43:55.020: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983173787s
  May  7 12:43:56.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.981224385s
  May  7 12:43:57.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.97922405s
  May  7 12:43:58.026: INFO: Verifying statefulset ss doesn't scale past 3 for another 977.287913ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2172 @ 05/07/23 12:43:59.027
  May  7 12:43:59.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-2172 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  7 12:43:59.129: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  7 12:43:59.129: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  7 12:43:59.129: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  7 12:43:59.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-2172 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  7 12:43:59.254: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  7 12:43:59.254: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  7 12:43:59.254: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  7 12:43:59.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-2172 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  7 12:43:59.389: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  7 12:43:59.389: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  7 12:43:59.389: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  7 12:43:59.389: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/07/23 12:44:09.397
  May  7 12:44:09.397: INFO: Deleting all statefulset in ns statefulset-2172
  May  7 12:44:09.398: INFO: Scaling statefulset ss to 0
  May  7 12:44:09.402: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 12:44:09.403: INFO: Deleting statefulset ss
  May  7 12:44:09.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2172" for this suite. @ 05/07/23 12:44:09.411
• [71.031 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/07/23 12:44:09.416
  May  7 12:44:09.416: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename containers @ 05/07/23 12:44:09.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:44:09.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:44:09.429
  STEP: Creating a pod to test override command @ 05/07/23 12:44:09.43
  STEP: Saw pod success @ 05/07/23 12:44:13.439
  May  7 12:44:13.440: INFO: Trying to get logs from node 10.255.0.202 pod client-containers-261a0a9e-f68b-45f3-b832-14212cf8b2bd container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 12:44:13.443
  May  7 12:44:13.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4782" for this suite. @ 05/07/23 12:44:13.452
• [4.038 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 05/07/23 12:44:13.455
  May  7 12:44:13.455: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-probe @ 05/07/23 12:44:13.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:44:13.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:44:13.467
  STEP: Creating pod test-grpc-285b0e42-301a-4d17-bb8f-0f077af16831 in namespace container-probe-3039 @ 05/07/23 12:44:13.468
  May  7 12:44:15.476: INFO: Started pod test-grpc-285b0e42-301a-4d17-bb8f-0f077af16831 in namespace container-probe-3039
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/07/23 12:44:15.476
  May  7 12:44:15.478: INFO: Initial restart count of pod test-grpc-285b0e42-301a-4d17-bb8f-0f077af16831 is 0
  May  7 12:48:15.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:48:15.751
  STEP: Destroying namespace "container-probe-3039" for this suite. @ 05/07/23 12:48:15.759
• [242.309 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 05/07/23 12:48:15.764
  May  7 12:48:15.764: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename statefulset @ 05/07/23 12:48:15.764
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:48:15.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:48:15.775
  STEP: Creating service test in namespace statefulset-5645 @ 05/07/23 12:48:15.777
  STEP: Creating stateful set ss in namespace statefulset-5645 @ 05/07/23 12:48:15.779
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5645 @ 05/07/23 12:48:15.784
  May  7 12:48:15.790: INFO: Found 0 stateful pods, waiting for 1
  May  7 12:48:25.794: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/07/23 12:48:25.794
  May  7 12:48:25.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5645 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  7 12:48:25.903: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  7 12:48:25.903: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  7 12:48:25.903: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  7 12:48:25.904: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May  7 12:48:35.907: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  7 12:48:35.907: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 12:48:35.915: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
  May  7 12:48:35.915: INFO: ss-0  10.255.0.202  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:15 +0000 UTC  }]
  May  7 12:48:35.915: INFO: 
  May  7 12:48:35.915: INFO: StatefulSet ss has not reached scale 3, at 1
  May  7 12:48:36.917: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997737586s
  May  7 12:48:37.919: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995504688s
  May  7 12:48:38.921: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.99350953s
  May  7 12:48:39.923: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.991400161s
  May  7 12:48:40.926: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.988421689s
  May  7 12:48:41.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.985999043s
  May  7 12:48:42.931: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.983437569s
  May  7 12:48:43.933: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.981558786s
  May  7 12:48:44.935: INFO: Verifying statefulset ss doesn't scale past 3 for another 979.438332ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5645 @ 05/07/23 12:48:45.935
  May  7 12:48:45.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5645 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  7 12:48:46.037: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May  7 12:48:46.037: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  7 12:48:46.037: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  7 12:48:46.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5645 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  7 12:48:46.123: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May  7 12:48:46.123: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  7 12:48:46.123: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  7 12:48:46.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5645 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May  7 12:48:46.229: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May  7 12:48:46.229: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May  7 12:48:46.229: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May  7 12:48:46.231: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May  7 12:48:46.231: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May  7 12:48:46.231: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/07/23 12:48:46.231
  May  7 12:48:46.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5645 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  7 12:48:46.317: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  7 12:48:46.317: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  7 12:48:46.317: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  7 12:48:46.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5645 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  7 12:48:46.400: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  7 12:48:46.400: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  7 12:48:46.400: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  7 12:48:46.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=statefulset-5645 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May  7 12:48:46.501: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May  7 12:48:46.501: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May  7 12:48:46.501: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May  7 12:48:46.501: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 12:48:46.503: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  May  7 12:48:56.508: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May  7 12:48:56.508: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May  7 12:48:56.508: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May  7 12:48:56.514: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
  May  7 12:48:56.514: INFO: ss-0  10.255.0.202  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:15 +0000 UTC  }]
  May  7 12:48:56.514: INFO: ss-1  10.255.0.203  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:35 +0000 UTC  }]
  May  7 12:48:56.514: INFO: ss-2  10.255.0.201  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:35 +0000 UTC  }]
  May  7 12:48:56.514: INFO: 
  May  7 12:48:56.514: INFO: StatefulSet ss has not reached scale 0, at 3
  May  7 12:48:57.525: INFO: POD   NODE          PHASE      GRACE  CONDITIONS
  May  7 12:48:57.525: INFO: ss-0  10.255.0.202  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:15 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:15 +0000 UTC  }]
  May  7 12:48:57.525: INFO: ss-1  10.255.0.203  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:35 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:35 +0000 UTC  }]
  May  7 12:48:57.525: INFO: ss-2  10.255.0.201  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:35 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:47 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-07 12:48:35 +0000 UTC  }]
  May  7 12:48:57.525: INFO: 
  May  7 12:48:57.525: INFO: StatefulSet ss has not reached scale 0, at 3
  May  7 12:48:58.527: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.985779599s
  May  7 12:48:59.529: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.984189738s
  May  7 12:49:00.531: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98220797s
  May  7 12:49:01.533: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.980536623s
  May  7 12:49:02.535: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.978201462s
  May  7 12:49:03.537: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.975699247s
  May  7 12:49:04.539: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.974062279s
  May  7 12:49:05.541: INFO: Verifying statefulset ss doesn't scale past 0 for another 972.370461ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5645 @ 05/07/23 12:49:06.541
  May  7 12:49:06.543: INFO: Scaling statefulset ss to 0
  May  7 12:49:06.553: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 12:49:06.555: INFO: Deleting all statefulset in ns statefulset-5645
  May  7 12:49:06.556: INFO: Scaling statefulset ss to 0
  May  7 12:49:06.559: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 12:49:06.560: INFO: Deleting statefulset ss
  May  7 12:49:06.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5645" for this suite. @ 05/07/23 12:49:06.576
• [50.815 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 05/07/23 12:49:06.578
  May  7 12:49:06.578: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:49:06.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:06.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:06.589
  STEP: Setting up server cert @ 05/07/23 12:49:06.608
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:49:06.808
  STEP: Deploying the webhook pod @ 05/07/23 12:49:06.812
  STEP: Wait for the deployment to be ready @ 05/07/23 12:49:06.818
  May  7 12:49:06.821: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/07/23 12:49:08.826
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:49:08.832
  May  7 12:49:09.832: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/07/23 12:49:09.834
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/07/23 12:49:09.842
  STEP: Creating a dummy validating-webhook-configuration object @ 05/07/23 12:49:09.85
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/07/23 12:49:09.854
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/07/23 12:49:09.856
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/07/23 12:49:09.86
  May  7 12:49:09.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1565" for this suite. @ 05/07/23 12:49:09.906
  STEP: Destroying namespace "webhook-markers-2215" for this suite. @ 05/07/23 12:49:09.909
• [3.335 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 05/07/23 12:49:09.913
  May  7 12:49:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename job @ 05/07/23 12:49:09.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:09.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:09.929
  STEP: Creating a job @ 05/07/23 12:49:09.93
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/07/23 12:49:09.933
  STEP: patching /status @ 05/07/23 12:49:11.935
  STEP: updating /status @ 05/07/23 12:49:11.939
  STEP: get /status @ 05/07/23 12:49:11.942
  May  7 12:49:11.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8334" for this suite. @ 05/07/23 12:49:11.945
• [2.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 05/07/23 12:49:11.948
  May  7 12:49:11.948: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 12:49:11.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:11.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:11.958
  STEP: creating all guestbook components @ 05/07/23 12:49:11.96
  May  7 12:49:11.960: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  May  7 12:49:11.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 create -f -'
  May  7 12:49:12.233: INFO: stderr: ""
  May  7 12:49:12.233: INFO: stdout: "service/agnhost-replica created\n"
  May  7 12:49:12.233: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  May  7 12:49:12.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 create -f -'
  May  7 12:49:12.452: INFO: stderr: ""
  May  7 12:49:12.452: INFO: stdout: "service/agnhost-primary created\n"
  May  7 12:49:12.452: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  May  7 12:49:12.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 create -f -'
  May  7 12:49:12.667: INFO: stderr: ""
  May  7 12:49:12.667: INFO: stdout: "service/frontend created\n"
  May  7 12:49:12.667: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  May  7 12:49:12.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 create -f -'
  May  7 12:49:12.872: INFO: stderr: ""
  May  7 12:49:12.872: INFO: stdout: "deployment.apps/frontend created\n"
  May  7 12:49:12.872: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May  7 12:49:12.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 create -f -'
  May  7 12:49:13.026: INFO: stderr: ""
  May  7 12:49:13.026: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  May  7 12:49:13.026: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May  7 12:49:13.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 create -f -'
  May  7 12:49:13.178: INFO: stderr: ""
  May  7 12:49:13.178: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/07/23 12:49:13.178
  May  7 12:49:13.178: INFO: Waiting for all frontend pods to be Running.
  May  7 12:49:18.230: INFO: Waiting for frontend to serve content.
  May  7 12:49:18.240: INFO: Trying to add a new entry to the guestbook.
  May  7 12:49:18.250: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 05/07/23 12:49:18.26
  May  7 12:49:18.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 delete --grace-period=0 --force -f -'
  May  7 12:49:18.324: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  7 12:49:18.324: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/07/23 12:49:18.324
  May  7 12:49:18.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 delete --grace-period=0 --force -f -'
  May  7 12:49:18.403: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  7 12:49:18.403: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/07/23 12:49:18.403
  May  7 12:49:18.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 delete --grace-period=0 --force -f -'
  May  7 12:49:18.469: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  7 12:49:18.469: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/07/23 12:49:18.469
  May  7 12:49:18.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 delete --grace-period=0 --force -f -'
  May  7 12:49:18.539: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  7 12:49:18.539: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/07/23 12:49:18.539
  May  7 12:49:18.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 delete --grace-period=0 --force -f -'
  May  7 12:49:18.600: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  7 12:49:18.600: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/07/23 12:49:18.6
  May  7 12:49:18.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1558 delete --grace-period=0 --force -f -'
  May  7 12:49:18.667: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  7 12:49:18.667: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  May  7 12:49:18.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1558" for this suite. @ 05/07/23 12:49:18.67
• [6.730 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 05/07/23 12:49:18.679
  May  7 12:49:18.679: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/07/23 12:49:18.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:18.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:18.713
  STEP: creating @ 05/07/23 12:49:18.715
  STEP: getting @ 05/07/23 12:49:18.741
  STEP: listing @ 05/07/23 12:49:18.749
  STEP: deleting @ 05/07/23 12:49:18.751
  May  7 12:49:18.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-7674" for this suite. @ 05/07/23 12:49:18.763
• [0.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 05/07/23 12:49:18.767
  May  7 12:49:18.767: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 12:49:18.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:18.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:18.782
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/07/23 12:49:18.785
  STEP: Saw pod success @ 05/07/23 12:49:22.827
  May  7 12:49:22.828: INFO: Trying to get logs from node 10.255.0.202 pod pod-0eeee594-3d4f-4228-bdd1-c8afff918c1f container test-container: <nil>
  STEP: delete the pod @ 05/07/23 12:49:22.84
  May  7 12:49:22.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5920" for this suite. @ 05/07/23 12:49:22.848
• [4.084 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 05/07/23 12:49:22.851
  May  7 12:49:22.851: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 12:49:22.852
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:22.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:22.863
  STEP: Creating a ResourceQuota with best effort scope @ 05/07/23 12:49:22.865
  STEP: Ensuring ResourceQuota status is calculated @ 05/07/23 12:49:22.867
  STEP: Creating a ResourceQuota with not best effort scope @ 05/07/23 12:49:24.868
  STEP: Ensuring ResourceQuota status is calculated @ 05/07/23 12:49:24.87
  STEP: Creating a best-effort pod @ 05/07/23 12:49:26.873
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/07/23 12:49:26.882
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/07/23 12:49:28.885
  STEP: Deleting the pod @ 05/07/23 12:49:30.887
  STEP: Ensuring resource quota status released the pod usage @ 05/07/23 12:49:30.897
  STEP: Creating a not best-effort pod @ 05/07/23 12:49:32.9
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/07/23 12:49:32.904
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/07/23 12:49:34.906
  STEP: Deleting the pod @ 05/07/23 12:49:36.909
  STEP: Ensuring resource quota status released the pod usage @ 05/07/23 12:49:36.918
  May  7 12:49:38.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3393" for this suite. @ 05/07/23 12:49:38.923
• [16.074 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 05/07/23 12:49:38.926
  May  7 12:49:38.926: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 12:49:38.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:38.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:38.94
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/07/23 12:49:38.942
  STEP: Saw pod success @ 05/07/23 12:49:40.951
  May  7 12:49:40.952: INFO: Trying to get logs from node 10.255.0.202 pod pod-72a13419-7593-45ac-ab7d-454b23868f82 container test-container: <nil>
  STEP: delete the pod @ 05/07/23 12:49:40.955
  May  7 12:49:40.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1470" for this suite. @ 05/07/23 12:49:40.963
• [2.039 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 05/07/23 12:49:40.965
  May  7 12:49:40.965: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 12:49:40.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:40.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:40.974
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/07/23 12:49:40.977
  May  7 12:49:40.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-6420 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May  7 12:49:41.023: INFO: stderr: ""
  May  7 12:49:41.023: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/07/23 12:49:41.023
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/07/23 12:49:46.077
  May  7 12:49:46.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-6420 get pod e2e-test-httpd-pod -o json'
  May  7 12:49:46.124: INFO: stderr: ""
  May  7 12:49:46.124: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-05-07T12:49:41Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6420\",\n        \"resourceVersion\": \"70355\",\n        \"uid\": \"b9c2862e-240e-4a13-9996-3dbde2b274a6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5fgnb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.255.0.202\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5fgnb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-07T12:49:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-07T12:49:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-07T12:49:41Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-07T12:49:41Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://37d43cb640e77bd80489821076296b0e28ce2d073d5256330b0b98c32ccaadc7\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-07T12:49:41Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.255.0.202\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.20.231.252\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.20.231.252\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-07T12:49:41Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/07/23 12:49:46.124
  May  7 12:49:46.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-6420 replace -f -'
  May  7 12:49:46.310: INFO: stderr: ""
  May  7 12:49:46.310: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 05/07/23 12:49:46.31
  May  7 12:49:46.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-6420 delete pods e2e-test-httpd-pod'
  May  7 12:49:47.797: INFO: stderr: ""
  May  7 12:49:47.798: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May  7 12:49:47.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6420" for this suite. @ 05/07/23 12:49:47.799
• [6.838 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 05/07/23 12:49:47.803
  May  7 12:49:47.803: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/07/23 12:49:47.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:47.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:47.815
  STEP: creating @ 05/07/23 12:49:47.816
  STEP: getting @ 05/07/23 12:49:47.825
  STEP: listing in namespace @ 05/07/23 12:49:47.83
  STEP: patching @ 05/07/23 12:49:47.831
  STEP: deleting @ 05/07/23 12:49:47.838
  May  7 12:49:47.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3454" for this suite. @ 05/07/23 12:49:47.844
• [0.043 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 05/07/23 12:49:47.847
  May  7 12:49:47.847: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/07/23 12:49:47.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:47.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:47.858
  May  7 12:49:47.859: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:49:50.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8156" for this suite. @ 05/07/23 12:49:50.927
• [3.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 05/07/23 12:49:50.937
  May  7 12:49:50.937: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename dns @ 05/07/23 12:49:50.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:49:50.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:49:50.951
  STEP: Creating a test headless service @ 05/07/23 12:49:50.952
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8666.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8666.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8666.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 238.68.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.68.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.68.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.68.238_tcp@PTR;sleep 1; done
   @ 05/07/23 12:49:50.961
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8666.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8666.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8666.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8666.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8666.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 238.68.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.68.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.68.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.68.238_tcp@PTR;sleep 1; done
   @ 05/07/23 12:49:50.961
  STEP: creating a pod to probe DNS @ 05/07/23 12:49:50.961
  STEP: submitting the pod to kubernetes @ 05/07/23 12:49:50.961
  STEP: retrieving the pod @ 05/07/23 12:49:52.977
  STEP: looking for the results for each expected name from probers @ 05/07/23 12:49:52.978
  May  7 12:49:52.987: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:52.995: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:52.997: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:52.998: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:53.038: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:53.048: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:53.058: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:53.060: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:53.088: INFO: Lookups using dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

  May  7 12:49:58.100: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:58.111: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:58.120: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:58.130: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:58.161: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:58.170: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:58.180: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:58.181: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:49:58.210: INFO: Lookups using dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

  May  7 12:50:03.091: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:03.101: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:03.111: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:03.121: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:03.172: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:03.182: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:03.192: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:03.201: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:03.242: INFO: Lookups using dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

  May  7 12:50:08.101: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:08.111: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:08.121: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:08.122: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:08.162: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:08.171: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:08.175: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:08.181: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:08.211: INFO: Lookups using dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

  May  7 12:50:13.102: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:13.112: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:13.125: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:13.132: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:13.172: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:13.182: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:13.192: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:13.202: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:13.233: INFO: Lookups using dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

  May  7 12:50:18.101: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:18.111: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:18.121: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:18.131: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:18.181: INFO: Unable to read jessie_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:18.191: INFO: Unable to read jessie_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:18.201: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:18.211: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:18.251: INFO: Lookups using dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_udp@dns-test-service.dns-8666.svc.cluster.local jessie_tcp@dns-test-service.dns-8666.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

  May  7 12:50:23.097: INFO: Unable to read wheezy_udp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:23.101: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:23.108: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:23.118: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local from pod dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a: the server could not find the requested resource (get pods dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a)
  May  7 12:50:23.204: INFO: Lookups using dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a failed for: [wheezy_udp@dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@dns-test-service.dns-8666.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8666.svc.cluster.local]

  May  7 12:50:28.223: INFO: DNS probes using dns-8666/dns-test-2f97a44d-81a0-49b0-93ec-9de7101b596a succeeded

  May  7 12:50:28.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:50:28.224
  STEP: deleting the test service @ 05/07/23 12:50:28.236
  STEP: deleting the test headless service @ 05/07/23 12:50:28.26
  STEP: Destroying namespace "dns-8666" for this suite. @ 05/07/23 12:50:28.268
• [37.336 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 05/07/23 12:50:28.273
  May  7 12:50:28.273: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename statefulset @ 05/07/23 12:50:28.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:50:28.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:50:28.287
  STEP: Creating service test in namespace statefulset-6941 @ 05/07/23 12:50:28.29
  STEP: Looking for a node to schedule stateful set and pod @ 05/07/23 12:50:28.293
  STEP: Creating pod with conflicting port in namespace statefulset-6941 @ 05/07/23 12:50:28.296
  STEP: Waiting until pod test-pod will start running in namespace statefulset-6941 @ 05/07/23 12:50:28.305
  STEP: Creating statefulset with conflicting port in namespace statefulset-6941 @ 05/07/23 12:50:30.309
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6941 @ 05/07/23 12:50:30.312
  May  7 12:50:30.320: INFO: Observed stateful pod in namespace: statefulset-6941, name: ss-0, uid: c796fd15-76a5-4178-8361-b5cc61a3f7bb, status phase: Pending. Waiting for statefulset controller to delete.
  May  7 12:50:30.328: INFO: Observed stateful pod in namespace: statefulset-6941, name: ss-0, uid: c796fd15-76a5-4178-8361-b5cc61a3f7bb, status phase: Failed. Waiting for statefulset controller to delete.
  May  7 12:50:30.333: INFO: Observed stateful pod in namespace: statefulset-6941, name: ss-0, uid: c796fd15-76a5-4178-8361-b5cc61a3f7bb, status phase: Failed. Waiting for statefulset controller to delete.
  May  7 12:50:30.335: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6941
  STEP: Removing pod with conflicting port in namespace statefulset-6941 @ 05/07/23 12:50:30.335
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6941 and will be in running state @ 05/07/23 12:50:30.345
  May  7 12:50:32.351: INFO: Deleting all statefulset in ns statefulset-6941
  May  7 12:50:32.353: INFO: Scaling statefulset ss to 0
  May  7 12:50:42.361: INFO: Waiting for statefulset status.replicas updated to 0
  May  7 12:50:42.362: INFO: Deleting statefulset ss
  May  7 12:50:42.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6941" for this suite. @ 05/07/23 12:50:42.371
• [14.100 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/07/23 12:50:42.374
  May  7 12:50:42.374: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-runtime @ 05/07/23 12:50:42.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:50:42.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:50:42.386
  STEP: create the container @ 05/07/23 12:50:42.388
  W0507 12:50:42.391813      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/07/23 12:50:42.391
  STEP: get the container status @ 05/07/23 12:50:45.399
  STEP: the container should be terminated @ 05/07/23 12:50:45.401
  STEP: the termination message should be set @ 05/07/23 12:50:45.401
  May  7 12:50:45.401: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/07/23 12:50:45.401
  May  7 12:50:45.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5261" for this suite. @ 05/07/23 12:50:45.416
• [3.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 05/07/23 12:50:45.421
  May  7 12:50:45.421: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 12:50:45.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:50:45.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:50:45.439
  STEP: validating api versions @ 05/07/23 12:50:45.44
  May  7 12:50:45.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-8424 api-versions'
  May  7 12:50:45.488: INFO: stderr: ""
  May  7 12:50:45.488: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  May  7 12:50:45.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8424" for this suite. @ 05/07/23 12:50:45.491
• [0.079 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 05/07/23 12:50:45.5
  May  7 12:50:45.500: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-probe @ 05/07/23 12:50:45.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:50:45.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:50:45.515
  STEP: Creating pod liveness-5bc42b90-4c9c-4dff-bd9b-aba654c1ac47 in namespace container-probe-7218 @ 05/07/23 12:50:45.516
  May  7 12:50:47.522: INFO: Started pod liveness-5bc42b90-4c9c-4dff-bd9b-aba654c1ac47 in namespace container-probe-7218
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/07/23 12:50:47.522
  May  7 12:50:47.523: INFO: Initial restart count of pod liveness-5bc42b90-4c9c-4dff-bd9b-aba654c1ac47 is 0
  May  7 12:51:07.546: INFO: Restart count of pod container-probe-7218/liveness-5bc42b90-4c9c-4dff-bd9b-aba654c1ac47 is now 1 (20.023100541s elapsed)
  May  7 12:51:07.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:51:07.548
  STEP: Destroying namespace "container-probe-7218" for this suite. @ 05/07/23 12:51:07.554
• [22.056 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 05/07/23 12:51:07.557
  May  7 12:51:07.557: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename svcaccounts @ 05/07/23 12:51:07.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:07.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:07.57
  STEP: Creating a pod to test service account token:  @ 05/07/23 12:51:07.573
  STEP: Saw pod success @ 05/07/23 12:51:11.583
  May  7 12:51:11.584: INFO: Trying to get logs from node 10.255.0.202 pod test-pod-85eec77c-dbce-45de-89c4-1a8cfa98975e container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 12:51:11.602
  May  7 12:51:11.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9877" for this suite. @ 05/07/23 12:51:11.61
• [4.055 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 05/07/23 12:51:11.612
  May  7 12:51:11.612: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/07/23 12:51:11.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:11.621
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:11.626
  STEP: create the container to handle the HTTPGet hook request. @ 05/07/23 12:51:11.629
  STEP: create the pod with lifecycle hook @ 05/07/23 12:51:13.639
  STEP: delete the pod with lifecycle hook @ 05/07/23 12:51:15.647
  STEP: check prestop hook @ 05/07/23 12:51:17.654
  May  7 12:51:17.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1129" for this suite. @ 05/07/23 12:51:17.666
• [6.056 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 05/07/23 12:51:17.668
  May  7 12:51:17.668: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename security-context-test @ 05/07/23 12:51:17.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:17.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:17.686
  May  7 12:51:21.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8716" for this suite. @ 05/07/23 12:51:21.7
• [4.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 05/07/23 12:51:21.704
  May  7 12:51:21.704: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 12:51:21.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:21.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:21.715
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/07/23 12:51:21.717
  STEP: Saw pod success @ 05/07/23 12:51:25.727
  May  7 12:51:25.729: INFO: Trying to get logs from node 10.255.0.202 pod pod-39fcbcb3-d0c6-4b1f-b055-31e691eb3b40 container test-container: <nil>
  STEP: delete the pod @ 05/07/23 12:51:25.732
  May  7 12:51:25.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2408" for this suite. @ 05/07/23 12:51:25.74
• [4.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 05/07/23 12:51:25.742
  May  7 12:51:25.742: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename namespaces @ 05/07/23 12:51:25.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:25.752
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:25.754
  STEP: Read namespace status @ 05/07/23 12:51:25.756
  May  7 12:51:25.757: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/07/23 12:51:25.757
  May  7 12:51:25.760: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/07/23 12:51:25.76
  May  7 12:51:25.763: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  May  7 12:51:25.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-856" for this suite. @ 05/07/23 12:51:25.765
• [0.025 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 05/07/23 12:51:25.767
  May  7 12:51:25.767: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename security-context-test @ 05/07/23 12:51:25.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:25.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:25.78
  May  7 12:51:29.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-1232" for this suite. @ 05/07/23 12:51:29.792
• [4.027 seconds]
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 05/07/23 12:51:29.795
  May  7 12:51:29.795: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replicaset @ 05/07/23 12:51:29.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:29.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:29.806
  STEP: Create a ReplicaSet @ 05/07/23 12:51:29.808
  STEP: Verify that the required pods have come up @ 05/07/23 12:51:29.812
  May  7 12:51:29.819: INFO: Pod name sample-pod: Found 0 pods out of 3
  May  7 12:51:34.823: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/07/23 12:51:34.823
  May  7 12:51:34.825: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/07/23 12:51:34.825
  STEP: DeleteCollection of the ReplicaSets @ 05/07/23 12:51:34.828
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/07/23 12:51:34.832
  May  7 12:51:34.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6599" for this suite. @ 05/07/23 12:51:34.837
• [5.047 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 05/07/23 12:51:34.843
  May  7 12:51:34.843: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 12:51:34.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:34.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:34.873
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/07/23 12:51:34.875
  STEP: Saw pod success @ 05/07/23 12:51:38.895
  May  7 12:51:38.896: INFO: Trying to get logs from node 10.255.0.202 pod pod-65017f3f-28e3-4ac6-a2f9-9ff228f69229 container test-container: <nil>
  STEP: delete the pod @ 05/07/23 12:51:38.899
  May  7 12:51:38.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9438" for this suite. @ 05/07/23 12:51:38.908
• [4.068 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 05/07/23 12:51:38.911
  May  7 12:51:38.911: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 12:51:38.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:38.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:38.922
  STEP: Counting existing ResourceQuota @ 05/07/23 12:51:38.924
  STEP: Creating a ResourceQuota @ 05/07/23 12:51:43.925
  STEP: Ensuring resource quota status is calculated @ 05/07/23 12:51:43.928
  STEP: Creating a ReplicaSet @ 05/07/23 12:51:45.93
  STEP: Ensuring resource quota status captures replicaset creation @ 05/07/23 12:51:45.936
  STEP: Deleting a ReplicaSet @ 05/07/23 12:51:47.938
  STEP: Ensuring resource quota status released usage @ 05/07/23 12:51:47.941
  May  7 12:51:49.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2655" for this suite. @ 05/07/23 12:51:49.945
• [11.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/07/23 12:51:49.949
  May  7 12:51:49.949: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename proxy @ 05/07/23 12:51:49.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:49.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:49.96
  May  7 12:51:49.962: INFO: Creating pod...
  May  7 12:51:51.972: INFO: Creating service...
  May  7 12:51:51.977: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/pods/agnhost/proxy?method=DELETE
  May  7 12:51:51.988: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  7 12:51:51.988: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/pods/agnhost/proxy?method=OPTIONS
  May  7 12:51:51.994: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  7 12:51:51.994: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/pods/agnhost/proxy?method=PATCH
  May  7 12:51:52.005: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  7 12:51:52.005: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/pods/agnhost/proxy?method=POST
  May  7 12:51:52.010: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  7 12:51:52.010: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/pods/agnhost/proxy?method=PUT
  May  7 12:51:52.014: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  7 12:51:52.014: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/services/e2e-proxy-test-service/proxy?method=DELETE
  May  7 12:51:52.018: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May  7 12:51:52.018: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/services/e2e-proxy-test-service/proxy?method=OPTIONS
  May  7 12:51:52.028: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May  7 12:51:52.028: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/services/e2e-proxy-test-service/proxy?method=PATCH
  May  7 12:51:52.038: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May  7 12:51:52.038: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/services/e2e-proxy-test-service/proxy?method=POST
  May  7 12:51:52.048: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May  7 12:51:52.048: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/services/e2e-proxy-test-service/proxy?method=PUT
  May  7 12:51:52.058: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May  7 12:51:52.058: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/pods/agnhost/proxy?method=GET
  May  7 12:51:52.061: INFO: http.Client request:GET StatusCode:301
  May  7 12:51:52.061: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/services/e2e-proxy-test-service/proxy?method=GET
  May  7 12:51:52.062: INFO: http.Client request:GET StatusCode:301
  May  7 12:51:52.062: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/pods/agnhost/proxy?method=HEAD
  May  7 12:51:52.063: INFO: http.Client request:HEAD StatusCode:301
  May  7 12:51:52.064: INFO: Starting http.Client for https://10.68.0.1:443/api/v1/namespaces/proxy-7128/services/e2e-proxy-test-service/proxy?method=HEAD
  May  7 12:51:52.065: INFO: http.Client request:HEAD StatusCode:301
  May  7 12:51:52.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7128" for this suite. @ 05/07/23 12:51:52.067
• [2.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/07/23 12:51:52.074
  May  7 12:51:52.074: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 12:51:52.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:52.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:52.087
  STEP: creating the pod @ 05/07/23 12:51:52.094
  STEP: submitting the pod to kubernetes @ 05/07/23 12:51:52.094
  W0507 12:51:52.103016      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 05/07/23 12:51:54.108
  STEP: updating the pod @ 05/07/23 12:51:54.109
  May  7 12:51:54.615: INFO: Successfully updated pod "pod-update-activedeadlineseconds-bfa9ac80-0888-4682-9135-51df55d4b844"
  May  7 12:51:58.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7529" for this suite. @ 05/07/23 12:51:58.624
• [6.553 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 05/07/23 12:51:58.627
  May  7 12:51:58.627: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename dns @ 05/07/23 12:51:58.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:51:58.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:51:58.638
  STEP: Creating a test externalName service @ 05/07/23 12:51:58.64
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 05/07/23 12:51:58.644
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 05/07/23 12:51:58.644
  STEP: creating a pod to probe DNS @ 05/07/23 12:51:58.644
  STEP: submitting the pod to kubernetes @ 05/07/23 12:51:58.644
  STEP: retrieving the pod @ 05/07/23 12:52:00.653
  STEP: looking for the results for each expected name from probers @ 05/07/23 12:52:00.655
  May  7 12:52:00.671: INFO: DNS probes using dns-test-78a62a48-5b00-4b52-bc3b-ccb9fdfd8b68 succeeded

  STEP: changing the externalName to bar.example.com @ 05/07/23 12:52:00.671
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 05/07/23 12:52:00.674
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 05/07/23 12:52:00.674
  STEP: creating a second pod to probe DNS @ 05/07/23 12:52:00.674
  STEP: submitting the pod to kubernetes @ 05/07/23 12:52:00.674
  STEP: retrieving the pod @ 05/07/23 12:52:02.682
  STEP: looking for the results for each expected name from probers @ 05/07/23 12:52:02.683
  May  7 12:52:02.693: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:02.703: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:02.703: INFO: Lookups using dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  May  7 12:52:07.715: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:07.725: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:07.725: INFO: Lookups using dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  May  7 12:52:12.715: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:12.725: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:12.725: INFO: Lookups using dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  May  7 12:52:17.705: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:17.715: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:17.715: INFO: Lookups using dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  May  7 12:52:22.715: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:22.725: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:22.725: INFO: Lookups using dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  May  7 12:52:27.715: INFO: File wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:27.725: INFO: File jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local from pod  dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May  7 12:52:27.725: INFO: Lookups using dns-8172/dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 failed for: [wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local]

  May  7 12:52:32.716: INFO: DNS probes using dns-test-cd8790c2-e9d2-479b-97a4-c9b6dc2910e3 succeeded

  STEP: changing the service to type=ClusterIP @ 05/07/23 12:52:32.716
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 05/07/23 12:52:32.725
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8172.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8172.svc.cluster.local; sleep 1; done
   @ 05/07/23 12:52:32.725
  STEP: creating a third pod to probe DNS @ 05/07/23 12:52:32.725
  STEP: submitting the pod to kubernetes @ 05/07/23 12:52:32.727
  STEP: retrieving the pod @ 05/07/23 12:52:34.738
  STEP: looking for the results for each expected name from probers @ 05/07/23 12:52:34.74
  May  7 12:52:34.753: INFO: DNS probes using dns-test-19ef4ea5-192c-467b-896a-df4172ab8a8b succeeded

  May  7 12:52:34.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:52:34.754
  STEP: deleting the pod @ 05/07/23 12:52:34.761
  STEP: deleting the pod @ 05/07/23 12:52:34.773
  STEP: deleting the test externalName service @ 05/07/23 12:52:34.785
  STEP: Destroying namespace "dns-8172" for this suite. @ 05/07/23 12:52:34.796
• [36.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 05/07/23 12:52:34.806
  May  7 12:52:34.806: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sched-preemption @ 05/07/23 12:52:34.806
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:52:34.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:52:34.824
  May  7 12:52:34.834: INFO: Waiting up to 1m0s for all nodes to be ready
  May  7 12:53:34.845: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/07/23 12:53:34.846
  May  7 12:53:34.846: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/07/23 12:53:34.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:53:34.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:53:34.858
  STEP: Finding an available node @ 05/07/23 12:53:34.86
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/07/23 12:53:34.86
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/07/23 12:53:36.868
  May  7 12:53:36.874: INFO: found a healthy node: 10.255.0.202
  May  7 12:53:42.908: INFO: pods created so far: [1 1 1]
  May  7 12:53:42.908: INFO: length of pods created so far: 3
  May  7 12:53:44.912: INFO: pods created so far: [2 2 1]
  May  7 12:53:51.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 12:53:51.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-633" for this suite. @ 05/07/23 12:53:51.95
  STEP: Destroying namespace "sched-preemption-7241" for this suite. @ 05/07/23 12:53:51.954
• [77.154 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 05/07/23 12:53:51.961
  May  7 12:53:51.961: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-probe @ 05/07/23 12:53:51.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:53:51.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:53:51.979
  STEP: Creating pod busybox-b2534ff5-49de-4ce8-852b-ea58d66fbd9b in namespace container-probe-2456 @ 05/07/23 12:53:51.982
  May  7 12:53:53.990: INFO: Started pod busybox-b2534ff5-49de-4ce8-852b-ea58d66fbd9b in namespace container-probe-2456
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/07/23 12:53:53.99
  May  7 12:53:53.992: INFO: Initial restart count of pod busybox-b2534ff5-49de-4ce8-852b-ea58d66fbd9b is 0
  May  7 12:57:54.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:57:54.289
  STEP: Destroying namespace "container-probe-2456" for this suite. @ 05/07/23 12:57:54.298
• [242.339 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 05/07/23 12:57:54.301
  May  7 12:57:54.301: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename dns @ 05/07/23 12:57:54.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:57:54.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:57:54.312
  STEP: Creating a test headless service @ 05/07/23 12:57:54.313
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2240 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2240;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2240 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2240;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2240.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2240.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2240.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2240.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2240.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2240.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2240.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2240.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2240.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2240.svc;check="$$(dig +notcp +noall +answer +search 198.173.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.173.198_udp@PTR;check="$$(dig +tcp +noall +answer +search 198.173.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.173.198_tcp@PTR;sleep 1; done
   @ 05/07/23 12:57:54.323
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2240 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2240;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2240 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2240;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2240.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2240.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2240.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2240.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2240.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2240.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2240.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2240.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2240.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2240.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2240.svc;check="$$(dig +notcp +noall +answer +search 198.173.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.173.198_udp@PTR;check="$$(dig +tcp +noall +answer +search 198.173.68.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.68.173.198_tcp@PTR;sleep 1; done
   @ 05/07/23 12:57:54.323
  STEP: creating a pod to probe DNS @ 05/07/23 12:57:54.324
  STEP: submitting the pod to kubernetes @ 05/07/23 12:57:54.324
  STEP: retrieving the pod @ 05/07/23 12:57:56.34
  STEP: looking for the results for each expected name from probers @ 05/07/23 12:57:56.342
  May  7 12:57:56.350: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.360: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.370: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.381: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.390: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.400: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.410: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.420: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.470: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.480: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.490: INFO: Unable to read jessie_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.500: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.510: INFO: Unable to read jessie_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.520: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.530: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.540: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:57:56.561: INFO: Lookups using dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2240 wheezy_tcp@dns-test-service.dns-2240 wheezy_udp@dns-test-service.dns-2240.svc wheezy_tcp@dns-test-service.dns-2240.svc wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2240 jessie_tcp@dns-test-service.dns-2240 jessie_udp@dns-test-service.dns-2240.svc jessie_tcp@dns-test-service.dns-2240.svc jessie_udp@_http._tcp.dns-test-service.dns-2240.svc jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc]

  May  7 12:58:01.567: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.577: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.587: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.597: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.607: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.617: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.628: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.637: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.667: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.668: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.677: INFO: Unable to read jessie_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.687: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.688: INFO: Unable to read jessie_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.697: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.698: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.707: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:01.737: INFO: Lookups using dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2240 wheezy_tcp@dns-test-service.dns-2240 wheezy_udp@dns-test-service.dns-2240.svc wheezy_tcp@dns-test-service.dns-2240.svc wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2240 jessie_tcp@dns-test-service.dns-2240 jessie_udp@dns-test-service.dns-2240.svc jessie_tcp@dns-test-service.dns-2240.svc jessie_udp@_http._tcp.dns-test-service.dns-2240.svc jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc]

  May  7 12:58:06.577: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.587: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.597: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.599: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.607: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.609: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.619: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.627: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.667: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.677: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.687: INFO: Unable to read jessie_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.697: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.713: INFO: Unable to read jessie_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.723: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.737: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.747: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:06.769: INFO: Lookups using dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2240 wheezy_tcp@dns-test-service.dns-2240 wheezy_udp@dns-test-service.dns-2240.svc wheezy_tcp@dns-test-service.dns-2240.svc wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2240 jessie_tcp@dns-test-service.dns-2240 jessie_udp@dns-test-service.dns-2240.svc jessie_tcp@dns-test-service.dns-2240.svc jessie_udp@_http._tcp.dns-test-service.dns-2240.svc jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc]

  May  7 12:58:11.577: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.579: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.589: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.599: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.607: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.609: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.619: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.629: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.677: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.688: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.698: INFO: Unable to read jessie_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.707: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.708: INFO: Unable to read jessie_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.718: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.728: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.737: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:11.768: INFO: Lookups using dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2240 wheezy_tcp@dns-test-service.dns-2240 wheezy_udp@dns-test-service.dns-2240.svc wheezy_tcp@dns-test-service.dns-2240.svc wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2240 jessie_tcp@dns-test-service.dns-2240 jessie_udp@dns-test-service.dns-2240.svc jessie_tcp@dns-test-service.dns-2240.svc jessie_udp@_http._tcp.dns-test-service.dns-2240.svc jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc]

  May  7 12:58:16.577: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.587: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.597: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.607: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.617: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.627: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.637: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.647: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.697: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.708: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.717: INFO: Unable to read jessie_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.727: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.738: INFO: Unable to read jessie_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.748: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.758: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.768: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:16.808: INFO: Lookups using dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2240 wheezy_tcp@dns-test-service.dns-2240 wheezy_udp@dns-test-service.dns-2240.svc wheezy_tcp@dns-test-service.dns-2240.svc wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2240 jessie_tcp@dns-test-service.dns-2240 jessie_udp@dns-test-service.dns-2240.svc jessie_tcp@dns-test-service.dns-2240.svc jessie_udp@_http._tcp.dns-test-service.dns-2240.svc jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc]

  May  7 12:58:21.577: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.587: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.597: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.598: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.607: INFO: Unable to read wheezy_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.608: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.618: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.628: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.667: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.677: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.678: INFO: Unable to read jessie_udp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.687: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240 from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.697: INFO: Unable to read jessie_udp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.707: INFO: Unable to read jessie_tcp@dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.717: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.718: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:21.757: INFO: Lookups using dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2240 wheezy_tcp@dns-test-service.dns-2240 wheezy_udp@dns-test-service.dns-2240.svc wheezy_tcp@dns-test-service.dns-2240.svc wheezy_udp@_http._tcp.dns-test-service.dns-2240.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2240.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2240 jessie_tcp@dns-test-service.dns-2240 jessie_udp@dns-test-service.dns-2240.svc jessie_tcp@dns-test-service.dns-2240.svc jessie_udp@_http._tcp.dns-test-service.dns-2240.svc jessie_tcp@_http._tcp.dns-test-service.dns-2240.svc]

  May  7 12:58:26.656: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:26.666: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e: the server could not find the requested resource (get pods dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e)
  May  7 12:58:26.766: INFO: Lookups using dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service]

  May  7 12:58:31.748: INFO: DNS probes using dns-2240/dns-test-6273b8fd-5f74-4632-ba90-0ca245d32a7e succeeded

  May  7 12:58:31.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 12:58:31.749
  STEP: deleting the test service @ 05/07/23 12:58:31.76
  STEP: deleting the test headless service @ 05/07/23 12:58:31.79
  STEP: Destroying namespace "dns-2240" for this suite. @ 05/07/23 12:58:31.799
• [37.503 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 05/07/23 12:58:31.804
  May  7 12:58:31.804: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 12:58:31.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:58:31.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:58:31.821
  STEP: Setting up server cert @ 05/07/23 12:58:31.838
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 12:58:32.058
  STEP: Deploying the webhook pod @ 05/07/23 12:58:32.061
  STEP: Wait for the deployment to be ready @ 05/07/23 12:58:32.072
  May  7 12:58:32.078: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/07/23 12:58:34.082
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 12:58:34.087
  May  7 12:58:35.087: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/07/23 12:58:35.089
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/07/23 12:58:35.097
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/07/23 12:58:35.102
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/07/23 12:58:35.106
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/07/23 12:58:35.111
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/07/23 12:58:35.115
  May  7 12:58:35.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2125" for this suite. @ 05/07/23 12:58:35.147
  STEP: Destroying namespace "webhook-markers-4869" for this suite. @ 05/07/23 12:58:35.151
• [3.355 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 05/07/23 12:58:35.16
  May  7 12:58:35.160: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 12:58:35.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:58:35.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:58:35.186
  May  7 12:58:35.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4358" for this suite. @ 05/07/23 12:58:35.213
• [0.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 05/07/23 12:58:35.226
  May  7 12:58:35.226: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename runtimeclass @ 05/07/23 12:58:35.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:58:35.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:58:35.243
  May  7 12:58:37.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5473" for this suite. @ 05/07/23 12:58:37.269
• [2.047 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/07/23 12:58:37.273
  May  7 12:58:37.273: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-runtime @ 05/07/23 12:58:37.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:58:37.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:58:37.284
  STEP: create the container @ 05/07/23 12:58:37.285
  W0507 12:58:37.289624      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/07/23 12:58:37.289
  STEP: get the container status @ 05/07/23 12:58:40.299
  STEP: the container should be terminated @ 05/07/23 12:58:40.302
  STEP: the termination message should be set @ 05/07/23 12:58:40.302
  May  7 12:58:40.302: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/07/23 12:58:40.302
  May  7 12:58:40.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-28" for this suite. @ 05/07/23 12:58:40.329
• [3.062 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/07/23 12:58:40.335
  May  7 12:58:40.335: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename watch @ 05/07/23 12:58:40.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:58:40.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:58:40.356
  STEP: getting a starting resourceVersion @ 05/07/23 12:58:40.358
  STEP: starting a background goroutine to produce watch events @ 05/07/23 12:58:40.36
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/07/23 12:58:40.36
  May  7 12:58:43.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6516" for this suite. @ 05/07/23 12:58:43.188
• [2.904 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 05/07/23 12:58:43.241
  May  7 12:58:43.241: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 12:58:43.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:58:43.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:58:43.252
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/07/23 12:58:43.254
  May  7 12:58:43.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1021 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  May  7 12:58:43.299: INFO: stderr: ""
  May  7 12:58:43.299: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/07/23 12:58:43.3
  May  7 12:58:43.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1021 delete pods e2e-test-httpd-pod'
  May  7 12:58:45.661: INFO: stderr: ""
  May  7 12:58:45.661: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May  7 12:58:45.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1021" for this suite. @ 05/07/23 12:58:45.662
• [2.424 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 05/07/23 12:58:45.666
  May  7 12:58:45.666: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 12:58:45.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:58:45.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:58:45.678
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/07/23 12:58:45.679
  STEP: Saw pod success @ 05/07/23 12:58:49.689
  May  7 12:58:49.690: INFO: Trying to get logs from node 10.255.0.202 pod pod-1d757bed-862c-49e2-b786-9407c1e44943 container test-container: <nil>
  STEP: delete the pod @ 05/07/23 12:58:49.7
  May  7 12:58:49.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1228" for this suite. @ 05/07/23 12:58:49.709
• [4.045 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 05/07/23 12:58:49.712
  May  7 12:58:49.712: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 12:58:49.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:58:49.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:58:49.722
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 12:58:49.724
  STEP: Saw pod success @ 05/07/23 12:58:53.734
  May  7 12:58:53.735: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-f2b8a25f-ce01-46d9-8e0e-1deb2cc5a6ac container client-container: <nil>
  STEP: delete the pod @ 05/07/23 12:58:53.738
  May  7 12:58:53.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9532" for this suite. @ 05/07/23 12:58:53.747
• [4.038 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 05/07/23 12:58:53.75
  May  7 12:58:53.750: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 12:58:53.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:58:53.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:58:53.765
  STEP: Creating configMap with name configmap-test-volume-map-13c243e7-e4fa-443d-a74c-0875902f19f4 @ 05/07/23 12:58:53.766
  STEP: Creating a pod to test consume configMaps @ 05/07/23 12:58:53.768
  STEP: Saw pod success @ 05/07/23 12:58:57.777
  May  7 12:58:57.778: INFO: Trying to get logs from node 10.255.0.202 pod pod-configmaps-ce6861dc-dcd5-4971-a4bc-1beac16858fc container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 12:58:57.781
  May  7 12:58:57.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2609" for this suite. @ 05/07/23 12:58:57.79
• [4.043 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 05/07/23 12:58:57.793
  May  7 12:58:57.793: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 12:58:57.793
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:58:57.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:58:57.804
  STEP: creating Agnhost RC @ 05/07/23 12:58:57.852
  May  7 12:58:57.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-6312 create -f -'
  May  7 12:58:58.095: INFO: stderr: ""
  May  7 12:58:58.095: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/07/23 12:58:58.095
  May  7 12:58:59.098: INFO: Selector matched 1 pods for map[app:agnhost]
  May  7 12:58:59.098: INFO: Found 0 / 1
  May  7 12:59:00.097: INFO: Selector matched 1 pods for map[app:agnhost]
  May  7 12:59:00.097: INFO: Found 1 / 1
  May  7 12:59:00.097: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/07/23 12:59:00.097
  May  7 12:59:00.099: INFO: Selector matched 1 pods for map[app:agnhost]
  May  7 12:59:00.099: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  7 12:59:00.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-6312 patch pod agnhost-primary-n7cd9 -p {"metadata":{"annotations":{"x":"y"}}}'
  May  7 12:59:00.145: INFO: stderr: ""
  May  7 12:59:00.145: INFO: stdout: "pod/agnhost-primary-n7cd9 patched\n"
  STEP: checking annotations @ 05/07/23 12:59:00.145
  May  7 12:59:00.148: INFO: Selector matched 1 pods for map[app:agnhost]
  May  7 12:59:00.148: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  7 12:59:00.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6312" for this suite. @ 05/07/23 12:59:00.149
• [2.359 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 05/07/23 12:59:00.155
  May  7 12:59:00.155: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename aggregator @ 05/07/23 12:59:00.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:59:00.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:59:00.167
  May  7 12:59:00.168: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Registering the sample API server. @ 05/07/23 12:59:00.168
  May  7 12:59:00.720: INFO: Found ClusterRoles; assuming RBAC is enabled.
  May  7 12:59:00.738: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  May  7 12:59:02.775: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:04.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:06.778: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:08.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:10.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:12.778: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:14.778: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:16.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:18.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:20.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:22.778: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May  7 12:59:24.886: INFO: Waited 105.461893ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/07/23 12:59:24.913
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/07/23 12:59:24.914
  STEP: List APIServices @ 05/07/23 12:59:24.917
  May  7 12:59:24.920: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/07/23 12:59:24.92
  May  7 12:59:24.927: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/07/23 12:59:24.927
  May  7 12:59:24.933: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.May, 7, 12, 59, 24, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/07/23 12:59:24.933
  May  7 12:59:24.934: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-05-07 12:59:24 +0000 UTC Passed all checks passed}
  May  7 12:59:24.934: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  7 12:59:24.934: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/07/23 12:59:24.934
  May  7 12:59:24.940: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-102307222" @ 05/07/23 12:59:24.94
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/07/23 12:59:24.954
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/07/23 12:59:24.961
  STEP: Patch APIService Status @ 05/07/23 12:59:24.962
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/07/23 12:59:24.966
  May  7 12:59:24.968: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-05-07 12:59:24 +0000 UTC Passed all checks passed}
  May  7 12:59:24.968: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  7 12:59:24.968: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  May  7 12:59:24.968: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 05/07/23 12:59:24.968
  STEP: Confirm that the generated APIService has been deleted @ 05/07/23 12:59:24.972
  May  7 12:59:24.972: INFO: Requesting list of APIServices to confirm quantity
  May  7 12:59:24.981: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  May  7 12:59:24.981: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  May  7 12:59:25.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-1442" for this suite. @ 05/07/23 12:59:25.066
• [24.915 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 05/07/23 12:59:25.071
  May  7 12:59:25.071: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 12:59:25.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:59:25.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:59:25.084
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/07/23 12:59:25.086
  STEP: Saw pod success @ 05/07/23 12:59:29.098
  May  7 12:59:29.100: INFO: Trying to get logs from node 10.255.0.202 pod pod-0917e334-d280-48a7-9596-8687530dc23c container test-container: <nil>
  STEP: delete the pod @ 05/07/23 12:59:29.102
  May  7 12:59:29.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1111" for this suite. @ 05/07/23 12:59:29.115
• [4.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 05/07/23 12:59:29.12
  May  7 12:59:29.120: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 12:59:29.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:59:29.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:59:29.13
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/07/23 12:59:29.132
  STEP: Saw pod success @ 05/07/23 12:59:31.142
  May  7 12:59:31.143: INFO: Trying to get logs from node 10.255.0.202 pod pod-36ffdb19-2bd2-461f-9eea-385429bea0b4 container test-container: <nil>
  STEP: delete the pod @ 05/07/23 12:59:31.146
  May  7 12:59:31.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6331" for this suite. @ 05/07/23 12:59:31.155
• [2.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 05/07/23 12:59:31.158
  May  7 12:59:31.158: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replicaset @ 05/07/23 12:59:31.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:59:31.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:59:31.168
  May  7 12:59:31.170: INFO: Creating ReplicaSet my-hostname-basic-24d1dff0-852e-4f3a-a33c-292a722e9074
  May  7 12:59:31.182: INFO: Pod name my-hostname-basic-24d1dff0-852e-4f3a-a33c-292a722e9074: Found 0 pods out of 1
  May  7 12:59:36.187: INFO: Pod name my-hostname-basic-24d1dff0-852e-4f3a-a33c-292a722e9074: Found 1 pods out of 1
  May  7 12:59:36.187: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-24d1dff0-852e-4f3a-a33c-292a722e9074" is running
  May  7 12:59:36.188: INFO: Pod "my-hostname-basic-24d1dff0-852e-4f3a-a33c-292a722e9074-ptd59" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-07 12:59:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-07 12:59:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-07 12:59:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-07 12:59:31 +0000 UTC Reason: Message:}])
  May  7 12:59:36.188: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/07/23 12:59:36.189
  May  7 12:59:36.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5382" for this suite. @ 05/07/23 12:59:36.196
• [5.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 05/07/23 12:59:36.2
  May  7 12:59:36.200: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename gc @ 05/07/23 12:59:36.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:59:36.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:59:36.22
  STEP: create the deployment @ 05/07/23 12:59:36.221
  W0507 12:59:36.224343      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/07/23 12:59:36.224
  STEP: delete the deployment @ 05/07/23 12:59:36.73
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/07/23 12:59:36.732
  STEP: Gathering metrics @ 05/07/23 12:59:37.251
  W0507 12:59:37.253393      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  7 12:59:37.253: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  7 12:59:37.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7135" for this suite. @ 05/07/23 12:59:37.255
• [1.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/07/23 12:59:37.259
  May  7 12:59:37.259: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 12:59:37.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:59:37.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:59:37.274
  STEP: creating the pod @ 05/07/23 12:59:37.275
  STEP: submitting the pod to kubernetes @ 05/07/23 12:59:37.275
  STEP: verifying the pod is in kubernetes @ 05/07/23 12:59:39.283
  STEP: updating the pod @ 05/07/23 12:59:39.285
  May  7 12:59:39.791: INFO: Successfully updated pod "pod-update-27f4dc7a-17f7-4864-b3fa-e6fcf6f43a0a"
  STEP: verifying the updated pod is in kubernetes @ 05/07/23 12:59:39.795
  May  7 12:59:39.797: INFO: Pod update OK
  May  7 12:59:39.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9955" for this suite. @ 05/07/23 12:59:39.799
• [2.542 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 05/07/23 12:59:39.801
  May  7 12:59:39.801: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename job @ 05/07/23 12:59:39.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:59:39.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:59:39.812
  STEP: Creating a suspended job @ 05/07/23 12:59:39.824
  STEP: Patching the Job @ 05/07/23 12:59:39.828
  STEP: Watching for Job to be patched @ 05/07/23 12:59:39.838
  May  7 12:59:39.839: INFO: Event ADDED observed for Job e2e-z247w in namespace job-1551 with labels: map[e2e-job-label:e2e-z247w] and annotations: map[batch.kubernetes.io/job-tracking:]
  May  7 12:59:39.839: INFO: Event MODIFIED observed for Job e2e-z247w in namespace job-1551 with labels: map[e2e-job-label:e2e-z247w] and annotations: map[batch.kubernetes.io/job-tracking:]
  May  7 12:59:39.839: INFO: Event MODIFIED found for Job e2e-z247w in namespace job-1551 with labels: map[e2e-job-label:e2e-z247w e2e-z247w:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 05/07/23 12:59:39.839
  STEP: Watching for Job to be updated @ 05/07/23 12:59:39.843
  May  7 12:59:39.844: INFO: Event MODIFIED found for Job e2e-z247w in namespace job-1551 with labels: map[e2e-job-label:e2e-z247w e2e-z247w:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  7 12:59:39.844: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/07/23 12:59:39.844
  May  7 12:59:39.846: INFO: Job: e2e-z247w as labels: map[e2e-job-label:e2e-z247w e2e-z247w:patched]
  STEP: Waiting for job to complete @ 05/07/23 12:59:39.846
  STEP: Delete a job collection with a labelselector @ 05/07/23 12:59:47.855
  STEP: Watching for Job to be deleted @ 05/07/23 12:59:47.861
  May  7 12:59:47.862: INFO: Event MODIFIED observed for Job e2e-z247w in namespace job-1551 with labels: map[e2e-job-label:e2e-z247w e2e-z247w:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  7 12:59:47.863: INFO: Event MODIFIED observed for Job e2e-z247w in namespace job-1551 with labels: map[e2e-job-label:e2e-z247w e2e-z247w:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  7 12:59:47.863: INFO: Event MODIFIED observed for Job e2e-z247w in namespace job-1551 with labels: map[e2e-job-label:e2e-z247w e2e-z247w:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  7 12:59:47.863: INFO: Event MODIFIED observed for Job e2e-z247w in namespace job-1551 with labels: map[e2e-job-label:e2e-z247w e2e-z247w:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  7 12:59:47.863: INFO: Event MODIFIED observed for Job e2e-z247w in namespace job-1551 with labels: map[e2e-job-label:e2e-z247w e2e-z247w:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May  7 12:59:47.863: INFO: Event DELETED found for Job e2e-z247w in namespace job-1551 with labels: map[e2e-job-label:e2e-z247w e2e-z247w:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 05/07/23 12:59:47.863
  May  7 12:59:47.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1551" for this suite. @ 05/07/23 12:59:47.867
• [8.072 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 05/07/23 12:59:47.874
  May  7 12:59:47.874: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replication-controller @ 05/07/23 12:59:47.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:59:47.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:59:47.897
  STEP: Given a ReplicationController is created @ 05/07/23 12:59:47.9
  STEP: When the matched label of one of its pods change @ 05/07/23 12:59:47.915
  May  7 12:59:47.921: INFO: Pod name pod-release: Found 0 pods out of 1
  May  7 12:59:52.924: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/07/23 12:59:52.93
  May  7 12:59:53.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1334" for this suite. @ 05/07/23 12:59:53.941
• [6.070 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 05/07/23 12:59:53.944
  May  7 12:59:53.944: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename var-expansion @ 05/07/23 12:59:53.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 12:59:53.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 12:59:53.963
  STEP: creating the pod @ 05/07/23 12:59:53.964
  STEP: waiting for pod running @ 05/07/23 12:59:53.969
  STEP: creating a file in subpath @ 05/07/23 12:59:55.974
  May  7 12:59:55.975: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5111 PodName:var-expansion-0a349cff-8be7-4ff7-ba87-934834334019 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:59:55.975: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:59:55.976: INFO: ExecWithOptions: Clientset creation
  May  7 12:59:55.976: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/var-expansion-5111/pods/var-expansion-0a349cff-8be7-4ff7-ba87-934834334019/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 05/07/23 12:59:56.041
  May  7 12:59:56.042: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5111 PodName:var-expansion-0a349cff-8be7-4ff7-ba87-934834334019 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 12:59:56.042: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 12:59:56.043: INFO: ExecWithOptions: Clientset creation
  May  7 12:59:56.043: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/var-expansion-5111/pods/var-expansion-0a349cff-8be7-4ff7-ba87-934834334019/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/07/23 12:59:56.106
  May  7 12:59:56.613: INFO: Successfully updated pod "var-expansion-0a349cff-8be7-4ff7-ba87-934834334019"
  STEP: waiting for annotated pod running @ 05/07/23 12:59:56.613
  STEP: deleting the pod gracefully @ 05/07/23 12:59:56.615
  May  7 12:59:56.615: INFO: Deleting pod "var-expansion-0a349cff-8be7-4ff7-ba87-934834334019" in namespace "var-expansion-5111"
  May  7 12:59:56.622: INFO: Wait up to 5m0s for pod "var-expansion-0a349cff-8be7-4ff7-ba87-934834334019" to be fully deleted
  May  7 13:00:28.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5111" for this suite. @ 05/07/23 13:00:28.661
• [34.722 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 05/07/23 13:00:28.666
  May  7 13:00:28.666: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename field-validation @ 05/07/23 13:00:28.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:00:28.675
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:00:28.676
  May  7 13:00:28.678: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  W0507 13:00:31.204507      20 warnings.go:70] unknown field "alpha"
  W0507 13:00:31.204519      20 warnings.go:70] unknown field "beta"
  W0507 13:00:31.204522      20 warnings.go:70] unknown field "delta"
  W0507 13:00:31.204526      20 warnings.go:70] unknown field "epsilon"
  W0507 13:00:31.204547      20 warnings.go:70] unknown field "gamma"
  May  7 13:00:31.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-28" for this suite. @ 05/07/23 13:00:31.217
• [2.553 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 05/07/23 13:00:31.219
  May  7 13:00:31.219: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 13:00:31.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:00:31.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:00:31.23
  STEP: Creating the pod @ 05/07/23 13:00:31.232
  May  7 13:00:33.758: INFO: Successfully updated pod "labelsupdated3c461f9-284f-49c1-a02b-e8ad979471c1"
  May  7 13:00:37.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7573" for this suite. @ 05/07/23 13:00:37.774
• [6.557 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 05/07/23 13:00:37.777
  May  7 13:00:37.777: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename subpath @ 05/07/23 13:00:37.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:00:37.785
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:00:37.792
  STEP: Setting up data @ 05/07/23 13:00:37.793
  STEP: Creating pod pod-subpath-test-downwardapi-4pln @ 05/07/23 13:00:37.797
  STEP: Creating a pod to test atomic-volume-subpath @ 05/07/23 13:00:37.797
  STEP: Saw pod success @ 05/07/23 13:01:01.832
  May  7 13:01:01.834: INFO: Trying to get logs from node 10.255.0.202 pod pod-subpath-test-downwardapi-4pln container test-container-subpath-downwardapi-4pln: <nil>
  STEP: delete the pod @ 05/07/23 13:01:01.838
  STEP: Deleting pod pod-subpath-test-downwardapi-4pln @ 05/07/23 13:01:01.848
  May  7 13:01:01.848: INFO: Deleting pod "pod-subpath-test-downwardapi-4pln" in namespace "subpath-8231"
  May  7 13:01:01.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8231" for this suite. @ 05/07/23 13:01:01.851
• [24.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 05/07/23 13:01:01.855
  May  7 13:01:01.855: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename dns @ 05/07/23 13:01:01.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:01:01.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:01:01.867
  STEP: Creating a test headless service @ 05/07/23 13:01:01.869
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-19.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-19.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local;sleep 1; done
   @ 05/07/23 13:01:01.871
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-19.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-19.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-19.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local;sleep 1; done
   @ 05/07/23 13:01:01.871
  STEP: creating a pod to probe DNS @ 05/07/23 13:01:01.871
  STEP: submitting the pod to kubernetes @ 05/07/23 13:01:01.871
  STEP: retrieving the pod @ 05/07/23 13:01:03.884
  STEP: looking for the results for each expected name from probers @ 05/07/23 13:01:03.885
  May  7 13:01:03.893: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:03.895: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:03.903: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:03.905: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:03.915: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:03.923: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:03.933: INFO: Unable to read jessie_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:03.943: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:03.943: INFO: Lookups using dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local]

  May  7 13:01:08.946: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:08.956: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:08.966: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:08.976: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:08.986: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:08.996: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:09.006: INFO: Unable to read jessie_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:09.016: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:09.016: INFO: Lookups using dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local]

  May  7 13:01:13.956: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:13.966: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:13.976: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:13.986: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:13.988: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:13.996: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:14.006: INFO: Unable to read jessie_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:14.016: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:14.016: INFO: Lookups using dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local]

  May  7 13:01:18.956: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:18.966: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:18.976: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:18.986: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:18.996: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:19.006: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:19.016: INFO: Unable to read jessie_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:19.026: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:19.026: INFO: Lookups using dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local]

  May  7 13:01:23.957: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:23.967: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:23.977: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:23.987: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:23.997: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:24.007: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:24.017: INFO: Unable to read jessie_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:24.027: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:24.027: INFO: Lookups using dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local]

  May  7 13:01:28.956: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:28.966: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:28.976: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:28.986: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:28.996: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:29.006: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:29.016: INFO: Unable to read jessie_udp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:29.026: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local from pod dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2: the server could not find the requested resource (get pods dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2)
  May  7 13:01:29.026: INFO: Lookups using dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local wheezy_udp@dns-test-service-2.dns-19.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-19.svc.cluster.local jessie_udp@dns-test-service-2.dns-19.svc.cluster.local jessie_tcp@dns-test-service-2.dns-19.svc.cluster.local]

  May  7 13:01:34.026: INFO: DNS probes using dns-19/dns-test-2bd35df0-dbf9-4aa1-aada-4a0810230fe2 succeeded

  May  7 13:01:34.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 13:01:34.028
  STEP: deleting the test headless service @ 05/07/23 13:01:34.039
  STEP: Destroying namespace "dns-19" for this suite. @ 05/07/23 13:01:34.049
• [32.199 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 05/07/23 13:01:34.055
  May  7 13:01:34.055: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:01:34.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:01:34.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:01:34.072
  STEP: Creating the pod @ 05/07/23 13:01:34.073
  May  7 13:01:36.591: INFO: Successfully updated pod "labelsupdate3541a925-f1ab-45da-aa06-ffda3d99724f"
  May  7 13:01:40.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7534" for this suite. @ 05/07/23 13:01:40.605
• [6.554 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 05/07/23 13:01:40.609
  May  7 13:01:40.609: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replication-controller @ 05/07/23 13:01:40.61
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:01:40.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:01:40.621
  STEP: creating a ReplicationController @ 05/07/23 13:01:40.623
  STEP: waiting for RC to be added @ 05/07/23 13:01:40.625
  STEP: waiting for available Replicas @ 05/07/23 13:01:40.626
  STEP: patching ReplicationController @ 05/07/23 13:01:42.026
  STEP: waiting for RC to be modified @ 05/07/23 13:01:42.033
  STEP: patching ReplicationController status @ 05/07/23 13:01:42.033
  STEP: waiting for RC to be modified @ 05/07/23 13:01:42.038
  STEP: waiting for available Replicas @ 05/07/23 13:01:42.038
  STEP: fetching ReplicationController status @ 05/07/23 13:01:42.043
  STEP: patching ReplicationController scale @ 05/07/23 13:01:42.044
  STEP: waiting for RC to be modified @ 05/07/23 13:01:42.047
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/07/23 13:01:42.047
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/07/23 13:01:42.704
  STEP: updating ReplicationController status @ 05/07/23 13:01:42.705
  STEP: waiting for RC to be modified @ 05/07/23 13:01:42.71
  STEP: listing all ReplicationControllers @ 05/07/23 13:01:42.71
  STEP: checking that ReplicationController has expected values @ 05/07/23 13:01:42.712
  STEP: deleting ReplicationControllers by collection @ 05/07/23 13:01:42.712
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/07/23 13:01:42.715
  May  7 13:01:42.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0507 13:01:42.748063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-1417" for this suite. @ 05/07/23 13:01:42.749
• [2.142 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 05/07/23 13:01:42.752
  May  7 13:01:42.752: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 13:01:42.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:01:42.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:01:42.766
  STEP: Creating a ResourceQuota with terminating scope @ 05/07/23 13:01:42.768
  STEP: Ensuring ResourceQuota status is calculated @ 05/07/23 13:01:42.77
  E0507 13:01:43.748383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:01:44.748524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 05/07/23 13:01:44.772
  STEP: Ensuring ResourceQuota status is calculated @ 05/07/23 13:01:44.775
  E0507 13:01:45.748895      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:01:46.748985      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 05/07/23 13:01:46.776
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/07/23 13:01:46.786
  E0507 13:01:47.749934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:01:48.750010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/07/23 13:01:48.788
  E0507 13:01:49.754167      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:01:50.754521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/07/23 13:01:50.791
  STEP: Ensuring resource quota status released the pod usage @ 05/07/23 13:01:50.8
  E0507 13:01:51.754618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:01:52.754749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 05/07/23 13:01:52.802
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/07/23 13:01:52.807
  E0507 13:01:53.755609      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:01:54.755783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/07/23 13:01:54.81
  E0507 13:01:55.755858      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:01:56.756715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/07/23 13:01:56.812
  STEP: Ensuring resource quota status released the pod usage @ 05/07/23 13:01:56.819
  E0507 13:01:57.756868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:01:58.757933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:01:58.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8018" for this suite. @ 05/07/23 13:01:58.823
• [16.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 05/07/23 13:01:58.826
  May  7 13:01:58.826: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 13:01:58.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:01:58.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:01:58.843
  STEP: Creating configMap with name configmap-test-volume-986a6f23-fad1-448e-b990-80c72557f44a @ 05/07/23 13:01:58.845
  STEP: Creating a pod to test consume configMaps @ 05/07/23 13:01:58.847
  E0507 13:01:59.758869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:00.758976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:01.759079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:02.759264      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:02:02.856
  May  7 13:02:02.857: INFO: Trying to get logs from node 10.255.0.202 pod pod-configmaps-20f09466-eae4-404a-9cee-b95cba4603c2 container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 13:02:02.86
  May  7 13:02:02.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7784" for this suite. @ 05/07/23 13:02:02.869
• [4.047 seconds]
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 05/07/23 13:02:02.873
  May  7 13:02:02.873: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pod-network-test @ 05/07/23 13:02:02.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:02:02.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:02:02.882
  STEP: Performing setup for networking test in namespace pod-network-test-4516 @ 05/07/23 13:02:02.885
  STEP: creating a selector @ 05/07/23 13:02:02.885
  STEP: Creating the service pods in kubernetes @ 05/07/23 13:02:02.885
  May  7 13:02:02.885: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0507 13:02:03.759355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:04.759432      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:05.759508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:06.759641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:07.760649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:08.760760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:09.761680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:10.761750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:11.761817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:12.761965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:13.762023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:14.762122      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/07/23 13:02:14.927
  E0507 13:02:15.762211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:16.762284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:02:16.935: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May  7 13:02:16.935: INFO: Breadth first check of 172.20.3.67 on host 10.255.0.201...
  May  7 13:02:16.936: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.231.225:9080/dial?request=hostname&protocol=udp&host=172.20.3.67&port=8081&tries=1'] Namespace:pod-network-test-4516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 13:02:16.936: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 13:02:16.937: INFO: ExecWithOptions: Clientset creation
  May  7 13:02:16.937: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-4516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.231.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.3.67%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  7 13:02:17.009: INFO: Waiting for responses: map[]
  May  7 13:02:17.009: INFO: reached 172.20.3.67 after 0/1 tries
  May  7 13:02:17.009: INFO: Breadth first check of 172.20.231.215 on host 10.255.0.202...
  May  7 13:02:17.011: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.231.225:9080/dial?request=hostname&protocol=udp&host=172.20.231.215&port=8081&tries=1'] Namespace:pod-network-test-4516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 13:02:17.011: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 13:02:17.011: INFO: ExecWithOptions: Clientset creation
  May  7 13:02:17.011: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-4516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.231.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.231.215%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  7 13:02:17.053: INFO: Waiting for responses: map[]
  May  7 13:02:17.053: INFO: reached 172.20.231.215 after 0/1 tries
  May  7 13:02:17.053: INFO: Breadth first check of 172.20.191.19 on host 10.255.0.203...
  May  7 13:02:17.055: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.231.225:9080/dial?request=hostname&protocol=udp&host=172.20.191.19&port=8081&tries=1'] Namespace:pod-network-test-4516 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 13:02:17.055: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 13:02:17.055: INFO: ExecWithOptions: Clientset creation
  May  7 13:02:17.055: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-4516/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.231.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.191.19%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May  7 13:02:17.080: INFO: Waiting for responses: map[]
  May  7 13:02:17.080: INFO: reached 172.20.191.19 after 0/1 tries
  May  7 13:02:17.080: INFO: Going to retry 0 out of 3 pods....
  May  7 13:02:17.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4516" for this suite. @ 05/07/23 13:02:17.082
• [14.212 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 05/07/23 13:02:17.085
  May  7 13:02:17.085: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 13:02:17.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:02:17.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:02:17.098
  STEP: Counting existing ResourceQuota @ 05/07/23 13:02:17.099
  E0507 13:02:17.763230      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:18.763294      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:19.763365      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:20.763779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:21.763835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/07/23 13:02:22.105
  STEP: Ensuring resource quota status is calculated @ 05/07/23 13:02:22.111
  E0507 13:02:22.764001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:23.764218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 05/07/23 13:02:24.114
  STEP: Creating a NodePort Service @ 05/07/23 13:02:24.122
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/07/23 13:02:24.133
  STEP: Ensuring resource quota status captures service creation @ 05/07/23 13:02:24.142
  E0507 13:02:24.764263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:25.764387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 05/07/23 13:02:26.145
  STEP: Ensuring resource quota status released usage @ 05/07/23 13:02:26.167
  E0507 13:02:26.764892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:27.765939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:02:28.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-228" for this suite. @ 05/07/23 13:02:28.17
• [11.088 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 05/07/23 13:02:28.173
  May  7 13:02:28.173: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename job @ 05/07/23 13:02:28.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:02:28.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:02:28.185
  STEP: Creating a job @ 05/07/23 13:02:28.186
  STEP: Ensuring active pods == parallelism @ 05/07/23 13:02:28.189
  E0507 13:02:28.766926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:29.767119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 05/07/23 13:02:30.191
  STEP: deleting Job.batch foo in namespace job-2299, will wait for the garbage collector to delete the pods @ 05/07/23 13:02:30.191
  May  7 13:02:30.247: INFO: Deleting Job.batch foo took: 3.430049ms
  May  7 13:02:30.347: INFO: Terminating Job.batch foo pods took: 100.082395ms
  E0507 13:02:30.767247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:31.768104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:32.769079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:33.769918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:34.770721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:35.771720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:36.772616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:37.773630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:38.774558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:39.775223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:40.776141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:41.777074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:42.777934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:43.778779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:44.779564      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:45.780439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:46.780683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:47.781152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:48.781901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:49.782748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:50.783446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:51.784237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:52.785124      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:53.785918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:54.786758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:55.787579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:56.788455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:57.789433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:58.790229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:02:59.791073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:00.791582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 05/07/23 13:03:01.248
  May  7 13:03:01.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2299" for this suite. @ 05/07/23 13:03:01.251
• [33.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 05/07/23 13:03:01.255
  May  7 13:03:01.255: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename field-validation @ 05/07/23 13:03:01.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:03:01.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:03:01.268
  May  7 13:03:01.269: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  W0507 13:03:01.269862      20 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc0012d70a0 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0507 13:03:01.792553      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:02.793113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:03.793632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0507 13:03:03.797346      20 warnings.go:70] unknown field "alpha"
  W0507 13:03:03.797355      20 warnings.go:70] unknown field "beta"
  W0507 13:03:03.797359      20 warnings.go:70] unknown field "delta"
  W0507 13:03:03.797363      20 warnings.go:70] unknown field "epsilon"
  W0507 13:03:03.797366      20 warnings.go:70] unknown field "gamma"
  May  7 13:03:03.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2084" for this suite. @ 05/07/23 13:03:03.808
• [2.557 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 05/07/23 13:03:03.813
  May  7 13:03:03.813: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-probe @ 05/07/23 13:03:03.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:03:03.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:03:03.825
  E0507 13:03:04.793682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:05.794474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:06.794521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:07.795287      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:08.795627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:09.795663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:10.796190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:11.796863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:12.797947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:13.797992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:14.798151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:15.798841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:16.798891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:17.798929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:18.798983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:19.799640      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:20.800009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:21.800840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:22.800921      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:23.800960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:24.801029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:25.801957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:26.802762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:27.802945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:28.803955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:29.804939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:30.805888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:31.805950      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:32.806806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:33.807641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:34.807844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:35.808626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:36.809561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:37.810580      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:38.811388      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:39.811754      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:40.812458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:41.812863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:42.813940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:43.813998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:44.814191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:45.814695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:46.814742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:47.814760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:48.814812      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:49.815664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:50.816015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:51.816111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:52.816720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:53.817615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:54.817771      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:55.818466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:56.819075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:57.820019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:58.820078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:03:59.820867      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:00.820936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:01.821946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:02.822016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:03.822953      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:04:03.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6168" for this suite. @ 05/07/23 13:04:03.833
• [60.023 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 05/07/23 13:04:03.836
  May  7 13:04:03.836: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename statefulset @ 05/07/23 13:04:03.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:04:03.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:04:03.849
  STEP: Creating service test in namespace statefulset-6558 @ 05/07/23 13:04:03.851
  May  7 13:04:03.860: INFO: Found 0 stateful pods, waiting for 1
  E0507 13:04:04.823408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:05.823580      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:06.823650      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:07.823909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:08.823965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:09.824036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:10.824125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:11.824242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:12.824451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:13.824512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:04:13.862: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/07/23 13:04:13.864
  W0507 13:04:13.868989      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May  7 13:04:13.871: INFO: Found 1 stateful pods, waiting for 2
  E0507 13:04:14.824564      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:15.824696      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:16.824859      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:17.824893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:18.825908      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:19.825973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:20.826114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:21.826166      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:22.826367      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:23.826410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:04:23.874: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May  7 13:04:23.874: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/07/23 13:04:23.876
  STEP: Delete all of the StatefulSets @ 05/07/23 13:04:23.878
  STEP: Verify that StatefulSets have been deleted @ 05/07/23 13:04:23.88
  May  7 13:04:23.882: INFO: Deleting all statefulset in ns statefulset-6558
  May  7 13:04:23.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6558" for this suite. @ 05/07/23 13:04:23.889
• [20.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 05/07/23 13:04:23.896
  May  7 13:04:23.896: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 13:04:23.896
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:04:23.912
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:04:23.914
  STEP: Creating configMap with name configmap-test-volume-map-1f953064-960e-4f55-9ab6-dc10a3623314 @ 05/07/23 13:04:23.916
  STEP: Creating a pod to test consume configMaps @ 05/07/23 13:04:23.918
  E0507 13:04:24.826506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:25.826588      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:26.827074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:27.827335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:04:27.932
  May  7 13:04:27.934: INFO: Trying to get logs from node 10.255.0.202 pod pod-configmaps-21ecfbf1-a46d-4ab7-82ec-5819fc16bddf container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 13:04:27.945
  May  7 13:04:27.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-305" for this suite. @ 05/07/23 13:04:27.954
• [4.061 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 05/07/23 13:04:27.957
  May  7 13:04:27.957: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 13:04:27.958
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:04:27.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:04:27.97
  STEP: Discovering how many secrets are in namespace by default @ 05/07/23 13:04:27.971
  E0507 13:04:28.827379      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:29.827647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:30.827713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:31.827779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:32.828362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/07/23 13:04:32.974
  E0507 13:04:33.828420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:34.829064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:35.829181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:36.829940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:37.830570      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/07/23 13:04:37.977
  STEP: Ensuring resource quota status is calculated @ 05/07/23 13:04:37.979
  E0507 13:04:38.831416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:39.831592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 05/07/23 13:04:39.981
  STEP: Ensuring resource quota status captures secret creation @ 05/07/23 13:04:39.987
  E0507 13:04:40.832327      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:41.832459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 05/07/23 13:04:41.989
  STEP: Ensuring resource quota status released usage @ 05/07/23 13:04:41.992
  E0507 13:04:42.833382      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:43.833442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:04:43.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7040" for this suite. @ 05/07/23 13:04:43.996
• [16.042 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 05/07/23 13:04:44
  May  7 13:04:44.000: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 13:04:44
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:04:44.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:04:44.017
  STEP: Counting existing ResourceQuota @ 05/07/23 13:04:44.018
  E0507 13:04:44.833923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:45.834015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:46.834073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:47.834128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:48.834806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/07/23 13:04:49.02
  STEP: Ensuring resource quota status is calculated @ 05/07/23 13:04:49.024
  E0507 13:04:49.834869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:50.834937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 05/07/23 13:04:51.025
  STEP: Ensuring resource quota status captures replication controller creation @ 05/07/23 13:04:51.032
  E0507 13:04:51.835199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:52.835322      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 05/07/23 13:04:53.034
  STEP: Ensuring resource quota status released usage @ 05/07/23 13:04:53.045
  E0507 13:04:53.835882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:54.835949      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:04:55.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4120" for this suite. @ 05/07/23 13:04:55.048
• [11.051 seconds]
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/07/23 13:04:55.051
  May  7 13:04:55.051: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename conformance-tests @ 05/07/23 13:04:55.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:04:55.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:04:55.062
  STEP: Getting node addresses @ 05/07/23 13:04:55.063
  May  7 13:04:55.063: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  May  7 13:04:55.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-4991" for this suite. @ 05/07/23 13:04:55.067
• [0.019 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 05/07/23 13:04:55.07
  May  7 13:04:55.071: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename init-container @ 05/07/23 13:04:55.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:04:55.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:04:55.082
  STEP: creating the pod @ 05/07/23 13:04:55.083
  May  7 13:04:55.083: INFO: PodSpec: initContainers in spec.initContainers
  E0507 13:04:55.836244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:56.836391      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:57.836444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:58.836515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:04:59.836644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:00.837585      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:01.837651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:02.837905      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:03.837836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:04.837939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:05.838080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:06.838232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:07.838524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:08.838651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:09.838764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:10.838821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:11.838892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:12.839052      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:13.839101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:14.839162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:15.839267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:16.839368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:17.839642      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:18.839971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:19.840046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:20.840153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:21.840212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:22.840336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:23.840400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:24.840463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:25.840548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:26.840626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:27.840887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:28.840951      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:29.841928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:30.842066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:31.842113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:32.842300      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:33.842425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:34.842582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:35.842716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:36.842878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:37.843250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:05:38.376: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-fe1b2286-2268-49ed-8fe8-05c2d7d19dc6", GenerateName:"", Namespace:"init-container-8674", SelfLink:"", UID:"96d136d1-2fb1-40a9-9efb-a283c18f82f2", ResourceVersion:"74994", Generation:0, CreationTimestamp:time.Date(2023, time.May, 7, 13, 4, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"83566969"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 7, 13, 4, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006a48b28), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 7, 13, 5, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006a48b58), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-z9jmm", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0040c53e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-z9jmm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-z9jmm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-z9jmm", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002d170d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.255.0.202", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0000fe690), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d17160)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002d17190)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002d17198), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002d1719c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0012d6030), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 7, 13, 4, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 7, 13, 4, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 7, 13, 4, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 7, 13, 4, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.255.0.202", PodIP:"172.20.231.207", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.20.231.207"}}, StartTime:time.Date(2023, time.May, 7, 13, 4, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000fe850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000fe9a0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://cec6080975551dc4ff7db5ca149840876c18fea2a59a0f297deff36983f71b1c", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0040c5520), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0040c5500), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc002d1722f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  May  7 13:05:38.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8674" for this suite. @ 05/07/23 13:05:38.378
• [43.310 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 05/07/23 13:05:38.38
  May  7 13:05:38.380: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 13:05:38.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:05:38.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:05:38.392
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 13:05:38.394
  E0507 13:05:38.843839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:39.843904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:40.844837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:41.844860      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:05:42.405
  May  7 13:05:42.406: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-f509db44-8e2f-4498-b2be-9c0a5bb9a4c6 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 13:05:42.409
  May  7 13:05:42.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1266" for this suite. @ 05/07/23 13:05:42.417
• [4.039 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 05/07/23 13:05:42.42
  May  7 13:05:42.420: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 13:05:42.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:05:42.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:05:42.482
  STEP: Creating projection with secret that has name secret-emptykey-test-12bd1d6a-b2e6-47f5-a45f-33f2aa85fb47 @ 05/07/23 13:05:42.483
  May  7 13:05:42.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5379" for this suite. @ 05/07/23 13:05:42.486
• [0.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 05/07/23 13:05:42.489
  May  7 13:05:42.489: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename dns @ 05/07/23 13:05:42.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:05:42.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:05:42.5
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/07/23 13:05:42.501
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/07/23 13:05:42.501
  STEP: creating a pod to probe DNS @ 05/07/23 13:05:42.501
  STEP: submitting the pod to kubernetes @ 05/07/23 13:05:42.501
  E0507 13:05:42.845649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:43.845917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/07/23 13:05:44.51
  STEP: looking for the results for each expected name from probers @ 05/07/23 13:05:44.511
  May  7 13:05:44.549: INFO: DNS probes using dns-3859/dns-test-25c472bd-26f8-4e48-bd3c-3efaac8bb177 succeeded

  May  7 13:05:44.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/07/23 13:05:44.551
  STEP: Destroying namespace "dns-3859" for this suite. @ 05/07/23 13:05:44.56
• [2.075 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 05/07/23 13:05:44.564
  May  7 13:05:44.564: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:05:44.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:05:44.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:05:44.574
  STEP: Creating configMap with name projected-configmap-test-volume-77e622c0-b425-4136-8504-c0e25a05b927 @ 05/07/23 13:05:44.576
  STEP: Creating a pod to test consume configMaps @ 05/07/23 13:05:44.578
  E0507 13:05:44.846718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:45.846821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:05:46.584
  May  7 13:05:46.585: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-configmaps-8e49a6ff-8384-45cc-a359-96c54fa95b93 container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 13:05:46.588
  May  7 13:05:46.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2324" for this suite. @ 05/07/23 13:05:46.596
• [2.034 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 05/07/23 13:05:46.599
  May  7 13:05:46.599: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-webhook @ 05/07/23 13:05:46.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:05:46.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:05:46.609
  STEP: Setting up server cert @ 05/07/23 13:05:46.613
  E0507 13:05:46.846963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/07/23 13:05:47.02
  STEP: Deploying the custom resource conversion webhook pod @ 05/07/23 13:05:47.023
  STEP: Wait for the deployment to be ready @ 05/07/23 13:05:47.028
  May  7 13:05:47.031: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0507 13:05:47.847456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:48.848194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/07/23 13:05:49.036
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 13:05:49.041
  E0507 13:05:49.848253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:05:50.041: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May  7 13:05:50.043: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  E0507 13:05:50.848306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:51.848416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/07/23 13:05:52.582
  STEP: v2 custom resource should be converted @ 05/07/23 13:05:52.585
  May  7 13:05:52.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0507 13:05:52.848867      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-3295" for this suite. @ 05/07/23 13:05:53.126
• [6.532 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 05/07/23 13:05:53.131
  May  7 13:05:53.131: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename daemonsets @ 05/07/23 13:05:53.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:05:53.151
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:05:53.164
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/07/23 13:05:53.176
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/07/23 13:05:53.181
  May  7 13:05:53.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 13:05:53.188: INFO: Node 10.255.0.201 is running 0 daemon pod, expected 1
  E0507 13:05:53.848937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:05:54.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  7 13:05:54.192: INFO: Node 10.255.0.202 is running 0 daemon pod, expected 1
  E0507 13:05:54.848983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:05:55.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May  7 13:05:55.192: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/07/23 13:05:55.194
  May  7 13:05:55.205: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May  7 13:05:55.206: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/07/23 13:05:55.206
  E0507 13:05:55.849851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting DaemonSet "daemon-set" @ 05/07/23 13:05:56.21
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3249, will wait for the garbage collector to delete the pods @ 05/07/23 13:05:56.21
  May  7 13:05:56.264: INFO: Deleting DaemonSet.extensions daemon-set took: 2.312ms
  May  7 13:05:56.364: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.466558ms
  E0507 13:05:56.850322      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:05:57.467: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 13:05:57.467: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  7 13:05:57.468: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75336"},"items":null}

  May  7 13:05:57.469: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75336"},"items":null}

  May  7 13:05:57.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3249" for this suite. @ 05/07/23 13:05:57.475
• [4.346 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 05/07/23 13:05:57.479
  May  7 13:05:57.479: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename taint-single-pod @ 05/07/23 13:05:57.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:05:57.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:05:57.49
  May  7 13:05:57.491: INFO: Waiting up to 1m0s for all nodes to be ready
  E0507 13:05:57.850714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:58.850795      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:05:59.851224      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:00.851310      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:01.852246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:02.852363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:03.852868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:04.852944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:05.853013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:06.853076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:07.853934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:08.854026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:09.854594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:10.854659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:11.854945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:12.855054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:13.855896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:14.855982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:15.856557      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:16.856632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:17.856706      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:18.856767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:19.857669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:20.857751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:21.858752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:22.858952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:23.859580      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:24.859653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:25.860082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:26.860162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:27.861081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:28.861919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:29.862272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:30.862354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:31.863208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:32.864054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:33.864680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:34.864721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:35.865303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:36.865930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:37.866577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:38.866652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:39.867304      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:40.867660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:41.867727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:42.867931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:43.868547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:44.868621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:45.868872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:46.868953      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:47.869748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:48.869929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:49.870853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:50.871155      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:51.871230      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:52.871342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:53.871924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:54.872024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:55.873021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:56.873097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:06:57.500: INFO: Waiting for terminating namespaces to be deleted...
  May  7 13:06:57.501: INFO: Starting informer...
  STEP: Starting pod... @ 05/07/23 13:06:57.501
  May  7 13:06:57.709: INFO: Pod is running on 10.255.0.202. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/07/23 13:06:57.709
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/07/23 13:06:57.715
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/07/23 13:06:57.718
  May  7 13:06:57.718: INFO: Pod wasn't evicted. Proceeding
  May  7 13:06:57.718: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/07/23 13:06:57.726
  STEP: Waiting some time to make sure that toleration time passed. @ 05/07/23 13:06:57.727
  E0507 13:06:57.873887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:58.874685      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:06:59.874895      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:00.875004      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:01.875181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:02.875405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:03.875605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:04.875666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:05.875747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:06.875914      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:07.876102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:08.876161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:09.877044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:10.877106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:11.877926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:12.878014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:13.878100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:14.878151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:15.878563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:16.878628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:17.878894      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:18.879000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:19.879098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:20.879232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:21.879340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:22.879440      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:23.879888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:24.880019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:25.880139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:26.880214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:27.880511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:28.881016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:29.881928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:30.882028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:31.882138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:32.882393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:33.882464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:34.882526      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:35.882645      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:36.882799      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:37.883106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:38.883219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:39.883339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:40.883449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:41.883549      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:42.883745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:43.883817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:44.883915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:45.884085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:46.884201      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:47.884478      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:48.884589      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:49.884656      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:50.884734      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:51.884855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:52.884903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:53.885944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:54.886095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:55.886209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:56.887172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:57.887442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:58.887491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:07:59.887602      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:00.887748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:01.887856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:02.888049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:03.888187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:04.888299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:05.888453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:06.888562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:07.888751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:08.888857      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:09.888884      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:10.889939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:11.890036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:08:12.728: INFO: Pod wasn't evicted. Test successful
  May  7 13:08:12.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-4571" for this suite. @ 05/07/23 13:08:12.73
• [135.254 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 05/07/23 13:08:12.734
  May  7 13:08:12.734: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename var-expansion @ 05/07/23 13:08:12.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:08:12.746
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:08:12.749
  E0507 13:08:12.890890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:13.890970      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:08:14.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 13:08:14.760: INFO: Deleting pod "var-expansion-8b192f71-0546-47dd-9c83-fb51075102c0" in namespace "var-expansion-2829"
  May  7 13:08:14.763: INFO: Wait up to 5m0s for pod "var-expansion-8b192f71-0546-47dd-9c83-fb51075102c0" to be fully deleted
  E0507 13:08:14.891065      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:15.891239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-2829" for this suite. @ 05/07/23 13:08:16.767
• [4.036 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 05/07/23 13:08:16.771
  May  7 13:08:16.771: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 13:08:16.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:08:16.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:08:16.782
  STEP: Setting up server cert @ 05/07/23 13:08:16.794
  E0507 13:08:16.892137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 13:08:17.066
  STEP: Deploying the webhook pod @ 05/07/23 13:08:17.07
  STEP: Wait for the deployment to be ready @ 05/07/23 13:08:17.076
  May  7 13:08:17.078: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0507 13:08:17.892226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:18.893031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/07/23 13:08:19.083
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 13:08:19.087
  E0507 13:08:19.893085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:08:20.088: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/07/23 13:08:20.09
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/07/23 13:08:20.099
  STEP: Creating a configMap that should not be mutated @ 05/07/23 13:08:20.102
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/07/23 13:08:20.106
  STEP: Creating a configMap that should be mutated @ 05/07/23 13:08:20.11
  May  7 13:08:20.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2101" for this suite. @ 05/07/23 13:08:20.15
  STEP: Destroying namespace "webhook-markers-2788" for this suite. @ 05/07/23 13:08:20.156
• [3.395 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 05/07/23 13:08:20.167
  May  7 13:08:20.167: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/07/23 13:08:20.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:08:20.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:08:20.18
  May  7 13:08:20.182: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 13:08:20.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5494" for this suite. @ 05/07/23 13:08:20.706
• [0.543 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 05/07/23 13:08:20.71
  May  7 13:08:20.710: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 13:08:20.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:08:20.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:08:20.723
  STEP: Setting up server cert @ 05/07/23 13:08:20.737
  E0507 13:08:20.893828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 13:08:20.973
  STEP: Deploying the webhook pod @ 05/07/23 13:08:20.976
  STEP: Wait for the deployment to be ready @ 05/07/23 13:08:20.983
  May  7 13:08:20.990: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0507 13:08:21.894552      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:22.894673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/07/23 13:08:22.995
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 13:08:22.999
  E0507 13:08:23.894699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:08:23.999: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/07/23 13:08:24.001
  STEP: create a configmap that should be updated by the webhook @ 05/07/23 13:08:24.01
  May  7 13:08:24.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2624" for this suite. @ 05/07/23 13:08:24.049
  STEP: Destroying namespace "webhook-markers-5853" for this suite. @ 05/07/23 13:08:24.055
• [3.356 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 05/07/23 13:08:24.066
  May  7 13:08:24.066: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 13:08:24.066
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:08:24.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:08:24.082
  STEP: Creating configMap with name configmap-test-volume-b9dccf12-5d92-4f4d-af15-aac82c9b0e8f @ 05/07/23 13:08:24.083
  STEP: Creating a pod to test consume configMaps @ 05/07/23 13:08:24.086
  E0507 13:08:24.894752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:25.894967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:26.895820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:27.896043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:08:28.099
  May  7 13:08:28.100: INFO: Trying to get logs from node 10.255.0.202 pod pod-configmaps-98fa7201-0554-4bb0-9f36-9098e6f0bcb6 container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 13:08:28.108
  May  7 13:08:28.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-803" for this suite. @ 05/07/23 13:08:28.118
• [4.054 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 05/07/23 13:08:28.12
  May  7 13:08:28.120: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sched-preemption @ 05/07/23 13:08:28.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:08:28.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:08:28.132
  May  7 13:08:28.142: INFO: Waiting up to 1m0s for all nodes to be ready
  E0507 13:08:28.896094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:29.896159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:30.896232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:31.896295      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:32.897240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:33.897934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:34.897986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:35.898102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:36.898139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:37.898386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:38.899308      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:39.899407      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:40.899667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:41.899766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:42.900359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:43.900403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:44.900860      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:45.900909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:46.901307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:47.901935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:48.901986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:49.902050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:50.902117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:51.902314      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:52.903249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:53.903298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:54.903366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:55.903563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:56.903586      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:57.903877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:58.903890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:08:59.903986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:00.904421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:01.905472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:02.905893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:03.905955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:04.906196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:05.906316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:06.907233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:07.907410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:08.908232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:09.908302      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:10.908369      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:11.908476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:12.908884      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:13.909935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:14.910425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:15.910535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:16.911427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:17.911742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:18.911915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:19.912879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:20.913909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:21.913975      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:22.914078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:23.914241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:24.914295      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:25.914405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:26.914464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:27.914551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:09:28.152: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/07/23 13:09:28.153
  May  7 13:09:28.164: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May  7 13:09:28.168: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May  7 13:09:28.188: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May  7 13:09:28.193: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  May  7 13:09:28.220: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  May  7 13:09:28.233: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/07/23 13:09:28.233
  E0507 13:09:28.914682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:29.914718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/07/23 13:09:30.245
  E0507 13:09:30.915654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:31.915727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:32.915796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:33.915865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:09:34.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-4424" for this suite. @ 05/07/23 13:09:34.298
• [66.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 05/07/23 13:09:34.304
  May  7 13:09:34.304: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:09:34.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:09:34.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:09:34.366
  STEP: Creating configMap with name cm-test-opt-del-28ab5871-0d79-4859-8653-6aeec4a25d50 @ 05/07/23 13:09:34.37
  STEP: Creating configMap with name cm-test-opt-upd-dd9fcc4d-788c-4201-9729-b2de46642257 @ 05/07/23 13:09:34.372
  STEP: Creating the pod @ 05/07/23 13:09:34.373
  E0507 13:09:34.916480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:35.916618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-28ab5871-0d79-4859-8653-6aeec4a25d50 @ 05/07/23 13:09:36.397
  STEP: Updating configmap cm-test-opt-upd-dd9fcc4d-788c-4201-9729-b2de46642257 @ 05/07/23 13:09:36.399
  STEP: Creating configMap with name cm-test-opt-create-6e8c14f2-1c9e-4187-b2c8-6cb7105ff8a8 @ 05/07/23 13:09:36.403
  STEP: waiting to observe update in volume @ 05/07/23 13:09:36.405
  E0507 13:09:36.917569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:37.917914      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:38.918600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:39.918744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:40.919601      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:41.919741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:42.920185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:43.920253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:44.921027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:45.921133      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:46.921589      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:47.921952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:48.922649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:49.922752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:50.922804      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:51.922914      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:52.923490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:53.923563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:54.923825      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:55.923892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:56.924425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:57.924704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:58.925052      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:09:59.925925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:00.926145      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:01.926246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:02.926467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:03.926583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:04.927141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:05.927249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:06.927299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:07.927608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:08.927927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:09.928033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:10.928667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:11.928735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:12.929458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:13.929514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:14.929876      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:15.929982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:16.930682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:17.931018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:18.932012      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:19.932090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:20.932650      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:21.932748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:22.933246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:23.933325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:24.933896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:25.933981      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:26.934776      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:27.935101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:28.935693      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:29.936422      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:30.937116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:31.937178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:32.937714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:33.937883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:34.937929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:35.938034      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:36.938732      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:37.939021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:38.939069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:39.939137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:40.939925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:41.940003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:42.940794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:43.940879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:44.941567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:45.941951      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:46.942959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:47.943213      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:48.943735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:49.943824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:50.944716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:51.944914      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:10:52.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2240" for this suite. @ 05/07/23 13:10:52.61
• [78.309 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 05/07/23 13:10:52.613
  May  7 13:10:52.613: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename field-validation @ 05/07/23 13:10:52.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:10:52.621
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:10:52.624
  May  7 13:10:52.627: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  E0507 13:10:52.945285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:53.945342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:54.945419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:10:55.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7589" for this suite. @ 05/07/23 13:10:55.167
• [2.556 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 05/07/23 13:10:55.169
  May  7 13:10:55.169: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename runtimeclass @ 05/07/23 13:10:55.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:10:55.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:10:55.185
  E0507 13:10:55.945935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:56.946009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:10:57.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9558" for this suite. @ 05/07/23 13:10:57.201
• [2.035 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/07/23 13:10:57.204
  May  7 13:10:57.204: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sched-pred @ 05/07/23 13:10:57.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:10:57.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:10:57.212
  May  7 13:10:57.215: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  7 13:10:57.218: INFO: Waiting for terminating namespaces to be deleted...
  May  7 13:10:57.220: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.201 before test
  May  7 13:10:57.222: INFO: calico-kube-controllers-7ccf856ff8-2v596 from kube-system started at 2023-05-07 12:02:36 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.222: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May  7 13:10:57.222: INFO: calico-node-w75tc from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.222: INFO: 	Container calico-node ready: true, restart count 0
  May  7 13:10:57.222: INFO: coredns-6557d7db9c-cgp7l from kube-system started at 2023-05-07 09:07:01 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.222: INFO: 	Container coredns ready: true, restart count 0
  May  7 13:10:57.222: INFO: dashboard-metrics-scraper-5c876f54bd-z7s9t from kube-system started at 2023-05-07 09:07:07 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.222: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May  7 13:10:57.222: INFO: metrics-server-57fbbb5957-fw6s6 from kube-system started at 2023-05-07 09:10:30 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.222: INFO: 	Container metrics-server ready: true, restart count 0
  May  7 13:10:57.222: INFO: node-local-dns-hwr8c from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.222: INFO: 	Container node-cache ready: true, restart count 0
  May  7 13:10:57.222: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-w58n4 from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 13:10:57.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 13:10:57.222: INFO: 	Container systemd-logs ready: true, restart count 0
  May  7 13:10:57.222: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.202 before test
  May  7 13:10:57.225: INFO: calico-node-j77rx from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.225: INFO: 	Container calico-node ready: true, restart count 0
  May  7 13:10:57.225: INFO: node-local-dns-4lrj5 from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.225: INFO: 	Container node-cache ready: true, restart count 0
  May  7 13:10:57.225: INFO: pod-projected-configmaps-5f0cc8b7-8691-47b2-8f7a-c6e5b0a7977c from projected-2240 started at 2023-05-07 13:09:34 +0000 UTC (3 container statuses recorded)
  May  7 13:10:57.225: INFO: 	Container createcm-volume-test ready: true, restart count 0
  May  7 13:10:57.225: INFO: 	Container delcm-volume-test ready: true, restart count 0
  May  7 13:10:57.225: INFO: 	Container updcm-volume-test ready: true, restart count 0
  May  7 13:10:57.225: INFO: test-runtimeclass-runtimeclass-9558-preconfigured-handler-kctz5 from runtimeclass-9558 started at 2023-05-07 13:10:55 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.225: INFO: 	Container test ready: false, restart count 0
  May  7 13:10:57.225: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-7v67t from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 13:10:57.225: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 13:10:57.225: INFO: 	Container systemd-logs ready: true, restart count 0
  May  7 13:10:57.225: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.203 before test
  May  7 13:10:57.227: INFO: calico-node-2m8cq from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.227: INFO: 	Container calico-node ready: true, restart count 0
  May  7 13:10:57.227: INFO: kubernetes-dashboard-89b5448d6-2d4rk from kube-system started at 2023-05-07 12:02:36 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.227: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May  7 13:10:57.228: INFO: node-local-dns-9mnsl from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.228: INFO: 	Container node-cache ready: true, restart count 0
  May  7 13:10:57.228: INFO: sonobuoy from sonobuoy started at 2023-05-07 11:54:26 +0000 UTC (1 container statuses recorded)
  May  7 13:10:57.228: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  7 13:10:57.228: INFO: sonobuoy-e2e-job-8208a86a1e4a4756 from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 13:10:57.228: INFO: 	Container e2e ready: true, restart count 0
  May  7 13:10:57.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 13:10:57.228: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-mhtmw from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 13:10:57.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 13:10:57.228: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/07/23 13:10:57.228
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.175cde8cd8b704a0], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 05/07/23 13:10:57.247
  E0507 13:10:57.946925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:10:58.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3084" for this suite. @ 05/07/23 13:10:58.245
• [1.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 05/07/23 13:10:58.248
  May  7 13:10:58.248: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename disruption @ 05/07/23 13:10:58.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:10:58.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:10:58.259
  STEP: creating the pdb @ 05/07/23 13:10:58.261
  STEP: Waiting for the pdb to be processed @ 05/07/23 13:10:58.263
  E0507 13:10:58.946990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:10:59.947091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 05/07/23 13:11:00.266
  STEP: Waiting for the pdb to be processed @ 05/07/23 13:11:00.27
  E0507 13:11:00.947803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:01.947937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 05/07/23 13:11:02.275
  STEP: Waiting for the pdb to be processed @ 05/07/23 13:11:02.281
  STEP: Waiting for the pdb to be deleted @ 05/07/23 13:11:02.289
  May  7 13:11:02.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3797" for this suite. @ 05/07/23 13:11:02.293
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/07/23 13:11:02.297
  May  7 13:11:02.297: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:11:02.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:11:02.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:11:02.317
  STEP: Creating projection with secret that has name projected-secret-test-534a07ae-ad99-4935-90bb-2b393b974962 @ 05/07/23 13:11:02.319
  STEP: Creating a pod to test consume secrets @ 05/07/23 13:11:02.321
  E0507 13:11:02.947976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:03.948066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:04.948931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:05.949950      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:11:06.332
  May  7 13:11:06.333: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-secrets-69b5bc7f-58cb-4b8e-adba-51c34798c9d8 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 13:11:06.336
  May  7 13:11:06.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8506" for this suite. @ 05/07/23 13:11:06.344
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 05/07/23 13:11:06.347
  May  7 13:11:06.347: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename runtimeclass @ 05/07/23 13:11:06.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:11:06.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:11:06.36
  May  7 13:11:06.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6569" for this suite. @ 05/07/23 13:11:06.365
• [0.020 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/07/23 13:11:06.368
  May  7 13:11:06.368: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename containers @ 05/07/23 13:11:06.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:11:06.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:11:06.38
  STEP: Creating a pod to test override all @ 05/07/23 13:11:06.381
  E0507 13:11:06.949989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:07.950229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:08.950893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:09.950962      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:11:10.39
  May  7 13:11:10.392: INFO: Trying to get logs from node 10.255.0.202 pod client-containers-00a7ca1e-dc57-4f7c-bbe2-2446981ea7c9 container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 13:11:10.395
  May  7 13:11:10.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1562" for this suite. @ 05/07/23 13:11:10.402
• [4.038 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 05/07/23 13:11:10.406
  May  7 13:11:10.406: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 13:11:10.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:11:10.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:11:10.417
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/07/23 13:11:10.418
  E0507 13:11:10.951019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:11.951095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:12.951912      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:13.951977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:11:14.426
  May  7 13:11:14.428: INFO: Trying to get logs from node 10.255.0.202 pod pod-5fd28d1b-7ad5-4885-b3ce-fdfd1f4fe155 container test-container: <nil>
  STEP: delete the pod @ 05/07/23 13:11:14.43
  May  7 13:11:14.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9020" for this suite. @ 05/07/23 13:11:14.439
• [4.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 05/07/23 13:11:14.443
  May  7 13:11:14.443: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 13:11:14.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:11:14.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:11:14.453
  STEP: Creating the pod @ 05/07/23 13:11:14.456
  E0507 13:11:14.952254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:15.953144      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:16.953830      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:11:16.974: INFO: Successfully updated pod "annotationupdate03f7b97a-b7a5-490f-88ae-5036f4ec64a1"
  E0507 13:11:17.954161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:18.954227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:19.954809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:20.954872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:11:20.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7291" for this suite. @ 05/07/23 13:11:20.987
• [6.547 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 05/07/23 13:11:20.991
  May  7 13:11:20.991: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 13:11:20.992
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:11:20.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:11:21.001
  STEP: creating service nodeport-test with type=NodePort in namespace services-1639 @ 05/07/23 13:11:21.004
  STEP: creating replication controller nodeport-test in namespace services-1639 @ 05/07/23 13:11:21.009
  I0507 13:11:21.022106      20 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-1639, replica count: 2
  E0507 13:11:21.954910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:22.955172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:23.955958      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0507 13:11:24.074220      20 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  7 13:11:24.074: INFO: Creating new exec pod
  E0507 13:11:24.955997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:25.956064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:26.956123      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:11:27.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-1639 exec execpodrlvvx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May  7 13:11:27.170: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May  7 13:11:27.170: INFO: stdout: "nodeport-test-nnb7l"
  May  7 13:11:27.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-1639 exec execpodrlvvx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.213.128 80'
  May  7 13:11:27.257: INFO: stderr: "+ nc -v -t -w 2 10.68.213.128 80\nConnection to 10.68.213.128 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  May  7 13:11:27.257: INFO: stdout: ""
  E0507 13:11:27.956956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:11:28.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-1639 exec execpodrlvvx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.213.128 80'
  May  7 13:11:28.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.68.213.128 80\nConnection to 10.68.213.128 80 port [tcp/http] succeeded!\n"
  May  7 13:11:28.361: INFO: stdout: "nodeport-test-nnb7l"
  May  7 13:11:28.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-1639 exec execpodrlvvx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.0.203 31860'
  May  7 13:11:28.445: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.0.203 31860\nConnection to 10.255.0.203 31860 port [tcp/*] succeeded!\n"
  May  7 13:11:28.445: INFO: stdout: "nodeport-test-nnb7l"
  May  7 13:11:28.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-1639 exec execpodrlvvx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.0.202 31860'
  May  7 13:11:28.529: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.0.202 31860\nConnection to 10.255.0.202 31860 port [tcp/*] succeeded!\n"
  May  7 13:11:28.529: INFO: stdout: "nodeport-test-nnb7l"
  May  7 13:11:28.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1639" for this suite. @ 05/07/23 13:11:28.53
• [7.542 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 05/07/23 13:11:28.534
  May  7 13:11:28.534: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename security-context @ 05/07/23 13:11:28.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:11:28.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:11:28.546
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/07/23 13:11:28.547
  E0507 13:11:28.957881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:29.958015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:30.958708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:31.959661      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:11:32.557
  May  7 13:11:32.558: INFO: Trying to get logs from node 10.255.0.202 pod security-context-7e827a05-fd2d-4059-8ec3-dc2efe10b75c container test-container: <nil>
  STEP: delete the pod @ 05/07/23 13:11:32.561
  May  7 13:11:32.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-6424" for this suite. @ 05/07/23 13:11:32.571
• [4.039 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 05/07/23 13:11:32.574
  May  7 13:11:32.574: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 13:11:32.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:11:32.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:11:32.588
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 13:11:32.59
  E0507 13:11:32.960228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:33.960312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:34.960829      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:35.960977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:11:36.601
  May  7 13:11:36.602: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-66d45a7e-c0f5-4b80-bd18-484c6a985069 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 13:11:36.605
  May  7 13:11:36.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1063" for this suite. @ 05/07/23 13:11:36.614
• [4.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 05/07/23 13:11:36.616
  May  7 13:11:36.616: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 13:11:36.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:11:36.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:11:36.628
  E0507 13:11:36.961788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:37.961840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:38.962534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:39.962731      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:40.963325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:41.964071      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:42.964134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:43.964788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:44.965518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:45.965891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:46.966663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:47.966704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:48.966847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:49.967136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:50.967271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:51.967341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:52.967864      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/07/23 13:11:53.631
  E0507 13:11:53.967932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:54.968543      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:55.969314      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:56.969742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:57.970100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/07/23 13:11:58.633
  STEP: Ensuring resource quota status is calculated @ 05/07/23 13:11:58.635
  E0507 13:11:58.971046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:11:59.971184      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 05/07/23 13:12:00.637
  STEP: Ensuring resource quota status captures configMap creation @ 05/07/23 13:12:00.642
  E0507 13:12:00.971307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:01.971436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 05/07/23 13:12:02.644
  STEP: Ensuring resource quota status released usage @ 05/07/23 13:12:02.647
  E0507 13:12:02.971483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:03.971632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:12:04.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2713" for this suite. @ 05/07/23 13:12:04.651
• [28.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 05/07/23 13:12:04.654
  May  7 13:12:04.654: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename configmap @ 05/07/23 13:12:04.654
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:12:04.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:12:04.666
  STEP: Creating configMap with name configmap-test-volume-map-dfc2e74a-7249-4138-8b39-f54f62b8df6d @ 05/07/23 13:12:04.668
  STEP: Creating a pod to test consume configMaps @ 05/07/23 13:12:04.67
  E0507 13:12:04.972285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:05.972385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:06.973322      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:07.973924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:12:08.678
  May  7 13:12:08.680: INFO: Trying to get logs from node 10.255.0.202 pod pod-configmaps-179b8fc9-8a54-4625-b54b-596817357af1 container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 13:12:08.682
  May  7 13:12:08.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8293" for this suite. @ 05/07/23 13:12:08.69
• [4.039 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 05/07/23 13:12:08.693
  May  7 13:12:08.693: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 13:12:08.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:12:08.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:12:08.705
  STEP: creating a replication controller @ 05/07/23 13:12:08.706
  May  7 13:12:08.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 create -f -'
  E0507 13:12:08.974136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:12:09.285: INFO: stderr: ""
  May  7 13:12:09.285: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/07/23 13:12:09.285
  May  7 13:12:09.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  7 13:12:09.338: INFO: stderr: ""
  May  7 13:12:09.338: INFO: stdout: "update-demo-nautilus-59hfk update-demo-nautilus-qrm54 "
  May  7 13:12:09.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 get pods update-demo-nautilus-59hfk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  7 13:12:09.380: INFO: stderr: ""
  May  7 13:12:09.380: INFO: stdout: ""
  May  7 13:12:09.380: INFO: update-demo-nautilus-59hfk is created but not running
  E0507 13:12:09.974643      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:10.974764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:11.974964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:12.975159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:13.975285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:12:14.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May  7 13:12:14.427: INFO: stderr: ""
  May  7 13:12:14.427: INFO: stdout: "update-demo-nautilus-59hfk update-demo-nautilus-qrm54 "
  May  7 13:12:14.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 get pods update-demo-nautilus-59hfk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  7 13:12:14.469: INFO: stderr: ""
  May  7 13:12:14.469: INFO: stdout: "true"
  May  7 13:12:14.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 get pods update-demo-nautilus-59hfk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  7 13:12:14.511: INFO: stderr: ""
  May  7 13:12:14.511: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  7 13:12:14.511: INFO: validating pod update-demo-nautilus-59hfk
  May  7 13:12:14.515: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  7 13:12:14.515: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  7 13:12:14.515: INFO: update-demo-nautilus-59hfk is verified up and running
  May  7 13:12:14.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 get pods update-demo-nautilus-qrm54 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May  7 13:12:14.558: INFO: stderr: ""
  May  7 13:12:14.558: INFO: stdout: "true"
  May  7 13:12:14.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 get pods update-demo-nautilus-qrm54 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May  7 13:12:14.601: INFO: stderr: ""
  May  7 13:12:14.601: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May  7 13:12:14.601: INFO: validating pod update-demo-nautilus-qrm54
  May  7 13:12:14.605: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May  7 13:12:14.606: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May  7 13:12:14.606: INFO: update-demo-nautilus-qrm54 is verified up and running
  STEP: using delete to clean up resources @ 05/07/23 13:12:14.606
  May  7 13:12:14.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 delete --grace-period=0 --force -f -'
  May  7 13:12:14.647: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May  7 13:12:14.647: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May  7 13:12:14.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 get rc,svc -l name=update-demo --no-headers'
  May  7 13:12:14.707: INFO: stderr: "No resources found in kubectl-1705 namespace.\n"
  May  7 13:12:14.707: INFO: stdout: ""
  May  7 13:12:14.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-1705 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May  7 13:12:14.767: INFO: stderr: ""
  May  7 13:12:14.767: INFO: stdout: ""
  May  7 13:12:14.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1705" for this suite. @ 05/07/23 13:12:14.769
• [6.079 seconds]
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 05/07/23 13:12:14.772
  May  7 13:12:14.772: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename svcaccounts @ 05/07/23 13:12:14.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:12:14.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:12:14.784
  STEP: creating a ServiceAccount @ 05/07/23 13:12:14.786
  STEP: watching for the ServiceAccount to be added @ 05/07/23 13:12:14.791
  STEP: patching the ServiceAccount @ 05/07/23 13:12:14.792
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/07/23 13:12:14.794
  STEP: deleting the ServiceAccount @ 05/07/23 13:12:14.796
  May  7 13:12:14.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7634" for this suite. @ 05/07/23 13:12:14.807
• [0.037 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 05/07/23 13:12:14.81
  May  7 13:12:14.810: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 13:12:14.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:12:14.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:12:14.821
  STEP: creating secret secrets-4365/secret-test-e9468040-17c3-4494-9141-54afbb18c89e @ 05/07/23 13:12:14.823
  STEP: Creating a pod to test consume secrets @ 05/07/23 13:12:14.826
  E0507 13:12:14.975865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:15.975977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:16.976154      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:17.977026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:12:18.838
  May  7 13:12:18.839: INFO: Trying to get logs from node 10.255.0.202 pod pod-configmaps-f5e0812f-54a5-4dcf-969a-24af245aa081 container env-test: <nil>
  STEP: delete the pod @ 05/07/23 13:12:18.843
  May  7 13:12:18.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4365" for this suite. @ 05/07/23 13:12:18.851
• [4.044 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 05/07/23 13:12:18.855
  May  7 13:12:18.855: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename podtemplate @ 05/07/23 13:12:18.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:12:18.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:12:18.866
  May  7 13:12:18.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3669" for this suite. @ 05/07/23 13:12:18.879
• [0.028 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 05/07/23 13:12:18.883
  May  7 13:12:18.883: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename var-expansion @ 05/07/23 13:12:18.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:12:18.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:12:18.898
  STEP: Creating a pod to test substitution in container's command @ 05/07/23 13:12:18.899
  E0507 13:12:18.977688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:19.977876      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:20.978444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:21.978589      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:12:22.907
  May  7 13:12:22.908: INFO: Trying to get logs from node 10.255.0.202 pod var-expansion-f9fb2f7d-a41b-4cd0-b3d3-61cd19020ca6 container dapi-container: <nil>
  STEP: delete the pod @ 05/07/23 13:12:22.911
  May  7 13:12:22.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3295" for this suite. @ 05/07/23 13:12:22.92
• [4.039 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 05/07/23 13:12:22.923
  May  7 13:12:22.923: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename namespaces @ 05/07/23 13:12:22.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:12:22.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:12:22.933
  STEP: Creating a test namespace @ 05/07/23 13:12:22.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:12:22.943
  STEP: Creating a pod in the namespace @ 05/07/23 13:12:22.945
  STEP: Waiting for the pod to have running status @ 05/07/23 13:12:22.949
  E0507 13:12:22.979147      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:23.979204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 05/07/23 13:12:24.953
  STEP: Waiting for the namespace to be removed. @ 05/07/23 13:12:24.956
  E0507 13:12:24.979468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:25.979906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:26.980777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:27.981494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:28.981909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:29.982051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:30.982317      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:31.982646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:32.983296      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:33.984359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:34.985386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/07/23 13:12:35.958
  STEP: Verifying there are no pods in the namespace @ 05/07/23 13:12:35.968
  May  7 13:12:35.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6417" for this suite. @ 05/07/23 13:12:35.972
  STEP: Destroying namespace "nsdeletetest-6722" for this suite. @ 05/07/23 13:12:35.975
  May  7 13:12:35.976: INFO: Namespace nsdeletetest-6722 was already deleted
  STEP: Destroying namespace "nsdeletetest-6560" for this suite. @ 05/07/23 13:12:35.976
• [13.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/07/23 13:12:35.983
  May  7 13:12:35.983: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 13:12:35.983
  E0507 13:12:35.985393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:12:35.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:12:35.996
  STEP: Creating secret with name s-test-opt-del-37cd7892-de18-42a4-82ed-9dcd570441e3 @ 05/07/23 13:12:35.999
  STEP: Creating secret with name s-test-opt-upd-6bade625-660a-4da0-8e5e-89607381700b @ 05/07/23 13:12:36
  STEP: Creating the pod @ 05/07/23 13:12:36.002
  E0507 13:12:36.986425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:37.987354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-37cd7892-de18-42a4-82ed-9dcd570441e3 @ 05/07/23 13:12:38.019
  STEP: Updating secret s-test-opt-upd-6bade625-660a-4da0-8e5e-89607381700b @ 05/07/23 13:12:38.021
  STEP: Creating secret with name s-test-opt-create-6ce1eecc-c3a5-4e45-8f95-b2fbfe0115dd @ 05/07/23 13:12:38.023
  STEP: waiting to observe update in volume @ 05/07/23 13:12:38.025
  E0507 13:12:38.988109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:39.988174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:40.988236      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:41.988609      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:42.988901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:43.988952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:44.989931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:45.990066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:46.991064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:47.991173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:48.991996      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:49.992066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:50.992410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:51.992716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:52.993417      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:53.993480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:54.993538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:55.993942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:56.994426      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:57.994520      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:58.995009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:12:59.995096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:00.995169      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:01.996180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:02.997237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:03.997308      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:04.997362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:05.997959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:06.998234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:07.998344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:08.999285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:09.999352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:10.999407      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:12.000371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:13.000841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:14.000879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:15.001442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:16.001544      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:17.002441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:18.002541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:19.002835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:20.002954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:21.003000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:22.003591      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:23.004634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:24.004705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:25.004873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:26.004956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:27.005898      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:28.005964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:29.006022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:30.006761      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:31.007245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:32.007427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:33.007934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:34.008207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:35.008783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:36.008907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:37.009677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:38.009744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:39.009809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:40.009869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:41.010525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:42.010538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:43.010624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:44.010676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:45.011207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:46.011295      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:47.011835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:48.011936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:49.012670      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:50.012751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:51.012862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:52.013587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:53.013928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:54.014041      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:55.014771      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:56.015681      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:13:56.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6238" for this suite. @ 05/07/23 13:13:56.263
• [80.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 05/07/23 13:13:56.266
  May  7 13:13:56.266: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 13:13:56.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:13:56.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:13:56.279
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 13:13:56.28
  E0507 13:13:57.015663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:58.015760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:13:59.015971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:00.016036      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:14:00.291
  May  7 13:14:00.292: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-c4832723-0cfd-4c3c-ab8a-af3dcb29e157 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 13:14:00.295
  May  7 13:14:00.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8349" for this suite. @ 05/07/23 13:14:00.303
• [4.040 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/07/23 13:14:00.306
  May  7 13:14:00.307: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubelet-test @ 05/07/23 13:14:00.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:14:00.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:14:00.315
  May  7 13:14:00.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5425" for this suite. @ 05/07/23 13:14:00.336
• [0.032 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 05/07/23 13:14:00.339
  May  7 13:14:00.339: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename events @ 05/07/23 13:14:00.34
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:14:00.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:14:00.349
  STEP: Create set of events @ 05/07/23 13:14:00.35
  May  7 13:14:00.353: INFO: created test-event-1
  May  7 13:14:00.354: INFO: created test-event-2
  May  7 13:14:00.356: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/07/23 13:14:00.356
  STEP: delete collection of events @ 05/07/23 13:14:00.357
  May  7 13:14:00.357: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/07/23 13:14:00.363
  May  7 13:14:00.363: INFO: requesting list of events to confirm quantity
  May  7 13:14:00.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8774" for this suite. @ 05/07/23 13:14:00.373
• [0.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/07/23 13:14:00.377
  May  7 13:14:00.377: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename proxy @ 05/07/23 13:14:00.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:14:00.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:14:00.389
  STEP: starting an echo server on multiple ports @ 05/07/23 13:14:00.395
  STEP: creating replication controller proxy-service-ln67g in namespace proxy-3041 @ 05/07/23 13:14:00.395
  I0507 13:14:00.407175      20 runners.go:194] Created replication controller with name: proxy-service-ln67g, namespace: proxy-3041, replica count: 1
  E0507 13:14:01.016194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0507 13:14:01.457605      20 runners.go:194] proxy-service-ln67g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0507 13:14:02.016350      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0507 13:14:02.458191      20 runners.go:194] proxy-service-ln67g Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  7 13:14:02.459: INFO: setup took 2.068758698s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/07/23 13:14:02.459
  May  7 13:14:02.465: INFO: (0) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 5.684317ms)
  May  7 13:14:02.466: INFO: (0) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.211934ms)
  May  7 13:14:02.468: INFO: (0) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 8.545248ms)
  May  7 13:14:02.468: INFO: (0) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 8.566613ms)
  May  7 13:14:02.471: INFO: (0) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 11.822197ms)
  May  7 13:14:02.471: INFO: (0) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 11.562166ms)
  May  7 13:14:02.471: INFO: (0) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 11.432318ms)
  May  7 13:14:02.471: INFO: (0) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 11.746634ms)
  May  7 13:14:02.472: INFO: (0) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 11.821754ms)
  May  7 13:14:02.472: INFO: (0) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 12.27477ms)
  May  7 13:14:02.472: INFO: (0) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 12.079122ms)
  May  7 13:14:02.472: INFO: (0) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 11.966696ms)
  May  7 13:14:02.472: INFO: (0) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 12.329935ms)
  May  7 13:14:02.472: INFO: (0) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 12.448933ms)
  May  7 13:14:02.472: INFO: (0) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 12.292036ms)
  May  7 13:14:02.472: INFO: (0) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 12.367161ms)
  May  7 13:14:02.476: INFO: (1) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 4.058064ms)
  May  7 13:14:02.476: INFO: (1) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 4.28011ms)
  May  7 13:14:02.477: INFO: (1) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 4.458175ms)
  May  7 13:14:02.477: INFO: (1) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 4.399776ms)
  May  7 13:14:02.477: INFO: (1) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 4.438149ms)
  May  7 13:14:02.479: INFO: (1) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 6.223273ms)
  May  7 13:14:02.479: INFO: (1) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.368448ms)
  May  7 13:14:02.479: INFO: (1) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 7.170397ms)
  May  7 13:14:02.480: INFO: (1) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 7.124297ms)
  May  7 13:14:02.480: INFO: (1) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 7.116173ms)
  May  7 13:14:02.480: INFO: (1) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 7.271256ms)
  May  7 13:14:02.480: INFO: (1) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 7.255581ms)
  May  7 13:14:02.480: INFO: (1) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 7.093211ms)
  May  7 13:14:02.480: INFO: (1) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 7.136011ms)
  May  7 13:14:02.480: INFO: (1) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 7.214377ms)
  May  7 13:14:02.480: INFO: (1) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 7.168569ms)
  May  7 13:14:02.484: INFO: (2) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 3.817706ms)
  May  7 13:14:02.485: INFO: (2) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 4.855902ms)
  May  7 13:14:02.485: INFO: (2) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 4.981126ms)
  May  7 13:14:02.485: INFO: (2) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 4.974804ms)
  May  7 13:14:02.485: INFO: (2) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 5.02494ms)
  May  7 13:14:02.485: INFO: (2) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 5.017599ms)
  May  7 13:14:02.485: INFO: (2) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 5.276728ms)
  May  7 13:14:02.487: INFO: (2) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 6.921193ms)
  May  7 13:14:02.487: INFO: (2) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 7.191371ms)
  May  7 13:14:02.487: INFO: (2) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 7.347043ms)
  May  7 13:14:02.487: INFO: (2) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 7.4794ms)
  May  7 13:14:02.487: INFO: (2) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 7.336019ms)
  May  7 13:14:02.487: INFO: (2) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 7.338617ms)
  May  7 13:14:02.487: INFO: (2) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 7.332328ms)
  May  7 13:14:02.487: INFO: (2) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 7.403198ms)
  May  7 13:14:02.487: INFO: (2) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 7.468352ms)
  May  7 13:14:02.491: INFO: (3) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 3.110096ms)
  May  7 13:14:02.491: INFO: (3) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 3.145539ms)
  May  7 13:14:02.491: INFO: (3) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 3.507049ms)
  May  7 13:14:02.491: INFO: (3) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 3.675367ms)
  May  7 13:14:02.491: INFO: (3) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 3.770027ms)
  May  7 13:14:02.491: INFO: (3) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 3.822404ms)
  May  7 13:14:02.494: INFO: (3) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 5.851471ms)
  May  7 13:14:02.495: INFO: (3) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 6.83518ms)
  May  7 13:14:02.495: INFO: (3) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 6.993279ms)
  May  7 13:14:02.495: INFO: (3) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 6.866752ms)
  May  7 13:14:02.495: INFO: (3) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 6.841243ms)
  May  7 13:14:02.495: INFO: (3) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 7.042541ms)
  May  7 13:14:02.495: INFO: (3) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 7.223277ms)
  May  7 13:14:02.495: INFO: (3) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 7.253142ms)
  May  7 13:14:02.495: INFO: (3) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 7.294325ms)
  May  7 13:14:02.495: INFO: (3) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 7.191339ms)
  May  7 13:14:02.498: INFO: (4) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 2.968915ms)
  May  7 13:14:02.502: INFO: (4) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 6.369279ms)
  May  7 13:14:02.502: INFO: (4) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 6.677988ms)
  May  7 13:14:02.502: INFO: (4) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 6.622636ms)
  May  7 13:14:02.502: INFO: (4) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 6.693032ms)
  May  7 13:14:02.502: INFO: (4) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 6.853833ms)
  May  7 13:14:02.502: INFO: (4) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 7.074095ms)
  May  7 13:14:02.502: INFO: (4) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 7.063046ms)
  May  7 13:14:02.502: INFO: (4) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 7.191335ms)
  May  7 13:14:02.502: INFO: (4) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 7.105292ms)
  May  7 13:14:02.503: INFO: (4) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 7.246492ms)
  May  7 13:14:02.503: INFO: (4) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 7.303877ms)
  May  7 13:14:02.503: INFO: (4) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 7.470028ms)
  May  7 13:14:02.503: INFO: (4) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 7.436785ms)
  May  7 13:14:02.503: INFO: (4) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 7.574671ms)
  May  7 13:14:02.503: INFO: (4) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 7.54211ms)
  May  7 13:14:02.507: INFO: (5) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 4.174244ms)
  May  7 13:14:02.508: INFO: (5) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 5.501687ms)
  May  7 13:14:02.509: INFO: (5) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 5.585149ms)
  May  7 13:14:02.509: INFO: (5) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 5.598777ms)
  May  7 13:14:02.509: INFO: (5) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 5.789947ms)
  May  7 13:14:02.509: INFO: (5) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.082036ms)
  May  7 13:14:02.509: INFO: (5) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.472742ms)
  May  7 13:14:02.510: INFO: (5) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 7.426925ms)
  May  7 13:14:02.511: INFO: (5) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 7.683677ms)
  May  7 13:14:02.511: INFO: (5) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 7.970029ms)
  May  7 13:14:02.512: INFO: (5) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 9.460962ms)
  May  7 13:14:02.512: INFO: (5) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 9.48195ms)
  May  7 13:14:02.512: INFO: (5) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 9.518058ms)
  May  7 13:14:02.513: INFO: (5) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 9.709475ms)
  May  7 13:14:02.513: INFO: (5) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 9.489711ms)
  May  7 13:14:02.513: INFO: (5) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 9.66376ms)
  May  7 13:14:02.517: INFO: (6) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 3.842308ms)
  May  7 13:14:02.517: INFO: (6) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 3.978113ms)
  May  7 13:14:02.517: INFO: (6) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 4.134466ms)
  May  7 13:14:02.517: INFO: (6) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 4.279935ms)
  May  7 13:14:02.517: INFO: (6) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 4.385093ms)
  May  7 13:14:02.517: INFO: (6) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 4.13381ms)
  May  7 13:14:02.518: INFO: (6) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 5.100193ms)
  May  7 13:14:02.518: INFO: (6) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 5.034799ms)
  May  7 13:14:02.519: INFO: (6) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 6.290834ms)
  May  7 13:14:02.519: INFO: (6) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 6.608961ms)
  May  7 13:14:02.520: INFO: (6) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 6.886635ms)
  May  7 13:14:02.520: INFO: (6) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 6.642445ms)
  May  7 13:14:02.520: INFO: (6) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.683285ms)
  May  7 13:14:02.520: INFO: (6) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 6.832605ms)
  May  7 13:14:02.520: INFO: (6) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 6.806059ms)
  May  7 13:14:02.520: INFO: (6) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 7.055419ms)
  May  7 13:14:02.526: INFO: (7) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 5.162208ms)
  May  7 13:14:02.526: INFO: (7) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 5.787556ms)
  May  7 13:14:02.526: INFO: (7) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 6.050653ms)
  May  7 13:14:02.526: INFO: (7) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 6.185386ms)
  May  7 13:14:02.526: INFO: (7) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 6.260551ms)
  May  7 13:14:02.526: INFO: (7) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 5.949085ms)
  May  7 13:14:02.526: INFO: (7) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 6.10739ms)
  May  7 13:14:02.527: INFO: (7) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 6.35526ms)
  May  7 13:14:02.527: INFO: (7) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 6.295964ms)
  May  7 13:14:02.527: INFO: (7) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.572471ms)
  May  7 13:14:02.527: INFO: (7) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 6.701353ms)
  May  7 13:14:02.527: INFO: (7) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 6.623619ms)
  May  7 13:14:02.527: INFO: (7) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 6.877945ms)
  May  7 13:14:02.527: INFO: (7) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 7.265223ms)
  May  7 13:14:02.527: INFO: (7) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 7.074165ms)
  May  7 13:14:02.528: INFO: (7) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 7.296498ms)
  May  7 13:14:02.533: INFO: (8) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 4.700683ms)
  May  7 13:14:02.533: INFO: (8) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 4.701187ms)
  May  7 13:14:02.533: INFO: (8) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 4.688546ms)
  May  7 13:14:02.533: INFO: (8) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 5.394924ms)
  May  7 13:14:02.534: INFO: (8) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 5.59295ms)
  May  7 13:14:02.534: INFO: (8) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 5.44431ms)
  May  7 13:14:02.534: INFO: (8) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 5.555692ms)
  May  7 13:14:02.534: INFO: (8) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 5.606813ms)
  May  7 13:14:02.534: INFO: (8) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 5.800339ms)
  May  7 13:14:02.534: INFO: (8) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 5.692574ms)
  May  7 13:14:02.535: INFO: (8) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 6.794924ms)
  May  7 13:14:02.535: INFO: (8) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.990135ms)
  May  7 13:14:02.535: INFO: (8) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 6.651509ms)
  May  7 13:14:02.535: INFO: (8) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 7.127135ms)
  May  7 13:14:02.535: INFO: (8) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 7.238372ms)
  May  7 13:14:02.535: INFO: (8) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 7.142679ms)
  May  7 13:14:02.540: INFO: (9) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 4.950243ms)
  May  7 13:14:02.541: INFO: (9) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 5.223294ms)
  May  7 13:14:02.541: INFO: (9) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 5.205645ms)
  May  7 13:14:02.541: INFO: (9) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 5.178924ms)
  May  7 13:14:02.541: INFO: (9) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 5.228608ms)
  May  7 13:14:02.541: INFO: (9) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 5.227961ms)
  May  7 13:14:02.541: INFO: (9) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 5.491568ms)
  May  7 13:14:02.542: INFO: (9) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 6.88102ms)
  May  7 13:14:02.542: INFO: (9) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 6.796849ms)
  May  7 13:14:02.542: INFO: (9) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 7.012617ms)
  May  7 13:14:02.543: INFO: (9) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 7.482245ms)
  May  7 13:14:02.543: INFO: (9) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 7.629992ms)
  May  7 13:14:02.543: INFO: (9) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 7.474112ms)
  May  7 13:14:02.543: INFO: (9) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 7.714001ms)
  May  7 13:14:02.549: INFO: (9) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 13.702096ms)
  May  7 13:14:02.549: INFO: (9) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 13.680333ms)
  May  7 13:14:02.554: INFO: (10) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 4.377137ms)
  May  7 13:14:02.554: INFO: (10) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 4.524285ms)
  May  7 13:14:02.554: INFO: (10) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 4.499504ms)
  May  7 13:14:02.555: INFO: (10) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 5.186973ms)
  May  7 13:14:02.555: INFO: (10) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 5.253788ms)
  May  7 13:14:02.555: INFO: (10) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 5.281646ms)
  May  7 13:14:02.555: INFO: (10) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 5.265294ms)
  May  7 13:14:02.555: INFO: (10) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 5.427571ms)
  May  7 13:14:02.555: INFO: (10) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 5.785293ms)
  May  7 13:14:02.556: INFO: (10) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 6.177868ms)
  May  7 13:14:02.556: INFO: (10) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 6.143562ms)
  May  7 13:14:02.556: INFO: (10) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 6.180778ms)
  May  7 13:14:02.557: INFO: (10) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 7.483451ms)
  May  7 13:14:02.557: INFO: (10) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 7.573336ms)
  May  7 13:14:02.557: INFO: (10) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 7.858926ms)
  May  7 13:14:02.558: INFO: (10) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 7.911505ms)
  May  7 13:14:02.561: INFO: (11) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 3.754092ms)
  May  7 13:14:02.562: INFO: (11) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 4.036478ms)
  May  7 13:14:02.562: INFO: (11) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 4.280108ms)
  May  7 13:14:02.563: INFO: (11) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 4.96655ms)
  May  7 13:14:02.563: INFO: (11) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 5.020029ms)
  May  7 13:14:02.563: INFO: (11) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 5.465806ms)
  May  7 13:14:02.563: INFO: (11) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 5.855624ms)
  May  7 13:14:02.565: INFO: (11) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 6.839447ms)
  May  7 13:14:02.565: INFO: (11) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 6.974796ms)
  May  7 13:14:02.565: INFO: (11) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 7.062554ms)
  May  7 13:14:02.565: INFO: (11) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 7.460626ms)
  May  7 13:14:02.565: INFO: (11) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 7.19931ms)
  May  7 13:14:02.565: INFO: (11) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 7.453564ms)
  May  7 13:14:02.565: INFO: (11) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 7.624922ms)
  May  7 13:14:02.565: INFO: (11) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 7.591257ms)
  May  7 13:14:02.565: INFO: (11) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 7.80275ms)
  May  7 13:14:02.569: INFO: (12) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 3.454835ms)
  May  7 13:14:02.569: INFO: (12) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 3.661256ms)
  May  7 13:14:02.570: INFO: (12) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 3.799945ms)
  May  7 13:14:02.570: INFO: (12) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 3.914041ms)
  May  7 13:14:02.570: INFO: (12) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 4.798483ms)
  May  7 13:14:02.571: INFO: (12) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 5.487904ms)
  May  7 13:14:02.571: INFO: (12) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 5.312932ms)
  May  7 13:14:02.571: INFO: (12) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 5.486006ms)
  May  7 13:14:02.571: INFO: (12) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 5.78116ms)
  May  7 13:14:02.572: INFO: (12) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 6.107442ms)
  May  7 13:14:02.572: INFO: (12) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 6.457287ms)
  May  7 13:14:02.572: INFO: (12) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 6.51737ms)
  May  7 13:14:02.573: INFO: (12) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 7.436284ms)
  May  7 13:14:02.574: INFO: (12) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 7.649393ms)
  May  7 13:14:02.574: INFO: (12) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 8.032923ms)
  May  7 13:14:02.574: INFO: (12) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 8.392292ms)
  May  7 13:14:02.580: INFO: (13) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 4.971497ms)
  May  7 13:14:02.580: INFO: (13) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 5.160287ms)
  May  7 13:14:02.580: INFO: (13) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 5.471853ms)
  May  7 13:14:02.580: INFO: (13) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 5.433615ms)
  May  7 13:14:02.580: INFO: (13) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 5.625284ms)
  May  7 13:14:02.580: INFO: (13) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 5.589377ms)
  May  7 13:14:02.580: INFO: (13) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 5.452325ms)
  May  7 13:14:02.580: INFO: (13) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 6.018046ms)
  May  7 13:14:02.580: INFO: (13) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 5.789771ms)
  May  7 13:14:02.581: INFO: (13) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 5.84052ms)
  May  7 13:14:02.581: INFO: (13) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 6.741666ms)
  May  7 13:14:02.582: INFO: (13) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 7.063121ms)
  May  7 13:14:02.582: INFO: (13) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 7.520458ms)
  May  7 13:14:02.582: INFO: (13) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 7.820204ms)
  May  7 13:14:02.583: INFO: (13) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 7.740065ms)
  May  7 13:14:02.583: INFO: (13) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 8.027492ms)
  May  7 13:14:02.586: INFO: (14) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 3.561588ms)
  May  7 13:14:02.587: INFO: (14) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 3.722564ms)
  May  7 13:14:02.588: INFO: (14) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 5.323599ms)
  May  7 13:14:02.589: INFO: (14) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 5.562003ms)
  May  7 13:14:02.589: INFO: (14) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 5.930054ms)
  May  7 13:14:02.589: INFO: (14) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 6.029354ms)
  May  7 13:14:02.589: INFO: (14) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.014277ms)
  May  7 13:14:02.589: INFO: (14) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.332195ms)
  May  7 13:14:02.590: INFO: (14) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 6.702422ms)
  May  7 13:14:02.590: INFO: (14) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 6.808558ms)
  May  7 13:14:02.591: INFO: (14) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 8.016813ms)
  May  7 13:14:02.591: INFO: (14) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 7.965621ms)
  May  7 13:14:02.591: INFO: (14) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 8.227253ms)
  May  7 13:14:02.591: INFO: (14) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 8.392194ms)
  May  7 13:14:02.592: INFO: (14) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 9.019036ms)
  May  7 13:14:02.592: INFO: (14) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 8.852496ms)
  May  7 13:14:02.596: INFO: (15) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 4.380204ms)
  May  7 13:14:02.598: INFO: (15) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 5.402124ms)
  May  7 13:14:02.598: INFO: (15) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 5.372788ms)
  May  7 13:14:02.598: INFO: (15) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 6.101383ms)
  May  7 13:14:02.598: INFO: (15) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.217527ms)
  May  7 13:14:02.598: INFO: (15) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 6.461474ms)
  May  7 13:14:02.599: INFO: (15) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 7.182917ms)
  May  7 13:14:02.599: INFO: (15) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 7.298226ms)
  May  7 13:14:02.601: INFO: (15) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 8.528703ms)
  May  7 13:14:02.601: INFO: (15) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 8.328857ms)
  May  7 13:14:02.601: INFO: (15) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 8.336894ms)
  May  7 13:14:02.601: INFO: (15) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 8.610841ms)
  May  7 13:14:02.601: INFO: (15) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 8.603177ms)
  May  7 13:14:02.601: INFO: (15) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 8.806336ms)
  May  7 13:14:02.601: INFO: (15) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 8.661702ms)
  May  7 13:14:02.601: INFO: (15) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 9.040919ms)
  May  7 13:14:02.605: INFO: (16) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 4.309563ms)
  May  7 13:14:02.605: INFO: (16) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 4.489992ms)
  May  7 13:14:02.609: INFO: (16) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 7.421727ms)
  May  7 13:14:02.610: INFO: (16) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 8.091965ms)
  May  7 13:14:02.610: INFO: (16) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 8.196108ms)
  May  7 13:14:02.610: INFO: (16) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 8.363162ms)
  May  7 13:14:02.610: INFO: (16) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 8.407635ms)
  May  7 13:14:02.610: INFO: (16) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 8.249655ms)
  May  7 13:14:02.610: INFO: (16) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 8.456769ms)
  May  7 13:14:02.610: INFO: (16) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 8.721595ms)
  May  7 13:14:02.610: INFO: (16) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 8.966198ms)
  May  7 13:14:02.610: INFO: (16) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 8.906629ms)
  May  7 13:14:02.610: INFO: (16) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 9.131622ms)
  May  7 13:14:02.611: INFO: (16) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 9.29154ms)
  May  7 13:14:02.611: INFO: (16) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 9.315396ms)
  May  7 13:14:02.619: INFO: (16) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 17.933865ms)
  May  7 13:14:02.626: INFO: (17) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 6.465243ms)
  May  7 13:14:02.626: INFO: (17) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 6.483418ms)
  May  7 13:14:02.626: INFO: (17) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 6.582228ms)
  May  7 13:14:02.626: INFO: (17) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 6.575031ms)
  May  7 13:14:02.626: INFO: (17) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 6.786987ms)
  May  7 13:14:02.626: INFO: (17) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 6.811931ms)
  May  7 13:14:02.627: INFO: (17) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 7.23547ms)
  May  7 13:14:02.627: INFO: (17) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 7.585729ms)
  May  7 13:14:02.627: INFO: (17) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 7.574426ms)
  May  7 13:14:02.627: INFO: (17) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 7.631545ms)
  May  7 13:14:02.628: INFO: (17) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 8.841874ms)
  May  7 13:14:02.629: INFO: (17) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 9.225053ms)
  May  7 13:14:02.629: INFO: (17) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 9.186644ms)
  May  7 13:14:02.629: INFO: (17) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 9.091941ms)
  May  7 13:14:02.629: INFO: (17) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 9.419796ms)
  May  7 13:14:02.630: INFO: (17) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 10.255881ms)
  May  7 13:14:02.634: INFO: (18) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 3.720154ms)
  May  7 13:14:02.634: INFO: (18) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 3.916834ms)
  May  7 13:14:02.636: INFO: (18) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 5.807422ms)
  May  7 13:14:02.636: INFO: (18) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 5.457533ms)
  May  7 13:14:02.638: INFO: (18) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 7.454156ms)
  May  7 13:14:02.638: INFO: (18) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 7.35133ms)
  May  7 13:14:02.638: INFO: (18) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 7.318801ms)
  May  7 13:14:02.638: INFO: (18) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 7.399341ms)
  May  7 13:14:02.638: INFO: (18) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 7.671786ms)
  May  7 13:14:02.638: INFO: (18) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 7.885991ms)
  May  7 13:14:02.639: INFO: (18) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 8.497024ms)
  May  7 13:14:02.639: INFO: (18) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 8.747558ms)
  May  7 13:14:02.639: INFO: (18) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 9.066276ms)
  May  7 13:14:02.639: INFO: (18) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 8.959288ms)
  May  7 13:14:02.639: INFO: (18) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 9.114081ms)
  May  7 13:14:02.639: INFO: (18) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 9.176535ms)
  May  7 13:14:02.646: INFO: (19) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:462/proxy/: tls qux (200; 6.080857ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:1080/proxy/rewriteme">... (200; 7.987015ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname2/proxy/: bar (200; 8.023467ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:1080/proxy/rewriteme">test<... (200; 8.160987ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:160/proxy/: foo (200; 8.133629ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname2/proxy/: tls qux (200; 8.461849ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/services/https:proxy-service-ln67g:tlsportname1/proxy/: tls baz (200; 8.4554ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb:162/proxy/: bar (200; 8.392874ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:460/proxy/: tls baz (200; 8.489586ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname1/proxy/: foo (200; 8.836336ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/https:proxy-service-ln67g-d7swb:443/proxy/tlsrewritem... (200; 8.638768ms)
  May  7 13:14:02.648: INFO: (19) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:162/proxy/: bar (200; 8.850444ms)
  May  7 13:14:02.649: INFO: (19) /api/v1/namespaces/proxy-3041/services/proxy-service-ln67g:portname2/proxy/: bar (200; 9.01604ms)
  May  7 13:14:02.649: INFO: (19) /api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/: <a href="/api/v1/namespaces/proxy-3041/pods/proxy-service-ln67g-d7swb/proxy/rewriteme">test</a> (200; 8.970088ms)
  May  7 13:14:02.649: INFO: (19) /api/v1/namespaces/proxy-3041/services/http:proxy-service-ln67g:portname1/proxy/: foo (200; 9.206092ms)
  May  7 13:14:02.649: INFO: (19) /api/v1/namespaces/proxy-3041/pods/http:proxy-service-ln67g-d7swb:160/proxy/: foo (200; 9.088076ms)
  May  7 13:14:02.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-ln67g in namespace proxy-3041, will wait for the garbage collector to delete the pods @ 05/07/23 13:14:02.65
  May  7 13:14:02.704: INFO: Deleting ReplicationController proxy-service-ln67g took: 2.16716ms
  May  7 13:14:02.805: INFO: Terminating ReplicationController proxy-service-ln67g pods took: 100.852733ms
  E0507 13:14:03.016759      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:04.017404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:05.018024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-3041" for this suite. @ 05/07/23 13:14:05.209
• [4.835 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 05/07/23 13:14:05.214
  May  7 13:14:05.214: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:14:05.215
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:14:05.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:14:05.228
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 13:14:05.229
  E0507 13:14:06.018091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:07.018978      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:08.019840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:09.019927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:14:09.241
  May  7 13:14:09.243: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-c74fa723-198e-43ca-97cc-16266f2d5b9a container client-container: <nil>
  STEP: delete the pod @ 05/07/23 13:14:09.245
  May  7 13:14:09.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5387" for this suite. @ 05/07/23 13:14:09.255
• [4.042 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/07/23 13:14:09.257
  May  7 13:14:09.257: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename subjectreview @ 05/07/23 13:14:09.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:14:09.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:14:09.266
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-2219" @ 05/07/23 13:14:09.269
  May  7 13:14:09.271: INFO: saUsername: "system:serviceaccount:subjectreview-2219:e2e"
  May  7 13:14:09.271: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-2219"}
  May  7 13:14:09.271: INFO: saUID: "760dba38-a61b-476e-855a-46c99136758b"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-2219:e2e" @ 05/07/23 13:14:09.271
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-2219:e2e" @ 05/07/23 13:14:09.271
  May  7 13:14:09.272: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-2219:e2e" api 'list' configmaps in "subjectreview-2219" namespace @ 05/07/23 13:14:09.272
  May  7 13:14:09.273: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-2219:e2e" @ 05/07/23 13:14:09.273
  May  7 13:14:09.274: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  May  7 13:14:09.274: INFO: LocalSubjectAccessReview has been verified
  May  7 13:14:09.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-2219" for this suite. @ 05/07/23 13:14:09.276
• [0.021 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 05/07/23 13:14:09.278
  May  7 13:14:09.278: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename webhook @ 05/07/23 13:14:09.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:14:09.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:14:09.29
  STEP: Setting up server cert @ 05/07/23 13:14:09.307
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/07/23 13:14:09.625
  STEP: Deploying the webhook pod @ 05/07/23 13:14:09.629
  STEP: Wait for the deployment to be ready @ 05/07/23 13:14:09.634
  May  7 13:14:09.637: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0507 13:14:10.020793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:11.021748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/07/23 13:14:11.642
  STEP: Verifying the service has paired with the endpoint @ 05/07/23 13:14:11.652
  E0507 13:14:12.022461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:14:12.653: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/07/23 13:14:12.655
  STEP: create a pod that should be updated by the webhook @ 05/07/23 13:14:12.663
  May  7 13:14:12.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1579" for this suite. @ 05/07/23 13:14:12.721
  STEP: Destroying namespace "webhook-markers-7240" for this suite. @ 05/07/23 13:14:12.727
• [3.457 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 05/07/23 13:14:12.737
  May  7 13:14:12.737: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename svcaccounts @ 05/07/23 13:14:12.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:14:12.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:14:12.774
  May  7 13:14:12.811: INFO: created pod pod-service-account-defaultsa
  May  7 13:14:12.811: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  May  7 13:14:12.837: INFO: created pod pod-service-account-mountsa
  May  7 13:14:12.837: INFO: pod pod-service-account-mountsa service account token volume mount: true
  May  7 13:14:12.843: INFO: created pod pod-service-account-nomountsa
  May  7 13:14:12.843: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  May  7 13:14:12.847: INFO: created pod pod-service-account-defaultsa-mountspec
  May  7 13:14:12.847: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  May  7 13:14:12.856: INFO: created pod pod-service-account-mountsa-mountspec
  May  7 13:14:12.856: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  May  7 13:14:12.868: INFO: created pod pod-service-account-nomountsa-mountspec
  May  7 13:14:12.869: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  May  7 13:14:12.879: INFO: created pod pod-service-account-defaultsa-nomountspec
  May  7 13:14:12.879: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  May  7 13:14:12.890: INFO: created pod pod-service-account-mountsa-nomountspec
  May  7 13:14:12.890: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  May  7 13:14:12.908: INFO: created pod pod-service-account-nomountsa-nomountspec
  May  7 13:14:12.908: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  May  7 13:14:12.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3738" for this suite. @ 05/07/23 13:14:12.913
• [0.218 seconds]
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 05/07/23 13:14:12.956
  May  7 13:14:12.956: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pod-network-test @ 05/07/23 13:14:12.956
  E0507 13:14:13.022819      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:14:13.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:14:13.082
  STEP: Performing setup for networking test in namespace pod-network-test-8148 @ 05/07/23 13:14:13.084
  STEP: creating a selector @ 05/07/23 13:14:13.085
  STEP: Creating the service pods in kubernetes @ 05/07/23 13:14:13.085
  May  7 13:14:13.085: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0507 13:14:14.022946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:15.023086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:16.024003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:17.024131      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:18.024462      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:19.024620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:20.024871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:21.024945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:22.025600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:23.025704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:24.025758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:25.026174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:26.026281      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:27.026463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:28.026494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:29.026529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:30.027177      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:31.027560      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:32.028321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:33.028351      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:34.028869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:35.029933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/07/23 13:14:35.192
  E0507 13:14:36.030022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:37.030406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:14:37.217: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May  7 13:14:37.217: INFO: Going to poll 172.20.3.77 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May  7 13:14:37.218: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.3.77:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8148 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 13:14:37.218: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 13:14:37.218: INFO: ExecWithOptions: Clientset creation
  May  7 13:14:37.218: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-8148/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.3.77%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  7 13:14:37.290: INFO: Found all 1 expected endpoints: [netserver-0]
  May  7 13:14:37.290: INFO: Going to poll 172.20.231.226 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May  7 13:14:37.292: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.231.226:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8148 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 13:14:37.292: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 13:14:37.292: INFO: ExecWithOptions: Clientset creation
  May  7 13:14:37.292: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-8148/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.231.226%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  7 13:14:37.335: INFO: Found all 1 expected endpoints: [netserver-1]
  May  7 13:14:37.335: INFO: Going to poll 172.20.191.47 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May  7 13:14:37.336: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.191.47:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8148 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May  7 13:14:37.336: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  May  7 13:14:37.336: INFO: ExecWithOptions: Clientset creation
  May  7 13:14:37.336: INFO: ExecWithOptions: execute(POST https://10.68.0.1:443/api/v1/namespaces/pod-network-test-8148/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.191.47%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May  7 13:14:37.362: INFO: Found all 1 expected endpoints: [netserver-2]
  May  7 13:14:37.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8148" for this suite. @ 05/07/23 13:14:37.364
• [24.412 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/07/23 13:14:37.369
  May  7 13:14:37.369: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/07/23 13:14:37.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:14:37.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:14:37.383
  STEP: Creating 50 configmaps @ 05/07/23 13:14:37.384
  STEP: Creating RC which spawns configmap-volume pods @ 05/07/23 13:14:37.622
  May  7 13:14:37.773: INFO: Pod name wrapped-volume-race-c8cdb9da-fae1-4737-a77e-8e950ef2b958: Found 3 pods out of 5
  E0507 13:14:38.031225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:39.031342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:40.031501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:41.032279      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:42.032680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:14:42.776: INFO: Pod name wrapped-volume-race-c8cdb9da-fae1-4737-a77e-8e950ef2b958: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/07/23 13:14:42.776
  STEP: Creating RC which spawns configmap-volume pods @ 05/07/23 13:14:42.789
  May  7 13:14:42.799: INFO: Pod name wrapped-volume-race-877940c0-2e1b-4979-bba5-d32def675a98: Found 0 pods out of 5
  E0507 13:14:43.032654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:44.032727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:45.032861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:46.033041      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:47.033928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:14:47.803: INFO: Pod name wrapped-volume-race-877940c0-2e1b-4979-bba5-d32def675a98: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/07/23 13:14:47.803
  STEP: Creating RC which spawns configmap-volume pods @ 05/07/23 13:14:47.811
  May  7 13:14:47.822: INFO: Pod name wrapped-volume-race-6228c18d-38ce-4915-b3d1-9372af26fb45: Found 0 pods out of 5
  E0507 13:14:48.034720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:49.034847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:50.034917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:51.034992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:52.035210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:14:52.827: INFO: Pod name wrapped-volume-race-6228c18d-38ce-4915-b3d1-9372af26fb45: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/07/23 13:14:52.827
  May  7 13:14:52.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-6228c18d-38ce-4915-b3d1-9372af26fb45 in namespace emptydir-wrapper-1942, will wait for the garbage collector to delete the pods @ 05/07/23 13:14:52.836
  May  7 13:14:52.894: INFO: Deleting ReplicationController wrapped-volume-race-6228c18d-38ce-4915-b3d1-9372af26fb45 took: 5.017485ms
  May  7 13:14:52.994: INFO: Terminating ReplicationController wrapped-volume-race-6228c18d-38ce-4915-b3d1-9372af26fb45 pods took: 100.560547ms
  E0507 13:14:53.035916      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-877940c0-2e1b-4979-bba5-d32def675a98 in namespace emptydir-wrapper-1942, will wait for the garbage collector to delete the pods @ 05/07/23 13:14:53.994
  E0507 13:14:54.036658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:14:54.049: INFO: Deleting ReplicationController wrapped-volume-race-877940c0-2e1b-4979-bba5-d32def675a98 took: 2.757287ms
  May  7 13:14:54.150: INFO: Terminating ReplicationController wrapped-volume-race-877940c0-2e1b-4979-bba5-d32def675a98 pods took: 100.933082ms
  E0507 13:14:55.037525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-c8cdb9da-fae1-4737-a77e-8e950ef2b958 in namespace emptydir-wrapper-1942, will wait for the garbage collector to delete the pods @ 05/07/23 13:14:55.45
  May  7 13:14:55.507: INFO: Deleting ReplicationController wrapped-volume-race-c8cdb9da-fae1-4737-a77e-8e950ef2b958 took: 2.46529ms
  May  7 13:14:55.608: INFO: Terminating ReplicationController wrapped-volume-race-c8cdb9da-fae1-4737-a77e-8e950ef2b958 pods took: 100.425305ms
  E0507 13:14:56.048986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 05/07/23 13:14:57.008
  E0507 13:14:57.049427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-1942" for this suite. @ 05/07/23 13:14:57.13
• [19.763 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 05/07/23 13:14:57.132
  May  7 13:14:57.132: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 13:14:57.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:14:57.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:14:57.152
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/07/23 13:14:57.158
  E0507 13:14:58.049941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:14:59.050002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:00.050401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:01.050518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:15:01.171
  May  7 13:15:01.173: INFO: Trying to get logs from node 10.255.0.202 pod pod-7b0ad11e-433e-4415-963a-deb5b3fe70fd container test-container: <nil>
  STEP: delete the pod @ 05/07/23 13:15:01.178
  May  7 13:15:01.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5954" for this suite. @ 05/07/23 13:15:01.189
• [4.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/07/23 13:15:01.194
  May  7 13:15:01.194: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename deployment @ 05/07/23 13:15:01.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:15:01.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:15:01.257
  May  7 13:15:01.258: INFO: Creating deployment "test-recreate-deployment"
  May  7 13:15:01.261: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  May  7 13:15:01.266: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0507 13:15:02.050544      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:03.050874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:15:03.270: INFO: Waiting deployment "test-recreate-deployment" to complete
  May  7 13:15:03.271: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  May  7 13:15:03.275: INFO: Updating deployment test-recreate-deployment
  May  7 13:15:03.275: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  May  7 13:15:03.333: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-6869  2c531436-2641-4fa2-8347-8627dea66881 78738 2 2023-05-07 13:15:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-07 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b4d228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-07 13:15:03 +0000 UTC,LastTransitionTime:2023-05-07 13:15:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-05-07 13:15:03 +0000 UTC,LastTransitionTime:2023-05-07 13:15:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  May  7 13:15:03.334: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-6869  632555be-f49a-4fa4-b542-42f7202a47af 78737 1 2023-05-07 13:15:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 2c531436-2641-4fa2-8347-8627dea66881 0xc0049ad677 0xc0049ad678}] [] [{kube-controller-manager Update apps/v1 2023-05-07 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c531436-2641-4fa2-8347-8627dea66881\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 13:15:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049ad718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  7 13:15:03.334: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  May  7 13:15:03.334: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-6869  64423c1b-7b3c-4bd4-9bf5-58a678f9f9c2 78726 2 2023-05-07 13:15:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 2c531436-2641-4fa2-8347-8627dea66881 0xc0049ad787 0xc0049ad788}] [] [{kube-controller-manager Update apps/v1 2023-05-07 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c531436-2641-4fa2-8347-8627dea66881\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 13:15:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049ad838 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  7 13:15:03.335: INFO: Pod "test-recreate-deployment-54757ffd6c-n8rqr" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-n8rqr test-recreate-deployment-54757ffd6c- deployment-6869  f8e2c866-c378-464f-a6af-ff47de486aeb 78736 0 2023-05-07 13:15:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 632555be-f49a-4fa4-b542-42f7202a47af 0xc003b4d617 0xc003b4d618}] [] [{kube-controller-manager Update v1 2023-05-07 13:15:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"632555be-f49a-4fa4-b542-42f7202a47af\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 13:15:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-np6kj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-np6kj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 13:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 13:15:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 13:15:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 13:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:,StartTime:2023-05-07 13:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 13:15:03.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6869" for this suite. @ 05/07/23 13:15:03.337
• [2.146 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 05/07/23 13:15:03.341
  May  7 13:15:03.341: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 13:15:03.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:15:03.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:15:03.352
  STEP: creating a secret @ 05/07/23 13:15:03.353
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/07/23 13:15:03.355
  STEP: patching the secret @ 05/07/23 13:15:03.356
  STEP: deleting the secret using a LabelSelector @ 05/07/23 13:15:03.36
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/07/23 13:15:03.362
  May  7 13:15:03.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6999" for this suite. @ 05/07/23 13:15:03.371
• [0.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 05/07/23 13:15:03.376
  May  7 13:15:03.376: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/07/23 13:15:03.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:15:03.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:15:03.387
  STEP: create the container to handle the HTTPGet hook request. @ 05/07/23 13:15:03.389
  E0507 13:15:04.051026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:05.051149      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/07/23 13:15:05.403
  E0507 13:15:06.051593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:07.052095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/07/23 13:15:07.411
  STEP: delete the pod with lifecycle hook @ 05/07/23 13:15:07.421
  E0507 13:15:08.053396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:09.053488      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:15:09.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3898" for this suite. @ 05/07/23 13:15:09.43
• [6.056 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/07/23 13:15:09.433
  May  7 13:15:09.433: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename cronjob @ 05/07/23 13:15:09.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:15:09.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:15:09.445
  STEP: Creating a suspended cronjob @ 05/07/23 13:15:09.446
  STEP: Ensuring no jobs are scheduled @ 05/07/23 13:15:09.449
  E0507 13:15:10.053512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:11.053577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:12.054205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:13.054277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:14.055075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:15.055197      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:16.056072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:17.056429      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:18.057307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:19.057456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:20.057878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:21.057943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:22.058586      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:23.059141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:24.059991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:25.060200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:26.061077      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:27.061932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:28.062718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:29.062785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:30.062844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:31.062925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:32.063265      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:33.063343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:34.064208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:35.064278      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:36.064342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:37.064428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:38.065140      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:39.065221      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:40.065930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:41.066874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:42.067597      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:43.067765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:44.068311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:45.068368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:46.069247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:47.069603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:48.069654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:49.069718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:50.069781      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:51.069845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:52.070763      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:53.070886      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:54.071828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:55.071898      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:56.071965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:57.072494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:58.072877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:15:59.073938      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:00.074654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:01.074773      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:02.075782      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:03.075882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:04.076683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:05.077176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:06.077903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:07.078332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:08.079035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:09.079154      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:10.079201      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:11.080164      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:12.080861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:13.080899      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:14.080957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:15.081936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:16.082004      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:17.082503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:18.082571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:19.082644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:20.082784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:21.083265      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:22.083678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:23.083784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:24.084441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:25.084501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:26.084936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:27.085445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:28.085796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:29.086800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:30.086861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:31.086948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:32.087412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:33.087609      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:34.087654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:35.087772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:36.087854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:37.088511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:38.089490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:39.089551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:40.089638      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:41.089681      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:42.090318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:43.090434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:44.091199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:45.092079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:46.092935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:47.093225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:48.093931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:49.094000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:50.094572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:51.094670      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:52.094870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:53.095049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:54.095974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:55.096721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:56.097228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:57.097954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:58.098451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:16:59.098522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:00.099363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:01.099439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:02.100225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:03.100378      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:04.101282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:05.101414      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:06.101907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:07.102741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:08.102796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:09.102879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:10.103375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:11.103438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:12.103571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:13.103751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:14.103767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:15.103909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:16.104540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:17.104827      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:18.105675      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:19.105727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:20.105743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:21.106316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:22.107032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:23.107174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:24.108093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:25.108295      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:26.109185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:27.109520      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:28.109574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:29.109934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:30.109996      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:31.110065      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:32.110806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:33.111208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:34.111253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:35.111321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:36.112241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:37.112591      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:38.112883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:39.113954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:40.114480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:41.114612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:42.115324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:43.115409      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:44.116264      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:45.116321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:46.116397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:47.117018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:48.117877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:49.117947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:50.118318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:51.118410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:52.118692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:53.118809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:54.119652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:55.119749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:56.119810      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:57.120317      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:58.121091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:17:59.121157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:00.121213      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:01.121942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:02.122846      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:03.122952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:04.123666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:05.123737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:06.123810      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:07.124533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:08.124586      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:09.124674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:10.124882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:11.125910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:12.126628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:13.126719      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:14.126906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:15.127838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:16.127905      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:17.128529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:18.128891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:19.129948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:20.130838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:21.130998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:22.131661      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:23.131777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:24.132627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:25.132697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:26.132886      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:27.133312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:28.133934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:29.134048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:30.134104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:31.134175      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:32.134868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:33.135046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:34.135741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:35.135803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:36.136454      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:37.137385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:38.137930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:39.138032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:40.138512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:41.138627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:42.138668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:43.138771      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:44.138827      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:45.138929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:46.139733      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:47.140098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:48.140881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:49.141890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:50.141949      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:51.142018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:52.142211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:53.142203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:54.142236      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:55.142344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:56.143241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:57.143590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:58.144210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:18:59.144302      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:00.144708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:01.144844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:02.145702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:03.145784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:04.146353      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:05.147169      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:06.147237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:07.147498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:08.148087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:09.148193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:10.149198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:11.149921      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:12.150711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:13.151298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:14.151945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:15.152486      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:16.153282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:17.153352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:18.154028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:19.154127      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:20.154632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:21.154747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:22.155336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:23.155439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:24.156255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:25.156287      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:26.157219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:27.157940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:28.158925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:29.158986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:30.159318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:31.159429      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:32.160449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:33.160518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:34.160868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:35.160947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:36.161945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:37.162568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:38.162624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:39.162697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:40.163673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:41.163743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:42.164346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:43.164828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:44.164873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:45.165002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:46.165942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:47.166526      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:48.167571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:49.167633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:50.168572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:51.168680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:52.169417      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:53.169922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:54.170718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:55.170781      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:56.170852      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:57.171696      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:58.171753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:19:59.171842      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:00.171957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:01.172108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:02.172330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:03.172428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:04.173334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:05.173377      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:06.173886      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:07.174022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:08.174465      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:09.174543      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/07/23 13:20:09.452
  STEP: Removing cronjob @ 05/07/23 13:20:09.453
  May  7 13:20:09.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9429" for this suite. @ 05/07/23 13:20:09.457
• [300.028 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 05/07/23 13:20:09.461
  May  7 13:20:09.461: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename daemonsets @ 05/07/23 13:20:09.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:20:09.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:20:09.473
  STEP: Creating simple DaemonSet "daemon-set" @ 05/07/23 13:20:09.483
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/07/23 13:20:09.485
  May  7 13:20:09.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 13:20:09.490: INFO: Node 10.255.0.201 is running 0 daemon pod, expected 1
  E0507 13:20:10.174663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:20:10.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  7 13:20:10.494: INFO: Node 10.255.0.202 is running 0 daemon pod, expected 1
  E0507 13:20:11.174991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:20:11.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May  7 13:20:11.494: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/07/23 13:20:11.495
  May  7 13:20:11.503: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  7 13:20:11.503: INFO: Node 10.255.0.202 is running 0 daemon pod, expected 1
  E0507 13:20:12.175705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:20:12.507: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  7 13:20:12.507: INFO: Node 10.255.0.202 is running 0 daemon pod, expected 1
  E0507 13:20:13.175890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:20:13.507: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  7 13:20:13.507: INFO: Node 10.255.0.202 is running 0 daemon pod, expected 1
  E0507 13:20:14.176097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:20:14.507: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May  7 13:20:14.507: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/07/23 13:20:14.508
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2079, will wait for the garbage collector to delete the pods @ 05/07/23 13:20:14.509
  May  7 13:20:14.563: INFO: Deleting DaemonSet.extensions daemon-set took: 2.671142ms
  May  7 13:20:14.664: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.390882ms
  E0507 13:20:15.176323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:20:15.771: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 13:20:15.771: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  7 13:20:15.772: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"79477"},"items":null}

  May  7 13:20:15.773: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"79477"},"items":null}

  May  7 13:20:15.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2079" for this suite. @ 05/07/23 13:20:15.779
• [6.321 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 05/07/23 13:20:15.783
  May  7 13:20:15.783: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename runtimeclass @ 05/07/23 13:20:15.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:20:15.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:20:15.796
  STEP: getting /apis @ 05/07/23 13:20:15.798
  STEP: getting /apis/node.k8s.io @ 05/07/23 13:20:15.804
  STEP: getting /apis/node.k8s.io/v1 @ 05/07/23 13:20:15.804
  STEP: creating @ 05/07/23 13:20:15.805
  STEP: watching @ 05/07/23 13:20:15.811
  May  7 13:20:15.811: INFO: starting watch
  STEP: getting @ 05/07/23 13:20:15.815
  STEP: listing @ 05/07/23 13:20:15.816
  STEP: patching @ 05/07/23 13:20:15.817
  STEP: updating @ 05/07/23 13:20:15.819
  May  7 13:20:15.821: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 05/07/23 13:20:15.821
  STEP: deleting a collection @ 05/07/23 13:20:15.825
  May  7 13:20:15.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7901" for this suite. @ 05/07/23 13:20:15.832
• [0.051 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 05/07/23 13:20:15.835
  May  7 13:20:15.835: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename svcaccounts @ 05/07/23 13:20:15.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:20:15.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:20:15.846
  STEP: Creating ServiceAccount "e2e-sa-kzfb5"  @ 05/07/23 13:20:15.848
  May  7 13:20:15.849: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-kzfb5"  @ 05/07/23 13:20:15.849
  May  7 13:20:15.853: INFO: AutomountServiceAccountToken: true
  May  7 13:20:15.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2774" for this suite. @ 05/07/23 13:20:15.855
• [0.023 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 05/07/23 13:20:15.858
  May  7 13:20:15.858: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 13:20:15.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:20:15.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:20:15.871
  STEP: validating cluster-info @ 05/07/23 13:20:15.873
  May  7 13:20:15.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-714 cluster-info'
  May  7 13:20:15.919: INFO: stderr: ""
  May  7 13:20:15.919: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.68.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  May  7 13:20:15.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-714" for this suite. @ 05/07/23 13:20:15.921
• [0.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 05/07/23 13:20:15.924
  May  7 13:20:15.924: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename endpointslice @ 05/07/23 13:20:15.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:20:15.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:20:15.935
  May  7 13:20:15.940: INFO: Endpoints addresses: [10.255.0.201] , ports: [6443]
  May  7 13:20:15.940: INFO: EndpointSlices addresses: [10.255.0.201] , ports: [6443]
  May  7 13:20:15.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3901" for this suite. @ 05/07/23 13:20:15.942
• [0.019 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 05/07/23 13:20:15.944
  May  7 13:20:15.944: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename field-validation @ 05/07/23 13:20:15.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:20:15.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:20:15.954
  STEP: apply creating a deployment @ 05/07/23 13:20:15.956
  May  7 13:20:15.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8445" for this suite. @ 05/07/23 13:20:15.962
• [0.022 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/07/23 13:20:15.966
  May  7 13:20:15.966: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:20:15.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:20:15.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:20:15.976
  STEP: Creating configMap with name configmap-projected-all-test-volume-418df860-6e92-4cf5-b667-5790ed4acb6e @ 05/07/23 13:20:15.977
  STEP: Creating secret with name secret-projected-all-test-volume-cce307ae-b9c6-4e86-b7c7-2f1ece60130b @ 05/07/23 13:20:15.979
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/07/23 13:20:15.981
  E0507 13:20:16.176331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:17.176434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:20:17.987
  May  7 13:20:17.988: INFO: Trying to get logs from node 10.255.0.202 pod projected-volume-c6061959-645a-4c01-bc69-59feb638231a container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 13:20:17.999
  May  7 13:20:18.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6380" for this suite. @ 05/07/23 13:20:18.009
• [2.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 05/07/23 13:20:18.012
  May  7 13:20:18.012: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename init-container @ 05/07/23 13:20:18.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:20:18.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:20:18.023
  STEP: creating the pod @ 05/07/23 13:20:18.025
  May  7 13:20:18.025: INFO: PodSpec: initContainers in spec.initContainers
  E0507 13:20:18.176691      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:19.176841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:20.176880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:21.177458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:22.178444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:23.179137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:24.180085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:20:24.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5339" for this suite. @ 05/07/23 13:20:24.854
• [6.845 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 05/07/23 13:20:24.857
  May  7 13:20:24.857: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename cronjob @ 05/07/23 13:20:24.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:20:24.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:20:24.868
  STEP: Creating a ReplaceConcurrent cronjob @ 05/07/23 13:20:24.87
  STEP: Ensuring a job is scheduled @ 05/07/23 13:20:24.872
  E0507 13:20:25.180941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:26.181952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:27.182718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:28.182839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:29.183606      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:30.183673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:31.184471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:32.184888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:33.185248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:34.185319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:35.186038      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:36.186088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:37.187017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:38.187093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:39.187853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:40.187914      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:41.188400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:42.188896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:43.189755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:44.189942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:45.190783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:46.191043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:47.191941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:48.192017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:49.192507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:50.192572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:51.193367      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:52.194844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:53.194682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:54.195405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:55.196483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:56.197318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:57.197926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:58.197999      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:20:59.198796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:00.198873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/07/23 13:21:00.875
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/07/23 13:21:00.876
  STEP: Ensuring the job is replaced with a new one @ 05/07/23 13:21:00.877
  E0507 13:21:01.199757      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:02.200974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:03.201556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:04.201628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:05.202168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:06.202223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:07.203216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:08.203320      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:09.203742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:10.203807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:11.204512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:12.204705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:13.204870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:14.204933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:15.205544      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:16.205620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:17.206515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:18.207242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:19.208069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:20.208147      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:21.208936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:22.209812      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:23.210579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:24.210677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:25.210711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:26.210764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:27.211713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:28.211856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:29.212620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:30.212689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:31.212764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:32.214873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:33.214930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:34.215045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:35.215756      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:36.215807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:37.216602      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:38.216667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:39.217378      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:40.217934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:41.218350      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:42.218483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:43.218990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:44.219064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:45.219247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:46.219284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:47.219685      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:48.219788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:49.220577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:50.220672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:51.221228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:52.221372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:53.221926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:54.222005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:55.222821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:56.222893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:57.223628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:58.223737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:21:59.223791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:00.223857      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/07/23 13:22:00.878
  May  7 13:22:00.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6381" for this suite. @ 05/07/23 13:22:00.883
• [96.033 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 05/07/23 13:22:00.89
  May  7 13:22:00.890: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:22:00.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:22:00.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:22:00.907
  STEP: Creating configMap with name projected-configmap-test-volume-map-1334c7ff-29e1-418e-8280-66f5d150ea23 @ 05/07/23 13:22:00.908
  STEP: Creating a pod to test consume configMaps @ 05/07/23 13:22:00.921
  E0507 13:22:01.224725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:02.224794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:03.225567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:04.225934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:22:04.935
  May  7 13:22:04.936: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-configmaps-7b68cee9-7afb-420b-aec0-4b3b4167dc70 container agnhost-container: <nil>
  STEP: delete the pod @ 05/07/23 13:22:04.944
  May  7 13:22:04.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4334" for this suite. @ 05/07/23 13:22:04.953
• [4.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/07/23 13:22:04.957
  May  7 13:22:04.957: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 13:22:04.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:22:04.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:22:04.968
  STEP: Create set of pods @ 05/07/23 13:22:04.969
  May  7 13:22:04.975: INFO: created test-pod-1
  May  7 13:22:04.978: INFO: created test-pod-2
  May  7 13:22:04.987: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/07/23 13:22:04.987
  E0507 13:22:05.226522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:06.226680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 05/07/23 13:22:07.019
  May  7 13:22:07.022: INFO: Pod quantity 3 is different from expected quantity 0
  E0507 13:22:07.227107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:22:08.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2249" for this suite. @ 05/07/23 13:22:08.026
• [3.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/07/23 13:22:08.03
  May  7 13:22:08.030: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename cronjob @ 05/07/23 13:22:08.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:22:08.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:22:08.039
  STEP: Creating a ForbidConcurrent cronjob @ 05/07/23 13:22:08.041
  STEP: Ensuring a job is scheduled @ 05/07/23 13:22:08.046
  E0507 13:22:08.227311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:09.227380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:10.228324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:11.228387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:12.228975      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:13.229062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:14.229078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:15.230336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:16.230817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:17.231640      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:18.231981      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:19.232047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:20.232735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:21.232865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:22.233948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:23.234038      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:24.234517      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:25.234688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:26.235162      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:27.236081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:28.236142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:29.236200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:30.236261      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:31.236336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:32.236958      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:33.237947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:34.238721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:35.238906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:36.239157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:37.240141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:38.240863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:39.241943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:40.242211      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:41.242328      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:42.243045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:43.243118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:44.243839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:45.243932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:46.244795      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:47.244881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:48.245766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:49.245829      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:50.245897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:51.245983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:52.246181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:53.246246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:54.247260      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:55.247352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:56.247897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:57.248637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:58.248990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:22:59.249055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:00.249917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:01.249990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/07/23 13:23:02.052
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/07/23 13:23:02.053
  STEP: Ensuring no more jobs are scheduled @ 05/07/23 13:23:02.055
  E0507 13:23:02.250661      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:03.250730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:04.251312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:05.251418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:06.251596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:07.251825      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:08.252501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:09.252594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:10.253043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:11.253920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:12.254195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:13.254272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:14.254324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:15.254421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:16.255231      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:17.256115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:18.256667      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:19.256734      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:20.257378      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:21.257445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:22.257669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:23.257956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:24.258653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:25.258732      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:26.259421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:27.260136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:28.261046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:29.261114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:30.261738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:31.261807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:32.262415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:33.262487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:34.263351      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:35.263496      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:36.264323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:37.265143      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:38.265582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:39.265651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:40.266669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:41.267317      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:42.267536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:43.267602      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:44.268410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:45.268522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:46.269336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:47.269926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:48.270416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:49.270484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:50.270500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:51.270644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:52.271441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:53.271514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:54.272012      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:55.272141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:56.272968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:57.273190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:58.273919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:23:59.274001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:00.274481      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:01.274573      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:02.274808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:03.274922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:04.275628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:05.275730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:06.276518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:07.277274      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:08.277913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:09.278010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:10.278408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:11.278471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:12.279240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:13.279369      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:14.280192      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:15.280282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:16.280702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:17.281670      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:18.282273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:19.282337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:20.282506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:21.282573      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:22.283594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:23.283682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:24.284421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:25.284529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:26.285441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:27.285513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:28.286370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:29.287204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:30.287433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:31.287534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:32.288552      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:33.288659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:34.288818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:35.288896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:36.289928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:37.290147      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:38.290318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:39.290424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:40.290876      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:41.290948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:42.291502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:43.292059      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:44.292728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:45.292816      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:46.293524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:47.294337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:48.295090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:49.295154      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:50.295670      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:51.296728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:52.296927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:53.297013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:54.297725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:55.297938      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:56.298740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:57.298906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:58.299651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:24:59.299741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:00.300452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:01.300548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:02.301298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:03.301978      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:04.302428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:05.302548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:06.303142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:07.304185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:08.304200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:09.304290      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:10.305339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:11.305404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:12.305726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:13.305918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:14.306638      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:15.306844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:16.307660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:17.308704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:18.308874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:19.308937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:20.309918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:21.309997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:22.310083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:23.310139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:24.310786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:25.310902      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:26.311545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:27.311624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:28.312014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:29.312073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:30.312581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:31.312657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:32.312679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:33.312825      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:34.313185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:35.313279      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:36.313931      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:37.314192      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:38.314945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:39.315059      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:40.315803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:41.315902      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:42.316801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:43.316872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:44.317250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:45.317941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:46.318001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:47.318733      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:48.318913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:49.319007      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:50.319744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:51.319829      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:52.320814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:53.320868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:54.321574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:55.321942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:56.322696      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:57.323709      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:58.324338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:25:59.324431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:00.325020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:01.325101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:02.325907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:03.326467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:04.327045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:05.327190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:06.327997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:07.328725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:08.329495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:09.329925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:10.329995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:11.330091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:12.331113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:13.331214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:14.331891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:15.332034      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:16.332522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:17.333289      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:18.333730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:19.333924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:20.334752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:21.334840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:22.335822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:23.335919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:24.336425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:25.336540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:26.337471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:27.337926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:28.338004      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:29.338094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:30.338872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:31.338977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:32.340035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:33.340132      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:34.340617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:35.340737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:36.341509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:37.341917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:38.342628      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:39.342750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:40.343319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:41.343383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:42.344022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:43.344079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:44.344627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:45.344756      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:46.345492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:47.345702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:48.346633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:49.346739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:50.347506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:51.347641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:52.347847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:53.347907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:54.348608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:55.348728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:56.349569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:57.349674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:58.350428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:26:59.350517      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:00.351288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:01.351300      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:02.351384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:03.351480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:04.352264      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:05.352438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:06.353257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:07.353319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:08.353489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:09.353923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:10.354596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:11.354686      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:12.354814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:13.354922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:14.355813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:15.355913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:16.356318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:17.357155      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:18.357218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:19.357925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:20.358567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:21.358665      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:22.358723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:23.358823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:24.359634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:25.359747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:26.359816      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:27.360033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:28.360429      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:29.360515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:30.361308      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:31.361374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:32.361466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:33.361928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:34.362627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:35.362738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:36.363546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:37.364324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:38.364447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:39.364546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:40.364600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:41.364709      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:42.364815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:43.364881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:44.365443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:45.365950      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:46.366498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:47.366522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:48.367498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:49.367585      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:50.368610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:51.368674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:52.368738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:53.368830      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:54.369519      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:55.369944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:56.370737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:57.370963      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:58.371281      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:27:59.371347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:00.371863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:01.371960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/07/23 13:28:02.058
  May  7 13:28:02.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8199" for this suite. @ 05/07/23 13:28:02.073
• [354.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 05/07/23 13:28:02.079
  May  7 13:28:02.079: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 13:28:02.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:28:02.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:28:02.129
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 13:28:02.135
  E0507 13:28:02.372394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:03.372473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:04.372937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:05.373030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:28:06.166
  May  7 13:28:06.167: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-6e9c1e62-3cd6-427f-9097-5bd659ffe2f3 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 13:28:06.179
  May  7 13:28:06.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5239" for this suite. @ 05/07/23 13:28:06.188
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 05/07/23 13:28:06.192
  May  7 13:28:06.192: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sched-preemption @ 05/07/23 13:28:06.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:28:06.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:28:06.205
  May  7 13:28:06.211: INFO: Waiting up to 1m0s for all nodes to be ready
  E0507 13:28:06.373800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:07.374750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:08.375182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:09.375270      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:10.375915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:11.375986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:12.376500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:13.376571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:14.376863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:15.376901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:16.376943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:17.377774      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:18.378412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:19.378479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:20.378829      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:21.378930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:22.378988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:23.379078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:24.379331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:25.379421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:26.379469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:27.379538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:28.379825      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:29.379897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:30.380288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:31.380373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:32.381065      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:33.381936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:34.381957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:35.382080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:36.382112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:37.382174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:38.382952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:39.383045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:40.383043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:41.383110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:42.383915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:43.383982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:44.384784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:45.384896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:46.385242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:47.385435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:48.386144      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:49.386209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:50.387096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:51.387197      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:52.387396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:53.387363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:54.387869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:55.387959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:56.388604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:57.388716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:58.389500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:28:59.389917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:00.390099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:01.390189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:02.390939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:03.391010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:04.391775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:05.391880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:29:06.220: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/07/23 13:29:06.221
  May  7 13:29:06.221: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/07/23 13:29:06.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:06.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:06.232
  May  7 13:29:06.239: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  May  7 13:29:06.240: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  May  7 13:29:06.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 13:29:06.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-6778" for this suite. @ 05/07/23 13:29:06.281
  STEP: Destroying namespace "sched-preemption-5713" for this suite. @ 05/07/23 13:29:06.284
• [60.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/07/23 13:29:06.287
  May  7 13:29:06.287: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/07/23 13:29:06.288
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:06.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:06.298
  STEP: mirroring a new custom Endpoint @ 05/07/23 13:29:06.303
  May  7 13:29:06.310: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0507 13:29:06.392147      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:07.392437      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 05/07/23 13:29:08.313
  May  7 13:29:08.316: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0507 13:29:08.392768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:09.392879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 05/07/23 13:29:10.318
  May  7 13:29:10.324: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0507 13:29:10.393378      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:11.393924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:29:12.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-9869" for this suite. @ 05/07/23 13:29:12.328
• [6.044 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/07/23 13:29:12.332
  May  7 13:29:12.332: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename pods @ 05/07/23 13:29:12.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:12.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:12.342
  May  7 13:29:12.343: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: creating the pod @ 05/07/23 13:29:12.344
  STEP: submitting the pod to kubernetes @ 05/07/23 13:29:12.344
  E0507 13:29:12.394704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:13.394771      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:14.395042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:29:14.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7122" for this suite. @ 05/07/23 13:29:14.426
• [2.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/07/23 13:29:14.432
  May  7 13:29:14.432: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename sched-pred @ 05/07/23 13:29:14.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:14.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:14.493
  May  7 13:29:14.494: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May  7 13:29:14.497: INFO: Waiting for terminating namespaces to be deleted...
  May  7 13:29:14.498: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.201 before test
  May  7 13:29:14.501: INFO: calico-kube-controllers-7ccf856ff8-2v596 from kube-system started at 2023-05-07 12:02:36 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.501: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May  7 13:29:14.501: INFO: calico-node-w75tc from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.501: INFO: 	Container calico-node ready: true, restart count 0
  May  7 13:29:14.501: INFO: coredns-6557d7db9c-cgp7l from kube-system started at 2023-05-07 09:07:01 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.501: INFO: 	Container coredns ready: true, restart count 0
  May  7 13:29:14.501: INFO: dashboard-metrics-scraper-5c876f54bd-z7s9t from kube-system started at 2023-05-07 09:07:07 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.501: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May  7 13:29:14.501: INFO: metrics-server-57fbbb5957-fw6s6 from kube-system started at 2023-05-07 09:10:30 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.501: INFO: 	Container metrics-server ready: true, restart count 0
  May  7 13:29:14.501: INFO: node-local-dns-hwr8c from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.501: INFO: 	Container node-cache ready: true, restart count 0
  May  7 13:29:14.501: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-w58n4 from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 13:29:14.501: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 13:29:14.501: INFO: 	Container systemd-logs ready: true, restart count 0
  May  7 13:29:14.501: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.202 before test
  May  7 13:29:14.503: INFO: calico-node-j77rx from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.503: INFO: 	Container calico-node ready: true, restart count 0
  May  7 13:29:14.503: INFO: node-local-dns-4lrj5 from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.503: INFO: 	Container node-cache ready: true, restart count 0
  May  7 13:29:14.503: INFO: pod-exec-websocket-35908838-e551-4165-98c2-6de67f7feb68 from pods-7122 started at 2023-05-07 13:29:12 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.503: INFO: 	Container main ready: true, restart count 0
  May  7 13:29:14.503: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-7v67t from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 13:29:14.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 13:29:14.503: INFO: 	Container systemd-logs ready: true, restart count 0
  May  7 13:29:14.503: INFO: 
  Logging pods the apiserver thinks is on node 10.255.0.203 before test
  May  7 13:29:14.506: INFO: calico-node-2m8cq from kube-system started at 2023-05-07 09:06:31 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.506: INFO: 	Container calico-node ready: true, restart count 0
  May  7 13:29:14.506: INFO: kubernetes-dashboard-89b5448d6-2d4rk from kube-system started at 2023-05-07 12:02:36 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.506: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May  7 13:29:14.506: INFO: node-local-dns-9mnsl from kube-system started at 2023-05-07 09:07:03 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.506: INFO: 	Container node-cache ready: true, restart count 0
  May  7 13:29:14.506: INFO: sonobuoy from sonobuoy started at 2023-05-07 11:54:26 +0000 UTC (1 container statuses recorded)
  May  7 13:29:14.506: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May  7 13:29:14.506: INFO: sonobuoy-e2e-job-8208a86a1e4a4756 from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 13:29:14.506: INFO: 	Container e2e ready: true, restart count 0
  May  7 13:29:14.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 13:29:14.506: INFO: sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-mhtmw from sonobuoy started at 2023-05-07 11:54:27 +0000 UTC (2 container statuses recorded)
  May  7 13:29:14.506: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May  7 13:29:14.506: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node 10.255.0.201 @ 05/07/23 13:29:14.514
  STEP: verifying the node has the label node 10.255.0.202 @ 05/07/23 13:29:14.533
  STEP: verifying the node has the label node 10.255.0.203 @ 05/07/23 13:29:14.549
  May  7 13:29:14.558: INFO: Pod calico-kube-controllers-7ccf856ff8-2v596 requesting resource cpu=0m on Node 10.255.0.201
  May  7 13:29:14.558: INFO: Pod calico-node-2m8cq requesting resource cpu=250m on Node 10.255.0.203
  May  7 13:29:14.558: INFO: Pod calico-node-j77rx requesting resource cpu=250m on Node 10.255.0.202
  May  7 13:29:14.558: INFO: Pod calico-node-w75tc requesting resource cpu=250m on Node 10.255.0.201
  May  7 13:29:14.558: INFO: Pod coredns-6557d7db9c-cgp7l requesting resource cpu=100m on Node 10.255.0.201
  May  7 13:29:14.558: INFO: Pod dashboard-metrics-scraper-5c876f54bd-z7s9t requesting resource cpu=0m on Node 10.255.0.201
  May  7 13:29:14.558: INFO: Pod kubernetes-dashboard-89b5448d6-2d4rk requesting resource cpu=0m on Node 10.255.0.203
  May  7 13:29:14.558: INFO: Pod metrics-server-57fbbb5957-fw6s6 requesting resource cpu=100m on Node 10.255.0.201
  May  7 13:29:14.558: INFO: Pod node-local-dns-4lrj5 requesting resource cpu=25m on Node 10.255.0.202
  May  7 13:29:14.558: INFO: Pod node-local-dns-9mnsl requesting resource cpu=25m on Node 10.255.0.203
  May  7 13:29:14.558: INFO: Pod node-local-dns-hwr8c requesting resource cpu=25m on Node 10.255.0.201
  May  7 13:29:14.558: INFO: Pod pod-exec-websocket-35908838-e551-4165-98c2-6de67f7feb68 requesting resource cpu=0m on Node 10.255.0.202
  May  7 13:29:14.558: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.255.0.203
  May  7 13:29:14.558: INFO: Pod sonobuoy-e2e-job-8208a86a1e4a4756 requesting resource cpu=0m on Node 10.255.0.203
  May  7 13:29:14.558: INFO: Pod sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-7v67t requesting resource cpu=0m on Node 10.255.0.202
  May  7 13:29:14.559: INFO: Pod sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-mhtmw requesting resource cpu=0m on Node 10.255.0.203
  May  7 13:29:14.559: INFO: Pod sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-w58n4 requesting resource cpu=0m on Node 10.255.0.201
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/07/23 13:29:14.559
  May  7 13:29:14.559: INFO: Creating a pod which consumes cpu=1067m on Node 10.255.0.201
  May  7 13:29:14.563: INFO: Creating a pod which consumes cpu=1207m on Node 10.255.0.202
  May  7 13:29:14.568: INFO: Creating a pod which consumes cpu=1207m on Node 10.255.0.203
  E0507 13:29:15.395159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:16.395235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/07/23 13:29:16.586
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-192e060e-42c1-4737-8ca4-d24c2f12d589.175cdf8c57456387], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8296/filler-pod-192e060e-42c1-4737-8ca4-d24c2f12d589 to 10.255.0.202] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-192e060e-42c1-4737-8ca4-d24c2f12d589.175cdf8cacef5d62], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-192e060e-42c1-4737-8ca4-d24c2f12d589.175cdf8cad89d28d], Reason = [Created], Message = [Created container filler-pod-192e060e-42c1-4737-8ca4-d24c2f12d589] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-192e060e-42c1-4737-8ca4-d24c2f12d589.175cdf8cb20acdc0], Reason = [Started], Message = [Started container filler-pod-192e060e-42c1-4737-8ca4-d24c2f12d589] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-bdf95ac4-0dda-4628-99c4-1dd0de9d967a.175cdf8c5833b0be], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8296/filler-pod-bdf95ac4-0dda-4628-99c4-1dd0de9d967a to 10.255.0.203] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-bdf95ac4-0dda-4628-99c4-1dd0de9d967a.175cdf8c788122d1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-bdf95ac4-0dda-4628-99c4-1dd0de9d967a.175cdf8c7921cc39], Reason = [Created], Message = [Created container filler-pod-bdf95ac4-0dda-4628-99c4-1dd0de9d967a] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-bdf95ac4-0dda-4628-99c4-1dd0de9d967a.175cdf8c7ed55367], Reason = [Started], Message = [Started container filler-pod-bdf95ac4-0dda-4628-99c4-1dd0de9d967a] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cecaf8d6-e54f-4152-bf4d-d3624b5f5bf7.175cdf8c574b39cc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8296/filler-pod-cecaf8d6-e54f-4152-bf4d-d3624b5f5bf7 to 10.255.0.201] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cecaf8d6-e54f-4152-bf4d-d3624b5f5bf7.175cdf8c78ea748b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cecaf8d6-e54f-4152-bf4d-d3624b5f5bf7.175cdf8c79b463b6], Reason = [Created], Message = [Created container filler-pod-cecaf8d6-e54f-4152-bf4d-d3624b5f5bf7] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-cecaf8d6-e54f-4152-bf4d-d3624b5f5bf7.175cdf8c7dea763b], Reason = [Started], Message = [Started container filler-pod-cecaf8d6-e54f-4152-bf4d-d3624b5f5bf7] @ 05/07/23 13:29:16.588
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.175cdf8ccf1dad45], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 05/07/23 13:29:16.597
  E0507 13:29:17.396320      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node 10.255.0.202 @ 05/07/23 13:29:17.595
  STEP: verifying the node doesn't have the label node @ 05/07/23 13:29:17.601
  STEP: removing the label node off the node 10.255.0.203 @ 05/07/23 13:29:17.605
  STEP: verifying the node doesn't have the label node @ 05/07/23 13:29:17.613
  STEP: removing the label node off the node 10.255.0.201 @ 05/07/23 13:29:17.615
  STEP: verifying the node doesn't have the label node @ 05/07/23 13:29:17.629
  May  7 13:29:17.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8296" for this suite. @ 05/07/23 13:29:17.638
• [3.212 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/07/23 13:29:17.649
  May  7 13:29:17.649: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:29:17.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:17.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:17.663
  STEP: Creating secret with name projected-secret-test-8a8e64f6-412b-457a-abc4-a8469ef9a9d5 @ 05/07/23 13:29:17.665
  STEP: Creating a pod to test consume secrets @ 05/07/23 13:29:17.671
  E0507 13:29:18.396882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:19.397935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:29:19.679
  May  7 13:29:19.681: INFO: Trying to get logs from node 10.255.0.202 pod pod-projected-secrets-1f4a383d-d7d1-468c-b819-14ad29ef7f59 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 13:29:19.683
  May  7 13:29:19.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2059" for this suite. @ 05/07/23 13:29:19.692
• [2.045 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 05/07/23 13:29:19.695
  May  7 13:29:19.695: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 13:29:19.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:19.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:19.706
  STEP: Creating a pod to test downward api env vars @ 05/07/23 13:29:19.708
  E0507 13:29:20.398598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:21.398783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:29:21.715
  May  7 13:29:21.716: INFO: Trying to get logs from node 10.255.0.202 pod downward-api-a2f8c0d3-276f-4646-b0ca-b92662ca1f98 container dapi-container: <nil>
  STEP: delete the pod @ 05/07/23 13:29:21.719
  May  7 13:29:21.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2705" for this suite. @ 05/07/23 13:29:21.727
• [2.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 05/07/23 13:29:21.73
  May  7 13:29:21.730: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:29:21.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:21.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:21.742
  STEP: Creating the pod @ 05/07/23 13:29:21.744
  E0507 13:29:22.398918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:23.399046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:29:24.265: INFO: Successfully updated pod "annotationupdate0e4a7657-17c5-46a5-b84f-b92c8acb4f79"
  E0507 13:29:24.400074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:25.400204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:29:26.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-356" for this suite. @ 05/07/23 13:29:26.273
• [4.546 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 05/07/23 13:29:26.276
  May  7 13:29:26.276: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename gc @ 05/07/23 13:29:26.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:26.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:26.289
  STEP: create the rc @ 05/07/23 13:29:26.291
  W0507 13:29:26.293563      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0507 13:29:26.400745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:27.401845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:28.401977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:29.402109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:30.402237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/07/23 13:29:31.296
  STEP: wait for all pods to be garbage collected @ 05/07/23 13:29:31.3
  E0507 13:29:31.402285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:32.402953      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:33.403027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:34.403092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:35.403173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/07/23 13:29:36.303
  W0507 13:29:36.306286      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  7 13:29:36.306: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  7 13:29:36.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8252" for this suite. @ 05/07/23 13:29:36.307
• [10.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 05/07/23 13:29:36.313
  May  7 13:29:36.313: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 13:29:36.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:36.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:36.325
  STEP: fetching services @ 05/07/23 13:29:36.326
  May  7 13:29:36.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9964" for this suite. @ 05/07/23 13:29:36.329
• [0.019 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 05/07/23 13:29:36.333
  May  7 13:29:36.333: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 13:29:36.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:36.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:36.346
  STEP: Starting the proxy @ 05/07/23 13:29:36.347
  May  7 13:29:36.347: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-7491 proxy --unix-socket=/tmp/kubectl-proxy-unix1585192642/test'
  STEP: retrieving proxy /api/ output @ 05/07/23 13:29:36.379
  May  7 13:29:36.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7491" for this suite. @ 05/07/23 13:29:36.381
• [0.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 05/07/23 13:29:36.384
  May  7 13:29:36.384: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename var-expansion @ 05/07/23 13:29:36.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:36.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:36.393
  STEP: Creating a pod to test substitution in volume subpath @ 05/07/23 13:29:36.395
  E0507 13:29:36.403918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:37.404142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:38.404630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:39.404687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:40.404760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:29:40.406
  May  7 13:29:40.407: INFO: Trying to get logs from node 10.255.0.202 pod var-expansion-8dfdb539-a708-49d7-8fdb-c38d4bbbde6c container dapi-container: <nil>
  STEP: delete the pod @ 05/07/23 13:29:40.41
  May  7 13:29:40.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-927" for this suite. @ 05/07/23 13:29:40.419
• [4.039 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 05/07/23 13:29:40.423
  May  7 13:29:40.423: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/07/23 13:29:40.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:40.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:40.435
  STEP: set up a multi version CRD @ 05/07/23 13:29:40.436
  May  7 13:29:40.437: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  E0507 13:29:41.405540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:42.406490      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:43.406869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 05/07/23 13:29:43.763
  STEP: check the new version name is served @ 05/07/23 13:29:43.776
  E0507 13:29:44.406912      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 05/07/23 13:29:44.97
  E0507 13:29:45.407136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/07/23 13:29:45.631
  E0507 13:29:46.407176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:47.407402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:29:48.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2981" for this suite. @ 05/07/23 13:29:48.108
• [7.687 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/07/23 13:29:48.11
  May  7 13:29:48.110: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename watch @ 05/07/23 13:29:48.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:29:48.119
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:29:48.121
  STEP: creating a watch on configmaps with label A @ 05/07/23 13:29:48.122
  STEP: creating a watch on configmaps with label B @ 05/07/23 13:29:48.123
  STEP: creating a watch on configmaps with label A or B @ 05/07/23 13:29:48.124
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/07/23 13:29:48.125
  May  7 13:29:48.127: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7994  c4b029ba-9218-4567-98f1-7937041975f7 81288 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 13:29:48.127: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7994  c4b029ba-9218-4567-98f1-7937041975f7 81288 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/07/23 13:29:48.127
  May  7 13:29:48.131: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7994  c4b029ba-9218-4567-98f1-7937041975f7 81289 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 13:29:48.131: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7994  c4b029ba-9218-4567-98f1-7937041975f7 81289 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/07/23 13:29:48.131
  May  7 13:29:48.134: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7994  c4b029ba-9218-4567-98f1-7937041975f7 81290 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 13:29:48.134: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7994  c4b029ba-9218-4567-98f1-7937041975f7 81290 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/07/23 13:29:48.134
  May  7 13:29:48.136: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7994  c4b029ba-9218-4567-98f1-7937041975f7 81291 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 13:29:48.140: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7994  c4b029ba-9218-4567-98f1-7937041975f7 81291 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/07/23 13:29:48.14
  May  7 13:29:48.142: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7994  1024b939-475b-43b2-af98-09aec13c6714 81292 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 13:29:48.142: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7994  1024b939-475b-43b2-af98-09aec13c6714 81292 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0507 13:29:48.408113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:49.408199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:50.408567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:51.408630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:52.408707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:53.408955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:54.409948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:55.410016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:56.410132      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:57.410248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/07/23 13:29:58.142
  May  7 13:29:58.153: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7994  1024b939-475b-43b2-af98-09aec13c6714 81324 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May  7 13:29:58.154: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7994  1024b939-475b-43b2-af98-09aec13c6714 81324 0 2023-05-07 13:29:48 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-07 13:29:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0507 13:29:58.410277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:29:59.410369      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:00.410467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:01.410590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:02.410804      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:03.410939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:04.411001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:05.411079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:06.411147      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:07.411208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:30:08.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7994" for this suite. @ 05/07/23 13:30:08.157
• [20.049 seconds]
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 05/07/23 13:30:08.16
  May  7 13:30:08.160: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename field-validation @ 05/07/23 13:30:08.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:30:08.17
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:30:08.172
  May  7 13:30:08.174: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  E0507 13:30:08.412163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:09.412315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:10.412393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0507 13:30:10.699944      20 warnings.go:70] unknown field "alpha"
  W0507 13:30:10.699953      20 warnings.go:70] unknown field "beta"
  W0507 13:30:10.699957      20 warnings.go:70] unknown field "delta"
  W0507 13:30:10.699960      20 warnings.go:70] unknown field "epsilon"
  W0507 13:30:10.699973      20 warnings.go:70] unknown field "gamma"
  May  7 13:30:10.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2823" for this suite. @ 05/07/23 13:30:10.71
• [2.553 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 05/07/23 13:30:10.713
  May  7 13:30:10.713: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:30:10.714
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:30:10.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:30:10.725
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-7b6dd2ee-f1d2-4200-8f57-7fed3aed2a55 @ 05/07/23 13:30:10.728
  STEP: Creating the pod @ 05/07/23 13:30:10.73
  E0507 13:30:11.412412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:12.412632      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-7b6dd2ee-f1d2-4200-8f57-7fed3aed2a55 @ 05/07/23 13:30:12.743
  STEP: waiting to observe update in volume @ 05/07/23 13:30:12.745
  E0507 13:30:13.412865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:14.412989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:15.413944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:16.414120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:17.414718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:18.415680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:19.415862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:20.415983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:21.416923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:22.417949      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:23.418483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:24.418553      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:25.418797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:26.418866      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:27.419286      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:28.419353      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:29.420198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:30.420288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:31.420722      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:32.420885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:33.421927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:34.422027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:35.422626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:36.422814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:37.423450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:38.423514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:39.423944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:40.424012      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:41.424373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:42.424461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:43.424514      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:44.424582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:45.424887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:46.425954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:47.426679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:48.426749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:49.427757      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:50.427831      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:51.427886      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:52.427969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:53.428404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:54.428480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:55.428853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:56.428881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:57.429919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:58.430000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:30:59.430102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:00.430168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:01.430201      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:02.430333      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:03.430400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:04.430470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:05.431334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:06.432329      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:07.432660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:08.432768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:09.432862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:10.433936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:11.434849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:12.434992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:13.435045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:14.435105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:15.435950      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:16.436075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:17.436781      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:18.436927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:19.437675      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:20.437939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:21.438563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:22.438731      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:23.439315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:24.440181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:25.440509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:26.440616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:27.441293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:28.441919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:29.442867      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:30.442954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:31.443012      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:32.444039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:33.444851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:34.444871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:31:35.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8296" for this suite. @ 05/07/23 13:31:35.008
• [84.297 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 05/07/23 13:31:35.011
  May  7 13:31:35.011: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 13:31:35.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:31:35.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:31:35.038
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/07/23 13:31:35.039
  E0507 13:31:35.445800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:36.445866      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:37.446423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:38.446494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:31:39.052
  May  7 13:31:39.053: INFO: Trying to get logs from node 10.255.0.202 pod pod-b528cdea-b7de-4c02-990d-cb0dfaf9cfff container test-container: <nil>
  STEP: delete the pod @ 05/07/23 13:31:39.056
  May  7 13:31:39.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2118" for this suite. @ 05/07/23 13:31:39.064
• [4.055 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/07/23 13:31:39.066
  May  7 13:31:39.066: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename secrets @ 05/07/23 13:31:39.066
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:31:39.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:31:39.126
  STEP: Creating secret with name secret-test-map-d6bc5633-bc15-470b-a70f-da69269f5c84 @ 05/07/23 13:31:39.128
  STEP: Creating a pod to test consume secrets @ 05/07/23 13:31:39.13
  E0507 13:31:39.446554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:40.447435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:41.448006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:42.449031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:31:43.14
  May  7 13:31:43.141: INFO: Trying to get logs from node 10.255.0.202 pod pod-secrets-778f4c02-4aa2-4ed0-857a-eacc9de66113 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/07/23 13:31:43.144
  May  7 13:31:43.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4340" for this suite. @ 05/07/23 13:31:43.152
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/07/23 13:31:43.155
  May  7 13:31:43.155: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename events @ 05/07/23 13:31:43.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:31:43.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:31:43.166
  STEP: creating a test event @ 05/07/23 13:31:43.168
  STEP: listing events in all namespaces @ 05/07/23 13:31:43.171
  STEP: listing events in test namespace @ 05/07/23 13:31:43.172
  STEP: listing events with field selection filtering on source @ 05/07/23 13:31:43.173
  STEP: listing events with field selection filtering on reportingController @ 05/07/23 13:31:43.175
  STEP: getting the test event @ 05/07/23 13:31:43.176
  STEP: patching the test event @ 05/07/23 13:31:43.177
  STEP: getting the test event @ 05/07/23 13:31:43.18
  STEP: updating the test event @ 05/07/23 13:31:43.181
  STEP: getting the test event @ 05/07/23 13:31:43.183
  STEP: deleting the test event @ 05/07/23 13:31:43.184
  STEP: listing events in all namespaces @ 05/07/23 13:31:43.187
  STEP: listing events in test namespace @ 05/07/23 13:31:43.189
  May  7 13:31:43.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9906" for this suite. @ 05/07/23 13:31:43.191
• [0.039 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 05/07/23 13:31:43.193
  May  7 13:31:43.193: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 13:31:43.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:31:43.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:31:43.203
  May  7 13:31:43.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-8106 version'
  May  7 13:31:43.243: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  May  7 13:31:43.243: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:14:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  May  7 13:31:43.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8106" for this suite. @ 05/07/23 13:31:43.245
• [0.054 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/07/23 13:31:43.247
  May  7 13:31:43.247: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename prestop @ 05/07/23 13:31:43.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:31:43.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:31:43.257
  STEP: Creating server pod server in namespace prestop-5506 @ 05/07/23 13:31:43.259
  STEP: Waiting for pods to come up. @ 05/07/23 13:31:43.262
  E0507 13:31:43.449523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:44.449600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-5506 @ 05/07/23 13:31:45.269
  E0507 13:31:45.450254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:46.450330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 05/07/23 13:31:47.275
  E0507 13:31:47.450660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:48.450889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:49.451078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:50.451071      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:51.451154      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:31:52.282: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  May  7 13:31:52.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 05/07/23 13:31:52.283
  STEP: Destroying namespace "prestop-5506" for this suite. @ 05/07/23 13:31:52.292
• [9.048 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 05/07/23 13:31:52.296
  May  7 13:31:52.296: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename emptydir @ 05/07/23 13:31:52.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:31:52.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:31:52.307
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/07/23 13:31:52.308
  E0507 13:31:52.451193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:53.451273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:54.451991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:55.452103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:31:56.318
  May  7 13:31:56.320: INFO: Trying to get logs from node 10.255.0.202 pod pod-ba1c9c5e-ff5c-4dc5-857d-fae6c243f3d2 container test-container: <nil>
  STEP: delete the pod @ 05/07/23 13:31:56.323
  May  7 13:31:56.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-224" for this suite. @ 05/07/23 13:31:56.332
• [4.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 05/07/23 13:31:56.335
  May  7 13:31:56.335: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename field-validation @ 05/07/23 13:31:56.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:31:56.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:31:56.346
  May  7 13:31:56.351: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  E0507 13:31:56.452784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:57.453651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:31:58.453719      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0507 13:31:58.876107      20 warnings.go:70] unknown field "alpha"
  W0507 13:31:58.876116      20 warnings.go:70] unknown field "beta"
  W0507 13:31:58.876120      20 warnings.go:70] unknown field "delta"
  W0507 13:31:58.876124      20 warnings.go:70] unknown field "epsilon"
  W0507 13:31:58.876127      20 warnings.go:70] unknown field "gamma"
  May  7 13:31:58.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-609" for this suite. @ 05/07/23 13:31:58.886
• [2.555 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 05/07/23 13:31:58.891
  May  7 13:31:58.891: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 13:31:58.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:31:58.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:31:58.901
  STEP: creating a collection of services @ 05/07/23 13:31:58.903
  May  7 13:31:58.903: INFO: Creating e2e-svc-a-2l5s5
  May  7 13:31:58.908: INFO: Creating e2e-svc-b-rb96j
  May  7 13:31:58.915: INFO: Creating e2e-svc-c-2bt5k
  STEP: deleting service collection @ 05/07/23 13:31:58.923
  May  7 13:31:58.940: INFO: Collection of services has been deleted
  May  7 13:31:58.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6137" for this suite. @ 05/07/23 13:31:58.943
• [0.057 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 05/07/23 13:31:58.948
  May  7 13:31:58.948: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename projected @ 05/07/23 13:31:58.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:31:58.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:31:58.969
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 13:31:58.97
  E0507 13:31:59.453783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:00.453869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:32:00.978
  May  7 13:32:00.980: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-7f4f3251-62ca-4550-98a5-ff6410a64dc2 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 13:32:00.982
  May  7 13:32:00.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4988" for this suite. @ 05/07/23 13:32:00.997
• [2.066 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 05/07/23 13:32:01.014
  May  7 13:32:01.014: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-probe @ 05/07/23 13:32:01.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:32:01.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:32:01.031
  E0507 13:32:01.453958      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:02.454119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:03.454171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:04.455114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:05.455897      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:06.456240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:07.456988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:08.457047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:09.457769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:10.457843      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:11.458583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:12.458803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:13.459576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:14.459648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:15.460466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:16.460717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:17.461464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:18.461558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:19.461896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:20.462051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:21.462114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:22.462212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:32:23.067: INFO: Container started at 2023-05-07 13:32:01 +0000 UTC, pod became ready at 2023-05-07 13:32:21 +0000 UTC
  May  7 13:32:23.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1252" for this suite. @ 05/07/23 13:32:23.068
• [22.057 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 05/07/23 13:32:23.071
  May  7 13:32:23.071: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename resourcequota @ 05/07/23 13:32:23.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:32:23.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:32:23.082
  STEP: Creating a ResourceQuota @ 05/07/23 13:32:23.084
  STEP: Getting a ResourceQuota @ 05/07/23 13:32:23.086
  STEP: Updating a ResourceQuota @ 05/07/23 13:32:23.087
  STEP: Verifying a ResourceQuota was modified @ 05/07/23 13:32:23.093
  STEP: Deleting a ResourceQuota @ 05/07/23 13:32:23.094
  STEP: Verifying the deleted ResourceQuota @ 05/07/23 13:32:23.098
  May  7 13:32:23.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-231" for this suite. @ 05/07/23 13:32:23.101
• [0.032 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 05/07/23 13:32:23.103
  May  7 13:32:23.103: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 13:32:23.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:32:23.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:32:23.114
  STEP: starting the proxy server @ 05/07/23 13:32:23.115
  May  7 13:32:23.115: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-6489 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 05/07/23 13:32:23.151
  May  7 13:32:23.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6489" for this suite. @ 05/07/23 13:32:23.16
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 05/07/23 13:32:23.164
  May  7 13:32:23.164: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 13:32:23.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:32:23.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:32:23.174
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 13:32:23.179
  E0507 13:32:23.462615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:24.462694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:25.463350      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:26.463419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:32:27.189
  May  7 13:32:27.191: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-d1b05739-0fc6-4da4-9974-b049e82afb88 container client-container: <nil>
  STEP: delete the pod @ 05/07/23 13:32:27.193
  May  7 13:32:27.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-479" for this suite. @ 05/07/23 13:32:27.202
• [4.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 05/07/23 13:32:27.206
  May  7 13:32:27.206: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename gc @ 05/07/23 13:32:27.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:32:27.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:32:27.217
  STEP: create the rc @ 05/07/23 13:32:27.221
  W0507 13:32:27.223041      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0507 13:32:27.464380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:28.464531      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:29.466869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:30.467031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:31.467542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:32.480958      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/07/23 13:32:33.255
  STEP: wait for the rc to be deleted @ 05/07/23 13:32:33.306
  E0507 13:32:33.482982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:32:34.329: INFO: 80 pods remaining
  May  7 13:32:34.330: INFO: 80 pods has nil DeletionTimestamp
  May  7 13:32:34.330: INFO: 
  E0507 13:32:34.492925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:32:35.342: INFO: 71 pods remaining
  May  7 13:32:35.348: INFO: 71 pods has nil DeletionTimestamp
  May  7 13:32:35.349: INFO: 
  E0507 13:32:35.493252      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:32:36.388: INFO: 60 pods remaining
  May  7 13:32:36.388: INFO: 60 pods has nil DeletionTimestamp
  May  7 13:32:36.388: INFO: 
  E0507 13:32:36.493930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:32:37.389: INFO: 40 pods remaining
  May  7 13:32:37.389: INFO: 40 pods has nil DeletionTimestamp
  May  7 13:32:37.389: INFO: 
  E0507 13:32:37.494683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:32:38.394: INFO: 33 pods remaining
  May  7 13:32:38.394: INFO: 31 pods has nil DeletionTimestamp
  May  7 13:32:38.394: INFO: 
  E0507 13:32:38.494988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:32:39.328: INFO: 20 pods remaining
  May  7 13:32:39.328: INFO: 20 pods has nil DeletionTimestamp
  May  7 13:32:39.328: INFO: 
  E0507 13:32:39.495086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/07/23 13:32:40.332
  W0507 13:32:40.345532      20 metrics_grabber.go:152] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
  May  7 13:32:40.345: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May  7 13:32:40.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-988" for this suite. @ 05/07/23 13:32:40.363
• [13.164 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 05/07/23 13:32:40.37
  May  7 13:32:40.370: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename replicaset @ 05/07/23 13:32:40.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:32:40.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:32:40.416
  STEP: Create a Replicaset @ 05/07/23 13:32:40.468
  STEP: Verify that the required pods have come up. @ 05/07/23 13:32:40.479
  E0507 13:32:40.495272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:32:40.506: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0507 13:32:41.495409      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:42.495476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:43.496337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:44.496463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:45.496540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:32:45.509: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/07/23 13:32:45.509
  STEP: Getting /status @ 05/07/23 13:32:45.509
  May  7 13:32:45.512: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/07/23 13:32:45.513
  May  7 13:32:45.522: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/07/23 13:32:45.522
  May  7 13:32:45.523: INFO: Observed &ReplicaSet event: ADDED
  May  7 13:32:45.523: INFO: Observed &ReplicaSet event: MODIFIED
  May  7 13:32:45.523: INFO: Observed &ReplicaSet event: MODIFIED
  May  7 13:32:45.523: INFO: Observed &ReplicaSet event: MODIFIED
  May  7 13:32:45.523: INFO: Found replicaset test-rs in namespace replicaset-3864 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May  7 13:32:45.523: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/07/23 13:32:45.524
  May  7 13:32:45.524: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May  7 13:32:45.537: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/07/23 13:32:45.537
  May  7 13:32:45.538: INFO: Observed &ReplicaSet event: ADDED
  May  7 13:32:45.539: INFO: Observed &ReplicaSet event: MODIFIED
  May  7 13:32:45.539: INFO: Observed &ReplicaSet event: MODIFIED
  May  7 13:32:45.539: INFO: Observed &ReplicaSet event: MODIFIED
  May  7 13:32:45.539: INFO: Observed replicaset test-rs in namespace replicaset-3864 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May  7 13:32:45.539: INFO: Observed &ReplicaSet event: MODIFIED
  May  7 13:32:45.539: INFO: Found replicaset test-rs in namespace replicaset-3864 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May  7 13:32:45.539: INFO: Replicaset test-rs has a patched status
  May  7 13:32:45.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3864" for this suite. @ 05/07/23 13:32:45.546
• [5.190 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 05/07/23 13:32:45.561
  May  7 13:32:45.561: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename field-validation @ 05/07/23 13:32:45.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:32:45.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:32:45.586
  STEP: apply creating a deployment @ 05/07/23 13:32:45.589
  May  7 13:32:45.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7397" for this suite. @ 05/07/23 13:32:45.637
• [0.106 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/07/23 13:32:45.668
  May  7 13:32:45.668: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-runtime @ 05/07/23 13:32:45.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:32:45.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:32:45.733
  STEP: create the container @ 05/07/23 13:32:45.747
  W0507 13:32:45.772727      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/07/23 13:32:45.772
  E0507 13:32:46.496989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:47.497994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:48.498301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/07/23 13:32:48.796
  STEP: the container should be terminated @ 05/07/23 13:32:48.797
  STEP: the termination message should be set @ 05/07/23 13:32:48.797
  May  7 13:32:48.797: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 05/07/23 13:32:48.797
  May  7 13:32:48.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-265" for this suite. @ 05/07/23 13:32:48.807
• [3.141 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/07/23 13:32:48.809
  May  7 13:32:48.809: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename container-runtime @ 05/07/23 13:32:48.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:32:48.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:32:48.82
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/07/23 13:32:48.826
  E0507 13:32:49.498851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:50.499502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:51.499566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:52.500238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:53.500943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:54.501016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:55.501095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:56.501938      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:57.502869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:58.503732      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:32:59.504604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:00.504868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:01.505699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:02.506683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:03.507271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:04.507927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:05.508755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:06.509535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:07.509929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/07/23 13:33:07.868
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/07/23 13:33:07.869
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/07/23 13:33:07.871
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 05/07/23 13:33:07.871
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/07/23 13:33:07.881
  E0507 13:33:08.509987      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:09.510870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:10.510995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/07/23 13:33:10.889
  E0507 13:33:11.511027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/07/23 13:33:11.892
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/07/23 13:33:11.894
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 05/07/23 13:33:11.894
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/07/23 13:33:11.905
  E0507 13:33:12.512039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/07/23 13:33:12.91
  E0507 13:33:13.512999      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:14.513796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/07/23 13:33:14.914
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/07/23 13:33:14.917
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 05/07/23 13:33:14.917
  May  7 13:33:14.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1472" for this suite. @ 05/07/23 13:33:14.928
• [26.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 05/07/23 13:33:14.93
  May  7 13:33:14.930: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 13:33:14.931
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:33:14.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:33:14.943
  STEP: Creating a pod to test downward api env vars @ 05/07/23 13:33:14.944
  E0507 13:33:15.514843      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:16.514914      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:17.515671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:18.515734      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:33:18.954
  May  7 13:33:18.956: INFO: Trying to get logs from node 10.255.0.202 pod downward-api-cd843d45-1d41-4387-8f3b-1cb339302f61 container dapi-container: <nil>
  STEP: delete the pod @ 05/07/23 13:33:18.96
  May  7 13:33:18.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2154" for this suite. @ 05/07/23 13:33:18.969
• [4.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 05/07/23 13:33:18.971
  May  7 13:33:18.971: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename subpath @ 05/07/23 13:33:18.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:33:18.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:33:18.981
  STEP: Setting up data @ 05/07/23 13:33:18.989
  STEP: Creating pod pod-subpath-test-configmap-2f5x @ 05/07/23 13:33:18.993
  STEP: Creating a pod to test atomic-volume-subpath @ 05/07/23 13:33:18.993
  E0507 13:33:19.515737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:20.515813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:21.516459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:22.516571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:23.517312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:24.517388      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:25.517946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:26.518007      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:27.518724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:28.518824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:29.519684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:30.519757      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:31.520406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:32.520485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:33.520544      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:34.520623      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:35.521479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:36.521954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:37.521991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:38.522107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:39.522875      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:40.522944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:41.523596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:42.523786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:33:43.025
  May  7 13:33:43.027: INFO: Trying to get logs from node 10.255.0.202 pod pod-subpath-test-configmap-2f5x container test-container-subpath-configmap-2f5x: <nil>
  STEP: delete the pod @ 05/07/23 13:33:43.03
  STEP: Deleting pod pod-subpath-test-configmap-2f5x @ 05/07/23 13:33:43.037
  May  7 13:33:43.037: INFO: Deleting pod "pod-subpath-test-configmap-2f5x" in namespace "subpath-5298"
  May  7 13:33:43.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5298" for this suite. @ 05/07/23 13:33:43.04
• [24.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 05/07/23 13:33:43.043
  May  7 13:33:43.043: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename kubectl @ 05/07/23 13:33:43.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:33:43.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:33:43.055
  May  7 13:33:43.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-8952 create -f -'
  E0507 13:33:43.524526      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:33:43.670: INFO: stderr: ""
  May  7 13:33:43.670: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  May  7 13:33:43.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-8952 create -f -'
  May  7 13:33:43.855: INFO: stderr: ""
  May  7 13:33:43.855: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/07/23 13:33:43.855
  E0507 13:33:44.524616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:33:44.857: INFO: Selector matched 1 pods for map[app:agnhost]
  May  7 13:33:44.857: INFO: Found 0 / 1
  E0507 13:33:45.524908      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:33:45.856: INFO: Selector matched 1 pods for map[app:agnhost]
  May  7 13:33:45.856: INFO: Found 1 / 1
  May  7 13:33:45.856: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May  7 13:33:45.858: INFO: Selector matched 1 pods for map[app:agnhost]
  May  7 13:33:45.858: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May  7 13:33:45.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-8952 describe pod agnhost-primary-2c2p7'
  May  7 13:33:45.907: INFO: stderr: ""
  May  7 13:33:45.907: INFO: stdout: "Name:             agnhost-primary-2c2p7\nNamespace:        kubectl-8952\nPriority:         0\nService Account:  default\nNode:             10.255.0.202/10.255.0.202\nStart Time:       Sun, 07 May 2023 13:33:43 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               172.20.231.209\nIPs:\n  IP:           172.20.231.209\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://8399073106b75b903cf061351eb5cb87b89363b943efe30ed8ca19d566cdcb81\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 07 May 2023 13:33:44 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6gzm2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-6gzm2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8952/agnhost-primary-2c2p7 to 10.255.0.202\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  May  7 13:33:45.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-8952 describe rc agnhost-primary'
  May  7 13:33:45.955: INFO: stderr: ""
  May  7 13:33:45.955: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8952\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-2c2p7\n"
  May  7 13:33:45.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-8952 describe service agnhost-primary'
  May  7 13:33:46.001: INFO: stderr: ""
  May  7 13:33:46.001: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8952\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.68.114.143\nIPs:               10.68.114.143\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.20.231.209:6379\nSession Affinity:  None\nEvents:            <none>\n"
  May  7 13:33:46.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-8952 describe node 10.255.0.201'
  May  7 13:33:46.059: INFO: stderr: ""
  May  7 13:33:46.059: INFO: stdout: "Name:               10.255.0.201\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.255.0.201\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=master\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 07 May 2023 09:05:41 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.255.0.201\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 07 May 2023 13:33:36 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 07 May 2023 09:06:46 +0000   Sun, 07 May 2023 09:06:46 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sun, 07 May 2023 13:33:39 +0000   Sun, 07 May 2023 09:05:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 07 May 2023 13:33:39 +0000   Sun, 07 May 2023 09:05:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 07 May 2023 13:33:39 +0000   Sun, 07 May 2023 09:05:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 07 May 2023 13:33:39 +0000   Sun, 07 May 2023 09:05:41 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.255.0.201\n  Hostname:    10.255.0.201\nCapacity:\n  cpu:                2\n  ephemeral-storage:  40901312Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7600940Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  37694649077\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7293740Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 96616f9e696f480c970862c129963a34\n  System UUID:                413179ed-ecc9-4eac-b3fb-d31fa3a958ed\n  Boot ID:                    e6c6339f-d629-474b-a256-9613aa764ff5\n  Kernel Version:             5.15.0-58-generic\n  OS Image:                   Ubuntu 22.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.20\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nPodCIDR:                      172.20.0.0/24\nPodCIDRs:                     172.20.0.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-7ccf856ff8-2v596                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 calico-node-w75tc                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         4h27m\n  kube-system                 coredns-6557d7db9c-cgp7l                                   100m (5%)     0 (0%)      70Mi (0%)        300Mi (4%)     4h26m\n  kube-system                 dashboard-metrics-scraper-5c876f54bd-z7s9t                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h26m\n  kube-system                 metrics-server-57fbbb5957-fw6s6                            100m (5%)     0 (0%)      200Mi (2%)       0 (0%)         4h23m\n  kube-system                 node-local-dns-hwr8c                                       25m (1%)      0 (0%)      5Mi (0%)         0 (0%)         4h26m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-cfd78467b9264864-w58n4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         99m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                475m (23%)  0 (0%)\n  memory             275Mi (3%)  300Mi (4%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
  May  7 13:33:46.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=kubectl-8952 describe namespace kubectl-8952'
  May  7 13:33:46.108: INFO: stderr: ""
  May  7 13:33:46.108: INFO: stdout: "Name:         kubectl-8952\nLabels:       e2e-framework=kubectl\n              e2e-run=a7c2dced-f026-4955-bba1-4e1a58282135\n              kubernetes.io/metadata.name=kubectl-8952\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  May  7 13:33:46.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8952" for this suite. @ 05/07/23 13:33:46.109
• [3.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 05/07/23 13:33:46.114
  May  7 13:33:46.114: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename namespaces @ 05/07/23 13:33:46.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:33:46.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:33:46.127
  STEP: creating a Namespace @ 05/07/23 13:33:46.129
  STEP: patching the Namespace @ 05/07/23 13:33:46.135
  STEP: get the Namespace and ensuring it has the label @ 05/07/23 13:33:46.139
  May  7 13:33:46.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9866" for this suite. @ 05/07/23 13:33:46.146
  STEP: Destroying namespace "nspatchtest-74711fb8-9998-4587-a3bc-ad35b97bdf5f-8689" for this suite. @ 05/07/23 13:33:46.148
• [0.037 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 05/07/23 13:33:46.151
  May  7 13:33:46.151: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename services @ 05/07/23 13:33:46.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:33:46.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:33:46.163
  STEP: creating service in namespace services-8675 @ 05/07/23 13:33:46.164
  STEP: creating service affinity-clusterip-transition in namespace services-8675 @ 05/07/23 13:33:46.164
  STEP: creating replication controller affinity-clusterip-transition in namespace services-8675 @ 05/07/23 13:33:46.169
  I0507 13:33:46.176374      20 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-8675, replica count: 3
  E0507 13:33:46.525665      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:47.526356      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:48.526569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0507 13:33:49.226732      20 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May  7 13:33:49.230: INFO: Creating new exec pod
  E0507 13:33:49.526728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:50.527139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:51.527567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:33:52.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-8675 exec execpod-affinitygzf9w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  May  7 13:33:52.326: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  May  7 13:33:52.326: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 13:33:52.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-8675 exec execpod-affinitygzf9w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.68.107.115 80'
  May  7 13:33:52.426: INFO: stderr: "+ nc -v -t -w 2 10.68.107.115 80\n+ echo hostName\nConnection to 10.68.107.115 80 port [tcp/http] succeeded!\n"
  May  7 13:33:52.426: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May  7 13:33:52.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-8675 exec execpod-affinitygzf9w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.68.107.115:80/ ; done'
  E0507 13:33:52.528433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:33:52.575: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n"
  May  7 13:33:52.575: INFO: stdout: "\naffinity-clusterip-transition-7vqx9\naffinity-clusterip-transition-cr8zq\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-7vqx9\naffinity-clusterip-transition-cr8zq\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-7vqx9\naffinity-clusterip-transition-cr8zq\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-7vqx9\naffinity-clusterip-transition-cr8zq\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-7vqx9\naffinity-clusterip-transition-cr8zq\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-7vqx9"
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-7vqx9
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-cr8zq
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-7vqx9
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-cr8zq
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-7vqx9
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-cr8zq
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-7vqx9
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-cr8zq
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-7vqx9
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-cr8zq
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.575: INFO: Received response from host: affinity-clusterip-transition-7vqx9
  May  7 13:33:52.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-376245163 --namespace=services-8675 exec execpod-affinitygzf9w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.68.107.115:80/ ; done'
  May  7 13:33:52.727: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.68.107.115:80/\n"
  May  7 13:33:52.727: INFO: stdout: "\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk\naffinity-clusterip-transition-znslk"
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Received response from host: affinity-clusterip-transition-znslk
  May  7 13:33:52.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May  7 13:33:52.729: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8675, will wait for the garbage collector to delete the pods @ 05/07/23 13:33:52.738
  May  7 13:33:52.793: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.439555ms
  May  7 13:33:52.893: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.871767ms
  E0507 13:33:53.529058      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:54.529715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-8675" for this suite. @ 05/07/23 13:33:55.107
• [8.961 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 05/07/23 13:33:55.112
  May  7 13:33:55.112: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename downward-api @ 05/07/23 13:33:55.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:33:55.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:33:55.177
  STEP: Creating a pod to test downward API volume plugin @ 05/07/23 13:33:55.178
  E0507 13:33:55.530594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:56.530812      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:57.531723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:33:58.531795      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/07/23 13:33:59.191
  May  7 13:33:59.192: INFO: Trying to get logs from node 10.255.0.202 pod downwardapi-volume-5f379515-adf4-499f-ab9d-6fae23188bfa container client-container: <nil>
  STEP: delete the pod @ 05/07/23 13:33:59.195
  May  7 13:33:59.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6045" for this suite. @ 05/07/23 13:33:59.205
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 05/07/23 13:33:59.209
  May  7 13:33:59.209: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename daemonsets @ 05/07/23 13:33:59.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:33:59.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:33:59.22
  May  7 13:33:59.229: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/07/23 13:33:59.232
  May  7 13:33:59.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 13:33:59.239: INFO: Node 10.255.0.201 is running 0 daemon pod, expected 1
  E0507 13:33:59.532076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:00.243: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May  7 13:34:00.243: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/07/23 13:34:00.247
  STEP: Check that daemon pods images are updated. @ 05/07/23 13:34:00.252
  May  7 13:34:00.254: INFO: Wrong image for pod: daemon-set-mr8kc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  7 13:34:00.254: INFO: Wrong image for pod: daemon-set-n4lmp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  7 13:34:00.254: INFO: Wrong image for pod: daemon-set-xbp5t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0507 13:34:00.532193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:01.257: INFO: Wrong image for pod: daemon-set-mr8kc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  7 13:34:01.258: INFO: Wrong image for pod: daemon-set-xbp5t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0507 13:34:01.532889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:02.258: INFO: Wrong image for pod: daemon-set-mr8kc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May  7 13:34:02.258: INFO: Pod daemon-set-qz6wp is not available
  May  7 13:34:02.258: INFO: Wrong image for pod: daemon-set-xbp5t. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0507 13:34:02.533563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:03.258: INFO: Wrong image for pod: daemon-set-mr8kc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0507 13:34:03.534407      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:04.258: INFO: Pod daemon-set-62ctk is not available
  May  7 13:34:04.258: INFO: Wrong image for pod: daemon-set-mr8kc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0507 13:34:04.535476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:34:05.535528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:06.257: INFO: Pod daemon-set-vscsd is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/07/23 13:34:06.259
  May  7 13:34:06.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May  7 13:34:06.262: INFO: Node 10.255.0.202 is running 0 daemon pod, expected 1
  E0507 13:34:06.536149      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:07.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May  7 13:34:07.266: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/07/23 13:34:07.272
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5746, will wait for the garbage collector to delete the pods @ 05/07/23 13:34:07.272
  May  7 13:34:07.326: INFO: Deleting DaemonSet.extensions daemon-set took: 2.340083ms
  May  7 13:34:07.426: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.2619ms
  E0507 13:34:07.536679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:34:08.537250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:09.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May  7 13:34:09.229: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May  7 13:34:09.230: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"84659"},"items":null}

  May  7 13:34:09.239: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"84659"},"items":null}

  May  7 13:34:09.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5746" for this suite. @ 05/07/23 13:34:09.245
• [10.039 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/07/23 13:34:09.248
  May  7 13:34:09.248: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename deployment @ 05/07/23 13:34:09.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:34:09.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:34:09.261
  STEP: creating a Deployment @ 05/07/23 13:34:09.264
  STEP: waiting for Deployment to be created @ 05/07/23 13:34:09.265
  STEP: waiting for all Replicas to be Ready @ 05/07/23 13:34:09.266
  May  7 13:34:09.267: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  7 13:34:09.267: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  7 13:34:09.273: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  7 13:34:09.273: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  7 13:34:09.289: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  7 13:34:09.289: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  7 13:34:09.308: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May  7 13:34:09.308: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0507 13:34:09.537591      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:09.961: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May  7 13:34:09.961: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May  7 13:34:10.136: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/07/23 13:34:10.136
  W0507 13:34:10.140151      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May  7 13:34:10.141: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/07/23 13:34:10.141
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 0
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:10.142: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:10.151: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:10.151: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:10.161: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:10.161: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:10.176: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:10.176: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:10.188: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:10.188: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  E0507 13:34:10.537881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:11.150: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:11.150: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:11.159: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  STEP: listing Deployments @ 05/07/23 13:34:11.159
  May  7 13:34:11.160: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/07/23 13:34:11.161
  May  7 13:34:11.168: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/07/23 13:34:11.168
  May  7 13:34:11.175: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  7 13:34:11.177: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  7 13:34:11.196: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  7 13:34:11.229: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  7 13:34:11.239: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May  7 13:34:11.246: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0507 13:34:11.538716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:12.114: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May  7 13:34:12.162: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  May  7 13:34:12.176: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May  7 13:34:12.193: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0507 13:34:12.539528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:12.976: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/07/23 13:34:12.984
  STEP: fetching the DeploymentStatus @ 05/07/23 13:34:12.987
  May  7 13:34:12.989: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:12.989: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:12.989: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:12.989: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:12.989: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:12.989: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 1
  May  7 13:34:12.989: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:12.989: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 3
  May  7 13:34:12.989: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:12.990: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 2
  May  7 13:34:12.990: INFO: observed Deployment test-deployment in namespace deployment-6759 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/07/23 13:34:12.99
  May  7 13:34:12.994: INFO: observed event type MODIFIED
  May  7 13:34:12.994: INFO: observed event type MODIFIED
  May  7 13:34:12.994: INFO: observed event type MODIFIED
  May  7 13:34:12.994: INFO: observed event type MODIFIED
  May  7 13:34:12.994: INFO: observed event type MODIFIED
  May  7 13:34:12.994: INFO: observed event type MODIFIED
  May  7 13:34:12.994: INFO: observed event type MODIFIED
  May  7 13:34:12.994: INFO: observed event type MODIFIED
  May  7 13:34:12.994: INFO: observed event type MODIFIED
  May  7 13:34:12.995: INFO: observed event type MODIFIED
  May  7 13:34:12.995: INFO: observed event type MODIFIED
  May  7 13:34:12.995: INFO: observed event type MODIFIED
  May  7 13:34:12.998: INFO: Log out all the ReplicaSets if there is no deployment created
  May  7 13:34:13.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6759" for this suite. @ 05/07/23 13:34:13.004
• [3.760 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/07/23 13:34:13.009
  May  7 13:34:13.009: INFO: >>> kubeConfig: /tmp/kubeconfig-376245163
  STEP: Building a namespace api object, basename deployment @ 05/07/23 13:34:13.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/07/23 13:34:13.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/07/23 13:34:13.022
  May  7 13:34:13.034: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0507 13:34:13.540063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:34:14.541154      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:34:15.541976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:34:16.542174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0507 13:34:17.542247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May  7 13:34:18.036: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/07/23 13:34:18.036
  May  7 13:34:18.036: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/07/23 13:34:18.042
  May  7 13:34:18.050: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8349  cb7435ff-ab74-4671-8dbc-0fd7cb9a50a0 84950 1 2023-05-07 13:34:18 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-07 13:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e81238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  May  7 13:34:18.054: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-8349  3afa2617-567d-4a29-af48-cdca4ec89285 84952 1 2023-05-07 13:34:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment cb7435ff-ab74-4671-8dbc-0fd7cb9a50a0 0xc004b3c3c7 0xc004b3c3c8}] [] [{kube-controller-manager Update apps/v1 2023-05-07 13:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cb7435ff-ab74-4671-8dbc-0fd7cb9a50a0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b3c458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May  7 13:34:18.054: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  May  7 13:34:18.054: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8349  da7d3fc9-e373-416b-9b23-bf67c9d6848c 84951 1 2023-05-07 13:34:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment cb7435ff-ab74-4671-8dbc-0fd7cb9a50a0 0xc004b3c297 0xc004b3c298}] [] [{e2e.test Update apps/v1 2023-05-07 13:34:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-07 13:34:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-07 13:34:18 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"cb7435ff-ab74-4671-8dbc-0fd7cb9a50a0\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004b3c358 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May  7 13:34:18.057: INFO: Pod "test-cleanup-controller-wkkw9" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-wkkw9 test-cleanup-controller- deployment-8349  034a470f-cca8-4483-9e89-cfd92c33b6cc 84885 0 2023-05-07 13:34:13 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller da7d3fc9-e373-416b-9b23-bf67c9d6848c 0xc004e81557 0xc004e81558}] [] [{kube-controller-manager Update v1 2023-05-07 13:34:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da7d3fc9-e373-416b-9b23-bf67c9d6848c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-07 13:34:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.231.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k27qc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k27qc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.255.0.202,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 13:34:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 13:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 13:34:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-07 13:34:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.0.202,PodIP:172.20.231.208,StartTime:2023-05-07 13:34:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-07 13:34:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ff7e269d2a73b8a276551a90db8dbc59932ce5aba7526cb52c08e03ee9347048,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.231.208,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May  7 13:34:18.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8349" for this suite. @ 05/07/23 13:34:18.073
• [5.074 seconds]
------------------------------
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  May  7 13:34:18.084: INFO: Running AfterSuite actions on node 1
  May  7 13:34:18.084: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.030 seconds]
------------------------------

Ran 378 of 7207 Specs in 5980.961 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h39m41.254493386s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

