  I0513 18:12:01.214911      21 e2e.go:117] Starting e2e run "f9beaaef-e856-41aa-8a17-4bd32496d1c5" on Ginkgo node 1
  May 13 18:12:01.235: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1684001521 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  May 13 18:12:01.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:12:01.365: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  May 13 18:12:01.383: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  May 13 18:12:01.385: INFO: e2e test version: v1.27.1
  May 13 18:12:01.385: INFO: kube-apiserver version: v1.27.1
  May 13 18:12:01.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:12:01.388: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.025 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 05/13/23 18:12:01.585
  May 13 18:12:01.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 18:12:01.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:12:01.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:12:01.601
  STEP: Creating a ResourceQuota with terminating scope @ 05/13/23 18:12:01.604
  STEP: Ensuring ResourceQuota status is calculated @ 05/13/23 18:12:01.608
  STEP: Creating a ResourceQuota with not terminating scope @ 05/13/23 18:12:03.613
  STEP: Ensuring ResourceQuota status is calculated @ 05/13/23 18:12:03.619
  STEP: Creating a long running pod @ 05/13/23 18:12:05.623
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/13/23 18:12:05.632
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/13/23 18:12:07.638
  STEP: Deleting the pod @ 05/13/23 18:12:09.642
  STEP: Ensuring resource quota status released the pod usage @ 05/13/23 18:12:09.66
  STEP: Creating a terminating pod @ 05/13/23 18:12:11.67
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/13/23 18:12:11.682
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/13/23 18:12:13.687
  STEP: Deleting the pod @ 05/13/23 18:12:15.692
  STEP: Ensuring resource quota status released the pod usage @ 05/13/23 18:12:15.711
  May 13 18:12:17.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2259" for this suite. @ 05/13/23 18:12:17.719
• [16.141 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 05/13/23 18:12:17.726
  May 13 18:12:17.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 18:12:17.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:12:17.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:12:17.742
  STEP: starting the proxy server @ 05/13/23 18:12:17.745
  May 13 18:12:17.745: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-172 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 05/13/23 18:12:17.782
  May 13 18:12:17.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-172" for this suite. @ 05/13/23 18:12:17.8
• [0.078 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/13/23 18:12:17.804
  May 13 18:12:17.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:12:17.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:12:17.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:12:17.814
  STEP: Creating secret with name s-test-opt-del-3e7f1b37-d68e-4b69-b7f4-1548acec1a6f @ 05/13/23 18:12:17.819
  STEP: Creating secret with name s-test-opt-upd-e31745fd-1f24-48bc-b25d-2c17ee5d125c @ 05/13/23 18:12:17.824
  STEP: Creating the pod @ 05/13/23 18:12:17.829
  STEP: Deleting secret s-test-opt-del-3e7f1b37-d68e-4b69-b7f4-1548acec1a6f @ 05/13/23 18:12:27.889
  STEP: Updating secret s-test-opt-upd-e31745fd-1f24-48bc-b25d-2c17ee5d125c @ 05/13/23 18:12:27.893
  STEP: Creating secret with name s-test-opt-create-39641c67-cce2-4f37-9a19-3b34e822820d @ 05/13/23 18:12:27.9
  STEP: waiting to observe update in volume @ 05/13/23 18:12:27.907
  May 13 18:13:56.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5525" for this suite. @ 05/13/23 18:13:56.417
• [98.629 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 05/13/23 18:13:56.437
  May 13 18:13:56.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:13:56.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:13:56.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:13:56.466
  STEP: creating a Service @ 05/13/23 18:13:56.474
  STEP: watching for the Service to be added @ 05/13/23 18:13:56.489
  May 13 18:13:56.494: INFO: Found Service test-service-4kxv7 in namespace services-9449 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  May 13 18:13:56.494: INFO: Service test-service-4kxv7 created
  STEP: Getting /status @ 05/13/23 18:13:56.494
  May 13 18:13:56.499: INFO: Service test-service-4kxv7 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/13/23 18:13:56.5
  STEP: watching for the Service to be patched @ 05/13/23 18:13:56.513
  May 13 18:13:56.520: INFO: observed Service test-service-4kxv7 in namespace services-9449 with annotations: map[] & LoadBalancer: {[]}
  May 13 18:13:56.520: INFO: Found Service test-service-4kxv7 in namespace services-9449 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  May 13 18:13:56.520: INFO: Service test-service-4kxv7 has service status patched
  STEP: updating the ServiceStatus @ 05/13/23 18:13:56.52
  May 13 18:13:56.554: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/13/23 18:13:56.554
  May 13 18:13:56.556: INFO: Observed Service test-service-4kxv7 in namespace services-9449 with annotations: map[] & Conditions: {[]}
  May 13 18:13:56.557: INFO: Observed event: &Service{ObjectMeta:{test-service-4kxv7  services-9449  0c116ec2-3b86-443b-a3d5-f64a8664b9c7 2748 0 2023-05-13 18:13:56 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-13 18:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-13 18:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.32.0.66,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.32.0.66],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  May 13 18:13:56.558: INFO: Observed event: &Service{ObjectMeta:{test-service-4kxv7  services-9449  0c116ec2-3b86-443b-a3d5-f64a8664b9c7 2749 0 2023-05-13 18:13:56 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-13 18:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-13 18:13:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.32.0.66,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.32.0.66],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
  May 13 18:13:56.558: INFO: Found Service test-service-4kxv7 in namespace services-9449 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 13 18:13:56.559: INFO: Service test-service-4kxv7 has service status updated
  STEP: patching the service @ 05/13/23 18:13:56.559
  STEP: watching for the Service to be patched @ 05/13/23 18:13:56.573
  May 13 18:13:56.578: INFO: observed Service test-service-4kxv7 in namespace services-9449 with labels: map[test-service-static:true]
  May 13 18:13:56.578: INFO: observed Service test-service-4kxv7 in namespace services-9449 with labels: map[test-service-static:true]
  May 13 18:13:56.578: INFO: observed Service test-service-4kxv7 in namespace services-9449 with labels: map[test-service-static:true]
  May 13 18:13:56.579: INFO: observed Service test-service-4kxv7 in namespace services-9449 with labels: map[test-service-static:true]
  May 13 18:13:56.579: INFO: Found Service test-service-4kxv7 in namespace services-9449 with labels: map[test-service:patched test-service-static:true]
  May 13 18:13:56.579: INFO: Service test-service-4kxv7 patched
  STEP: deleting the service @ 05/13/23 18:13:56.58
  STEP: watching for the Service to be deleted @ 05/13/23 18:13:56.594
  May 13 18:13:56.598: INFO: Observed event: ADDED
  May 13 18:13:56.598: INFO: Observed event: MODIFIED
  May 13 18:13:56.599: INFO: Observed event: MODIFIED
  May 13 18:13:56.599: INFO: Observed event: MODIFIED
  May 13 18:13:56.599: INFO: Observed event: MODIFIED
  May 13 18:13:56.599: INFO: Found Service test-service-4kxv7 in namespace services-9449 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  May 13 18:13:56.599: INFO: Service test-service-4kxv7 deleted
  May 13 18:13:56.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9449" for this suite. @ 05/13/23 18:13:56.605
• [0.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/13/23 18:13:56.619
  May 13 18:13:56.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 18:13:56.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:13:56.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:13:56.648
  May 13 18:13:56.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8742" for this suite. @ 05/13/23 18:13:56.691
• [0.081 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 05/13/23 18:13:56.7
  May 13 18:13:56.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:13:56.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:13:56.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:13:56.726
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-552 @ 05/13/23 18:13:56.731
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/13/23 18:13:56.756
  STEP: creating service externalsvc in namespace services-552 @ 05/13/23 18:13:56.757
  STEP: creating replication controller externalsvc in namespace services-552 @ 05/13/23 18:13:56.77
  I0513 18:13:56.792598      21 runners.go:194] Created replication controller with name: externalsvc, namespace: services-552, replica count: 2
  I0513 18:13:59.845612      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0513 18:14:02.845905      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0513 18:14:05.847563      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0513 18:14:08.850657      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0513 18:14:11.850920      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0513 18:14:14.851545      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0513 18:14:17.852727      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0513 18:14:20.853383      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/13/23 18:14:20.858
  May 13 18:14:20.877: INFO: Creating new exec pod
  May 13 18:14:22.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-552 exec execpodtpp7w -- /bin/sh -x -c nslookup nodeport-service.services-552.svc.cluster.local'
  May 13 18:14:23.076: INFO: stderr: "+ nslookup nodeport-service.services-552.svc.cluster.local\n"
  May 13 18:14:23.076: INFO: stdout: "Server:\t\t10.32.0.10\nAddress:\t10.32.0.10#53\n\nnodeport-service.services-552.svc.cluster.local\tcanonical name = externalsvc.services-552.svc.cluster.local.\nName:\texternalsvc.services-552.svc.cluster.local\nAddress: 10.32.0.98\n\n"
  May 13 18:14:23.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-552, will wait for the garbage collector to delete the pods @ 05/13/23 18:14:23.081
  May 13 18:14:23.153: INFO: Deleting ReplicationController externalsvc took: 19.315356ms
  May 13 18:14:23.254: INFO: Terminating ReplicationController externalsvc pods took: 100.673578ms
  May 13 18:14:25.672: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-552" for this suite. @ 05/13/23 18:14:25.683
• [28.991 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 05/13/23 18:14:25.692
  May 13 18:14:25.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename init-container @ 05/13/23 18:14:25.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:14:25.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:14:25.716
  STEP: creating the pod @ 05/13/23 18:14:25.719
  May 13 18:14:25.719: INFO: PodSpec: initContainers in spec.initContainers
  May 13 18:15:12.254: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b97c3cdf-cd55-4d5d-8210-3b8c58471f4e", GenerateName:"", Namespace:"init-container-3232", SelfLink:"", UID:"5697d260-23f6-4eb4-8494-a89602ada0d9", ResourceVersion:"3220", Generation:0, CreationTimestamp:time.Date(2023, time.May, 13, 18, 14, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"719216322"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b798e7100c3b026cb4a0078c0ae5c554aa30ee858fd91afbd629d6c3dfa04d3e", "cni.projectcalico.org/podIP":"10.200.131.134/32", "cni.projectcalico.org/podIPs":"10.200.131.134/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 13, 18, 14, 25, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000494090), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 13, 18, 14, 26, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000494258), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 13, 18, 15, 12, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000494438), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jvr6p", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000b8b680), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jvr6p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jvr6p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jvr6p", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000b6c7c0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker00", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0004a9810), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000b6c840)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000b6c860)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000b6c868), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000b6c86c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000ed50c0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 13, 18, 14, 25, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 13, 18, 14, 25, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 13, 18, 14, 25, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 13, 18, 14, 25, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.58.100", PodIP:"10.200.131.134", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.200.131.134"}}, StartTime:time.Date(2023, time.May, 13, 18, 14, 25, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004a9960)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0004a99d0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://76ffcf8175c2562118fa116d4e201923222b72266b72c9a0c58a8a71fbc9e4e6", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b8b7a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000b8b780), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc000b6c8e4), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  May 13 18:15:12.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3232" for this suite. @ 05/13/23 18:15:12.258
• [46.574 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 05/13/23 18:15:12.269
  May 13 18:15:12.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename var-expansion @ 05/13/23 18:15:12.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:15:12.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:15:12.294
  May 13 18:15:14.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 18:15:14.316: INFO: Deleting pod "var-expansion-a5e74919-e5a9-4cb2-a1d3-b267b828b89d" in namespace "var-expansion-3260"
  May 13 18:15:14.324: INFO: Wait up to 5m0s for pod "var-expansion-a5e74919-e5a9-4cb2-a1d3-b267b828b89d" to be fully deleted
  STEP: Destroying namespace "var-expansion-3260" for this suite. @ 05/13/23 18:15:18.337
• [6.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/13/23 18:15:18.348
  May 13 18:15:18.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 18:15:18.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:15:18.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:15:18.363
  May 13 18:15:18.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: creating the pod @ 05/13/23 18:15:18.366
  STEP: submitting the pod to kubernetes @ 05/13/23 18:15:18.366
  May 13 18:15:20.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7041" for this suite. @ 05/13/23 18:15:20.402
• [2.062 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 05/13/23 18:15:20.412
  May 13 18:15:20.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replicaset @ 05/13/23 18:15:20.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:15:20.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:15:20.439
  STEP: Create a ReplicaSet @ 05/13/23 18:15:20.443
  STEP: Verify that the required pods have come up @ 05/13/23 18:15:20.449
  May 13 18:15:20.452: INFO: Pod name sample-pod: Found 0 pods out of 3
  May 13 18:15:25.456: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/13/23 18:15:25.457
  May 13 18:15:35.488: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/13/23 18:15:35.488
  STEP: DeleteCollection of the ReplicaSets @ 05/13/23 18:15:35.493
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/13/23 18:15:35.501
  May 13 18:15:35.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-372" for this suite. @ 05/13/23 18:15:35.513
• [15.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 05/13/23 18:15:35.528
  May 13 18:15:35.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename dns @ 05/13/23 18:15:35.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:15:35.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:15:35.574
  STEP: Creating a test headless service @ 05/13/23 18:15:35.586
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4038.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4038.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4038.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4038.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4038.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4038.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4038.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4038.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 213.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.213_udp@PTR;check="$$(dig +tcp +noall +answer +search 213.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.213_tcp@PTR;sleep 1; done
   @ 05/13/23 18:15:35.626
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4038.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4038.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4038.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4038.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4038.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4038.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4038.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4038.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4038.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 213.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.213_udp@PTR;check="$$(dig +tcp +noall +answer +search 213.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.213_tcp@PTR;sleep 1; done
   @ 05/13/23 18:15:35.626
  STEP: creating a pod to probe DNS @ 05/13/23 18:15:35.626
  STEP: submitting the pod to kubernetes @ 05/13/23 18:15:35.626
  STEP: retrieving the pod @ 05/13/23 18:15:53.708
  STEP: looking for the results for each expected name from probers @ 05/13/23 18:15:53.715
  May 13 18:15:53.721: INFO: Unable to read wheezy_udp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:53.725: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:53.728: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:53.732: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:53.747: INFO: Unable to read jessie_udp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:53.752: INFO: Unable to read jessie_tcp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:53.761: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:53.766: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:53.781: INFO: Lookups using dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67 failed for: [wheezy_udp@dns-test-service.dns-4038.svc.cluster.local wheezy_tcp@dns-test-service.dns-4038.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local jessie_udp@dns-test-service.dns-4038.svc.cluster.local jessie_tcp@dns-test-service.dns-4038.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local]

  May 13 18:15:58.787: INFO: Unable to read wheezy_udp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:58.795: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:58.803: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:58.811: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:58.851: INFO: Unable to read jessie_udp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:58.854: INFO: Unable to read jessie_tcp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:58.866: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:58.870: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:15:58.892: INFO: Lookups using dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67 failed for: [wheezy_udp@dns-test-service.dns-4038.svc.cluster.local wheezy_tcp@dns-test-service.dns-4038.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local jessie_udp@dns-test-service.dns-4038.svc.cluster.local jessie_tcp@dns-test-service.dns-4038.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local]

  May 13 18:16:03.787: INFO: Unable to read wheezy_udp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:16:03.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:16:03.794: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:16:03.796: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:16:03.808: INFO: Unable to read jessie_udp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:16:03.810: INFO: Unable to read jessie_tcp@dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:16:03.812: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:16:03.815: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local from pod dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67: the server could not find the requested resource (get pods dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67)
  May 13 18:16:03.823: INFO: Lookups using dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67 failed for: [wheezy_udp@dns-test-service.dns-4038.svc.cluster.local wheezy_tcp@dns-test-service.dns-4038.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local jessie_udp@dns-test-service.dns-4038.svc.cluster.local jessie_tcp@dns-test-service.dns-4038.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4038.svc.cluster.local]

  May 13 18:16:08.817: INFO: DNS probes using dns-4038/dns-test-2f3cf9e8-5a1c-43eb-9b0d-2e0b2bc46e67 succeeded

  May 13 18:16:08.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 18:16:08.818
  STEP: deleting the test service @ 05/13/23 18:16:08.831
  STEP: deleting the test headless service @ 05/13/23 18:16:08.923
  STEP: Destroying namespace "dns-4038" for this suite. @ 05/13/23 18:16:08.959
• [33.441 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 05/13/23 18:16:08.969
  May 13 18:16:08.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename security-context-test @ 05/13/23 18:16:08.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:16:08.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:16:08.988
  May 13 18:16:13.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9733" for this suite. @ 05/13/23 18:16:13.014
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 05/13/23 18:16:13.021
  May 13 18:16:13.022: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename dns @ 05/13/23 18:16:13.022
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:16:13.035
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:16:13.038
  STEP: Creating a test headless service @ 05/13/23 18:16:13.041
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local;sleep 1; done
   @ 05/13/23 18:16:13.046
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8329.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local;sleep 1; done
   @ 05/13/23 18:16:13.047
  STEP: creating a pod to probe DNS @ 05/13/23 18:16:13.048
  STEP: submitting the pod to kubernetes @ 05/13/23 18:16:13.049
  STEP: retrieving the pod @ 05/13/23 18:16:15.072
  STEP: looking for the results for each expected name from probers @ 05/13/23 18:16:15.075
  May 13 18:16:15.079: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:15.083: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:15.086: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:15.090: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:15.093: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:15.095: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:15.099: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:15.101: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:15.102: INFO: Lookups using dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local]

  May 13 18:16:20.105: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:20.107: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:20.110: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:20.114: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:20.120: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:20.124: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:20.127: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:20.130: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:20.130: INFO: Lookups using dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local]

  May 13 18:16:25.107: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:25.112: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:25.116: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:25.119: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:25.120: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:25.124: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:25.126: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:25.129: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:25.129: INFO: Lookups using dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local]

  May 13 18:16:30.109: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:30.113: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:30.117: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:30.121: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:30.124: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:30.127: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:30.129: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:30.132: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:30.132: INFO: Lookups using dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local]

  May 13 18:16:35.107: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:35.109: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:35.111: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:35.113: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:35.117: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:35.121: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:35.123: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:35.126: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:35.126: INFO: Lookups using dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local]

  May 13 18:16:40.105: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:40.118: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:40.124: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:40.126: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:40.138: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:40.141: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:40.144: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:40.149: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local from pod dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12: the server could not find the requested resource (get pods dns-test-a0f84188-db57-4a26-919a-5d494e822a12)
  May 13 18:16:40.149: INFO: Lookups using dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8329.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8329.svc.cluster.local jessie_udp@dns-test-service-2.dns-8329.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8329.svc.cluster.local]

  May 13 18:16:45.130: INFO: DNS probes using dns-8329/dns-test-a0f84188-db57-4a26-919a-5d494e822a12 succeeded

  May 13 18:16:45.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 18:16:45.132
  STEP: deleting the test headless service @ 05/13/23 18:16:45.151
  STEP: Destroying namespace "dns-8329" for this suite. @ 05/13/23 18:16:45.215
• [32.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/13/23 18:16:45.227
  May 13 18:16:45.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 18:16:45.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:16:45.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:16:45.247
  STEP: Creating secret with name secret-test-1dbfe7bb-38ca-43f2-8415-725764f7a172 @ 05/13/23 18:16:45.249
  STEP: Creating a pod to test consume secrets @ 05/13/23 18:16:45.252
  STEP: Saw pod success @ 05/13/23 18:16:49.271
  May 13 18:16:49.273: INFO: Trying to get logs from node worker00 pod pod-secrets-ff8b90fd-adba-47a1-9365-f707cf4939ae container secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 18:16:49.279
  May 13 18:16:49.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8455" for this suite. @ 05/13/23 18:16:49.296
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 05/13/23 18:16:49.307
  May 13 18:16:49.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 18:16:49.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:16:49.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:16:49.325
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/13/23 18:16:49.327
  STEP: Saw pod success @ 05/13/23 18:16:53.349
  May 13 18:16:53.354: INFO: Trying to get logs from node worker00 pod pod-15ed8225-eeee-44a7-b462-454d3fdb706e container test-container: <nil>
  STEP: delete the pod @ 05/13/23 18:16:53.361
  May 13 18:16:53.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3827" for this suite. @ 05/13/23 18:16:53.377
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 05/13/23 18:16:53.387
  May 13 18:16:53.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pod-network-test @ 05/13/23 18:16:53.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:16:53.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:16:53.402
  STEP: Performing setup for networking test in namespace pod-network-test-372 @ 05/13/23 18:16:53.405
  STEP: creating a selector @ 05/13/23 18:16:53.405
  STEP: Creating the service pods in kubernetes @ 05/13/23 18:16:53.405
  May 13 18:16:53.405: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/13/23 18:17:15.5
  May 13 18:17:17.530: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 13 18:17:17.530: INFO: Breadth first check of 10.200.131.144 on host 192.168.58.100...
  May 13 18:17:17.534: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.131.145:9080/dial?request=hostname&protocol=http&host=10.200.131.144&port=8083&tries=1'] Namespace:pod-network-test-372 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:17:17.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:17:17.536: INFO: ExecWithOptions: Clientset creation
  May 13 18:17:17.536: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/pod-network-test-372/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.200.131.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.200.131.144%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 13 18:17:17.607: INFO: Waiting for responses: map[]
  May 13 18:17:17.607: INFO: reached 10.200.131.144 after 0/1 tries
  May 13 18:17:17.607: INFO: Breadth first check of 10.200.5.17 on host 192.168.58.101...
  May 13 18:17:17.610: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.131.145:9080/dial?request=hostname&protocol=http&host=10.200.5.17&port=8083&tries=1'] Namespace:pod-network-test-372 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:17:17.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:17:17.610: INFO: ExecWithOptions: Clientset creation
  May 13 18:17:17.610: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/pod-network-test-372/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.200.131.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.200.5.17%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 13 18:17:17.669: INFO: Waiting for responses: map[]
  May 13 18:17:17.670: INFO: reached 10.200.5.17 after 0/1 tries
  May 13 18:17:17.670: INFO: Going to retry 0 out of 2 pods....
  May 13 18:17:17.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-372" for this suite. @ 05/13/23 18:17:17.672
• [24.290 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/13/23 18:17:17.678
  May 13 18:17:17.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:17:17.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:17:17.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:17:17.695
  STEP: Creating configMap with name configmap-projected-all-test-volume-7fb6be59-13cd-414b-8b26-37f58850a40e @ 05/13/23 18:17:17.698
  STEP: Creating secret with name secret-projected-all-test-volume-faf70034-521c-43e4-8084-dc40f20181db @ 05/13/23 18:17:17.702
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/13/23 18:17:17.705
  STEP: Saw pod success @ 05/13/23 18:17:21.728
  May 13 18:17:21.732: INFO: Trying to get logs from node worker00 pod projected-volume-9f260a1b-545e-43f2-bd5a-be3602567eb2 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 18:17:21.737
  May 13 18:17:21.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4826" for this suite. @ 05/13/23 18:17:21.755
• [4.082 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 05/13/23 18:17:21.759
  May 13 18:17:21.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:17:21.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:17:21.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:17:21.776
  STEP: Creating the pod @ 05/13/23 18:17:21.78
  May 13 18:17:24.332: INFO: Successfully updated pod "labelsupdatea4737d4d-7c8f-48b9-8504-db45432cab49"
  May 13 18:17:28.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6505" for this suite. @ 05/13/23 18:17:28.364
• [6.610 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 05/13/23 18:17:28.37
  May 13 18:17:28.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/13/23 18:17:28.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:17:28.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:17:28.389
  May 13 18:17:28.391: INFO: Waiting up to 1m0s for all nodes to be ready
  May 13 18:18:28.422: INFO: Waiting for terminating namespaces to be deleted...
  May 13 18:18:28.425: INFO: Starting informer...
  STEP: Starting pods... @ 05/13/23 18:18:28.425
  May 13 18:18:28.649: INFO: Pod1 is running on worker00. Tainting Node
  May 13 18:18:32.903: INFO: Pod2 is running on worker00. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/13/23 18:18:32.903
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/13/23 18:18:32.932
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/13/23 18:18:32.945
  May 13 18:18:39.123: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  May 13 18:18:58.486: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  May 13 18:18:58.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/13/23 18:18:58.499
  STEP: Destroying namespace "taint-multiple-pods-7696" for this suite. @ 05/13/23 18:18:58.503
• [90.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 05/13/23 18:18:58.521
  May 13 18:18:58.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/13/23 18:18:58.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:18:58.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:18:58.574
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/13/23 18:18:58.581
  May 13 18:18:58.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:18:59.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:19:04.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2382" for this suite. @ 05/13/23 18:19:04.722
• [6.217 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 05/13/23 18:19:04.738
  May 13 18:19:04.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:19:04.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:04.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:04.754
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 18:19:04.757
  STEP: Saw pod success @ 05/13/23 18:19:08.78
  May 13 18:19:08.785: INFO: Trying to get logs from node worker00 pod downwardapi-volume-9d6e4552-cd76-48e4-be0d-490a35750378 container client-container: <nil>
  STEP: delete the pod @ 05/13/23 18:19:08.792
  May 13 18:19:08.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6007" for this suite. @ 05/13/23 18:19:08.823
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 05/13/23 18:19:08.832
  May 13 18:19:08.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename daemonsets @ 05/13/23 18:19:08.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:08.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:08.863
  May 13 18:19:08.897: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/13/23 18:19:08.911
  May 13 18:19:08.919: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 18:19:08.919: INFO: Node worker00 is running 0 daemon pod, expected 1
  May 13 18:19:09.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 18:19:09.929: INFO: Node worker00 is running 0 daemon pod, expected 1
  May 13 18:19:10.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 13 18:19:10.929: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/13/23 18:19:10.939
  STEP: Check that daemon pods images are updated. @ 05/13/23 18:19:10.95
  May 13 18:19:10.953: INFO: Wrong image for pod: daemon-set-2fgjh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 13 18:19:10.953: INFO: Wrong image for pod: daemon-set-bgfpf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 13 18:19:11.960: INFO: Wrong image for pod: daemon-set-2fgjh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 13 18:19:12.964: INFO: Wrong image for pod: daemon-set-2fgjh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 13 18:19:13.957: INFO: Wrong image for pod: daemon-set-2fgjh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 13 18:19:13.957: INFO: Pod daemon-set-pcn6w is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/13/23 18:19:15.959
  May 13 18:19:15.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 13 18:19:15.963: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/13/23 18:19:15.979
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-430, will wait for the garbage collector to delete the pods @ 05/13/23 18:19:15.979
  May 13 18:19:16.040: INFO: Deleting DaemonSet.extensions daemon-set took: 7.552787ms
  May 13 18:19:16.141: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.795614ms
  May 13 18:19:19.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 18:19:19.048: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 13 18:19:19.050: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"5007"},"items":null}

  May 13 18:19:19.052: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"5007"},"items":null}

  May 13 18:19:19.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-430" for this suite. @ 05/13/23 18:19:19.065
• [10.240 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/13/23 18:19:19.074
  May 13 18:19:19.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubelet-test @ 05/13/23 18:19:19.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:19.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:19.092
  May 13 18:19:23.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9909" for this suite. @ 05/13/23 18:19:23.112
• [4.044 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 05/13/23 18:19:23.119
  May 13 18:19:23.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replicaset @ 05/13/23 18:19:23.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:23.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:23.137
  May 13 18:19:23.139: INFO: Creating ReplicaSet my-hostname-basic-1dc83bee-72b2-4297-bff9-2a3a837dde24
  May 13 18:19:23.148: INFO: Pod name my-hostname-basic-1dc83bee-72b2-4297-bff9-2a3a837dde24: Found 0 pods out of 1
  May 13 18:19:28.151: INFO: Pod name my-hostname-basic-1dc83bee-72b2-4297-bff9-2a3a837dde24: Found 1 pods out of 1
  May 13 18:19:28.151: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1dc83bee-72b2-4297-bff9-2a3a837dde24" is running
  May 13 18:19:28.154: INFO: Pod "my-hostname-basic-1dc83bee-72b2-4297-bff9-2a3a837dde24-4d5qm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-13 18:19:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-13 18:19:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-13 18:19:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-13 18:19:23 +0000 UTC Reason: Message:}])
  May 13 18:19:28.154: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/13/23 18:19:28.154
  May 13 18:19:28.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6082" for this suite. @ 05/13/23 18:19:28.176
• [5.061 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 05/13/23 18:19:28.18
  May 13 18:19:28.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename svcaccounts @ 05/13/23 18:19:28.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:28.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:28.201
  STEP: creating a ServiceAccount @ 05/13/23 18:19:28.203
  STEP: watching for the ServiceAccount to be added @ 05/13/23 18:19:28.213
  STEP: patching the ServiceAccount @ 05/13/23 18:19:28.216
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/13/23 18:19:28.223
  STEP: deleting the ServiceAccount @ 05/13/23 18:19:28.228
  May 13 18:19:28.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4120" for this suite. @ 05/13/23 18:19:28.245
• [0.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 05/13/23 18:19:28.254
  May 13 18:19:28.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/13/23 18:19:28.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:28.272
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:28.278
  STEP: creating @ 05/13/23 18:19:28.281
  STEP: getting @ 05/13/23 18:19:28.295
  STEP: listing @ 05/13/23 18:19:28.298
  STEP: deleting @ 05/13/23 18:19:28.301
  May 13 18:19:28.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5269" for this suite. @ 05/13/23 18:19:28.32
• [0.072 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 05/13/23 18:19:28.33
  May 13 18:19:28.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 18:19:28.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:28.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:28.35
  STEP: Creating a pod to test downward api env vars @ 05/13/23 18:19:28.353
  STEP: Saw pod success @ 05/13/23 18:19:32.377
  May 13 18:19:32.381: INFO: Trying to get logs from node worker00 pod downward-api-81deb6e0-e118-4b05-841d-d704d7c70da0 container dapi-container: <nil>
  STEP: delete the pod @ 05/13/23 18:19:32.387
  May 13 18:19:32.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8948" for this suite. @ 05/13/23 18:19:32.408
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 05/13/23 18:19:32.414
  May 13 18:19:32.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename statefulset @ 05/13/23 18:19:32.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:32.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:32.431
  STEP: Creating service test in namespace statefulset-6401 @ 05/13/23 18:19:32.434
  STEP: Creating statefulset ss in namespace statefulset-6401 @ 05/13/23 18:19:32.444
  May 13 18:19:32.456: INFO: Found 0 stateful pods, waiting for 1
  May 13 18:19:42.461: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/13/23 18:19:42.465
  STEP: updating a scale subresource @ 05/13/23 18:19:42.468
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/13/23 18:19:42.473
  STEP: Patch a scale subresource @ 05/13/23 18:19:42.476
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/13/23 18:19:42.506
  May 13 18:19:42.517: INFO: Deleting all statefulset in ns statefulset-6401
  May 13 18:19:42.529: INFO: Scaling statefulset ss to 0
  May 13 18:19:52.555: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 18:19:52.558: INFO: Deleting statefulset ss
  May 13 18:19:52.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6401" for this suite. @ 05/13/23 18:19:52.576
• [20.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 05/13/23 18:19:52.588
  May 13 18:19:52.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 18:19:52.59
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:52.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:52.619
  STEP: Setting up server cert @ 05/13/23 18:19:52.647
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 18:19:53.137
  STEP: Deploying the webhook pod @ 05/13/23 18:19:53.154
  STEP: Wait for the deployment to be ready @ 05/13/23 18:19:53.168
  May 13 18:19:53.175: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/13/23 18:19:55.191
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 18:19:55.209
  May 13 18:19:56.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/13/23 18:19:56.216
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/13/23 18:19:56.257
  STEP: Creating a configMap that should not be mutated @ 05/13/23 18:19:56.265
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/13/23 18:19:56.275
  STEP: Creating a configMap that should be mutated @ 05/13/23 18:19:56.284
  May 13 18:19:56.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3390" for this suite. @ 05/13/23 18:19:56.349
  STEP: Destroying namespace "webhook-markers-7518" for this suite. @ 05/13/23 18:19:56.357
• [3.782 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/13/23 18:19:56.371
  May 13 18:19:56.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-runtime @ 05/13/23 18:19:56.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:56.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:56.407
  STEP: create the container @ 05/13/23 18:19:56.411
  W0513 18:19:56.420728      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/13/23 18:19:56.423
  STEP: get the container status @ 05/13/23 18:19:59.443
  STEP: the container should be terminated @ 05/13/23 18:19:59.447
  STEP: the termination message should be set @ 05/13/23 18:19:59.447
  May 13 18:19:59.447: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/13/23 18:19:59.447
  May 13 18:19:59.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6050" for this suite. @ 05/13/23 18:19:59.493
• [3.134 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 05/13/23 18:19:59.505
  May 13 18:19:59.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 18:19:59.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:19:59.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:19:59.535
  STEP: Creating configMap with name configmap-test-volume-814d0cd2-fbae-45e8-8783-cc5f3895a9fd @ 05/13/23 18:19:59.538
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:19:59.544
  STEP: Saw pod success @ 05/13/23 18:20:03.578
  May 13 18:20:03.581: INFO: Trying to get logs from node worker00 pod pod-configmaps-a929f11c-baba-4653-9343-f5b052a716c7 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 18:20:03.585
  May 13 18:20:03.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9980" for this suite. @ 05/13/23 18:20:03.61
• [4.112 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 05/13/23 18:20:03.618
  May 13 18:20:03.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename init-container @ 05/13/23 18:20:03.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:20:03.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:20:03.639
  STEP: creating the pod @ 05/13/23 18:20:03.641
  May 13 18:20:03.641: INFO: PodSpec: initContainers in spec.initContainers
  May 13 18:20:06.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4579" for this suite. @ 05/13/23 18:20:06.7
• [3.090 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/13/23 18:20:06.708
  May 13 18:20:06.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 18:20:06.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:20:06.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:20:06.742
  STEP: creating the pod @ 05/13/23 18:20:06.745
  STEP: setting up watch @ 05/13/23 18:20:06.745
  STEP: submitting the pod to kubernetes @ 05/13/23 18:20:06.849
  STEP: verifying the pod is in kubernetes @ 05/13/23 18:20:06.853
  STEP: verifying pod creation was observed @ 05/13/23 18:20:06.856
  STEP: deleting the pod gracefully @ 05/13/23 18:20:08.872
  STEP: verifying pod deletion was observed @ 05/13/23 18:20:08.891
  May 13 18:20:11.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9402" for this suite. @ 05/13/23 18:20:11.591
• [4.891 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 05/13/23 18:20:11.599
  May 13 18:20:11.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 18:20:11.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:20:11.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:20:11.625
  STEP: Setting up server cert @ 05/13/23 18:20:11.651
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 18:20:11.89
  STEP: Deploying the webhook pod @ 05/13/23 18:20:11.894
  STEP: Wait for the deployment to be ready @ 05/13/23 18:20:11.905
  May 13 18:20:11.915: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/13/23 18:20:13.928
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 18:20:13.948
  May 13 18:20:14.950: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 13 18:20:14.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/13/23 18:20:15.471
  STEP: Creating a custom resource that should be denied by the webhook @ 05/13/23 18:20:15.495
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/13/23 18:20:17.515
  STEP: Updating the custom resource with disallowed data should be denied @ 05/13/23 18:20:17.526
  STEP: Deleting the custom resource should be denied @ 05/13/23 18:20:17.537
  STEP: Remove the offending key and value from the custom resource data @ 05/13/23 18:20:17.55
  STEP: Deleting the updated custom resource should be successful @ 05/13/23 18:20:17.569
  May 13 18:20:17.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7341" for this suite. @ 05/13/23 18:20:18.224
  STEP: Destroying namespace "webhook-markers-6415" for this suite. @ 05/13/23 18:20:18.234
• [6.650 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/13/23 18:20:18.249
  May 13 18:20:18.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 18:20:18.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:20:18.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:20:18.293
  STEP: creating the pod @ 05/13/23 18:20:18.3
  STEP: submitting the pod to kubernetes @ 05/13/23 18:20:18.3
  STEP: verifying the pod is in kubernetes @ 05/13/23 18:20:20.327
  STEP: updating the pod @ 05/13/23 18:20:20.332
  May 13 18:20:20.847: INFO: Successfully updated pod "pod-update-51b34f28-080a-40fc-9b9f-0ac4b2f824b0"
  STEP: verifying the updated pod is in kubernetes @ 05/13/23 18:20:20.853
  May 13 18:20:20.859: INFO: Pod update OK
  May 13 18:20:20.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1288" for this suite. @ 05/13/23 18:20:20.867
• [2.631 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 05/13/23 18:20:20.886
  May 13 18:20:20.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename ingressclass @ 05/13/23 18:20:20.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:20:20.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:20:20.904
  STEP: getting /apis @ 05/13/23 18:20:20.906
  STEP: getting /apis/networking.k8s.io @ 05/13/23 18:20:20.91
  STEP: getting /apis/networking.k8s.iov1 @ 05/13/23 18:20:20.912
  STEP: creating @ 05/13/23 18:20:20.914
  STEP: getting @ 05/13/23 18:20:20.928
  STEP: listing @ 05/13/23 18:20:20.931
  STEP: watching @ 05/13/23 18:20:20.934
  May 13 18:20:20.934: INFO: starting watch
  STEP: patching @ 05/13/23 18:20:20.936
  STEP: updating @ 05/13/23 18:20:20.943
  May 13 18:20:20.951: INFO: waiting for watch events with expected annotations
  May 13 18:20:20.951: INFO: saw patched and updated annotations
  STEP: deleting @ 05/13/23 18:20:20.951
  STEP: deleting a collection @ 05/13/23 18:20:20.965
  May 13 18:20:20.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-1307" for this suite. @ 05/13/23 18:20:20.979
• [0.098 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/13/23 18:20:20.985
  May 13 18:20:20.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename containers @ 05/13/23 18:20:20.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:20:21.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:20:21.006
  STEP: Creating a pod to test override all @ 05/13/23 18:20:21.008
  STEP: Saw pod success @ 05/13/23 18:20:25.031
  May 13 18:20:25.036: INFO: Trying to get logs from node worker00 pod client-containers-3edd2013-89b1-4b35-ac94-5aa274432188 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 18:20:25.048
  May 13 18:20:25.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-973" for this suite. @ 05/13/23 18:20:25.076
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 05/13/23 18:20:25.09
  May 13 18:20:25.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:20:25.091
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:20:25.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:20:25.122
  STEP: creating service nodeport-test with type=NodePort in namespace services-4184 @ 05/13/23 18:20:25.128
  STEP: creating replication controller nodeport-test in namespace services-4184 @ 05/13/23 18:20:25.147
  I0513 18:20:25.161158      21 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-4184, replica count: 2
  I0513 18:20:28.212144      21 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 13 18:20:28.212: INFO: Creating new exec pod
  May 13 18:20:31.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-4184 exec execpod5gw8t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May 13 18:20:31.478: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May 13 18:20:31.478: INFO: stdout: "nodeport-test-9hgd5"
  May 13 18:20:31.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-4184 exec execpod5gw8t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.42 80'
  May 13 18:20:31.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.42 80\nConnection to 10.32.0.42 80 port [tcp/http] succeeded!\n"
  May 13 18:20:31.707: INFO: stdout: "nodeport-test-9hgd5"
  May 13 18:20:31.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-4184 exec execpod5gw8t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.100 31117'
  May 13 18:20:31.992: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.100 31117\nConnection to 192.168.58.100 31117 port [tcp/*] succeeded!\n"
  May 13 18:20:31.992: INFO: stdout: ""
  May 13 18:20:32.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-4184 exec execpod5gw8t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.100 31117'
  May 13 18:20:33.172: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.100 31117\nConnection to 192.168.58.100 31117 port [tcp/*] succeeded!\n"
  May 13 18:20:33.172: INFO: stdout: "nodeport-test-9hgd5"
  May 13 18:20:33.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-4184 exec execpod5gw8t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.101 31117'
  May 13 18:20:33.371: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 192.168.58.101 31117\nConnection to 192.168.58.101 31117 port [tcp/*] succeeded!\n"
  May 13 18:20:33.371: INFO: stdout: ""
  May 13 18:20:34.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-4184 exec execpod5gw8t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.101 31117'
  May 13 18:20:34.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.101 31117\nConnection to 192.168.58.101 31117 port [tcp/*] succeeded!\n"
  May 13 18:20:34.567: INFO: stdout: ""
  May 13 18:20:35.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-4184 exec execpod5gw8t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.101 31117'
  May 13 18:20:35.587: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.101 31117\nConnection to 192.168.58.101 31117 port [tcp/*] succeeded!\n"
  May 13 18:20:35.587: INFO: stdout: "nodeport-test-pwjbp"
  May 13 18:20:35.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4184" for this suite. @ 05/13/23 18:20:35.595
• [10.516 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 05/13/23 18:20:35.612
  May 13 18:20:35.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename job @ 05/13/23 18:20:35.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:20:35.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:20:35.644
  STEP: Creating a job @ 05/13/23 18:20:35.653
  STEP: Ensuring active pods == parallelism @ 05/13/23 18:20:35.666
  STEP: delete a job @ 05/13/23 18:20:39.682
  STEP: deleting Job.batch foo in namespace job-7024, will wait for the garbage collector to delete the pods @ 05/13/23 18:20:39.682
  May 13 18:20:39.747: INFO: Deleting Job.batch foo took: 9.281904ms
  May 13 18:20:39.850: INFO: Terminating Job.batch foo pods took: 103.731006ms
  STEP: Ensuring job was deleted @ 05/13/23 18:21:10.951
  May 13 18:21:10.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7024" for this suite. @ 05/13/23 18:21:10.959
• [35.356 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 05/13/23 18:21:10.971
  May 13 18:21:10.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename dns @ 05/13/23 18:21:10.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:21:10.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:21:10.999
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-389.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-389.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/13/23 18:21:11.002
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-389.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-389.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/13/23 18:21:11.002
  STEP: creating a pod to probe /etc/hosts @ 05/13/23 18:21:11.002
  STEP: submitting the pod to kubernetes @ 05/13/23 18:21:11.003
  STEP: retrieving the pod @ 05/13/23 18:21:13.028
  STEP: looking for the results for each expected name from probers @ 05/13/23 18:21:13.03
  May 13 18:21:13.038: INFO: DNS probes using dns-389/dns-test-db9e0794-073a-448c-9514-51d2af95d0a5 succeeded

  May 13 18:21:13.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 18:21:13.04
  STEP: Destroying namespace "dns-389" for this suite. @ 05/13/23 18:21:13.061
• [2.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 05/13/23 18:21:13.075
  May 13 18:21:13.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:21:13.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:21:13.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:21:13.097
  STEP: Creating configMap with name projected-configmap-test-volume-1ce03002-0e7d-4b2e-849e-093255228aae @ 05/13/23 18:21:13.1
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:21:13.104
  STEP: Saw pod success @ 05/13/23 18:21:15.123
  May 13 18:21:15.125: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-b0e5b4e9-b28e-4453-b46b-ea716f8462fa container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 18:21:15.128
  May 13 18:21:15.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3546" for this suite. @ 05/13/23 18:21:15.149
• [2.080 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 05/13/23 18:21:15.155
  May 13 18:21:15.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:21:15.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:21:15.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:21:15.17
  STEP: Creating configMap with name projected-configmap-test-volume-map-9d24cf29-9ee8-4496-832d-27d3912dd13a @ 05/13/23 18:21:15.171
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:21:15.174
  STEP: Saw pod success @ 05/13/23 18:21:17.187
  May 13 18:21:17.190: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-bebf38a3-ecaf-449b-b1f3-1880e27074bf container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 18:21:17.195
  May 13 18:21:17.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2074" for this suite. @ 05/13/23 18:21:17.211
• [2.064 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 05/13/23 18:21:17.221
  May 13 18:21:17.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:21:17.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:21:17.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:21:17.235
  STEP: Creating configMap with name projected-configmap-test-volume-map-66d30f99-1e72-4ef5-a49d-b7fd294f531c @ 05/13/23 18:21:17.237
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:21:17.24
  STEP: Saw pod success @ 05/13/23 18:21:19.257
  May 13 18:21:19.258: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-2486ec4a-4a02-4a77-81cd-b3b3fed03e11 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 18:21:19.263
  May 13 18:21:19.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3466" for this suite. @ 05/13/23 18:21:19.287
• [2.073 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 05/13/23 18:21:19.294
  May 13 18:21:19.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename gc @ 05/13/23 18:21:19.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:21:19.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:21:19.312
  STEP: create the deployment @ 05/13/23 18:21:19.314
  W0513 18:21:19.318855      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/13/23 18:21:19.319
  STEP: delete the deployment @ 05/13/23 18:21:19.827
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/13/23 18:21:19.832
  STEP: Gathering metrics @ 05/13/23 18:21:20.377
  May 13 18:22:20.490: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
  May 13 18:22:20.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-117" for this suite. @ 05/13/23 18:22:20.493
• [61.204 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 05/13/23 18:22:20.499
  May 13 18:22:20.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 18:22:20.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:22:20.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:22:20.515
  STEP: Creating configMap with name configmap-test-upd-27263247-0efc-4653-9e51-3b97599e093e @ 05/13/23 18:22:20.519
  STEP: Creating the pod @ 05/13/23 18:22:20.522
  STEP: Updating configmap configmap-test-upd-27263247-0efc-4653-9e51-3b97599e093e @ 05/13/23 18:22:22.537
  STEP: waiting to observe update in volume @ 05/13/23 18:22:22.551
  May 13 18:22:24.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4156" for this suite. @ 05/13/23 18:22:24.568
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 05/13/23 18:22:24.585
  May 13 18:22:24.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename lease-test @ 05/13/23 18:22:24.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:22:24.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:22:24.603
  May 13 18:22:24.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-8486" for this suite. @ 05/13/23 18:22:24.637
• [0.060 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 05/13/23 18:22:24.647
  May 13 18:22:24.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename limitrange @ 05/13/23 18:22:24.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:22:24.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:22:24.661
  STEP: Creating LimitRange "e2e-limitrange-vf2k8" in namespace "limitrange-2036" @ 05/13/23 18:22:24.664
  STEP: Creating another limitRange in another namespace @ 05/13/23 18:22:24.669
  May 13 18:22:24.682: INFO: Namespace "e2e-limitrange-vf2k8-2359" created
  May 13 18:22:24.682: INFO: Creating LimitRange "e2e-limitrange-vf2k8" in namespace "e2e-limitrange-vf2k8-2359"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-vf2k8" @ 05/13/23 18:22:24.686
  May 13 18:22:24.688: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-vf2k8" in "limitrange-2036" namespace @ 05/13/23 18:22:24.688
  May 13 18:22:24.694: INFO: LimitRange "e2e-limitrange-vf2k8" has been patched
  STEP: Delete LimitRange "e2e-limitrange-vf2k8" by Collection with labelSelector: "e2e-limitrange-vf2k8=patched" @ 05/13/23 18:22:24.694
  STEP: Confirm that the limitRange "e2e-limitrange-vf2k8" has been deleted @ 05/13/23 18:22:24.698
  May 13 18:22:24.698: INFO: Requesting list of LimitRange to confirm quantity
  May 13 18:22:24.700: INFO: Found 0 LimitRange with label "e2e-limitrange-vf2k8=patched"
  May 13 18:22:24.700: INFO: LimitRange "e2e-limitrange-vf2k8" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-vf2k8" @ 05/13/23 18:22:24.7
  May 13 18:22:24.702: INFO: Found 1 limitRange
  May 13 18:22:24.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-2036" for this suite. @ 05/13/23 18:22:24.706
  STEP: Destroying namespace "e2e-limitrange-vf2k8-2359" for this suite. @ 05/13/23 18:22:24.71
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 05/13/23 18:22:24.719
  May 13 18:22:24.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename svcaccounts @ 05/13/23 18:22:24.72
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:22:24.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:22:24.73
  May 13 18:22:24.741: INFO: created pod
  STEP: Saw pod success @ 05/13/23 18:22:28.757
  May 13 18:22:58.759: INFO: polling logs
  May 13 18:22:58.774: INFO: Pod logs: 
  I0513 18:22:25.438741       1 log.go:198] OK: Got token
  I0513 18:22:25.438830       1 log.go:198] validating with in-cluster discovery
  I0513 18:22:25.439023       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0513 18:22:25.439051       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2801:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684002745, NotBefore:1684002145, IssuedAt:1684002145, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2801", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"31e3a108-adae-4ed6-853b-7cec5073b506"}}}
  I0513 18:22:25.448911       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0513 18:22:25.452904       1 log.go:198] OK: Validated signature on JWT
  I0513 18:22:25.453264       1 log.go:198] OK: Got valid claims from token!
  I0513 18:22:25.453283       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2801:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684002745, NotBefore:1684002145, IssuedAt:1684002145, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2801", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"31e3a108-adae-4ed6-853b-7cec5073b506"}}}

  May 13 18:22:58.774: INFO: completed pod
  May 13 18:22:58.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2801" for this suite. @ 05/13/23 18:22:58.794
• [34.082 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 05/13/23 18:22:58.802
  May 13 18:22:58.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename job @ 05/13/23 18:22:58.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:22:58.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:22:58.82
  STEP: Creating a suspended job @ 05/13/23 18:22:58.826
  STEP: Patching the Job @ 05/13/23 18:22:58.836
  STEP: Watching for Job to be patched @ 05/13/23 18:22:58.858
  May 13 18:22:58.861: INFO: Event ADDED observed for Job e2e-86ws8 in namespace job-4597 with labels: map[e2e-job-label:e2e-86ws8] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 13 18:22:58.861: INFO: Event MODIFIED observed for Job e2e-86ws8 in namespace job-4597 with labels: map[e2e-job-label:e2e-86ws8] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 13 18:22:58.861: INFO: Event MODIFIED found for Job e2e-86ws8 in namespace job-4597 with labels: map[e2e-86ws8:patched e2e-job-label:e2e-86ws8] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 05/13/23 18:22:58.861
  STEP: Watching for Job to be updated @ 05/13/23 18:22:58.881
  May 13 18:22:58.885: INFO: Event MODIFIED found for Job e2e-86ws8 in namespace job-4597 with labels: map[e2e-86ws8:patched e2e-job-label:e2e-86ws8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 13 18:22:58.885: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/13/23 18:22:58.885
  May 13 18:22:58.896: INFO: Job: e2e-86ws8 as labels: map[e2e-86ws8:patched e2e-job-label:e2e-86ws8]
  STEP: Waiting for job to complete @ 05/13/23 18:22:58.896
  STEP: Delete a job collection with a labelselector @ 05/13/23 18:23:06.901
  STEP: Watching for Job to be deleted @ 05/13/23 18:23:06.919
  May 13 18:23:06.924: INFO: Event MODIFIED observed for Job e2e-86ws8 in namespace job-4597 with labels: map[e2e-86ws8:patched e2e-job-label:e2e-86ws8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 13 18:23:06.924: INFO: Event MODIFIED observed for Job e2e-86ws8 in namespace job-4597 with labels: map[e2e-86ws8:patched e2e-job-label:e2e-86ws8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 13 18:23:06.924: INFO: Event MODIFIED observed for Job e2e-86ws8 in namespace job-4597 with labels: map[e2e-86ws8:patched e2e-job-label:e2e-86ws8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 13 18:23:06.924: INFO: Event MODIFIED observed for Job e2e-86ws8 in namespace job-4597 with labels: map[e2e-86ws8:patched e2e-job-label:e2e-86ws8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 13 18:23:06.924: INFO: Event MODIFIED observed for Job e2e-86ws8 in namespace job-4597 with labels: map[e2e-86ws8:patched e2e-job-label:e2e-86ws8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 13 18:23:06.924: INFO: Event DELETED found for Job e2e-86ws8 in namespace job-4597 with labels: map[e2e-86ws8:patched e2e-job-label:e2e-86ws8] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 05/13/23 18:23:06.924
  May 13 18:23:06.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4597" for this suite. @ 05/13/23 18:23:06.93
• [8.139 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/13/23 18:23:06.947
  May 13 18:23:06.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 18:23:06.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:06.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:06.973
  STEP: creating pod @ 05/13/23 18:23:06.977
  May 13 18:23:09.000: INFO: Pod pod-hostip-45292896-1152-4d49-9197-202e112503e6 has hostIP: 192.168.58.100
  May 13 18:23:09.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7359" for this suite. @ 05/13/23 18:23:09.003
• [2.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 05/13/23 18:23:09.011
  May 13 18:23:09.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 18:23:09.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:09.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:09.033
  STEP: Creating a pod to test downward api env vars @ 05/13/23 18:23:09.035
  STEP: Saw pod success @ 05/13/23 18:23:13.049
  May 13 18:23:13.051: INFO: Trying to get logs from node worker00 pod downward-api-826b087a-044e-46f6-9288-62726992601d container dapi-container: <nil>
  STEP: delete the pod @ 05/13/23 18:23:13.057
  May 13 18:23:13.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6509" for this suite. @ 05/13/23 18:23:13.076
• [4.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 05/13/23 18:23:13.084
  May 13 18:23:13.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename dns @ 05/13/23 18:23:13.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:13.096
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:13.099
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/13/23 18:23:13.102
  May 13 18:23:13.111: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9668  a06b0d75-263f-474a-898c-51022aad210d 7102 0 2023-05-13 18:23:13 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-13 18:23:13 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8b4cx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8b4cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/13/23 18:23:15.125
  May 13 18:23:15.125: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9668 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:23:15.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:23:15.126: INFO: ExecWithOptions: Clientset creation
  May 13 18:23:15.126: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/dns-9668/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/13/23 18:23:15.211
  May 13 18:23:15.211: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9668 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:23:15.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:23:15.212: INFO: ExecWithOptions: Clientset creation
  May 13 18:23:15.212: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/dns-9668/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 13 18:23:15.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 18:23:15.289: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-9668" for this suite. @ 05/13/23 18:23:15.3
• [2.225 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 05/13/23 18:23:15.31
  May 13 18:23:15.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replication-controller @ 05/13/23 18:23:15.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:15.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:15.329
  STEP: Given a ReplicationController is created @ 05/13/23 18:23:15.332
  STEP: When the matched label of one of its pods change @ 05/13/23 18:23:15.338
  May 13 18:23:15.340: INFO: Pod name pod-release: Found 0 pods out of 1
  May 13 18:23:20.342: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/13/23 18:23:20.351
  May 13 18:23:20.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8914" for this suite. @ 05/13/23 18:23:20.372
• [5.070 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/13/23 18:23:20.383
  May 13 18:23:20.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:23:20.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:20.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:20.406
  STEP: Creating projection with secret that has name projected-secret-test-0cc83c34-4147-4f10-b81e-977f20a3577e @ 05/13/23 18:23:20.411
  STEP: Creating a pod to test consume secrets @ 05/13/23 18:23:20.415
  STEP: Saw pod success @ 05/13/23 18:23:24.436
  May 13 18:23:24.439: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-6b4beb96-894b-40fb-839c-037367088a8d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 18:23:24.449
  May 13 18:23:24.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4052" for this suite. @ 05/13/23 18:23:24.474
• [4.097 seconds]
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/13/23 18:23:24.482
  May 13 18:23:24.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename events @ 05/13/23 18:23:24.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:24.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:24.5
  STEP: creating a test event @ 05/13/23 18:23:24.503
  STEP: listing events in all namespaces @ 05/13/23 18:23:24.511
  STEP: listing events in test namespace @ 05/13/23 18:23:24.517
  STEP: listing events with field selection filtering on source @ 05/13/23 18:23:24.518
  STEP: listing events with field selection filtering on reportingController @ 05/13/23 18:23:24.52
  STEP: getting the test event @ 05/13/23 18:23:24.522
  STEP: patching the test event @ 05/13/23 18:23:24.523
  STEP: getting the test event @ 05/13/23 18:23:24.529
  STEP: updating the test event @ 05/13/23 18:23:24.534
  STEP: getting the test event @ 05/13/23 18:23:24.539
  STEP: deleting the test event @ 05/13/23 18:23:24.541
  STEP: listing events in all namespaces @ 05/13/23 18:23:24.544
  STEP: listing events in test namespace @ 05/13/23 18:23:24.552
  May 13 18:23:24.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1517" for this suite. @ 05/13/23 18:23:24.557
• [0.079 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 05/13/23 18:23:24.562
  May 13 18:23:24.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:23:24.564
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:24.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:24.582
  STEP: creating service endpoint-test2 in namespace services-87 @ 05/13/23 18:23:24.584
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-87 to expose endpoints map[] @ 05/13/23 18:23:24.593
  May 13 18:23:24.601: INFO: successfully validated that service endpoint-test2 in namespace services-87 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-87 @ 05/13/23 18:23:24.601
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-87 to expose endpoints map[pod1:[80]] @ 05/13/23 18:23:26.614
  May 13 18:23:26.623: INFO: successfully validated that service endpoint-test2 in namespace services-87 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/13/23 18:23:26.623
  May 13 18:23:26.623: INFO: Creating new exec pod
  May 13 18:23:29.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-87 exec execpod5nhsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 13 18:23:29.797: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 13 18:23:29.797: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:23:29.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-87 exec execpod5nhsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.189 80'
  May 13 18:23:29.918: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.189 80\nConnection to 10.32.0.189 80 port [tcp/http] succeeded!\n"
  May 13 18:23:29.918: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-87 @ 05/13/23 18:23:29.918
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-87 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/13/23 18:23:31.939
  May 13 18:23:31.951: INFO: successfully validated that service endpoint-test2 in namespace services-87 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/13/23 18:23:31.951
  May 13 18:23:32.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-87 exec execpod5nhsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 13 18:23:33.072: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 13 18:23:33.072: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:23:33.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-87 exec execpod5nhsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.189 80'
  May 13 18:23:33.182: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 10.32.0.189 80\nConnection to 10.32.0.189 80 port [tcp/http] succeeded!\n"
  May 13 18:23:33.182: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-87 @ 05/13/23 18:23:33.182
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-87 to expose endpoints map[pod2:[80]] @ 05/13/23 18:23:33.209
  May 13 18:23:34.236: INFO: successfully validated that service endpoint-test2 in namespace services-87 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/13/23 18:23:34.236
  May 13 18:23:35.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-87 exec execpod5nhsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 13 18:23:35.376: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 13 18:23:35.377: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:23:35.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-87 exec execpod5nhsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.189 80'
  May 13 18:23:35.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.189 80\nConnection to 10.32.0.189 80 port [tcp/http] succeeded!\n"
  May 13 18:23:35.496: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-87 @ 05/13/23 18:23:35.496
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-87 to expose endpoints map[] @ 05/13/23 18:23:35.514
  May 13 18:23:35.526: INFO: successfully validated that service endpoint-test2 in namespace services-87 exposes endpoints map[]
  May 13 18:23:35.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-87" for this suite. @ 05/13/23 18:23:35.552
• [10.997 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 05/13/23 18:23:35.559
  May 13 18:23:35.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replication-controller @ 05/13/23 18:23:35.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:35.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:35.581
  STEP: creating a ReplicationController @ 05/13/23 18:23:35.587
  STEP: waiting for RC to be added @ 05/13/23 18:23:35.593
  STEP: waiting for available Replicas @ 05/13/23 18:23:35.593
  STEP: patching ReplicationController @ 05/13/23 18:23:36.601
  STEP: waiting for RC to be modified @ 05/13/23 18:23:36.606
  STEP: patching ReplicationController status @ 05/13/23 18:23:36.607
  STEP: waiting for RC to be modified @ 05/13/23 18:23:36.614
  STEP: waiting for available Replicas @ 05/13/23 18:23:36.617
  STEP: fetching ReplicationController status @ 05/13/23 18:23:36.618
  STEP: patching ReplicationController scale @ 05/13/23 18:23:36.622
  STEP: waiting for RC to be modified @ 05/13/23 18:23:36.63
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/13/23 18:23:36.633
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/13/23 18:23:38.339
  STEP: updating ReplicationController status @ 05/13/23 18:23:38.341
  STEP: waiting for RC to be modified @ 05/13/23 18:23:38.347
  STEP: listing all ReplicationControllers @ 05/13/23 18:23:38.347
  STEP: checking that ReplicationController has expected values @ 05/13/23 18:23:38.351
  STEP: deleting ReplicationControllers by collection @ 05/13/23 18:23:38.351
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/13/23 18:23:38.354
  May 13 18:23:38.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0513 18:23:38.392856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-5433" for this suite. @ 05/13/23 18:23:38.395
• [2.839 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 05/13/23 18:23:38.398
  May 13 18:23:38.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 18:23:38.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:38.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:38.415
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/13/23 18:23:38.42
  E0513 18:23:39.394243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:40.394736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:41.396019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:42.396467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:23:42.444
  May 13 18:23:42.448: INFO: Trying to get logs from node worker00 pod pod-d2fde2c8-8376-4e21-a04a-07e5e6154344 container test-container: <nil>
  STEP: delete the pod @ 05/13/23 18:23:42.453
  May 13 18:23:42.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9410" for this suite. @ 05/13/23 18:23:42.47
• [4.076 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 05/13/23 18:23:42.475
  May 13 18:23:42.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 18:23:42.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:42.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:42.489
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/13/23 18:23:42.491
  E0513 18:23:43.396696      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:44.398526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:45.400102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:46.402273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:23:46.517
  May 13 18:23:46.519: INFO: Trying to get logs from node worker00 pod pod-36e598b8-5bc8-4ef1-9642-c29e2a02d9a7 container test-container: <nil>
  STEP: delete the pod @ 05/13/23 18:23:46.522
  May 13 18:23:46.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5261" for this suite. @ 05/13/23 18:23:46.534
• [4.064 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 05/13/23 18:23:46.539
  May 13 18:23:46.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 18:23:46.54
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:46.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:46.556
  STEP: Creating configMap with name configmap-test-volume-b24babf7-2163-46a2-9ee9-39bf9178bf52 @ 05/13/23 18:23:46.558
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:23:46.561
  E0513 18:23:47.404159      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:48.404453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:49.405327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:50.407704      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:23:50.581
  May 13 18:23:50.586: INFO: Trying to get logs from node worker00 pod pod-configmaps-3223c8f1-5017-4bc2-8fe0-2c9063757423 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 18:23:50.595
  May 13 18:23:50.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5282" for this suite. @ 05/13/23 18:23:50.617
• [4.084 seconds]
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 05/13/23 18:23:50.623
  May 13 18:23:50.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename svcaccounts @ 05/13/23 18:23:50.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:50.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:50.641
  May 13 18:23:50.657: INFO: created pod pod-service-account-defaultsa
  May 13 18:23:50.657: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  May 13 18:23:50.662: INFO: created pod pod-service-account-mountsa
  May 13 18:23:50.662: INFO: pod pod-service-account-mountsa service account token volume mount: true
  May 13 18:23:50.680: INFO: created pod pod-service-account-nomountsa
  May 13 18:23:50.680: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  May 13 18:23:50.686: INFO: created pod pod-service-account-defaultsa-mountspec
  May 13 18:23:50.686: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  May 13 18:23:50.695: INFO: created pod pod-service-account-mountsa-mountspec
  May 13 18:23:50.695: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  May 13 18:23:50.721: INFO: created pod pod-service-account-nomountsa-mountspec
  May 13 18:23:50.721: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  May 13 18:23:50.727: INFO: created pod pod-service-account-defaultsa-nomountspec
  May 13 18:23:50.727: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  May 13 18:23:50.739: INFO: created pod pod-service-account-mountsa-nomountspec
  May 13 18:23:50.739: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  May 13 18:23:50.753: INFO: created pod pod-service-account-nomountsa-nomountspec
  May 13 18:23:50.753: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  May 13 18:23:50.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5732" for this suite. @ 05/13/23 18:23:50.766
• [0.153 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 05/13/23 18:23:50.776
  May 13 18:23:50.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename namespaces @ 05/13/23 18:23:50.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:50.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:50.817
  STEP: Read namespace status @ 05/13/23 18:23:50.82
  May 13 18:23:50.829: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/13/23 18:23:50.829
  May 13 18:23:50.844: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/13/23 18:23:50.845
  May 13 18:23:50.863: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  May 13 18:23:50.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3802" for this suite. @ 05/13/23 18:23:50.866
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 05/13/23 18:23:50.874
  May 13 18:23:50.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 18:23:50.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:23:50.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:23:50.893
  STEP: Setting up server cert @ 05/13/23 18:23:50.93
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 18:23:51.366
  STEP: Deploying the webhook pod @ 05/13/23 18:23:51.373
  STEP: Wait for the deployment to be ready @ 05/13/23 18:23:51.396
  May 13 18:23:51.406: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 18:23:51.407801      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:52.408429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:53.408512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 18:23:53.418
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 18:23:53.439
  E0513 18:23:54.409799      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:23:54.440: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/13/23 18:23:54.445
  STEP: create a pod that should be denied by the webhook @ 05/13/23 18:23:54.479
  STEP: create a pod that causes the webhook to hang @ 05/13/23 18:23:54.491
  E0513 18:23:55.410743      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:56.411570      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:57.412983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:58.417356      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:23:59.418338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:00.418662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:01.419771      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:02.420322      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:03.420749      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:04.422126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 05/13/23 18:24:04.504
  STEP: create a configmap that should be admitted by the webhook @ 05/13/23 18:24:04.527
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/13/23 18:24:04.538
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/13/23 18:24:04.545
  STEP: create a namespace that bypass the webhook @ 05/13/23 18:24:04.552
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/13/23 18:24:04.563
  May 13 18:24:04.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6951" for this suite. @ 05/13/23 18:24:04.622
  STEP: Destroying namespace "webhook-markers-6527" for this suite. @ 05/13/23 18:24:04.63
  STEP: Destroying namespace "exempted-namespace-9825" for this suite. @ 05/13/23 18:24:04.639
• [13.772 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 05/13/23 18:24:04.654
  May 13 18:24:04.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename statefulset @ 05/13/23 18:24:04.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:24:04.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:24:04.674
  STEP: Creating service test in namespace statefulset-227 @ 05/13/23 18:24:04.679
  STEP: Creating stateful set ss in namespace statefulset-227 @ 05/13/23 18:24:04.685
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-227 @ 05/13/23 18:24:04.695
  May 13 18:24:04.697: INFO: Found 0 stateful pods, waiting for 1
  E0513 18:24:05.422971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:06.423562      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:07.423709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:08.426567      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:09.427153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:10.427717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:11.430664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:12.431614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:13.432077      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:14.433673      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:14.706: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/13/23 18:24:14.706
  May 13 18:24:14.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-227 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 13 18:24:14.829: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 13 18:24:14.829: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 13 18:24:14.829: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 13 18:24:14.831: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0513 18:24:15.437171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:16.437563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:17.438593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:18.439364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:19.440305      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:20.440985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:21.443235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:22.443127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:23.444148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:24.451838      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:24.834: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 13 18:24:24.834: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 18:24:24.844: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
  May 13 18:24:24.844: INFO: ss-0  worker00  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:04 +0000 UTC  }]
  May 13 18:24:24.844: INFO: 
  May 13 18:24:24.844: INFO: StatefulSet ss has not reached scale 3, at 1
  E0513 18:24:25.453778      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:25.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996753914s
  E0513 18:24:26.455037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:26.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986308083s
  E0513 18:24:27.455480      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:27.864: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9801733s
  E0513 18:24:28.455775      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:28.876: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976295366s
  E0513 18:24:29.457742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:29.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.965782973s
  E0513 18:24:30.458088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:30.887: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961217695s
  E0513 18:24:31.459536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:31.894: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.952971318s
  E0513 18:24:32.460697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:32.900: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.947398161s
  E0513 18:24:33.461327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:33.908: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.212828ms
  E0513 18:24:34.464140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-227 @ 05/13/23 18:24:34.909
  May 13 18:24:34.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-227 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 13 18:24:35.035: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 13 18:24:35.035: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 13 18:24:35.035: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 13 18:24:35.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-227 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 13 18:24:35.222: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 13 18:24:35.222: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 13 18:24:35.222: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 13 18:24:35.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-227 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 13 18:24:35.339: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 13 18:24:35.339: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 13 18:24:35.339: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 13 18:24:35.342: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 13 18:24:35.342: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 13 18:24:35.342: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/13/23 18:24:35.342
  May 13 18:24:35.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-227 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 13 18:24:35.452: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 13 18:24:35.453: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 13 18:24:35.453: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 13 18:24:35.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-227 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0513 18:24:35.464608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:35.605: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 13 18:24:35.606: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 13 18:24:35.606: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 13 18:24:35.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-227 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 13 18:24:35.748: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 13 18:24:35.748: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 13 18:24:35.748: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 13 18:24:35.748: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 18:24:35.751: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0513 18:24:36.465788      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:37.466958      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:38.467502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:39.467574      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:40.467837      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:41.468287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:42.468647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:43.469717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:44.469607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:24:45.471588      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:45.760: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 13 18:24:45.760: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 13 18:24:45.760: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 13 18:24:45.769: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
  May 13 18:24:45.769: INFO: ss-0  worker00  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:04 +0000 UTC  }]
  May 13 18:24:45.769: INFO: ss-1  worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:24 +0000 UTC  }]
  May 13 18:24:45.769: INFO: ss-2  worker00  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:24 +0000 UTC  }]
  May 13 18:24:45.769: INFO: 
  May 13 18:24:45.769: INFO: StatefulSet ss has not reached scale 0, at 3
  E0513 18:24:46.474067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:46.772: INFO: POD   NODE      PHASE      GRACE  CONDITIONS
  May 13 18:24:46.772: INFO: ss-0  worker00  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:04 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:36 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:36 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:04 +0000 UTC  }]
  May 13 18:24:46.772: INFO: ss-2  worker00  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:24 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:36 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:36 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:24:24 +0000 UTC  }]
  May 13 18:24:46.772: INFO: 
  May 13 18:24:46.772: INFO: StatefulSet ss has not reached scale 0, at 2
  E0513 18:24:47.474798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:47.777: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.994948282s
  E0513 18:24:48.475775      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:48.782: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.989676134s
  E0513 18:24:49.476232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:49.786: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.985163355s
  E0513 18:24:50.477015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:50.792: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.979607935s
  E0513 18:24:51.478313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:51.798: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.972717808s
  E0513 18:24:52.485200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:52.803: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.968365117s
  E0513 18:24:53.487804      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:53.808: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.962753918s
  E0513 18:24:54.488816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:54.814: INFO: Verifying statefulset ss doesn't scale past 0 for another 958.338573ms
  E0513 18:24:55.489216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-227 @ 05/13/23 18:24:55.817
  May 13 18:24:55.824: INFO: Scaling statefulset ss to 0
  May 13 18:24:55.837: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 18:24:55.840: INFO: Deleting all statefulset in ns statefulset-227
  May 13 18:24:55.842: INFO: Scaling statefulset ss to 0
  May 13 18:24:55.848: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 18:24:55.850: INFO: Deleting statefulset ss
  May 13 18:24:55.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-227" for this suite. @ 05/13/23 18:24:55.86
• [51.212 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/13/23 18:24:55.872
  May 13 18:24:55.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename svc-latency @ 05/13/23 18:24:55.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:24:55.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:24:55.888
  May 13 18:24:55.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-5741 @ 05/13/23 18:24:55.892
  I0513 18:24:55.898421      21 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5741, replica count: 1
  E0513 18:24:56.490717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:24:56.950114      21 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0513 18:24:57.491962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:24:57.951358      21 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 13 18:24:58.058: INFO: Created: latency-svc-rvjxz
  May 13 18:24:58.068: INFO: Got endpoints: latency-svc-rvjxz [16.303049ms]
  May 13 18:24:58.087: INFO: Created: latency-svc-xqcjc
  May 13 18:24:58.095: INFO: Created: latency-svc-9jd5f
  May 13 18:24:58.103: INFO: Got endpoints: latency-svc-9jd5f [32.738564ms]
  May 13 18:24:58.103: INFO: Got endpoints: latency-svc-xqcjc [34.1016ms]
  May 13 18:24:58.108: INFO: Created: latency-svc-svvl9
  May 13 18:24:58.114: INFO: Got endpoints: latency-svc-svvl9 [43.146345ms]
  May 13 18:24:58.121: INFO: Created: latency-svc-85b8v
  May 13 18:24:58.128: INFO: Got endpoints: latency-svc-85b8v [57.264841ms]
  May 13 18:24:58.138: INFO: Created: latency-svc-nr4rp
  May 13 18:24:58.141: INFO: Created: latency-svc-vftns
  May 13 18:24:58.142: INFO: Got endpoints: latency-svc-vftns [71.108265ms]
  May 13 18:24:58.146: INFO: Got endpoints: latency-svc-nr4rp [75.509963ms]
  May 13 18:24:58.148: INFO: Created: latency-svc-7zl6g
  May 13 18:24:58.152: INFO: Created: latency-svc-pgbn7
  May 13 18:24:58.155: INFO: Got endpoints: latency-svc-7zl6g [84.037369ms]
  May 13 18:24:58.159: INFO: Got endpoints: latency-svc-pgbn7 [88.775505ms]
  May 13 18:24:58.164: INFO: Created: latency-svc-9pmq8
  May 13 18:24:58.170: INFO: Got endpoints: latency-svc-9pmq8 [99.778265ms]
  May 13 18:24:58.197: INFO: Created: latency-svc-7h4wr
  May 13 18:24:58.197: INFO: Created: latency-svc-krcwk
  May 13 18:24:58.201: INFO: Created: latency-svc-qvdtb
  May 13 18:24:58.204: INFO: Created: latency-svc-lcpbd
  May 13 18:24:58.212: INFO: Created: latency-svc-cbrhf
  May 13 18:24:58.212: INFO: Created: latency-svc-6sv9j
  May 13 18:24:58.212: INFO: Created: latency-svc-4g6cv
  May 13 18:24:58.212: INFO: Created: latency-svc-zbd2w
  May 13 18:24:58.212: INFO: Created: latency-svc-h7w7d
  May 13 18:24:58.212: INFO: Created: latency-svc-c6cwd
  May 13 18:24:58.212: INFO: Created: latency-svc-x4gdz
  May 13 18:24:58.212: INFO: Created: latency-svc-hmtng
  May 13 18:24:58.212: INFO: Created: latency-svc-5drng
  May 13 18:24:58.212: INFO: Created: latency-svc-ktvws
  May 13 18:24:58.212: INFO: Created: latency-svc-tgbmf
  May 13 18:24:58.221: INFO: Got endpoints: latency-svc-krcwk [150.675502ms]
  May 13 18:24:58.221: INFO: Got endpoints: latency-svc-7h4wr [50.834417ms]
  May 13 18:24:58.221: INFO: Got endpoints: latency-svc-5drng [151.226991ms]
  May 13 18:24:58.221: INFO: Got endpoints: latency-svc-6sv9j [150.973871ms]
  May 13 18:24:58.224: INFO: Got endpoints: latency-svc-4g6cv [154.321938ms]
  May 13 18:24:58.234: INFO: Got endpoints: latency-svc-zbd2w [105.5809ms]
  May 13 18:24:58.239: INFO: Got endpoints: latency-svc-qvdtb [84.447615ms]
  May 13 18:24:58.241: INFO: Got endpoints: latency-svc-h7w7d [136.110693ms]
  May 13 18:24:58.242: INFO: Got endpoints: latency-svc-c6cwd [139.038035ms]
  May 13 18:24:58.243: INFO: Got endpoints: latency-svc-cbrhf [96.890011ms]
  May 13 18:24:58.255: INFO: Got endpoints: latency-svc-ktvws [96.114409ms]
  May 13 18:24:58.256: INFO: Got endpoints: latency-svc-tgbmf [185.577413ms]
  May 13 18:24:58.256: INFO: Got endpoints: latency-svc-x4gdz [185.529081ms]
  May 13 18:24:58.261: INFO: Got endpoints: latency-svc-hmtng [118.305632ms]
  May 13 18:24:58.261: INFO: Got endpoints: latency-svc-lcpbd [147.19461ms]
  May 13 18:24:58.267: INFO: Created: latency-svc-d5784
  May 13 18:24:58.274: INFO: Got endpoints: latency-svc-d5784 [53.440132ms]
  May 13 18:24:58.280: INFO: Created: latency-svc-mh8hw
  May 13 18:24:58.287: INFO: Created: latency-svc-jtl6b
  May 13 18:24:58.287: INFO: Got endpoints: latency-svc-jtl6b [66.055406ms]
  May 13 18:24:58.295: INFO: Got endpoints: latency-svc-mh8hw [73.404108ms]
  May 13 18:24:58.297: INFO: Created: latency-svc-v82x7
  May 13 18:24:58.297: INFO: Got endpoints: latency-svc-v82x7 [72.917695ms]
  May 13 18:24:58.303: INFO: Created: latency-svc-fbxnx
  May 13 18:24:58.303: INFO: Created: latency-svc-wt5nq
  May 13 18:24:58.308: INFO: Got endpoints: latency-svc-wt5nq [86.953635ms]
  May 13 18:24:58.308: INFO: Got endpoints: latency-svc-fbxnx [73.946893ms]
  May 13 18:24:58.311: INFO: Created: latency-svc-74xvm
  May 13 18:24:58.318: INFO: Got endpoints: latency-svc-74xvm [75.133212ms]
  May 13 18:24:58.321: INFO: Created: latency-svc-g22rp
  May 13 18:24:58.327: INFO: Got endpoints: latency-svc-g22rp [83.679498ms]
  May 13 18:24:58.331: INFO: Created: latency-svc-r8jm5
  May 13 18:24:58.343: INFO: Created: latency-svc-tqfv2
  May 13 18:24:58.344: INFO: Got endpoints: latency-svc-r8jm5 [101.135217ms]
  May 13 18:24:58.348: INFO: Got endpoints: latency-svc-tqfv2 [102.395256ms]
  May 13 18:24:58.351: INFO: Created: latency-svc-g7445
  May 13 18:24:58.355: INFO: Created: latency-svc-687rm
  May 13 18:24:58.368: INFO: Got endpoints: latency-svc-g7445 [111.814316ms]
  May 13 18:24:58.371: INFO: Created: latency-svc-j62n6
  May 13 18:24:58.377: INFO: Created: latency-svc-56tww
  May 13 18:24:58.385: INFO: Created: latency-svc-c7j8w
  May 13 18:24:58.389: INFO: Created: latency-svc-9nb2d
  May 13 18:24:58.394: INFO: Created: latency-svc-cjrw8
  May 13 18:24:58.396: INFO: Created: latency-svc-cvfrx
  May 13 18:24:58.408: INFO: Created: latency-svc-vfc4s
  May 13 18:24:58.415: INFO: Got endpoints: latency-svc-687rm [158.9374ms]
  May 13 18:24:58.416: INFO: Created: latency-svc-t2jvr
  May 13 18:24:58.423: INFO: Created: latency-svc-8n7m2
  May 13 18:24:58.431: INFO: Created: latency-svc-scmkz
  May 13 18:24:58.435: INFO: Created: latency-svc-dkhc6
  May 13 18:24:58.440: INFO: Created: latency-svc-s4gzp
  May 13 18:24:58.443: INFO: Created: latency-svc-n8klv
  May 13 18:24:58.448: INFO: Created: latency-svc-7cn99
  May 13 18:24:58.451: INFO: Created: latency-svc-fvhp6
  May 13 18:24:58.465: INFO: Got endpoints: latency-svc-j62n6 [209.929745ms]
  May 13 18:24:58.474: INFO: Created: latency-svc-vm5m6
  E0513 18:24:58.494455      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:58.516: INFO: Got endpoints: latency-svc-56tww [255.478383ms]
  May 13 18:24:58.525: INFO: Created: latency-svc-vsnqc
  May 13 18:24:58.566: INFO: Got endpoints: latency-svc-c7j8w [305.264691ms]
  May 13 18:24:58.574: INFO: Created: latency-svc-zbz8t
  May 13 18:24:58.622: INFO: Got endpoints: latency-svc-9nb2d [344.943195ms]
  May 13 18:24:58.633: INFO: Created: latency-svc-tbfhv
  May 13 18:24:58.668: INFO: Got endpoints: latency-svc-cjrw8 [380.508863ms]
  May 13 18:24:58.679: INFO: Created: latency-svc-hfvdp
  May 13 18:24:58.720: INFO: Got endpoints: latency-svc-cvfrx [425.2037ms]
  May 13 18:24:58.729: INFO: Created: latency-svc-fpjdh
  May 13 18:24:58.768: INFO: Got endpoints: latency-svc-vfc4s [470.561312ms]
  May 13 18:24:58.778: INFO: Created: latency-svc-4cc49
  May 13 18:24:58.815: INFO: Got endpoints: latency-svc-t2jvr [506.931025ms]
  May 13 18:24:58.821: INFO: Created: latency-svc-2jd94
  May 13 18:24:58.883: INFO: Got endpoints: latency-svc-8n7m2 [574.322978ms]
  May 13 18:24:58.892: INFO: Created: latency-svc-j6jfj
  May 13 18:24:58.919: INFO: Got endpoints: latency-svc-scmkz [601.523383ms]
  May 13 18:24:58.930: INFO: Created: latency-svc-rhdz8
  May 13 18:24:58.970: INFO: Got endpoints: latency-svc-dkhc6 [642.972205ms]
  May 13 18:24:58.979: INFO: Created: latency-svc-c4mqf
  May 13 18:24:59.016: INFO: Got endpoints: latency-svc-s4gzp [672.753392ms]
  May 13 18:24:59.027: INFO: Created: latency-svc-pdf2c
  May 13 18:24:59.068: INFO: Got endpoints: latency-svc-n8klv [719.87736ms]
  May 13 18:24:59.083: INFO: Created: latency-svc-z56kr
  May 13 18:24:59.118: INFO: Got endpoints: latency-svc-7cn99 [750.24868ms]
  May 13 18:24:59.133: INFO: Created: latency-svc-ln4fx
  May 13 18:24:59.168: INFO: Got endpoints: latency-svc-fvhp6 [753.26188ms]
  May 13 18:24:59.181: INFO: Created: latency-svc-29vvg
  May 13 18:24:59.217: INFO: Got endpoints: latency-svc-vm5m6 [752.077645ms]
  May 13 18:24:59.229: INFO: Created: latency-svc-fzld7
  May 13 18:24:59.269: INFO: Got endpoints: latency-svc-vsnqc [752.25953ms]
  May 13 18:24:59.279: INFO: Created: latency-svc-6djps
  May 13 18:24:59.321: INFO: Got endpoints: latency-svc-zbz8t [754.596108ms]
  May 13 18:24:59.331: INFO: Created: latency-svc-p69v6
  May 13 18:24:59.374: INFO: Got endpoints: latency-svc-tbfhv [752.291187ms]
  May 13 18:24:59.385: INFO: Created: latency-svc-pvt42
  May 13 18:24:59.418: INFO: Got endpoints: latency-svc-hfvdp [749.686848ms]
  May 13 18:24:59.428: INFO: Created: latency-svc-8csdz
  May 13 18:24:59.465: INFO: Got endpoints: latency-svc-fpjdh [744.725968ms]
  May 13 18:24:59.479: INFO: Created: latency-svc-snc8m
  E0513 18:24:59.494435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:24:59.557: INFO: Got endpoints: latency-svc-4cc49 [789.439869ms]
  May 13 18:24:59.570: INFO: Got endpoints: latency-svc-2jd94 [754.181058ms]
  May 13 18:24:59.571: INFO: Created: latency-svc-5p5jl
  May 13 18:24:59.588: INFO: Created: latency-svc-dr9xn
  May 13 18:24:59.614: INFO: Got endpoints: latency-svc-j6jfj [729.676021ms]
  May 13 18:24:59.627: INFO: Created: latency-svc-xjqt7
  May 13 18:24:59.668: INFO: Got endpoints: latency-svc-rhdz8 [748.414894ms]
  May 13 18:24:59.677: INFO: Created: latency-svc-nbdqw
  May 13 18:24:59.714: INFO: Got endpoints: latency-svc-c4mqf [744.310685ms]
  May 13 18:24:59.724: INFO: Created: latency-svc-q9lcr
  May 13 18:24:59.768: INFO: Got endpoints: latency-svc-pdf2c [752.113586ms]
  May 13 18:24:59.777: INFO: Created: latency-svc-bncz7
  May 13 18:24:59.819: INFO: Got endpoints: latency-svc-z56kr [750.808306ms]
  May 13 18:24:59.827: INFO: Created: latency-svc-55xhv
  May 13 18:24:59.867: INFO: Got endpoints: latency-svc-ln4fx [748.922438ms]
  May 13 18:24:59.882: INFO: Created: latency-svc-hsqc8
  May 13 18:24:59.919: INFO: Got endpoints: latency-svc-29vvg [750.464601ms]
  May 13 18:24:59.928: INFO: Created: latency-svc-rtvdk
  May 13 18:24:59.968: INFO: Got endpoints: latency-svc-fzld7 [750.236828ms]
  May 13 18:24:59.977: INFO: Created: latency-svc-g52dc
  May 13 18:25:00.017: INFO: Got endpoints: latency-svc-6djps [747.917208ms]
  May 13 18:25:00.025: INFO: Created: latency-svc-7h7f4
  May 13 18:25:00.069: INFO: Got endpoints: latency-svc-p69v6 [747.89151ms]
  May 13 18:25:00.082: INFO: Created: latency-svc-r774x
  May 13 18:25:00.115: INFO: Got endpoints: latency-svc-pvt42 [738.960587ms]
  May 13 18:25:00.128: INFO: Created: latency-svc-9767m
  May 13 18:25:00.168: INFO: Got endpoints: latency-svc-8csdz [749.801743ms]
  May 13 18:25:00.178: INFO: Created: latency-svc-k6h8n
  May 13 18:25:00.214: INFO: Got endpoints: latency-svc-snc8m [748.944576ms]
  May 13 18:25:00.223: INFO: Created: latency-svc-lcjjj
  May 13 18:25:00.266: INFO: Got endpoints: latency-svc-5p5jl [708.192361ms]
  May 13 18:25:00.277: INFO: Created: latency-svc-swp8x
  May 13 18:25:00.316: INFO: Got endpoints: latency-svc-dr9xn [746.078809ms]
  May 13 18:25:00.326: INFO: Created: latency-svc-g9sv9
  May 13 18:25:00.372: INFO: Got endpoints: latency-svc-xjqt7 [757.848102ms]
  May 13 18:25:00.383: INFO: Created: latency-svc-xzs8m
  May 13 18:25:00.420: INFO: Got endpoints: latency-svc-nbdqw [751.598861ms]
  May 13 18:25:00.427: INFO: Created: latency-svc-8gp7p
  May 13 18:25:00.465: INFO: Got endpoints: latency-svc-q9lcr [750.652903ms]
  May 13 18:25:00.473: INFO: Created: latency-svc-jpdlh
  E0513 18:25:00.495610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:00.516: INFO: Got endpoints: latency-svc-bncz7 [746.842701ms]
  May 13 18:25:00.523: INFO: Created: latency-svc-kpx7x
  May 13 18:25:00.567: INFO: Got endpoints: latency-svc-55xhv [748.282456ms]
  May 13 18:25:00.574: INFO: Created: latency-svc-hqfwl
  May 13 18:25:00.620: INFO: Got endpoints: latency-svc-hsqc8 [752.32074ms]
  May 13 18:25:00.632: INFO: Created: latency-svc-66lrk
  May 13 18:25:00.670: INFO: Got endpoints: latency-svc-rtvdk [750.950388ms]
  May 13 18:25:00.678: INFO: Created: latency-svc-r22vq
  May 13 18:25:00.716: INFO: Got endpoints: latency-svc-g52dc [748.193388ms]
  May 13 18:25:00.726: INFO: Created: latency-svc-2ktnc
  May 13 18:25:00.771: INFO: Got endpoints: latency-svc-7h7f4 [754.810923ms]
  May 13 18:25:00.782: INFO: Created: latency-svc-6xb5j
  May 13 18:25:00.816: INFO: Got endpoints: latency-svc-r774x [747.758226ms]
  May 13 18:25:00.833: INFO: Created: latency-svc-7gk55
  May 13 18:25:00.866: INFO: Got endpoints: latency-svc-9767m [751.737436ms]
  May 13 18:25:00.881: INFO: Created: latency-svc-z8lgl
  May 13 18:25:00.915: INFO: Got endpoints: latency-svc-k6h8n [747.427731ms]
  May 13 18:25:00.925: INFO: Created: latency-svc-zdzqc
  May 13 18:25:00.965: INFO: Got endpoints: latency-svc-lcjjj [751.132041ms]
  May 13 18:25:00.973: INFO: Created: latency-svc-jcl6q
  May 13 18:25:01.016: INFO: Got endpoints: latency-svc-swp8x [749.211225ms]
  May 13 18:25:01.026: INFO: Created: latency-svc-dp68n
  May 13 18:25:01.065: INFO: Got endpoints: latency-svc-g9sv9 [749.27629ms]
  May 13 18:25:01.076: INFO: Created: latency-svc-xv86f
  May 13 18:25:01.117: INFO: Got endpoints: latency-svc-xzs8m [744.53748ms]
  May 13 18:25:01.131: INFO: Created: latency-svc-m8pc7
  May 13 18:25:01.170: INFO: Got endpoints: latency-svc-8gp7p [749.498795ms]
  May 13 18:25:01.181: INFO: Created: latency-svc-xr9gz
  May 13 18:25:01.216: INFO: Got endpoints: latency-svc-jpdlh [750.547056ms]
  May 13 18:25:01.227: INFO: Created: latency-svc-wkvlh
  May 13 18:25:01.268: INFO: Got endpoints: latency-svc-kpx7x [752.574226ms]
  May 13 18:25:01.275: INFO: Created: latency-svc-57v4j
  May 13 18:25:01.317: INFO: Got endpoints: latency-svc-hqfwl [749.504979ms]
  May 13 18:25:01.328: INFO: Created: latency-svc-xqmqq
  May 13 18:25:01.366: INFO: Got endpoints: latency-svc-66lrk [746.464857ms]
  May 13 18:25:01.377: INFO: Created: latency-svc-jkjsv
  May 13 18:25:01.419: INFO: Got endpoints: latency-svc-r22vq [748.768776ms]
  May 13 18:25:01.430: INFO: Created: latency-svc-x9bxj
  May 13 18:25:01.466: INFO: Got endpoints: latency-svc-2ktnc [749.525758ms]
  May 13 18:25:01.475: INFO: Created: latency-svc-s28bk
  E0513 18:25:01.496322      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:01.513: INFO: Got endpoints: latency-svc-6xb5j [741.819191ms]
  May 13 18:25:01.520: INFO: Created: latency-svc-t7bcv
  May 13 18:25:01.567: INFO: Got endpoints: latency-svc-7gk55 [750.882228ms]
  May 13 18:25:01.583: INFO: Created: latency-svc-4fknw
  May 13 18:25:01.615: INFO: Got endpoints: latency-svc-z8lgl [748.854885ms]
  May 13 18:25:01.631: INFO: Created: latency-svc-cql5n
  May 13 18:25:01.716: INFO: Got endpoints: latency-svc-zdzqc [801.080432ms]
  May 13 18:25:01.725: INFO: Created: latency-svc-vpjvq
  May 13 18:25:01.767: INFO: Got endpoints: latency-svc-jcl6q [802.462243ms]
  May 13 18:25:01.776: INFO: Created: latency-svc-q5jks
  May 13 18:25:01.815: INFO: Got endpoints: latency-svc-dp68n [799.338297ms]
  May 13 18:25:01.827: INFO: Created: latency-svc-x7w9n
  May 13 18:25:01.871: INFO: Got endpoints: latency-svc-xv86f [805.789113ms]
  May 13 18:25:01.880: INFO: Created: latency-svc-6zndm
  May 13 18:25:01.921: INFO: Got endpoints: latency-svc-m8pc7 [804.211538ms]
  May 13 18:25:01.932: INFO: Created: latency-svc-7sqk5
  May 13 18:25:01.966: INFO: Got endpoints: latency-svc-xr9gz [796.421479ms]
  May 13 18:25:01.981: INFO: Created: latency-svc-fkg6m
  May 13 18:25:02.014: INFO: Got endpoints: latency-svc-wkvlh [798.493839ms]
  May 13 18:25:02.023: INFO: Created: latency-svc-dvtsl
  May 13 18:25:02.065: INFO: Got endpoints: latency-svc-57v4j [796.436364ms]
  May 13 18:25:02.078: INFO: Created: latency-svc-dpm6m
  May 13 18:25:02.117: INFO: Got endpoints: latency-svc-xqmqq [799.942135ms]
  May 13 18:25:02.129: INFO: Created: latency-svc-7sh87
  May 13 18:25:02.171: INFO: Got endpoints: latency-svc-jkjsv [804.391996ms]
  May 13 18:25:02.181: INFO: Created: latency-svc-zw7q8
  May 13 18:25:02.220: INFO: Got endpoints: latency-svc-x9bxj [801.77132ms]
  May 13 18:25:02.230: INFO: Created: latency-svc-lb88s
  May 13 18:25:02.264: INFO: Got endpoints: latency-svc-s28bk [798.130129ms]
  May 13 18:25:02.271: INFO: Created: latency-svc-9slgq
  May 13 18:25:02.318: INFO: Got endpoints: latency-svc-t7bcv [804.97985ms]
  May 13 18:25:02.328: INFO: Created: latency-svc-2tsqt
  May 13 18:25:02.367: INFO: Got endpoints: latency-svc-4fknw [799.819932ms]
  May 13 18:25:02.378: INFO: Created: latency-svc-fzddx
  May 13 18:25:02.416: INFO: Got endpoints: latency-svc-cql5n [800.735585ms]
  May 13 18:25:02.427: INFO: Created: latency-svc-blwrw
  May 13 18:25:02.467: INFO: Got endpoints: latency-svc-vpjvq [750.333629ms]
  May 13 18:25:02.475: INFO: Created: latency-svc-gvrg2
  E0513 18:25:02.496987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:02.515: INFO: Got endpoints: latency-svc-q5jks [746.813582ms]
  May 13 18:25:02.522: INFO: Created: latency-svc-7rdjt
  May 13 18:25:02.566: INFO: Got endpoints: latency-svc-x7w9n [750.314638ms]
  May 13 18:25:02.577: INFO: Created: latency-svc-lhpzk
  May 13 18:25:02.614: INFO: Got endpoints: latency-svc-6zndm [743.395508ms]
  May 13 18:25:02.627: INFO: Created: latency-svc-2thtp
  May 13 18:25:02.670: INFO: Got endpoints: latency-svc-7sqk5 [748.444153ms]
  May 13 18:25:02.678: INFO: Created: latency-svc-4jx8p
  May 13 18:25:02.713: INFO: Got endpoints: latency-svc-fkg6m [747.138219ms]
  May 13 18:25:02.723: INFO: Created: latency-svc-lvvtd
  May 13 18:25:02.765: INFO: Got endpoints: latency-svc-dvtsl [749.876385ms]
  May 13 18:25:02.775: INFO: Created: latency-svc-7tp6n
  May 13 18:25:02.820: INFO: Got endpoints: latency-svc-dpm6m [754.808469ms]
  May 13 18:25:02.831: INFO: Created: latency-svc-q7nfb
  May 13 18:25:02.867: INFO: Got endpoints: latency-svc-7sh87 [750.117346ms]
  May 13 18:25:02.880: INFO: Created: latency-svc-mxzlq
  May 13 18:25:02.922: INFO: Got endpoints: latency-svc-zw7q8 [750.752824ms]
  May 13 18:25:02.931: INFO: Created: latency-svc-k8ckv
  May 13 18:25:02.967: INFO: Got endpoints: latency-svc-lb88s [746.779355ms]
  May 13 18:25:02.975: INFO: Created: latency-svc-jhgpf
  May 13 18:25:03.017: INFO: Got endpoints: latency-svc-9slgq [753.045446ms]
  May 13 18:25:03.025: INFO: Created: latency-svc-56sq8
  May 13 18:25:03.070: INFO: Got endpoints: latency-svc-2tsqt [751.219527ms]
  May 13 18:25:03.078: INFO: Created: latency-svc-fzwhw
  May 13 18:25:03.116: INFO: Got endpoints: latency-svc-fzddx [748.257647ms]
  May 13 18:25:03.129: INFO: Created: latency-svc-6b7rk
  May 13 18:25:03.170: INFO: Got endpoints: latency-svc-blwrw [753.452529ms]
  May 13 18:25:03.181: INFO: Created: latency-svc-m6fhf
  May 13 18:25:03.214: INFO: Got endpoints: latency-svc-gvrg2 [746.891704ms]
  May 13 18:25:03.223: INFO: Created: latency-svc-9jlqt
  May 13 18:25:03.267: INFO: Got endpoints: latency-svc-7rdjt [751.898622ms]
  May 13 18:25:03.281: INFO: Created: latency-svc-qff64
  May 13 18:25:03.315: INFO: Got endpoints: latency-svc-lhpzk [747.818849ms]
  May 13 18:25:03.326: INFO: Created: latency-svc-jdq9n
  May 13 18:25:03.362: INFO: Got endpoints: latency-svc-2thtp [747.078715ms]
  May 13 18:25:03.374: INFO: Created: latency-svc-l8qhk
  May 13 18:25:03.421: INFO: Got endpoints: latency-svc-4jx8p [751.287821ms]
  May 13 18:25:03.432: INFO: Created: latency-svc-cq7jl
  May 13 18:25:03.466: INFO: Got endpoints: latency-svc-lvvtd [752.455047ms]
  May 13 18:25:03.475: INFO: Created: latency-svc-d7qpb
  E0513 18:25:03.497158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:03.516: INFO: Got endpoints: latency-svc-7tp6n [750.347982ms]
  May 13 18:25:03.529: INFO: Created: latency-svc-7w4bs
  May 13 18:25:03.564: INFO: Got endpoints: latency-svc-q7nfb [744.073085ms]
  May 13 18:25:03.574: INFO: Created: latency-svc-xkvwp
  May 13 18:25:03.613: INFO: Got endpoints: latency-svc-mxzlq [745.595382ms]
  May 13 18:25:03.628: INFO: Created: latency-svc-knl2s
  May 13 18:25:03.667: INFO: Got endpoints: latency-svc-k8ckv [744.586512ms]
  May 13 18:25:03.677: INFO: Created: latency-svc-w47p8
  May 13 18:25:03.718: INFO: Got endpoints: latency-svc-jhgpf [750.557142ms]
  May 13 18:25:03.726: INFO: Created: latency-svc-kvdqw
  May 13 18:25:03.769: INFO: Got endpoints: latency-svc-56sq8 [751.327375ms]
  May 13 18:25:03.778: INFO: Created: latency-svc-4pxm8
  May 13 18:25:03.818: INFO: Got endpoints: latency-svc-fzwhw [747.950392ms]
  May 13 18:25:03.825: INFO: Created: latency-svc-rz8vx
  May 13 18:25:03.866: INFO: Got endpoints: latency-svc-6b7rk [749.007681ms]
  May 13 18:25:03.877: INFO: Created: latency-svc-fjdrp
  May 13 18:25:03.917: INFO: Got endpoints: latency-svc-m6fhf [747.504907ms]
  May 13 18:25:03.926: INFO: Created: latency-svc-c62r4
  May 13 18:25:03.967: INFO: Got endpoints: latency-svc-9jlqt [753.192315ms]
  May 13 18:25:03.977: INFO: Created: latency-svc-4cnzh
  May 13 18:25:04.017: INFO: Got endpoints: latency-svc-qff64 [749.627691ms]
  May 13 18:25:04.027: INFO: Created: latency-svc-2dd7t
  May 13 18:25:04.073: INFO: Got endpoints: latency-svc-jdq9n [757.746305ms]
  May 13 18:25:04.090: INFO: Created: latency-svc-ntqnh
  May 13 18:25:04.116: INFO: Got endpoints: latency-svc-l8qhk [754.12589ms]
  May 13 18:25:04.131: INFO: Created: latency-svc-vnxjl
  May 13 18:25:04.167: INFO: Got endpoints: latency-svc-cq7jl [746.243814ms]
  May 13 18:25:04.177: INFO: Created: latency-svc-7kw2z
  May 13 18:25:04.216: INFO: Got endpoints: latency-svc-d7qpb [750.148353ms]
  May 13 18:25:04.227: INFO: Created: latency-svc-d8lkl
  May 13 18:25:04.266: INFO: Got endpoints: latency-svc-7w4bs [750.080072ms]
  May 13 18:25:04.274: INFO: Created: latency-svc-k8ltm
  May 13 18:25:04.319: INFO: Got endpoints: latency-svc-xkvwp [754.361797ms]
  May 13 18:25:04.329: INFO: Created: latency-svc-jq9h9
  May 13 18:25:04.368: INFO: Got endpoints: latency-svc-knl2s [755.379996ms]
  May 13 18:25:04.383: INFO: Created: latency-svc-68zdw
  May 13 18:25:04.416: INFO: Got endpoints: latency-svc-w47p8 [749.318128ms]
  May 13 18:25:04.428: INFO: Created: latency-svc-7gxp9
  May 13 18:25:04.466: INFO: Got endpoints: latency-svc-kvdqw [747.512885ms]
  May 13 18:25:04.477: INFO: Created: latency-svc-nglk7
  E0513 18:25:04.497552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:04.519: INFO: Got endpoints: latency-svc-4pxm8 [750.94005ms]
  May 13 18:25:04.527: INFO: Created: latency-svc-456gw
  May 13 18:25:04.568: INFO: Got endpoints: latency-svc-rz8vx [750.840874ms]
  May 13 18:25:04.579: INFO: Created: latency-svc-m59b7
  May 13 18:25:04.613: INFO: Got endpoints: latency-svc-fjdrp [746.485974ms]
  May 13 18:25:04.625: INFO: Created: latency-svc-68dlg
  May 13 18:25:04.671: INFO: Got endpoints: latency-svc-c62r4 [753.147048ms]
  May 13 18:25:04.679: INFO: Created: latency-svc-jlqbj
  May 13 18:25:04.718: INFO: Got endpoints: latency-svc-4cnzh [750.500344ms]
  May 13 18:25:04.726: INFO: Created: latency-svc-t5mdr
  May 13 18:25:04.765: INFO: Got endpoints: latency-svc-2dd7t [748.70596ms]
  May 13 18:25:04.773: INFO: Created: latency-svc-vjdzs
  May 13 18:25:04.821: INFO: Got endpoints: latency-svc-ntqnh [746.781474ms]
  May 13 18:25:04.833: INFO: Created: latency-svc-vvh2j
  May 13 18:25:04.865: INFO: Got endpoints: latency-svc-vnxjl [748.374474ms]
  May 13 18:25:04.877: INFO: Created: latency-svc-76rbt
  May 13 18:25:04.916: INFO: Got endpoints: latency-svc-7kw2z [748.490875ms]
  May 13 18:25:04.925: INFO: Created: latency-svc-7jn7b
  May 13 18:25:04.965: INFO: Got endpoints: latency-svc-d8lkl [748.933755ms]
  May 13 18:25:04.975: INFO: Created: latency-svc-zxg9w
  May 13 18:25:05.018: INFO: Got endpoints: latency-svc-k8ltm [751.773291ms]
  May 13 18:25:05.026: INFO: Created: latency-svc-xgz46
  May 13 18:25:05.063: INFO: Got endpoints: latency-svc-jq9h9 [743.856678ms]
  May 13 18:25:05.072: INFO: Created: latency-svc-87tcq
  May 13 18:25:05.112: INFO: Got endpoints: latency-svc-68zdw [743.738591ms]
  May 13 18:25:05.126: INFO: Created: latency-svc-bnzzw
  May 13 18:25:05.164: INFO: Got endpoints: latency-svc-7gxp9 [747.484193ms]
  May 13 18:25:05.174: INFO: Created: latency-svc-9bnqd
  May 13 18:25:05.215: INFO: Got endpoints: latency-svc-nglk7 [749.181572ms]
  May 13 18:25:05.226: INFO: Created: latency-svc-vfmk6
  May 13 18:25:05.268: INFO: Got endpoints: latency-svc-456gw [748.204856ms]
  May 13 18:25:05.275: INFO: Created: latency-svc-fszl6
  May 13 18:25:05.317: INFO: Got endpoints: latency-svc-m59b7 [748.013694ms]
  May 13 18:25:05.327: INFO: Created: latency-svc-6pkcl
  May 13 18:25:05.368: INFO: Got endpoints: latency-svc-68dlg [754.085713ms]
  May 13 18:25:05.378: INFO: Created: latency-svc-w5rb5
  May 13 18:25:05.417: INFO: Got endpoints: latency-svc-jlqbj [746.616965ms]
  May 13 18:25:05.432: INFO: Created: latency-svc-8g7hh
  May 13 18:25:05.472: INFO: Got endpoints: latency-svc-t5mdr [753.396414ms]
  May 13 18:25:05.478: INFO: Created: latency-svc-sqxct
  E0513 18:25:05.497944      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:05.515: INFO: Got endpoints: latency-svc-vjdzs [749.286309ms]
  May 13 18:25:05.522: INFO: Created: latency-svc-8cwmj
  May 13 18:25:05.566: INFO: Got endpoints: latency-svc-vvh2j [743.498164ms]
  May 13 18:25:05.576: INFO: Created: latency-svc-vz87n
  May 13 18:25:05.618: INFO: Got endpoints: latency-svc-76rbt [753.545882ms]
  May 13 18:25:05.631: INFO: Created: latency-svc-6knsd
  May 13 18:25:05.670: INFO: Got endpoints: latency-svc-7jn7b [754.633434ms]
  May 13 18:25:05.681: INFO: Created: latency-svc-gxfkf
  May 13 18:25:05.718: INFO: Got endpoints: latency-svc-zxg9w [752.476758ms]
  May 13 18:25:05.726: INFO: Created: latency-svc-6qp29
  May 13 18:25:05.768: INFO: Got endpoints: latency-svc-xgz46 [750.715537ms]
  May 13 18:25:05.777: INFO: Created: latency-svc-lz5g7
  May 13 18:25:05.818: INFO: Got endpoints: latency-svc-87tcq [755.520018ms]
  May 13 18:25:05.830: INFO: Created: latency-svc-9lcsr
  May 13 18:25:05.870: INFO: Got endpoints: latency-svc-bnzzw [758.147896ms]
  May 13 18:25:05.883: INFO: Created: latency-svc-dmkw8
  May 13 18:25:05.917: INFO: Got endpoints: latency-svc-9bnqd [752.696111ms]
  May 13 18:25:05.928: INFO: Created: latency-svc-jdn7m
  May 13 18:25:05.967: INFO: Got endpoints: latency-svc-vfmk6 [751.863275ms]
  May 13 18:25:06.019: INFO: Got endpoints: latency-svc-fszl6 [751.018894ms]
  May 13 18:25:06.071: INFO: Got endpoints: latency-svc-6pkcl [754.028206ms]
  May 13 18:25:06.113: INFO: Got endpoints: latency-svc-w5rb5 [744.910204ms]
  May 13 18:25:06.168: INFO: Got endpoints: latency-svc-8g7hh [750.523199ms]
  May 13 18:25:06.222: INFO: Got endpoints: latency-svc-sqxct [749.863015ms]
  May 13 18:25:06.265: INFO: Got endpoints: latency-svc-8cwmj [750.57754ms]
  May 13 18:25:06.319: INFO: Got endpoints: latency-svc-vz87n [751.809588ms]
  May 13 18:25:06.368: INFO: Got endpoints: latency-svc-6knsd [749.114076ms]
  May 13 18:25:06.421: INFO: Got endpoints: latency-svc-gxfkf [750.200614ms]
  May 13 18:25:06.467: INFO: Got endpoints: latency-svc-6qp29 [748.379639ms]
  E0513 18:25:06.498553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:06.516: INFO: Got endpoints: latency-svc-lz5g7 [747.699118ms]
  May 13 18:25:06.566: INFO: Got endpoints: latency-svc-9lcsr [747.027583ms]
  May 13 18:25:06.617: INFO: Got endpoints: latency-svc-dmkw8 [746.260508ms]
  May 13 18:25:06.665: INFO: Got endpoints: latency-svc-jdn7m [747.750009ms]
  May 13 18:25:06.665: INFO: Latencies: [32.738564ms 34.1016ms 43.146345ms 50.834417ms 53.440132ms 57.264841ms 66.055406ms 71.108265ms 72.917695ms 73.404108ms 73.946893ms 75.133212ms 75.509963ms 83.679498ms 84.037369ms 84.447615ms 86.953635ms 88.775505ms 96.114409ms 96.890011ms 99.778265ms 101.135217ms 102.395256ms 105.5809ms 111.814316ms 118.305632ms 136.110693ms 139.038035ms 147.19461ms 150.675502ms 150.973871ms 151.226991ms 154.321938ms 158.9374ms 185.529081ms 185.577413ms 209.929745ms 255.478383ms 305.264691ms 344.943195ms 380.508863ms 425.2037ms 470.561312ms 506.931025ms 574.322978ms 601.523383ms 642.972205ms 672.753392ms 708.192361ms 719.87736ms 729.676021ms 738.960587ms 741.819191ms 743.395508ms 743.498164ms 743.738591ms 743.856678ms 744.073085ms 744.310685ms 744.53748ms 744.586512ms 744.725968ms 744.910204ms 745.595382ms 746.078809ms 746.243814ms 746.260508ms 746.464857ms 746.485974ms 746.616965ms 746.779355ms 746.781474ms 746.813582ms 746.842701ms 746.891704ms 747.027583ms 747.078715ms 747.138219ms 747.427731ms 747.484193ms 747.504907ms 747.512885ms 747.699118ms 747.750009ms 747.758226ms 747.818849ms 747.89151ms 747.917208ms 747.950392ms 748.013694ms 748.193388ms 748.204856ms 748.257647ms 748.282456ms 748.374474ms 748.379639ms 748.414894ms 748.444153ms 748.490875ms 748.70596ms 748.768776ms 748.854885ms 748.922438ms 748.933755ms 748.944576ms 749.007681ms 749.114076ms 749.181572ms 749.211225ms 749.27629ms 749.286309ms 749.318128ms 749.498795ms 749.504979ms 749.525758ms 749.627691ms 749.686848ms 749.801743ms 749.863015ms 749.876385ms 750.080072ms 750.117346ms 750.148353ms 750.200614ms 750.236828ms 750.24868ms 750.314638ms 750.333629ms 750.347982ms 750.464601ms 750.500344ms 750.523199ms 750.547056ms 750.557142ms 750.57754ms 750.652903ms 750.715537ms 750.752824ms 750.808306ms 750.840874ms 750.882228ms 750.94005ms 750.950388ms 751.018894ms 751.132041ms 751.219527ms 751.287821ms 751.327375ms 751.598861ms 751.737436ms 751.773291ms 751.809588ms 751.863275ms 751.898622ms 752.077645ms 752.113586ms 752.25953ms 752.291187ms 752.32074ms 752.455047ms 752.476758ms 752.574226ms 752.696111ms 753.045446ms 753.147048ms 753.192315ms 753.26188ms 753.396414ms 753.452529ms 753.545882ms 754.028206ms 754.085713ms 754.12589ms 754.181058ms 754.361797ms 754.596108ms 754.633434ms 754.808469ms 754.810923ms 755.379996ms 755.520018ms 757.746305ms 757.848102ms 758.147896ms 789.439869ms 796.421479ms 796.436364ms 798.130129ms 798.493839ms 799.338297ms 799.819932ms 799.942135ms 800.735585ms 801.080432ms 801.77132ms 802.462243ms 804.211538ms 804.391996ms 804.97985ms 805.789113ms]
  May 13 18:25:06.665: INFO: 50 %ile: 748.768776ms
  May 13 18:25:06.665: INFO: 90 %ile: 755.520018ms
  May 13 18:25:06.665: INFO: 99 %ile: 804.97985ms
  May 13 18:25:06.665: INFO: Total sample count: 200
  May 13 18:25:06.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-5741" for this suite. @ 05/13/23 18:25:06.672
• [10.806 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 05/13/23 18:25:06.684
  May 13 18:25:06.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename statefulset @ 05/13/23 18:25:06.685
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:25:06.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:25:06.699
  STEP: Creating service test in namespace statefulset-736 @ 05/13/23 18:25:06.703
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/13/23 18:25:06.71
  STEP: Creating stateful set ss in namespace statefulset-736 @ 05/13/23 18:25:06.718
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-736 @ 05/13/23 18:25:06.723
  May 13 18:25:06.727: INFO: Found 0 stateful pods, waiting for 1
  E0513 18:25:07.499111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:08.499360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:09.499456      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:10.500297      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:11.508232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:12.508290      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:13.508682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:14.508865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:15.508916      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:16.509730      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:16.747: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/13/23 18:25:16.747
  May 13 18:25:16.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-736 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 13 18:25:16.873: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 13 18:25:16.873: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 13 18:25:16.873: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 13 18:25:16.877: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0513 18:25:17.510225      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:18.510810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:19.511059      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:20.511818      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:21.516660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:22.516954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:23.517612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:24.518184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:25.518210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:26.518761      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:26.882: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 13 18:25:26.882: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 18:25:26.896: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999766s
  E0513 18:25:27.521143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:27.901: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996984199s
  E0513 18:25:28.523017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:28.908: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987903179s
  E0513 18:25:29.524779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:29.916: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983890988s
  E0513 18:25:30.524844      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:30.922: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.97592165s
  E0513 18:25:31.525785      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:31.928: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.969820504s
  E0513 18:25:32.527238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:32.932: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.964750076s
  E0513 18:25:33.527895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:33.939: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.960132212s
  E0513 18:25:34.529234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:34.944: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.954717625s
  E0513 18:25:35.530120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:35.949: INFO: Verifying statefulset ss doesn't scale past 1 for another 949.744273ms
  E0513 18:25:36.531694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-736 @ 05/13/23 18:25:36.95
  May 13 18:25:36.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-736 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 13 18:25:37.069: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 13 18:25:37.069: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 13 18:25:37.069: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 13 18:25:37.076: INFO: Found 1 stateful pods, waiting for 3
  E0513 18:25:37.533004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:38.537636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:39.539171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:40.541952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:41.542706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:42.543337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:43.543992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:44.544622      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:45.546106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:46.547114      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:47.085: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 13 18:25:47.085: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 13 18:25:47.085: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/13/23 18:25:47.085
  STEP: Scale down will halt with unhealthy stateful pod @ 05/13/23 18:25:47.085
  May 13 18:25:47.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-736 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 13 18:25:47.213: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 13 18:25:47.213: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 13 18:25:47.213: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 13 18:25:47.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-736 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 13 18:25:47.340: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 13 18:25:47.340: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 13 18:25:47.340: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 13 18:25:47.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-736 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 13 18:25:47.473: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 13 18:25:47.473: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 13 18:25:47.473: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 13 18:25:47.473: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 18:25:47.475: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  E0513 18:25:47.548900      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:48.549982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:49.550516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:50.551171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:51.551533      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:52.551606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:53.554918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:54.556024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:55.558346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:25:56.563294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:57.485: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 13 18:25:57.485: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 13 18:25:57.485: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 13 18:25:57.497: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999801s
  E0513 18:25:57.564408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:58.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996472931s
  E0513 18:25:58.564920      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:25:59.506: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992573398s
  E0513 18:25:59.565652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:00.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983958619s
  E0513 18:26:00.566122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:01.525: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974900916s
  E0513 18:26:01.566178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:02.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967655505s
  E0513 18:26:02.566606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:03.538: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.960904101s
  E0513 18:26:03.567695      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:04.546: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.954590496s
  E0513 18:26:04.567814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:05.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.948021039s
  E0513 18:26:05.571288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:06.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 938.736278ms
  E0513 18:26:06.571726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-736 @ 05/13/23 18:26:07.571
  E0513 18:26:07.573414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:07.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-736 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 13 18:26:07.719: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 13 18:26:07.719: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 13 18:26:07.719: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 13 18:26:07.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-736 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 13 18:26:07.919: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 13 18:26:07.919: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 13 18:26:07.919: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 13 18:26:07.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-736 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 13 18:26:08.122: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 13 18:26:08.122: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 13 18:26:08.122: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 13 18:26:08.122: INFO: Scaling statefulset ss to 0
  E0513 18:26:08.574118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:09.575061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:10.575306      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:11.576046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:12.576829      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:13.577967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:14.578546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:15.580072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:16.580355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:17.580989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/13/23 18:26:18.139
  May 13 18:26:18.139: INFO: Deleting all statefulset in ns statefulset-736
  May 13 18:26:18.144: INFO: Scaling statefulset ss to 0
  May 13 18:26:18.155: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 18:26:18.160: INFO: Deleting statefulset ss
  May 13 18:26:18.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-736" for this suite. @ 05/13/23 18:26:18.182
• [71.505 seconds]
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/13/23 18:26:18.19
  May 13 18:26:18.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename watch @ 05/13/23 18:26:18.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:26:18.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:26:18.215
  STEP: getting a starting resourceVersion @ 05/13/23 18:26:18.219
  STEP: starting a background goroutine to produce watch events @ 05/13/23 18:26:18.221
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/13/23 18:26:18.221
  E0513 18:26:18.582023      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:19.582450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:20.583462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:20.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7537" for this suite. @ 05/13/23 18:26:21.045
• [2.912 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 05/13/23 18:26:21.104
  May 13 18:26:21.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 18:26:21.105
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:26:21.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:26:21.125
  STEP: creating all guestbook components @ 05/13/23 18:26:21.13
  May 13 18:26:21.130: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  May 13 18:26:21.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 create -f -'
  E0513 18:26:21.584315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:21.798: INFO: stderr: ""
  May 13 18:26:21.798: INFO: stdout: "service/agnhost-replica created\n"
  May 13 18:26:21.798: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  May 13 18:26:21.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 create -f -'
  May 13 18:26:22.061: INFO: stderr: ""
  May 13 18:26:22.061: INFO: stdout: "service/agnhost-primary created\n"
  May 13 18:26:22.061: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  May 13 18:26:22.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 create -f -'
  May 13 18:26:22.293: INFO: stderr: ""
  May 13 18:26:22.293: INFO: stdout: "service/frontend created\n"
  May 13 18:26:22.294: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  May 13 18:26:22.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 create -f -'
  May 13 18:26:22.495: INFO: stderr: ""
  May 13 18:26:22.495: INFO: stdout: "deployment.apps/frontend created\n"
  May 13 18:26:22.495: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 13 18:26:22.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 create -f -'
  E0513 18:26:22.584755      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:22.770: INFO: stderr: ""
  May 13 18:26:22.770: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  May 13 18:26:22.772: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 13 18:26:22.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 create -f -'
  May 13 18:26:23.113: INFO: stderr: ""
  May 13 18:26:23.113: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/13/23 18:26:23.113
  May 13 18:26:23.113: INFO: Waiting for all frontend pods to be Running.
  E0513 18:26:23.587351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:24.588805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:25.588566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:26.588909      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:27.592586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:28.165: INFO: Waiting for frontend to serve content.
  May 13 18:26:28.180: INFO: Trying to add a new entry to the guestbook.
  May 13 18:26:28.194: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 05/13/23 18:26:28.201
  May 13 18:26:28.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 delete --grace-period=0 --force -f -'
  May 13 18:26:28.260: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 13 18:26:28.260: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/13/23 18:26:28.26
  May 13 18:26:28.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 delete --grace-period=0 --force -f -'
  May 13 18:26:28.321: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 13 18:26:28.321: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/13/23 18:26:28.321
  May 13 18:26:28.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 delete --grace-period=0 --force -f -'
  May 13 18:26:28.382: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 13 18:26:28.382: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/13/23 18:26:28.382
  May 13 18:26:28.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 delete --grace-period=0 --force -f -'
  May 13 18:26:28.439: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 13 18:26:28.439: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/13/23 18:26:28.439
  May 13 18:26:28.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 delete --grace-period=0 --force -f -'
  May 13 18:26:28.562: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 13 18:26:28.562: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/13/23 18:26:28.562
  May 13 18:26:28.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5182 delete --grace-period=0 --force -f -'
  E0513 18:26:28.596561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:28.721: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 13 18:26:28.724: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  May 13 18:26:28.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5182" for this suite. @ 05/13/23 18:26:28.728
• [7.631 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 05/13/23 18:26:28.739
  May 13 18:26:28.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/13/23 18:26:28.74
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:26:28.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:26:28.781
  STEP: set up a multi version CRD @ 05/13/23 18:26:28.783
  May 13 18:26:28.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:26:29.597579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:30.598037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:31.598174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 05/13/23 18:26:32.232
  STEP: check the new version name is served @ 05/13/23 18:26:32.249
  E0513 18:26:32.600177      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 05/13/23 18:26:33.532
  E0513 18:26:33.601552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/13/23 18:26:34.437
  E0513 18:26:34.601779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:35.651264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:36.652746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:37.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3755" for this suite. @ 05/13/23 18:26:37.388
• [8.656 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/13/23 18:26:37.396
  May 13 18:26:37.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename deployment @ 05/13/23 18:26:37.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:26:37.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:26:37.419
  May 13 18:26:37.425: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  May 13 18:26:37.435: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0513 18:26:37.654323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:38.654879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:39.656046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:40.655504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:41.655763      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:42.437: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/13/23 18:26:42.437
  May 13 18:26:42.437: INFO: Creating deployment "test-rolling-update-deployment"
  May 13 18:26:42.443: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  May 13 18:26:42.452: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0513 18:26:42.656857      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:43.657240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:44.460: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  May 13 18:26:44.462: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  May 13 18:26:44.467: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5831  f72b69fd-414b-4608-af7a-e3ed089e84ca 11116 1 2023-05-13 18:26:42 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-13 18:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 18:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007099bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-13 18:26:42 +0000 UTC,LastTransitionTime:2023-05-13 18:26:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-05-13 18:26:43 +0000 UTC,LastTransitionTime:2023-05-13 18:26:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 13 18:26:44.470: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-5831  ca1738a4-0fca-4b2a-8782-e96bd28915ec 11106 1 2023-05-13 18:26:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f72b69fd-414b-4608-af7a-e3ed089e84ca 0xc00701e107 0xc00701e108}] [] [{kube-controller-manager Update apps/v1 2023-05-13 18:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f72b69fd-414b-4608-af7a-e3ed089e84ca\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 18:26:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00701e1c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 13 18:26:44.470: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  May 13 18:26:44.470: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5831  8842aeb6-13dd-4c5f-8fbe-7eba01a6a632 11115 2 2023-05-13 18:26:37 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f72b69fd-414b-4608-af7a-e3ed089e84ca 0xc007099fc7 0xc007099fc8}] [] [{e2e.test Update apps/v1 2023-05-13 18:26:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 18:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f72b69fd-414b-4608-af7a-e3ed089e84ca\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-13 18:26:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00701e098 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 13 18:26:44.474: INFO: Pod "test-rolling-update-deployment-656d657cd8-l6px9" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-l6px9 test-rolling-update-deployment-656d657cd8- deployment-5831  87d145d9-4648-4af4-9ff6-6ef03a7a4d2e 11105 0 2023-05-13 18:26:42 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[cni.projectcalico.org/containerID:b9efc9e431e3fec7e7a2c8648394c647704d7d8b1fc1a16e83a2df089a50e5ed cni.projectcalico.org/podIP:10.200.131.142/32 cni.projectcalico.org/podIPs:10.200.131.142/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 ca1738a4-0fca-4b2a-8782-e96bd28915ec 0xc00701e707 0xc00701e708}] [] [{calico Update v1 2023-05-13 18:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca1738a4-0fca-4b2a-8782-e96bd28915ec\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:26:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47vb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47vb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:26:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:26:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:10.200.131.142,StartTime:2023-05-13 18:26:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 18:26:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://0203d2d085085a26fe5443a6e5e88d8bd85d60d4295eb45aed97c0d30bcd6dfb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.142,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:26:44.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5831" for this suite. @ 05/13/23 18:26:44.478
• [7.087 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 05/13/23 18:26:44.484
  May 13 18:26:44.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 18:26:44.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:26:44.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:26:44.502
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/13/23 18:26:44.505
  E0513 18:26:44.657468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:45.657739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:46.658139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:47.658825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:26:48.52
  May 13 18:26:48.524: INFO: Trying to get logs from node worker00 pod pod-ae1f8692-279a-42bf-9337-1dd63619afc0 container test-container: <nil>
  STEP: delete the pod @ 05/13/23 18:26:48.533
  May 13 18:26:48.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9535" for this suite. @ 05/13/23 18:26:48.566
• [4.089 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 05/13/23 18:26:48.574
  May 13 18:26:48.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-probe @ 05/13/23 18:26:48.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:26:48.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:26:48.603
  STEP: Creating pod busybox-646e9df5-99b9-4da2-ad84-92da733b7dcc in namespace container-probe-4376 @ 05/13/23 18:26:48.608
  E0513 18:26:48.659004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:49.660649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:26:50.626: INFO: Started pod busybox-646e9df5-99b9-4da2-ad84-92da733b7dcc in namespace container-probe-4376
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/13/23 18:26:50.626
  May 13 18:26:50.630: INFO: Initial restart count of pod busybox-646e9df5-99b9-4da2-ad84-92da733b7dcc is 0
  E0513 18:26:50.661562      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:51.662533      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:52.663395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:53.663733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:54.665065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:55.665862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:56.666687      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:57.668280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:58.668783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:26:59.669395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:00.669317      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:01.670922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:02.671753      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:03.672130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:04.672340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:05.672812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:06.673793      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:07.674636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:08.675160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:09.675822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:10.677771      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:11.678272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:12.681892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:13.681812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:14.682293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:15.682798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:16.683671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:17.683991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:18.685031      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:19.685861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:20.686180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:21.686557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:22.686894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:23.688049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:24.690865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:25.691568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:26.691868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:27.692510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:28.692861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:29.693346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:30.696535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:31.696135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:32.697290      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:33.697937      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:34.699125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:35.700687      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:36.701241      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:37.702904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:38.703967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:39.705185      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:40.706528      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:41.709845      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:42.710713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:43.710682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:44.727578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:45.727239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:46.729339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:47.729367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:48.729634      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:49.730798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:50.731034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:51.732423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:52.733047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:53.733343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:54.734293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:55.735559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:56.736812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:57.737704      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:58.737894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:27:59.738293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:00.738469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:01.738988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:02.738944      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:03.747010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:04.747422      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:05.747720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:06.748163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:07.749354      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:08.750859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:09.756140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:10.756833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:11.757608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:12.758190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:13.758584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:14.758797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:15.761553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:16.762548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:17.763414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:18.765715      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:19.765920      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:20.768618      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:21.769130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:22.769734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:23.772824      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:24.773389      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:25.775060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:26.775516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:27.776358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:28.777002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:29.782078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:30.781828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:31.783193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:32.783311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:33.784119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:34.784662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:35.786038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:36.787033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:37.787294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:38.787667      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:39.788106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:40.788375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:41.789239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:42.790067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:43.790816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:44.791896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:45.792389      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:46.792566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:47.792605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:48.793732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:49.796767      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:50.797870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:51.797834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:52.798986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:53.799790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:54.801174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:55.802063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:56.803885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:57.803849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:58.804674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:28:59.824547      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:00.827472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:01.827687      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:02.828596      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:03.830659      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:04.831497      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:05.832625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:06.833392      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:07.834827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:08.835319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:09.835717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:10.835852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:11.836258      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:12.838807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:13.838475      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:14.838724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:15.839964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:16.841115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:17.841199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:18.845462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:19.845822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:20.846392      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:21.846944      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:22.847768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:23.847959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:24.848818      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:25.848823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:26.851771      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:27.852798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:28.853970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:29.854204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:30.855259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:31.855465      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:32.856357      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:33.857084      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:34.857562      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:35.858238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:36.858633      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:37.861317      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:38.862941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:39.864791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:40.865821      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:41.865765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:42.866521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:43.868691      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:44.869151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:45.869630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:46.870169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:47.872428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:48.873137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:49.873636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:50.878172      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:51.878640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:52.879232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:53.880393      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:54.880960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:55.881887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:56.882394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:57.884427      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:58.888485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:29:59.887684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:00.888156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:01.888866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:02.889911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:03.890530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:04.890532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:05.890771      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:06.892119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:07.892430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:08.892917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:09.893680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:10.894218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:11.894697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:12.896382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:13.896045      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:14.896732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:15.896886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:16.897893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:17.898139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:18.898498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:19.898636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:20.899269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:21.899310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:22.899985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:23.900022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:24.900397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:25.901275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:26.903618      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:27.904843      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:28.906293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:29.906739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:30.907909      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:31.908140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:32.910504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:33.910759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:34.910785      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:35.911342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:36.912436      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:37.912702      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:38.917532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:39.913922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:40.914733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:41.915093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:42.915742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:43.916088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:44.917039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:45.917271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:46.917950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:47.918077      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:48.918798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:49.919256      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:50.919460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:30:51.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 18:30:51.256
  STEP: Destroying namespace "container-probe-4376" for this suite. @ 05/13/23 18:30:51.274
• [242.709 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 05/13/23 18:30:51.283
  May 13 18:30:51.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-probe @ 05/13/23 18:30:51.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:30:51.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:30:51.304
  STEP: Creating pod busybox-2c7082b8-cd6f-4e90-9364-ba72a64217c5 in namespace container-probe-7608 @ 05/13/23 18:30:51.308
  E0513 18:30:51.920155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:52.920653      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:30:53.325: INFO: Started pod busybox-2c7082b8-cd6f-4e90-9364-ba72a64217c5 in namespace container-probe-7608
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/13/23 18:30:53.326
  May 13 18:30:53.330: INFO: Initial restart count of pod busybox-2c7082b8-cd6f-4e90-9364-ba72a64217c5 is 0
  E0513 18:30:53.920981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:54.921309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:55.921682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:56.934487      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:57.931569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:58.932232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:30:59.934631      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:00.935187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:01.936160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:02.937783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:03.938768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:04.939054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:05.941329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:06.942198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:07.942494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:08.942607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:09.944779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:10.945741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:11.945932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:12.946121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:13.947079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:14.947575      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:15.947837      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:16.948246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:17.949209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:18.949442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:19.952137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:20.952478      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:21.953358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:22.954153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:23.954744      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:24.954868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:25.955660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:26.955781      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:27.955998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:28.956705      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:29.958637      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:30.959042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:31.959209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:32.960381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:33.961170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:34.961698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:35.962021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:36.962500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:37.963028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:38.963037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:39.964139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:40.964998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:41.965950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:42.966261      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:31:43.508: INFO: Restart count of pod container-probe-7608/busybox-2c7082b8-cd6f-4e90-9364-ba72a64217c5 is now 1 (50.177836063s elapsed)
  May 13 18:31:43.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 18:31:43.512
  STEP: Destroying namespace "container-probe-7608" for this suite. @ 05/13/23 18:31:43.542
• [52.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 05/13/23 18:31:43.55
  May 13 18:31:43.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename ingress @ 05/13/23 18:31:43.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:31:43.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:31:43.571
  STEP: getting /apis @ 05/13/23 18:31:43.574
  STEP: getting /apis/networking.k8s.io @ 05/13/23 18:31:43.583
  STEP: getting /apis/networking.k8s.iov1 @ 05/13/23 18:31:43.584
  STEP: creating @ 05/13/23 18:31:43.585
  STEP: getting @ 05/13/23 18:31:43.595
  STEP: listing @ 05/13/23 18:31:43.597
  STEP: watching @ 05/13/23 18:31:43.599
  May 13 18:31:43.599: INFO: starting watch
  STEP: cluster-wide listing @ 05/13/23 18:31:43.6
  STEP: cluster-wide watching @ 05/13/23 18:31:43.601
  May 13 18:31:43.601: INFO: starting watch
  STEP: patching @ 05/13/23 18:31:43.602
  STEP: updating @ 05/13/23 18:31:43.606
  May 13 18:31:43.613: INFO: waiting for watch events with expected annotations
  May 13 18:31:43.613: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/13/23 18:31:43.613
  STEP: updating /status @ 05/13/23 18:31:43.621
  STEP: get /status @ 05/13/23 18:31:43.631
  STEP: deleting @ 05/13/23 18:31:43.633
  STEP: deleting a collection @ 05/13/23 18:31:43.64
  May 13 18:31:43.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-8651" for this suite. @ 05/13/23 18:31:43.65
• [0.105 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/13/23 18:31:43.656
  May 13 18:31:43.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename proxy @ 05/13/23 18:31:43.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:31:43.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:31:43.671
  May 13 18:31:43.673: INFO: Creating pod...
  E0513 18:31:43.970538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:44.976103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:31:45.690: INFO: Creating service...
  May 13 18:31:45.716: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/pods/agnhost/proxy/some/path/with/DELETE
  May 13 18:31:45.724: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 13 18:31:45.724: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/pods/agnhost/proxy/some/path/with/GET
  May 13 18:31:45.727: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 13 18:31:45.727: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/pods/agnhost/proxy/some/path/with/HEAD
  May 13 18:31:45.730: INFO: http.Client request:HEAD | StatusCode:200
  May 13 18:31:45.730: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/pods/agnhost/proxy/some/path/with/OPTIONS
  May 13 18:31:45.732: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 13 18:31:45.732: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/pods/agnhost/proxy/some/path/with/PATCH
  May 13 18:31:45.734: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 13 18:31:45.734: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/pods/agnhost/proxy/some/path/with/POST
  May 13 18:31:45.735: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 13 18:31:45.735: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/pods/agnhost/proxy/some/path/with/PUT
  May 13 18:31:45.737: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 13 18:31:45.737: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/services/test-service/proxy/some/path/with/DELETE
  May 13 18:31:45.739: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 13 18:31:45.739: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/services/test-service/proxy/some/path/with/GET
  May 13 18:31:45.741: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 13 18:31:45.741: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/services/test-service/proxy/some/path/with/HEAD
  May 13 18:31:45.743: INFO: http.Client request:HEAD | StatusCode:200
  May 13 18:31:45.743: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/services/test-service/proxy/some/path/with/OPTIONS
  May 13 18:31:45.745: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 13 18:31:45.745: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/services/test-service/proxy/some/path/with/PATCH
  May 13 18:31:45.749: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 13 18:31:45.749: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/services/test-service/proxy/some/path/with/POST
  May 13 18:31:45.756: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 13 18:31:45.757: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-3107/services/test-service/proxy/some/path/with/PUT
  May 13 18:31:45.763: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 13 18:31:45.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-3107" for this suite. @ 05/13/23 18:31:45.766
• [2.115 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 05/13/23 18:31:45.772
  May 13 18:31:45.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename statefulset @ 05/13/23 18:31:45.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:31:45.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:31:45.791
  STEP: Creating service test in namespace statefulset-900 @ 05/13/23 18:31:45.797
  STEP: Creating a new StatefulSet @ 05/13/23 18:31:45.802
  May 13 18:31:45.814: INFO: Found 0 stateful pods, waiting for 3
  E0513 18:31:45.976954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:46.977194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:47.977423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:48.977591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:49.977906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:50.981339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:52.001064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:53.001548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:54.002421      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:55.002732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:31:55.822: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 13 18:31:55.822: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 13 18:31:55.822: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/13/23 18:31:55.83
  May 13 18:31:55.850: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/13/23 18:31:55.85
  E0513 18:31:56.003268      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:57.003778      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:58.004280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:31:59.004626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:00.005201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:01.005468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:02.006926      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:03.008222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:04.009484      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:05.009975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/13/23 18:32:05.867
  STEP: Performing a canary update @ 05/13/23 18:32:05.867
  May 13 18:32:05.888: INFO: Updating stateful set ss2
  May 13 18:32:05.900: INFO: Waiting for Pod statefulset-900/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0513 18:32:06.010505      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:07.011775      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:08.013321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:09.013884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:10.014098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:11.017308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:12.018157      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:13.018765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:14.019597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:15.023173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/13/23 18:32:15.908
  May 13 18:32:15.977: INFO: Found 2 stateful pods, waiting for 3
  E0513 18:32:16.023104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:17.024079      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:18.024222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:19.025188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:20.026939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:21.027750      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:22.027860      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:23.028042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:24.028491      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:25.029176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:32:25.981: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 13 18:32:25.982: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 13 18:32:25.983: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/13/23 18:32:25.988
  May 13 18:32:26.008: INFO: Updating stateful set ss2
  May 13 18:32:26.015: INFO: Waiting for Pod statefulset-900/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0513 18:32:26.030824      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:27.030669      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:28.030949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:29.033223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:30.033374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:31.037576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:32.038355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:33.038337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:34.038618      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:35.040122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:36.040539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:32:36.058: INFO: Updating stateful set ss2
  May 13 18:32:36.076: INFO: Waiting for StatefulSet statefulset-900/ss2 to complete update
  May 13 18:32:36.076: INFO: Waiting for Pod statefulset-900/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0513 18:32:37.040824      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:38.041413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:39.041968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:40.042717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:41.043282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:42.044995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:43.048644      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:44.048175      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:45.048764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:46.049325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:32:46.090: INFO: Deleting all statefulset in ns statefulset-900
  May 13 18:32:46.094: INFO: Scaling statefulset ss2 to 0
  E0513 18:32:47.050211      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:48.051087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:49.051670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:50.053764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:51.054006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:52.055073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:53.056472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:54.057376      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:55.057826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:56.058035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:32:56.111: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 18:32:56.115: INFO: Deleting statefulset ss2
  May 13 18:32:56.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-900" for this suite. @ 05/13/23 18:32:56.142
• [70.375 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 05/13/23 18:32:56.148
  May 13 18:32:56.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 18:32:56.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:32:56.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:32:56.162
  STEP: Setting up server cert @ 05/13/23 18:32:56.173
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 18:32:56.32
  STEP: Deploying the webhook pod @ 05/13/23 18:32:56.325
  STEP: Wait for the deployment to be ready @ 05/13/23 18:32:56.33
  May 13 18:32:56.335: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 18:32:57.063477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:32:58.063655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 18:32:58.341
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 18:32:58.353
  E0513 18:32:59.063846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:32:59.353: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/13/23 18:32:59.356
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/13/23 18:32:59.356
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/13/23 18:32:59.367
  E0513 18:33:00.064005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/13/23 18:33:00.378
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/13/23 18:33:00.378
  E0513 18:33:01.064161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 05/13/23 18:33:01.408
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/13/23 18:33:01.408
  E0513 18:33:02.064256      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:03.064582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:04.065134      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:05.065582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:06.065803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/13/23 18:33:06.432
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/13/23 18:33:06.432
  E0513 18:33:07.066707      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:08.067376      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:09.067253      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:10.068018      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:11.069076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:33:11.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9613" for this suite. @ 05/13/23 18:33:11.51
  STEP: Destroying namespace "webhook-markers-4414" for this suite. @ 05/13/23 18:33:11.516
• [15.375 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 05/13/23 18:33:11.523
  May 13 18:33:11.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:33:11.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:33:11.537
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:33:11.54
  STEP: creating service in namespace services-6910 @ 05/13/23 18:33:11.544
  STEP: creating service affinity-clusterip in namespace services-6910 @ 05/13/23 18:33:11.544
  STEP: creating replication controller affinity-clusterip in namespace services-6910 @ 05/13/23 18:33:11.548
  I0513 18:33:11.554143      21 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-6910, replica count: 3
  E0513 18:33:12.070117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:13.070920      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:14.071888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:33:14.607669      21 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 13 18:33:14.611: INFO: Creating new exec pod
  E0513 18:33:15.072920      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:16.074209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:17.074470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:33:17.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-6910 exec execpod-affinitywxmmg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  May 13 18:33:17.740: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  May 13 18:33:17.740: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:33:17.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-6910 exec execpod-affinitywxmmg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.151 80'
  May 13 18:33:17.835: INFO: stderr: "+ nc -v -t -w 2 10.32.0.151 80\n+ echo hostName\nConnection to 10.32.0.151 80 port [tcp/http] succeeded!\n"
  May 13 18:33:17.835: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:33:17.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-6910 exec execpod-affinitywxmmg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.32.0.151:80/ ; done'
  May 13 18:33:18.000: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.151:80/\n"
  May 13 18:33:18.000: INFO: stdout: "\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9\naffinity-clusterip-gsmt9"
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Received response from host: affinity-clusterip-gsmt9
  May 13 18:33:18.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 18:33:18.002: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-6910, will wait for the garbage collector to delete the pods @ 05/13/23 18:33:18.016
  E0513 18:33:18.076586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:33:18.077: INFO: Deleting ReplicationController affinity-clusterip took: 7.795628ms
  May 13 18:33:18.178: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.08436ms
  E0513 18:33:19.076767      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:20.076664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-6910" for this suite. @ 05/13/23 18:33:20.4
• [8.880 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 05/13/23 18:33:20.404
  May 13 18:33:20.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename security-context @ 05/13/23 18:33:20.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:33:20.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:33:20.411
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/13/23 18:33:20.412
  E0513 18:33:21.077796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:22.080210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:33:22.419
  May 13 18:33:22.421: INFO: Trying to get logs from node worker00 pod security-context-88153b4e-52c8-4328-80b8-f4ef5c51f1ab container test-container: <nil>
  STEP: delete the pod @ 05/13/23 18:33:22.428
  May 13 18:33:22.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-730" for this suite. @ 05/13/23 18:33:22.439
• [2.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 05/13/23 18:33:22.443
  May 13 18:33:22.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename gc @ 05/13/23 18:33:22.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:33:22.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:33:22.452
  STEP: create the rc @ 05/13/23 18:33:22.455
  W0513 18:33:22.457677      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0513 18:33:23.080115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:24.080328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:25.094596      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:26.098260      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:27.119190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:28.127072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/13/23 18:33:28.511
  STEP: wait for the rc to be deleted @ 05/13/23 18:33:28.542
  E0513 18:33:29.122564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:30.122810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:31.122875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:32.125786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:33.126879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/13/23 18:33:33.544
  E0513 18:33:34.126619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:35.127417      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:36.128092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:37.128856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:38.129413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:39.129485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:40.130187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:41.130610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:42.131117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:43.131655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:44.131808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:45.133415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:46.133876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:47.134181      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:48.134682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:49.136370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:50.138855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:51.139133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:52.139812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:53.140360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:54.141342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:55.142781      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:56.143023      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:57.143495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:58.145363      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:33:59.145517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:00.145742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:01.147805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:02.147810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:03.148437      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/13/23 18:34:03.558
  E0513 18:34:04.148490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:05.150092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:06.150485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:07.151694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:08.154924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:09.156042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:10.160056      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:11.160957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:12.161581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:13.164118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:14.164066      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:15.165352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:16.165382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:17.167066      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:18.168272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:19.168568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:20.170700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:21.171731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:22.172390      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:23.173497      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:24.172895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:25.173218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:26.173862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:27.174899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:28.176975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:29.177586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:30.178141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:31.178711      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:32.179569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:33.182639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:34.182750      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:35.183878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:36.191250      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:37.191629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:38.191739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:39.192242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:40.193237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:41.194114      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:42.197347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:43.198330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:44.201685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:45.201679      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:46.202102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:47.202228      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:48.202662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:49.203169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:50.204292      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:51.206135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:52.206942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:53.207409      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:54.207802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:55.208304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:56.209638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:57.211758      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:58.211999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:34:59.214483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:00.214993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:01.215835      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:02.216310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:03.216538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:03.629: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
  May 13 18:35:03.629: INFO: Deleting pod "simpletest.rc-2gj5z" in namespace "gc-4707"
  May 13 18:35:03.646: INFO: Deleting pod "simpletest.rc-2h5pc" in namespace "gc-4707"
  May 13 18:35:03.663: INFO: Deleting pod "simpletest.rc-2sbjm" in namespace "gc-4707"
  May 13 18:35:03.678: INFO: Deleting pod "simpletest.rc-4rzb2" in namespace "gc-4707"
  May 13 18:35:03.702: INFO: Deleting pod "simpletest.rc-4tj9l" in namespace "gc-4707"
  May 13 18:35:03.719: INFO: Deleting pod "simpletest.rc-58f57" in namespace "gc-4707"
  May 13 18:35:03.736: INFO: Deleting pod "simpletest.rc-5cjwl" in namespace "gc-4707"
  May 13 18:35:03.754: INFO: Deleting pod "simpletest.rc-5kcgk" in namespace "gc-4707"
  May 13 18:35:03.775: INFO: Deleting pod "simpletest.rc-5lx72" in namespace "gc-4707"
  May 13 18:35:03.808: INFO: Deleting pod "simpletest.rc-694wl" in namespace "gc-4707"
  May 13 18:35:03.828: INFO: Deleting pod "simpletest.rc-6fd5c" in namespace "gc-4707"
  May 13 18:35:03.846: INFO: Deleting pod "simpletest.rc-79mks" in namespace "gc-4707"
  May 13 18:35:03.881: INFO: Deleting pod "simpletest.rc-7cz2s" in namespace "gc-4707"
  May 13 18:35:03.927: INFO: Deleting pod "simpletest.rc-7dgns" in namespace "gc-4707"
  May 13 18:35:03.964: INFO: Deleting pod "simpletest.rc-7m2qt" in namespace "gc-4707"
  May 13 18:35:04.013: INFO: Deleting pod "simpletest.rc-7qgnr" in namespace "gc-4707"
  May 13 18:35:04.026: INFO: Deleting pod "simpletest.rc-7rbf2" in namespace "gc-4707"
  May 13 18:35:04.043: INFO: Deleting pod "simpletest.rc-7sgrp" in namespace "gc-4707"
  May 13 18:35:04.096: INFO: Deleting pod "simpletest.rc-7tmmd" in namespace "gc-4707"
  May 13 18:35:04.105: INFO: Deleting pod "simpletest.rc-7x44f" in namespace "gc-4707"
  May 13 18:35:04.125: INFO: Deleting pod "simpletest.rc-7xlth" in namespace "gc-4707"
  May 13 18:35:04.151: INFO: Deleting pod "simpletest.rc-84np9" in namespace "gc-4707"
  May 13 18:35:04.191: INFO: Deleting pod "simpletest.rc-85pn4" in namespace "gc-4707"
  May 13 18:35:04.214: INFO: Deleting pod "simpletest.rc-9fgvj" in namespace "gc-4707"
  E0513 18:35:04.217094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:04.236: INFO: Deleting pod "simpletest.rc-9hnn9" in namespace "gc-4707"
  May 13 18:35:04.279: INFO: Deleting pod "simpletest.rc-9l7m9" in namespace "gc-4707"
  May 13 18:35:04.311: INFO: Deleting pod "simpletest.rc-9nd95" in namespace "gc-4707"
  May 13 18:35:04.350: INFO: Deleting pod "simpletest.rc-9tpft" in namespace "gc-4707"
  May 13 18:35:04.374: INFO: Deleting pod "simpletest.rc-9z7s6" in namespace "gc-4707"
  May 13 18:35:04.408: INFO: Deleting pod "simpletest.rc-bp4lg" in namespace "gc-4707"
  May 13 18:35:04.443: INFO: Deleting pod "simpletest.rc-bts5g" in namespace "gc-4707"
  May 13 18:35:04.498: INFO: Deleting pod "simpletest.rc-cdpxf" in namespace "gc-4707"
  May 13 18:35:04.519: INFO: Deleting pod "simpletest.rc-cm4qg" in namespace "gc-4707"
  May 13 18:35:04.539: INFO: Deleting pod "simpletest.rc-cnmlg" in namespace "gc-4707"
  May 13 18:35:04.557: INFO: Deleting pod "simpletest.rc-cwq6m" in namespace "gc-4707"
  May 13 18:35:04.570: INFO: Deleting pod "simpletest.rc-dsw4f" in namespace "gc-4707"
  May 13 18:35:04.589: INFO: Deleting pod "simpletest.rc-f6hjj" in namespace "gc-4707"
  May 13 18:35:04.629: INFO: Deleting pod "simpletest.rc-fcvpv" in namespace "gc-4707"
  May 13 18:35:04.655: INFO: Deleting pod "simpletest.rc-fgpvj" in namespace "gc-4707"
  May 13 18:35:04.687: INFO: Deleting pod "simpletest.rc-frfr7" in namespace "gc-4707"
  May 13 18:35:04.700: INFO: Deleting pod "simpletest.rc-fsxlb" in namespace "gc-4707"
  May 13 18:35:04.731: INFO: Deleting pod "simpletest.rc-ftvq6" in namespace "gc-4707"
  May 13 18:35:04.789: INFO: Deleting pod "simpletest.rc-fvtl7" in namespace "gc-4707"
  May 13 18:35:04.801: INFO: Deleting pod "simpletest.rc-g72cg" in namespace "gc-4707"
  May 13 18:35:04.937: INFO: Deleting pod "simpletest.rc-gfdrz" in namespace "gc-4707"
  May 13 18:35:04.975: INFO: Deleting pod "simpletest.rc-glf7m" in namespace "gc-4707"
  May 13 18:35:05.015: INFO: Deleting pod "simpletest.rc-gxj48" in namespace "gc-4707"
  May 13 18:35:05.068: INFO: Deleting pod "simpletest.rc-hkvct" in namespace "gc-4707"
  May 13 18:35:05.092: INFO: Deleting pod "simpletest.rc-hlmrh" in namespace "gc-4707"
  May 13 18:35:05.116: INFO: Deleting pod "simpletest.rc-hmsn7" in namespace "gc-4707"
  May 13 18:35:05.133: INFO: Deleting pod "simpletest.rc-hvhc5" in namespace "gc-4707"
  May 13 18:35:05.163: INFO: Deleting pod "simpletest.rc-j5t2d" in namespace "gc-4707"
  May 13 18:35:05.200: INFO: Deleting pod "simpletest.rc-jd9fv" in namespace "gc-4707"
  E0513 18:35:05.218820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:05.222: INFO: Deleting pod "simpletest.rc-jfhld" in namespace "gc-4707"
  May 13 18:35:05.241: INFO: Deleting pod "simpletest.rc-jscwb" in namespace "gc-4707"
  May 13 18:35:05.273: INFO: Deleting pod "simpletest.rc-jtb9j" in namespace "gc-4707"
  May 13 18:35:05.298: INFO: Deleting pod "simpletest.rc-k8gbl" in namespace "gc-4707"
  May 13 18:35:05.336: INFO: Deleting pod "simpletest.rc-khmw2" in namespace "gc-4707"
  May 13 18:35:05.372: INFO: Deleting pod "simpletest.rc-kjbkz" in namespace "gc-4707"
  May 13 18:35:05.396: INFO: Deleting pod "simpletest.rc-kkxsj" in namespace "gc-4707"
  May 13 18:35:05.406: INFO: Deleting pod "simpletest.rc-ktxjf" in namespace "gc-4707"
  May 13 18:35:05.437: INFO: Deleting pod "simpletest.rc-lb22w" in namespace "gc-4707"
  May 13 18:35:05.450: INFO: Deleting pod "simpletest.rc-lrn7d" in namespace "gc-4707"
  May 13 18:35:05.492: INFO: Deleting pod "simpletest.rc-n4ss7" in namespace "gc-4707"
  May 13 18:35:05.503: INFO: Deleting pod "simpletest.rc-nwp85" in namespace "gc-4707"
  May 13 18:35:05.519: INFO: Deleting pod "simpletest.rc-p7r87" in namespace "gc-4707"
  May 13 18:35:05.547: INFO: Deleting pod "simpletest.rc-p9cf8" in namespace "gc-4707"
  May 13 18:35:05.591: INFO: Deleting pod "simpletest.rc-plhdh" in namespace "gc-4707"
  May 13 18:35:05.603: INFO: Deleting pod "simpletest.rc-qfkdb" in namespace "gc-4707"
  May 13 18:35:05.615: INFO: Deleting pod "simpletest.rc-qntpw" in namespace "gc-4707"
  May 13 18:35:05.633: INFO: Deleting pod "simpletest.rc-qxgkl" in namespace "gc-4707"
  May 13 18:35:05.690: INFO: Deleting pod "simpletest.rc-qzds5" in namespace "gc-4707"
  May 13 18:35:05.747: INFO: Deleting pod "simpletest.rc-rhmkj" in namespace "gc-4707"
  May 13 18:35:05.768: INFO: Deleting pod "simpletest.rc-rjcwx" in namespace "gc-4707"
  May 13 18:35:05.784: INFO: Deleting pod "simpletest.rc-rrfhb" in namespace "gc-4707"
  May 13 18:35:05.821: INFO: Deleting pod "simpletest.rc-s5vt5" in namespace "gc-4707"
  May 13 18:35:05.831: INFO: Deleting pod "simpletest.rc-sqpcc" in namespace "gc-4707"
  May 13 18:35:05.843: INFO: Deleting pod "simpletest.rc-sqxv6" in namespace "gc-4707"
  May 13 18:35:05.855: INFO: Deleting pod "simpletest.rc-ss2nb" in namespace "gc-4707"
  May 13 18:35:05.869: INFO: Deleting pod "simpletest.rc-ssr4b" in namespace "gc-4707"
  May 13 18:35:05.915: INFO: Deleting pod "simpletest.rc-tcr9l" in namespace "gc-4707"
  May 13 18:35:05.944: INFO: Deleting pod "simpletest.rc-tj94v" in namespace "gc-4707"
  May 13 18:35:05.966: INFO: Deleting pod "simpletest.rc-tnq2n" in namespace "gc-4707"
  May 13 18:35:05.987: INFO: Deleting pod "simpletest.rc-tphnr" in namespace "gc-4707"
  May 13 18:35:06.010: INFO: Deleting pod "simpletest.rc-v66c4" in namespace "gc-4707"
  May 13 18:35:06.026: INFO: Deleting pod "simpletest.rc-vbbhk" in namespace "gc-4707"
  May 13 18:35:06.046: INFO: Deleting pod "simpletest.rc-vgxgw" in namespace "gc-4707"
  May 13 18:35:06.057: INFO: Deleting pod "simpletest.rc-vwt7d" in namespace "gc-4707"
  May 13 18:35:06.089: INFO: Deleting pod "simpletest.rc-w7448" in namespace "gc-4707"
  May 13 18:35:06.132: INFO: Deleting pod "simpletest.rc-wbvh8" in namespace "gc-4707"
  May 13 18:35:06.145: INFO: Deleting pod "simpletest.rc-wd5x4" in namespace "gc-4707"
  May 13 18:35:06.158: INFO: Deleting pod "simpletest.rc-wf7km" in namespace "gc-4707"
  May 13 18:35:06.174: INFO: Deleting pod "simpletest.rc-wkqwj" in namespace "gc-4707"
  May 13 18:35:06.186: INFO: Deleting pod "simpletest.rc-wnz2h" in namespace "gc-4707"
  May 13 18:35:06.211: INFO: Deleting pod "simpletest.rc-x6f4d" in namespace "gc-4707"
  E0513 18:35:06.218925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:06.252: INFO: Deleting pod "simpletest.rc-xlhnd" in namespace "gc-4707"
  May 13 18:35:06.265: INFO: Deleting pod "simpletest.rc-xm7s4" in namespace "gc-4707"
  May 13 18:35:06.279: INFO: Deleting pod "simpletest.rc-z727z" in namespace "gc-4707"
  May 13 18:35:06.296: INFO: Deleting pod "simpletest.rc-zxsnw" in namespace "gc-4707"
  May 13 18:35:06.368: INFO: Deleting pod "simpletest.rc-zzmxv" in namespace "gc-4707"
  May 13 18:35:06.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4707" for this suite. @ 05/13/23 18:35:06.4
• [104.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 05/13/23 18:35:06.481
  May 13 18:35:06.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-webhook @ 05/13/23 18:35:06.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:06.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:06.536
  STEP: Setting up server cert @ 05/13/23 18:35:06.542
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/13/23 18:35:06.914
  STEP: Deploying the custom resource conversion webhook pod @ 05/13/23 18:35:06.93
  STEP: Wait for the deployment to be ready @ 05/13/23 18:35:06.971
  May 13 18:35:06.987: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0513 18:35:07.220352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:08.222643      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:09.023: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 18, 35, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 18, 35, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 18, 35, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 18, 35, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 18:35:09.222877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:10.224115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:11.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 18, 35, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 18, 35, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 18, 35, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 18, 35, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 18:35:11.225291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:12.226320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 18:35:13.025
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 18:35:13.032
  E0513 18:35:13.230047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:14.034: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 13 18:35:14.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:35:14.230061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:15.230662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:16.232571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/13/23 18:35:16.615
  STEP: Create a v2 custom resource @ 05/13/23 18:35:16.629
  STEP: List CRs in v1 @ 05/13/23 18:35:16.645
  STEP: List CRs in v2 @ 05/13/23 18:35:16.65
  May 13 18:35:16.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-1349" for this suite. @ 05/13/23 18:35:17.18
• [10.704 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 05/13/23 18:35:17.191
  May 13 18:35:17.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 18:35:17.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:17.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:17.208
  STEP: Creating configMap with name configmap-test-volume-map-2dd7540b-b616-4622-b9d6-4ae8ecc12471 @ 05/13/23 18:35:17.21
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:35:17.213
  E0513 18:35:17.232952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:18.233121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:19.234150      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:20.234257      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:35:21.224
  May 13 18:35:21.226: INFO: Trying to get logs from node worker00 pod pod-configmaps-94f2e4e6-6833-4f5f-8cb6-7ad602cfb673 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 18:35:21.231
  E0513 18:35:21.234332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:21.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2498" for this suite. @ 05/13/23 18:35:21.248
• [4.065 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/13/23 18:35:21.255
  May 13 18:35:21.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-runtime @ 05/13/23 18:35:21.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:21.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:21.267
  STEP: create the container @ 05/13/23 18:35:21.269
  W0513 18:35:21.274406      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/13/23 18:35:21.274
  E0513 18:35:22.234940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:23.235313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:24.236148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/13/23 18:35:24.291
  STEP: the container should be terminated @ 05/13/23 18:35:24.292
  STEP: the termination message should be set @ 05/13/23 18:35:24.292
  May 13 18:35:24.292: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/13/23 18:35:24.292
  May 13 18:35:24.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5546" for this suite. @ 05/13/23 18:35:24.324
• [3.076 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/13/23 18:35:24.331
  May 13 18:35:24.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename deployment @ 05/13/23 18:35:24.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:24.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:24.35
  May 13 18:35:24.352: INFO: Creating deployment "test-recreate-deployment"
  May 13 18:35:24.361: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  May 13 18:35:24.375: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0513 18:35:25.236254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:26.237719      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:26.379: INFO: Waiting deployment "test-recreate-deployment" to complete
  May 13 18:35:26.380: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  May 13 18:35:26.385: INFO: Updating deployment test-recreate-deployment
  May 13 18:35:26.385: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  May 13 18:35:26.457: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3802  cc582d60-5ccb-42a4-89fc-aa42bb3e72fa 16324 2 2023-05-13 18:35:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-13 18:35:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 18:35:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039e4b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-13 18:35:26 +0000 UTC,LastTransitionTime:2023-05-13 18:35:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-05-13 18:35:26 +0000 UTC,LastTransitionTime:2023-05-13 18:35:24 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  May 13 18:35:26.461: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-3802  4c6383c2-43a2-42ca-bc6d-ec295b6d6395 16322 1 2023-05-13 18:35:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment cc582d60-5ccb-42a4-89fc-aa42bb3e72fa 0xc002045e07 0xc002045e08}] [] [{kube-controller-manager Update apps/v1 2023-05-13 18:35:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc582d60-5ccb-42a4-89fc-aa42bb3e72fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 18:35:26 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002045ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 13 18:35:26.462: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  May 13 18:35:26.463: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-3802  fe577528-5b88-495b-a68a-8f03fea1dcce 16313 2 2023-05-13 18:35:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment cc582d60-5ccb-42a4-89fc-aa42bb3e72fa 0xc002045f17 0xc002045f18}] [] [{kube-controller-manager Update apps/v1 2023-05-13 18:35:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc582d60-5ccb-42a4-89fc-aa42bb3e72fa\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 18:35:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002045fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 13 18:35:26.465: INFO: Pod "test-recreate-deployment-54757ffd6c-q9dkn" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-q9dkn test-recreate-deployment-54757ffd6c- deployment-3802  522b7736-b46d-4ad8-99bc-17d1519f5658 16325 0 2023-05-13 18:35:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 4c6383c2-43a2-42ca-bc6d-ec295b6d6395 0xc0039e4ef7 0xc0039e4ef8}] [] [{kube-controller-manager Update v1 2023-05-13 18:35:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4c6383c2-43a2-42ca-bc6d-ec295b6d6395\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:35:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-krz4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krz4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:35:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:35:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:35:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:35:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:,StartTime:2023-05-13 18:35:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:35:26.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3802" for this suite. @ 05/13/23 18:35:26.468
• [2.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 05/13/23 18:35:26.472
  May 13 18:35:26.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename namespaces @ 05/13/23 18:35:26.473
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:26.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:26.482
  STEP: Creating a test namespace @ 05/13/23 18:35:26.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:26.493
  STEP: Creating a pod in the namespace @ 05/13/23 18:35:26.496
  STEP: Waiting for the pod to have running status @ 05/13/23 18:35:26.5
  E0513 18:35:27.238429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:28.238777      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 05/13/23 18:35:28.505
  STEP: Waiting for the namespace to be removed. @ 05/13/23 18:35:28.51
  E0513 18:35:29.239494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:30.239817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:31.241001      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:32.242812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:33.243660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:34.245473      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:35.245454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:36.246485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:37.248405      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:38.250695      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:39.251163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/13/23 18:35:39.517
  STEP: Verifying there are no pods in the namespace @ 05/13/23 18:35:39.531
  May 13 18:35:39.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1933" for this suite. @ 05/13/23 18:35:39.534
  STEP: Destroying namespace "nsdeletetest-1022" for this suite. @ 05/13/23 18:35:39.536
  May 13 18:35:39.537: INFO: Namespace nsdeletetest-1022 was already deleted
  STEP: Destroying namespace "nsdeletetest-1835" for this suite. @ 05/13/23 18:35:39.537
• [13.068 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 05/13/23 18:35:39.54
  May 13 18:35:39.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/13/23 18:35:39.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:39.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:39.554
  May 13 18:35:39.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:35:40.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4482" for this suite. @ 05/13/23 18:35:40.107
• [0.570 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 05/13/23 18:35:40.112
  May 13 18:35:40.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:35:40.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:40.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:40.122
  STEP: Creating configMap with name projected-configmap-test-volume-aea85925-1245-4091-badd-d0a6e0961248 @ 05/13/23 18:35:40.123
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:35:40.125
  E0513 18:35:40.251910      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:41.252065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:35:42.143
  May 13 18:35:42.147: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-4fc7bd84-4063-40b1-872e-dd96d761b202 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 18:35:42.151
  May 13 18:35:42.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4034" for this suite. @ 05/13/23 18:35:42.17
• [2.061 seconds]
------------------------------
SSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/13/23 18:35:42.174
  May 13 18:35:42.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/13/23 18:35:42.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:42.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:42.185
  STEP: mirroring a new custom Endpoint @ 05/13/23 18:35:42.193
  May 13 18:35:42.196: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0513 18:35:42.254433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:43.255759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 05/13/23 18:35:44.203
  May 13 18:35:44.212: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0513 18:35:44.256170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:45.257684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 05/13/23 18:35:46.215
  May 13 18:35:46.224: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0513 18:35:46.258540      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:47.259875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:48.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-7784" for this suite. @ 05/13/23 18:35:48.237
• [6.083 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/13/23 18:35:48.259
  May 13 18:35:48.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:35:48.260502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename conformance-tests @ 05/13/23 18:35:48.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:48.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:48.271
  STEP: Getting node addresses @ 05/13/23 18:35:48.274
  May 13 18:35:48.274: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  May 13 18:35:48.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-2765" for this suite. @ 05/13/23 18:35:48.279
• [0.024 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/13/23 18:35:48.283
  May 13 18:35:48.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubelet-test @ 05/13/23 18:35:48.283
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:48.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:48.292
  May 13 18:35:48.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6905" for this suite. @ 05/13/23 18:35:48.303
• [0.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/13/23 18:35:48.307
  May 13 18:35:48.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 18:35:48.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:48.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:48.315
  STEP: Creating secret with name s-test-opt-del-0e0cad1d-054d-41eb-a519-505fa4b61f2c @ 05/13/23 18:35:48.318
  STEP: Creating secret with name s-test-opt-upd-3f99be97-c731-4047-994f-a371ef474069 @ 05/13/23 18:35:48.32
  STEP: Creating the pod @ 05/13/23 18:35:48.324
  E0513 18:35:49.260914      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:50.262011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-0e0cad1d-054d-41eb-a519-505fa4b61f2c @ 05/13/23 18:35:50.356
  STEP: Updating secret s-test-opt-upd-3f99be97-c731-4047-994f-a371ef474069 @ 05/13/23 18:35:50.37
  STEP: Creating secret with name s-test-opt-create-1e6e1a45-b70d-4f84-8287-f7b5d367fcff @ 05/13/23 18:35:50.373
  STEP: waiting to observe update in volume @ 05/13/23 18:35:50.376
  E0513 18:35:51.262758      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:52.263280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:52.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6475" for this suite. @ 05/13/23 18:35:52.391
• [4.096 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 05/13/23 18:35:52.403
  May 13 18:35:52.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replication-controller @ 05/13/23 18:35:52.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:52.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:52.415
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/13/23 18:35:52.416
  E0513 18:35:53.264266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:54.265230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 05/13/23 18:35:54.425
  STEP: Then the orphan pod is adopted @ 05/13/23 18:35:54.439
  E0513 18:35:55.265730      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:35:55.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4767" for this suite. @ 05/13/23 18:35:55.454
• [3.056 seconds]
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/13/23 18:35:55.459
  May 13 18:35:55.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sched-pred @ 05/13/23 18:35:55.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:55.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:55.475
  May 13 18:35:55.476: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 13 18:35:55.479: INFO: Waiting for terminating namespaces to be deleted...
  May 13 18:35:55.480: INFO: 
  Logging pods the apiserver thinks is on node worker00 before test
  May 13 18:35:55.483: INFO: etcd-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.483: INFO: 	Container etcd ready: true, restart count 0
  May 13 18:35:55.483: INFO: gobetween-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.483: INFO: 	Container gobetween ready: true, restart count 0
  May 13 18:35:55.483: INFO: kube-apiserver-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.483: INFO: 	Container kube-apiserver ready: true, restart count 0
  May 13 18:35:55.483: INFO: kube-controller-manager-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.483: INFO: 	Container kube-controller-manager ready: true, restart count 0
  May 13 18:35:55.483: INFO: kube-proxy-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.483: INFO: 	Container kube-proxy ready: true, restart count 0
  May 13 18:35:55.483: INFO: kube-scheduler-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.483: INFO: 	Container kube-scheduler ready: true, restart count 0
  May 13 18:35:55.483: INFO: calico-node-xsr47 from networking started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.484: INFO: 	Container calico-node ready: true, restart count 0
  May 13 18:35:55.484: INFO: metallb-speaker-v7flx from networking started at 2023-05-13 18:18:58 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.484: INFO: 	Container speaker ready: true, restart count 0
  May 13 18:35:55.484: INFO: pod-adoption from replication-controller-4767 started at 2023-05-13 18:35:52 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.484: INFO: 	Container pod-adoption ready: true, restart count 0
  May 13 18:35:55.484: INFO: pod-secrets-328bec61-7ec8-4c77-9e6b-4cda2f71d0ef from secrets-6475 started at 2023-05-13 18:35:48 +0000 UTC (3 container statuses recorded)
  May 13 18:35:55.484: INFO: 	Container creates-volume-test ready: true, restart count 0
  May 13 18:35:55.484: INFO: 	Container dels-volume-test ready: true, restart count 0
  May 13 18:35:55.484: INFO: 	Container upds-volume-test ready: true, restart count 0
  May 13 18:35:55.484: INFO: sonobuoy from sonobuoy started at 2023-05-13 18:11:13 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.484: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 13 18:35:55.484: INFO: sonobuoy-e2e-job-dc09634ed87e4d2d from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 18:35:55.484: INFO: 	Container e2e ready: true, restart count 0
  May 13 18:35:55.484: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 18:35:55.484: INFO: sonobuoy-systemd-logs-daemon-set-0619921c091243c5-6tddn from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 18:35:55.484: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 18:35:55.484: INFO: 	Container systemd-logs ready: true, restart count 0
  May 13 18:35:55.484: INFO: ceph-csi-cephfs-nodeplugin-rhhl2 from storage started at 2023-05-13 18:18:58 +0000 UTC (3 container statuses recorded)
  May 13 18:35:55.484: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 18:35:55.484: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 18:35:55.484: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 18:35:55.484: INFO: ceph-csi-rbd-nodeplugin-t44kh from storage started at 2023-05-13 18:18:58 +0000 UTC (3 container statuses recorded)
  May 13 18:35:55.484: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 18:35:55.484: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 18:35:55.484: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 18:35:55.484: INFO: ceph-mon-worker00-79cd8cd599-5skt9 from storage started at 2023-05-13 18:18:58 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.484: INFO: 	Container ceph-mon ready: true, restart count 0
  May 13 18:35:55.484: INFO: 
  Logging pods the apiserver thinks is on node worker01 before test
  May 13 18:35:55.489: INFO: coredns-5bf6b5d445-frtlk from kube-system started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container coredns ready: true, restart count 0
  May 13 18:35:55.490: INFO: coredns-5bf6b5d445-wv86k from kube-system started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container coredns ready: true, restart count 0
  May 13 18:35:55.490: INFO: gobetween-worker01 from kube-system started at 2023-05-13 18:05:13 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container gobetween ready: true, restart count 0
  May 13 18:35:55.490: INFO: kube-proxy-worker01 from kube-system started at 2023-05-13 18:05:13 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container kube-proxy ready: true, restart count 0
  May 13 18:35:55.490: INFO: kubernetes-dashboard-67c9dc7898-qhknx from kube-system started at 2023-05-13 18:06:09 +0000 UTC (2 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May 13 18:35:55.490: INFO: calico-kube-controllers-849bffbf8c-mh9pf from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 13 18:35:55.490: INFO: calico-node-bsrzk from networking started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container calico-node ready: true, restart count 0
  May 13 18:35:55.490: INFO: calico-typha-7f5df955fb-2bg4l from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container calico-typha ready: true, restart count 0
  May 13 18:35:55.490: INFO: metallb-controller-64cc69b5c4-wd4dh from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container controller ready: true, restart count 0
  May 13 18:35:55.490: INFO: metallb-speaker-wntmv from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container speaker ready: true, restart count 0
  May 13 18:35:55.490: INFO: sonobuoy-systemd-logs-daemon-set-0619921c091243c5-wlgqk from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container systemd-logs ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-csi-cephfs-nodeplugin-c7nqc from storage started at 2023-05-13 18:05:58 +0000 UTC (3 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-csi-cephfs-provisioner-b795fb565-6gcfc from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-csi-cephfs-provisioner-b795fb565-8656n from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-csi-cephfs-provisioner-b795fb565-m982k from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-csi-rbd-nodeplugin-xgqs8 from storage started at 2023-05-13 18:05:58 +0000 UTC (3 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-csi-rbd-provisioner-674549bd88-lkl8l from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-csi-rbd-provisioner-674549bd88-n9pkj from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-csi-rbd-provisioner-674549bd88-nk4kp from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 18:35:55.490: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-mds-worker01-9b6db8bb9-q6w24 from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container ceph-mds ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-mgr-worker01-5bc7cddf9f-pmtvr from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container ceph-mgr ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-osd-worker01-594bc8755f-r5bdz from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container ceph-osd ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-rgw-worker01-64f8846fdb-jzw8c from storage started at 2023-05-13 18:05:59 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container ceph-rgw ready: true, restart count 0
  May 13 18:35:55.490: INFO: ceph-setup-q5j8d from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container ceph ready: false, restart count 0
  May 13 18:35:55.490: INFO: snapshot-controller-7dd49c567-9rfhc from storage started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container snapshot-controller ready: true, restart count 0
  May 13 18:35:55.490: INFO: snapshot-controller-7dd49c567-h5tj7 from storage started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 18:35:55.490: INFO: 	Container snapshot-controller ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/13/23 18:35:55.49
  E0513 18:35:56.266435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:57.266658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/13/23 18:35:57.505
  STEP: Trying to apply a random label on the found node. @ 05/13/23 18:35:57.52
  STEP: verifying the node has the label kubernetes.io/e2e-5bf300ee-d945-4bde-b5f3-ecdca99f3f7d 42 @ 05/13/23 18:35:57.531
  STEP: Trying to relaunch the pod, now with labels. @ 05/13/23 18:35:57.541
  E0513 18:35:58.267822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:35:59.268465      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-5bf300ee-d945-4bde-b5f3-ecdca99f3f7d off the node worker00 @ 05/13/23 18:35:59.561
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-5bf300ee-d945-4bde-b5f3-ecdca99f3f7d @ 05/13/23 18:35:59.588
  May 13 18:35:59.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3492" for this suite. @ 05/13/23 18:35:59.595
• [4.141 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 05/13/23 18:35:59.602
  May 13 18:35:59.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:35:59.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:35:59.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:35:59.617
  STEP: creating service in namespace services-3489 @ 05/13/23 18:35:59.618
  STEP: creating service affinity-nodeport in namespace services-3489 @ 05/13/23 18:35:59.619
  STEP: creating replication controller affinity-nodeport in namespace services-3489 @ 05/13/23 18:35:59.628
  I0513 18:35:59.632214      21 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-3489, replica count: 3
  E0513 18:36:00.269511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:01.271514      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:02.272397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:36:02.684575      21 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 13 18:36:02.687: INFO: Creating new exec pod
  E0513 18:36:03.274535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:04.274862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:05.275307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:36:05.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-3489 exec execpod-affinity5ttsk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  May 13 18:36:05.814: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  May 13 18:36:05.814: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:36:05.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-3489 exec execpod-affinity5ttsk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.180 80'
  May 13 18:36:05.888: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.180 80\nConnection to 10.32.0.180 80 port [tcp/http] succeeded!\n"
  May 13 18:36:05.888: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:36:05.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-3489 exec execpod-affinity5ttsk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.100 31937'
  May 13 18:36:05.960: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.100 31937\nConnection to 192.168.58.100 31937 port [tcp/*] succeeded!\n"
  May 13 18:36:05.960: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:36:05.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-3489 exec execpod-affinity5ttsk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.101 31937'
  May 13 18:36:06.057: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.101 31937\nConnection to 192.168.58.101 31937 port [tcp/*] succeeded!\n"
  May 13 18:36:06.057: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:36:06.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-3489 exec execpod-affinity5ttsk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.58.100:31937/ ; done'
  May 13 18:36:06.196: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31937/\n"
  May 13 18:36:06.196: INFO: stdout: "\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq\naffinity-nodeport-pkcvq"
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Received response from host: affinity-nodeport-pkcvq
  May 13 18:36:06.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 18:36:06.198: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-3489, will wait for the garbage collector to delete the pods @ 05/13/23 18:36:06.21
  May 13 18:36:06.270: INFO: Deleting ReplicationController affinity-nodeport took: 3.868664ms
  E0513 18:36:06.277608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:36:06.370: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.29539ms
  E0513 18:36:07.279072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:08.279117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-3489" for this suite. @ 05/13/23 18:36:08.396
• [8.798 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 05/13/23 18:36:08.401
  May 13 18:36:08.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 18:36:08.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:36:08.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:36:08.409
  STEP: Creating configMap with name configmap-test-upd-6e105b61-1063-4512-9a09-6f9dc08f4467 @ 05/13/23 18:36:08.411
  STEP: Creating the pod @ 05/13/23 18:36:08.413
  E0513 18:36:09.279663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:10.279830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 05/13/23 18:36:10.426
  STEP: Waiting for pod with binary data @ 05/13/23 18:36:10.435
  May 13 18:36:10.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1728" for this suite. @ 05/13/23 18:36:10.441
• [2.054 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 05/13/23 18:36:10.455
  May 13 18:36:10.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename podtemplate @ 05/13/23 18:36:10.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:36:10.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:36:10.464
  May 13 18:36:10.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-9352" for this suite. @ 05/13/23 18:36:10.482
• [0.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 05/13/23 18:36:10.484
  May 13 18:36:10.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 18:36:10.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:36:10.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:36:10.492
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/13/23 18:36:10.494
  May 13 18:36:10.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-9878 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 13 18:36:10.537: INFO: stderr: ""
  May 13 18:36:10.537: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/13/23 18:36:10.537
  May 13 18:36:10.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-9878 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  May 13 18:36:10.589: INFO: stderr: ""
  May 13 18:36:10.589: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/13/23 18:36:10.589
  May 13 18:36:10.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-9878 delete pods e2e-test-httpd-pod'
  E0513 18:36:11.280840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:12.280814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:36:13.021: INFO: stderr: ""
  May 13 18:36:13.021: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 13 18:36:13.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9878" for this suite. @ 05/13/23 18:36:13.024
• [2.542 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/13/23 18:36:13.027
  May 13 18:36:13.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 18:36:13.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:36:13.035
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:36:13.036
  STEP: creating a Pod with a static label @ 05/13/23 18:36:13.041
  STEP: watching for Pod to be ready @ 05/13/23 18:36:13.046
  May 13 18:36:13.047: INFO: observed Pod pod-test in namespace pods-8404 in phase Pending with labels: map[test-pod-static:true] & conditions []
  May 13 18:36:13.055: INFO: observed Pod pod-test in namespace pods-8404 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC  }]
  May 13 18:36:13.060: INFO: observed Pod pod-test in namespace pods-8404 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC  }]
  E0513 18:36:13.281996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:36:13.521: INFO: observed Pod pod-test in namespace pods-8404 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC  }]
  May 13 18:36:14.022: INFO: Found Pod pod-test in namespace pods-8404 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-13 18:36:13 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 05/13/23 18:36:14.026
  STEP: getting the Pod and ensuring that it's patched @ 05/13/23 18:36:14.029
  STEP: replacing the Pod's status Ready condition to False @ 05/13/23 18:36:14.032
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/13/23 18:36:14.037
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/13/23 18:36:14.037
  STEP: watching for the Pod to be deleted @ 05/13/23 18:36:14.045
  May 13 18:36:14.046: INFO: observed event type MODIFIED
  E0513 18:36:14.283547      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:15.284238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:36:16.039: INFO: observed event type MODIFIED
  May 13 18:36:16.154: INFO: observed event type MODIFIED
  May 13 18:36:16.236: INFO: observed event type MODIFIED
  E0513 18:36:16.287536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:36:17.038: INFO: observed event type MODIFIED
  May 13 18:36:17.046: INFO: observed event type MODIFIED
  May 13 18:36:17.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8404" for this suite. @ 05/13/23 18:36:17.053
• [4.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 05/13/23 18:36:17.059
  May 13 18:36:17.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename dns @ 05/13/23 18:36:17.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:36:17.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:36:17.074
  STEP: Creating a test headless service @ 05/13/23 18:36:17.075
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3155.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3155.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/13/23 18:36:17.078
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3155.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3155.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/13/23 18:36:17.078
  STEP: creating a pod to probe DNS @ 05/13/23 18:36:17.078
  STEP: submitting the pod to kubernetes @ 05/13/23 18:36:17.078
  E0513 18:36:17.288456      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:18.289071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/13/23 18:36:19.09
  STEP: looking for the results for each expected name from probers @ 05/13/23 18:36:19.094
  May 13 18:36:19.101: INFO: DNS probes using dns-3155/dns-test-fdbe7acf-1291-42d2-9a92-3bda3864509e succeeded

  May 13 18:36:19.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 18:36:19.103
  STEP: deleting the test headless service @ 05/13/23 18:36:19.111
  STEP: Destroying namespace "dns-3155" for this suite. @ 05/13/23 18:36:19.127
• [2.070 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 05/13/23 18:36:19.13
  May 13 18:36:19.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sysctl @ 05/13/23 18:36:19.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:36:19.138
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:36:19.14
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/13/23 18:36:19.143
  STEP: Watching for error events or started pod @ 05/13/23 18:36:19.148
  E0513 18:36:19.289981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:20.291803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 05/13/23 18:36:21.15
  E0513 18:36:21.292316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:22.292443      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 05/13/23 18:36:23.156
  STEP: Getting logs from the pod @ 05/13/23 18:36:23.156
  STEP: Checking that the sysctl is actually updated @ 05/13/23 18:36:23.159
  May 13 18:36:23.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-1969" for this suite. @ 05/13/23 18:36:23.161
• [4.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 05/13/23 18:36:23.168
  May 13 18:36:23.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename subpath @ 05/13/23 18:36:23.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:36:23.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:36:23.177
  STEP: Setting up data @ 05/13/23 18:36:23.178
  STEP: Creating pod pod-subpath-test-configmap-4ztf @ 05/13/23 18:36:23.182
  STEP: Creating a pod to test atomic-volume-subpath @ 05/13/23 18:36:23.182
  E0513 18:36:23.294075      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:24.294612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:25.295417      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:26.299713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:27.300402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:28.300461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:29.300580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:30.301065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:31.302303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:32.303322      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:33.304498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:34.305410      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:35.306864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:36.308076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:37.311085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:38.312008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:39.312425      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:40.313407      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:41.315030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:42.317310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:43.318387      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:44.318193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:45.319264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:46.319522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:36:47.242
  May 13 18:36:47.246: INFO: Trying to get logs from node worker00 pod pod-subpath-test-configmap-4ztf container test-container-subpath-configmap-4ztf: <nil>
  STEP: delete the pod @ 05/13/23 18:36:47.253
  STEP: Deleting pod pod-subpath-test-configmap-4ztf @ 05/13/23 18:36:47.26
  May 13 18:36:47.260: INFO: Deleting pod "pod-subpath-test-configmap-4ztf" in namespace "subpath-8376"
  May 13 18:36:47.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8376" for this suite. @ 05/13/23 18:36:47.263
• [24.098 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 05/13/23 18:36:47.267
  May 13 18:36:47.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:36:47.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:36:47.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:36:47.276
  STEP: Creating the pod @ 05/13/23 18:36:47.277
  E0513 18:36:47.319640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:48.319760      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:49.320348      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:36:49.799: INFO: Successfully updated pod "annotationupdate580b889b-f395-494a-8c8c-fe83496ccc7f"
  E0513 18:36:50.321571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:51.321957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:52.322648      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:53.322853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:36:53.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7786" for this suite. @ 05/13/23 18:36:53.836
• [6.573 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 05/13/23 18:36:53.84
  May 13 18:36:53.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 18:36:53.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:36:53.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:36:53.85
  STEP: Counting existing ResourceQuota @ 05/13/23 18:36:53.851
  E0513 18:36:54.323379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:55.324103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:56.324657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:57.326617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:36:58.328697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/13/23 18:36:58.853
  STEP: Ensuring resource quota status is calculated @ 05/13/23 18:36:58.861
  E0513 18:36:59.329631      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:00.329907      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 05/13/23 18:37:00.866
  STEP: Ensuring resource quota status captures replication controller creation @ 05/13/23 18:37:00.882
  E0513 18:37:01.330623      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:02.330729      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 05/13/23 18:37:02.892
  STEP: Ensuring resource quota status released usage @ 05/13/23 18:37:02.898
  E0513 18:37:03.330842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:04.331230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:37:04.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3001" for this suite. @ 05/13/23 18:37:04.908
• [11.073 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 05/13/23 18:37:04.912
  May 13 18:37:04.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename dns @ 05/13/23 18:37:04.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:37:04.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:37:04.928
  STEP: Creating a test headless service @ 05/13/23 18:37:04.93
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8779 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8779;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8779 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8779;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8779.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8779.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8779.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8779.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8779.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8779.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8779.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8779.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8779.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8779.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8779.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8779.svc;check="$$(dig +notcp +noall +answer +search 118.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.118_tcp@PTR;sleep 1; done
   @ 05/13/23 18:37:04.94
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8779 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8779;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8779 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8779;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8779.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8779.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8779.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8779.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8779.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8779.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8779.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8779.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8779.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8779.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8779.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8779.svc;check="$$(dig +notcp +noall +answer +search 118.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.118_tcp@PTR;sleep 1; done
   @ 05/13/23 18:37:04.94
  STEP: creating a pod to probe DNS @ 05/13/23 18:37:04.94
  STEP: submitting the pod to kubernetes @ 05/13/23 18:37:04.94
  E0513 18:37:05.332054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:06.332329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/13/23 18:37:06.959
  STEP: looking for the results for each expected name from probers @ 05/13/23 18:37:06.963
  May 13 18:37:06.968: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.972: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.974: INFO: Unable to read wheezy_udp@dns-test-service.dns-8779 from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.977: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8779 from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.978: INFO: Unable to read wheezy_udp@dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.981: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.983: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.984: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.986: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.988: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.989: INFO: Unable to read 10.32.0.118_udp@PTR from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.990: INFO: Unable to read 10.32.0.118_tcp@PTR from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.991: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.993: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.994: INFO: Unable to read jessie_udp@dns-test-service.dns-8779 from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.995: INFO: Unable to read jessie_tcp@dns-test-service.dns-8779 from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.996: INFO: Unable to read jessie_udp@dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.997: INFO: Unable to read jessie_tcp@dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.998: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:06.999: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:07.001: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:07.003: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:07.003: INFO: Unable to read 10.32.0.118_udp@PTR from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:07.004: INFO: Unable to read 10.32.0.118_tcp@PTR from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:07.004: INFO: Lookups using dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8779 wheezy_tcp@dns-test-service.dns-8779 wheezy_udp@dns-test-service.dns-8779.svc wheezy_tcp@dns-test-service.dns-8779.svc wheezy_udp@_http._tcp.dns-test-service.dns-8779.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8779.svc wheezy_udp@_http._tcp.test-service-2.dns-8779.svc wheezy_tcp@_http._tcp.test-service-2.dns-8779.svc 10.32.0.118_udp@PTR 10.32.0.118_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8779 jessie_tcp@dns-test-service.dns-8779 jessie_udp@dns-test-service.dns-8779.svc jessie_tcp@dns-test-service.dns-8779.svc jessie_udp@_http._tcp.dns-test-service.dns-8779.svc jessie_tcp@_http._tcp.dns-test-service.dns-8779.svc jessie_udp@_http._tcp.test-service-2.dns-8779.svc jessie_tcp@_http._tcp.test-service-2.dns-8779.svc 10.32.0.118_udp@PTR 10.32.0.118_tcp@PTR]

  E0513 18:37:07.332993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:08.333349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:09.338685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:10.341341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:11.341812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:37:12.008: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.011: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.013: INFO: Unable to read wheezy_udp@dns-test-service.dns-8779 from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.016: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8779 from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.018: INFO: Unable to read wheezy_udp@dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.020: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.022: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.024: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.025: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.026: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.030: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.031: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.035: INFO: Unable to read jessie_udp@dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:12.043: INFO: Lookups using dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8779 wheezy_tcp@dns-test-service.dns-8779 wheezy_udp@dns-test-service.dns-8779.svc wheezy_tcp@dns-test-service.dns-8779.svc wheezy_udp@_http._tcp.dns-test-service.dns-8779.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8779.svc wheezy_udp@_http._tcp.test-service-2.dns-8779.svc wheezy_tcp@_http._tcp.test-service-2.dns-8779.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8779.svc]

  E0513 18:37:12.342833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:13.343332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:14.343502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:15.344560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:16.345084      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:37:17.011: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:17.017: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:17.020: INFO: Unable to read wheezy_udp@dns-test-service.dns-8779 from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:17.022: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8779 from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:17.033: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:17.038: INFO: Unable to read jessie_udp@dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:17.044: INFO: Lookups using dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8779 wheezy_tcp@dns-test-service.dns-8779 jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-8779.svc]

  E0513 18:37:17.347321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:18.349026      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:19.349326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:20.351571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:21.352398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:37:22.010: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:22.016: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:22.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-8779 from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:22.025: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8779 from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:22.038: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:22.044: INFO: Unable to read jessie_udp@dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:22.053: INFO: Lookups using dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8779 wheezy_tcp@dns-test-service.dns-8779 jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-8779.svc]

  E0513 18:37:22.354047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:23.354754      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:24.355337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:25.360339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:26.362343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:37:27.011: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:27.017: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:27.036: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:27.044: INFO: Unable to read jessie_udp@dns-test-service.dns-8779.svc from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:27.054: INFO: Lookups using dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-8779.svc]

  E0513 18:37:27.363265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:28.364007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:29.364269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:30.364920      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:31.365301      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:37:32.052: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0: the server could not find the requested resource (get pods dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0)
  May 13 18:37:32.072: INFO: Lookups using dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0 failed for: [jessie_udp@dns-test-service]

  E0513 18:37:32.365624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:33.365953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:34.366617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:35.367156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:36.368121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:37:37.037: INFO: DNS probes using dns-8779/dns-test-56f17b20-b135-4ce4-bea4-3be89d8854e0 succeeded

  May 13 18:37:37.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 18:37:37.039
  STEP: deleting the test service @ 05/13/23 18:37:37.071
  STEP: deleting the test headless service @ 05/13/23 18:37:37.087
  STEP: Destroying namespace "dns-8779" for this suite. @ 05/13/23 18:37:37.108
• [32.201 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/13/23 18:37:37.113
  May 13 18:37:37.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 18:37:37.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:37:37.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:37:37.126
  STEP: Creating secret with name secret-test-92961061-00cb-4fb6-8e40-2f6d88ae2611 @ 05/13/23 18:37:37.128
  STEP: Creating a pod to test consume secrets @ 05/13/23 18:37:37.131
  E0513 18:37:37.369303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:38.370911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:39.372442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:40.373915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:37:41.148
  May 13 18:37:41.150: INFO: Trying to get logs from node worker00 pod pod-secrets-4318466d-daaf-4bc6-a518-3c931908452e container secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 18:37:41.152
  May 13 18:37:41.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6356" for this suite. @ 05/13/23 18:37:41.166
• [4.056 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 05/13/23 18:37:41.171
  May 13 18:37:41.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 18:37:41.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:37:41.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:37:41.181
  STEP: Creating projection with secret that has name secret-emptykey-test-47d7bf9f-91d9-4417-a342-a65bbdbbbc5a @ 05/13/23 18:37:41.182
  May 13 18:37:41.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8072" for this suite. @ 05/13/23 18:37:41.186
• [0.017 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 05/13/23 18:37:41.189
  May 13 18:37:41.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:37:41.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:37:41.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:37:41.197
  STEP: Creating configMap with name projected-configmap-test-volume-a9bd0d3f-cb80-409d-a4c8-58b70707bbd4 @ 05/13/23 18:37:41.198
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:37:41.201
  E0513 18:37:41.375319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:42.375328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:43.375596      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:44.376219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:37:45.22
  May 13 18:37:45.224: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-6e4730f9-0319-42a4-b577-b0da5f07943c container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 18:37:45.231
  May 13 18:37:45.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-695" for this suite. @ 05/13/23 18:37:45.255
• [4.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/13/23 18:37:45.26
  May 13 18:37:45.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 18:37:45.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:37:45.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:37:45.272
  STEP: Creating secret with name secret-test-7c305a93-bda8-4474-8c56-cd2ec5f47ab9 @ 05/13/23 18:37:45.273
  STEP: Creating a pod to test consume secrets @ 05/13/23 18:37:45.276
  E0513 18:37:45.377628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:46.379082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:47.379627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:48.381255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:37:49.29
  May 13 18:37:49.293: INFO: Trying to get logs from node worker00 pod pod-secrets-da176bcf-43b2-432d-bd90-e248fe04d403 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 18:37:49.297
  May 13 18:37:49.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-764" for this suite. @ 05/13/23 18:37:49.307
• [4.051 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/13/23 18:37:49.312
  May 13 18:37:49.312: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename deployment @ 05/13/23 18:37:49.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:37:49.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:37:49.325
  May 13 18:37:49.334: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0513 18:37:49.381774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:50.381943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:51.382850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:52.383877      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:53.384156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:37:54.335: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/13/23 18:37:54.335
  May 13 18:37:54.335: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/13/23 18:37:54.34
  May 13 18:37:54.344: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-119  97b577f2-cccd-48ff-81ad-a81d4ebf56d4 17788 1 2023-05-13 18:37:54 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-13 18:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d2b838 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  May 13 18:37:54.347: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-119  04d42866-7963-488e-84ce-ca431b93761b 17791 1 2023-05-13 18:37:54 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 97b577f2-cccd-48ff-81ad-a81d4ebf56d4 0xc005893d97 0xc005893d98}] [] [{kube-controller-manager Update apps/v1 2023-05-13 18:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97b577f2-cccd-48ff-81ad-a81d4ebf56d4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005893e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 13 18:37:54.347: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  May 13 18:37:54.347: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-119  3a83cafd-bfa1-4d4f-a383-080b07244270 17790 1 2023-05-13 18:37:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 97b577f2-cccd-48ff-81ad-a81d4ebf56d4 0xc005893c67 0xc005893c68}] [] [{e2e.test Update apps/v1 2023-05-13 18:37:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 18:37:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-13 18:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"97b577f2-cccd-48ff-81ad-a81d4ebf56d4\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005893d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 13 18:37:54.350: INFO: Pod "test-cleanup-controller-zhqb5" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-zhqb5 test-cleanup-controller- deployment-119  4358389d-f24f-44c1-ac46-d74e3901aa10 17769 0 2023-05-13 18:37:49 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:f13b4f7cec0f8625abc2466142c7a8f365bb1aa6f676f604b3da2a8c868e6071 cni.projectcalico.org/podIP:10.200.131.136/32 cni.projectcalico.org/podIPs:10.200.131.136/32] [{apps/v1 ReplicaSet test-cleanup-controller 3a83cafd-bfa1-4d4f-a383-080b07244270 0xc0050a8767 0xc0050a8768}] [] [{calico Update v1 2023-05-13 18:37:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:37:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3a83cafd-bfa1-4d4f-a383-080b07244270\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:37:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4vjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4vjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:37:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:37:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:37:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:37:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:10.200.131.136,StartTime:2023-05-13 18:37:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 18:37:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://956fca4fabf853ca0882462936bc3934f388a87a91df1a04e52d0884ed0d8b12,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.136,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:37:54.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-119" for this suite. @ 05/13/23 18:37:54.352
• [5.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 05/13/23 18:37:54.369
  May 13 18:37:54.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-probe @ 05/13/23 18:37:54.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:37:54.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:37:54.379
  STEP: Creating pod test-webserver-8d19d20c-47b6-456b-9a31-4308a709b022 in namespace container-probe-7830 @ 05/13/23 18:37:54.381
  E0513 18:37:54.384912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:55.385548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:56.385690      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:37:56.391: INFO: Started pod test-webserver-8d19d20c-47b6-456b-9a31-4308a709b022 in namespace container-probe-7830
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/13/23 18:37:56.391
  May 13 18:37:56.394: INFO: Initial restart count of pod test-webserver-8d19d20c-47b6-456b-9a31-4308a709b022 is 0
  E0513 18:37:57.386522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:58.386657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:37:59.387985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:00.389479      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:01.389607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:02.389982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:03.390885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:04.391893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:05.393420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:06.396850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:07.396890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:08.397275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:09.397419      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:10.397733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:11.397822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:12.401796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:13.402556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:14.402773      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:15.404145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:16.404543      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:17.404742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:18.405536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:19.406063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:20.407154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:21.408625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:22.408799      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:23.409812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:24.410734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:25.411131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:26.412276      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:27.416697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:28.417925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:29.418311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:30.418638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:31.419221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:32.422003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:33.422355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:34.422779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:35.424981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:36.425406      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:37.425969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:38.426183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:39.428302      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:40.429732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:41.430493      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:42.433000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:43.433511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:44.433879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:45.435121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:46.435913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:47.438523      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:48.438925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:49.439375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:50.439650      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:51.440217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:52.440764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:53.442121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:54.442444      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:55.443195      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:56.444059      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:57.446754      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:58.446565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:38:59.447848      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:00.448546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:01.449774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:02.450077      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:03.451191      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:04.452174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:05.452336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:06.452622      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:07.457845      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:08.459509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:09.459551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:10.459565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:11.461097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:12.462308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:13.463340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:14.463375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:15.464110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:16.464256      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:17.467546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:18.469653      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:19.470776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:20.473695      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:21.474246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:22.475532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:23.476397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:24.477040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:25.477254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:26.477385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:27.477422      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:28.477992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:29.478476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:30.479388      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:31.480282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:32.482661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:33.483217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:34.485132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:35.485849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:36.485972      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:37.487329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:38.488240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:39.488586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:40.488887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:41.489863      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:42.491406      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:43.494987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:44.494278      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:45.497086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:46.498219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:47.499349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:48.499398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:49.499694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:50.500860      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:51.500741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:52.501898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:53.504822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:54.506032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:55.506981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:56.509246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:57.511994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:58.512097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:39:59.512556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:00.513109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:01.515821      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:02.516006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:03.516594      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:04.516768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:05.519119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:06.520495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:07.525333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:08.528349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:09.528486      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:10.529942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:11.529813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:12.530236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:13.530617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:14.530751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:15.533713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:16.534165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:17.537591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:18.538559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:19.540289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:20.541353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:21.542188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:22.542768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:23.542855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:24.544785      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:25.545597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:26.547586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:27.547684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:28.549268      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:29.549320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:30.550470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:31.552597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:32.553663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:33.554186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:34.556400      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:35.556827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:36.556941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:37.559081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:38.562957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:39.562170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:40.562999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:41.564034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:42.564321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:43.566272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:44.566337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:45.566587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:46.568736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:47.568997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:48.569646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:49.571387      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:50.573409      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:51.574628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:52.577208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:53.577783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:54.578721      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:55.580010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:56.580053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:57.580308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:58.582651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:40:59.585734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:00.585308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:01.585447      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:02.586838      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:03.586657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:04.587261      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:05.587703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:06.589234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:07.639595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:08.644584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:09.644517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:10.645836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:11.646294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:12.646482      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:13.647213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:14.648029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:15.648544      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:16.649969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:17.650554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:18.651196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:19.656342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:20.655698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:21.660329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:22.661188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:23.661914      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:24.662511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:25.664714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:26.666554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:27.668016      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:28.669765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:29.670274      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:30.670749      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:31.671115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:32.671358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:33.671683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:34.673239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:35.677174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:36.677370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:37.679287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:38.681605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:39.680577      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:40.680770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:41.681008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:42.684360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:43.686648      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:44.689681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:45.690681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:46.691606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:47.691858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:48.692413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:49.694246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:50.694722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:51.695894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:52.695923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:53.697325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:54.698614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:55.699170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:56.700159      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:41:57.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 18:41:57.159
  STEP: Destroying namespace "container-probe-7830" for this suite. @ 05/13/23 18:41:57.183
• [242.822 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 05/13/23 18:41:57.193
  May 13 18:41:57.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:41:57.193
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:41:57.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:41:57.212
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-75 @ 05/13/23 18:41:57.215
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/13/23 18:41:57.224
  STEP: creating service externalsvc in namespace services-75 @ 05/13/23 18:41:57.224
  STEP: creating replication controller externalsvc in namespace services-75 @ 05/13/23 18:41:57.232
  I0513 18:41:57.238911      21 runners.go:194] Created replication controller with name: externalsvc, namespace: services-75, replica count: 2
  E0513 18:41:57.710710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:58.712043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:41:59.714115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:42:00.290711      21 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/13/23 18:42:00.296
  May 13 18:42:00.306: INFO: Creating new exec pod
  E0513 18:42:00.715039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:01.715737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:42:02.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-75 exec execpod4f9f6 -- /bin/sh -x -c nslookup clusterip-service.services-75.svc.cluster.local'
  May 13 18:42:02.429: INFO: stderr: "+ nslookup clusterip-service.services-75.svc.cluster.local\n"
  May 13 18:42:02.429: INFO: stdout: "Server:\t\t10.32.0.10\nAddress:\t10.32.0.10#53\n\nclusterip-service.services-75.svc.cluster.local\tcanonical name = externalsvc.services-75.svc.cluster.local.\nName:\texternalsvc.services-75.svc.cluster.local\nAddress: 10.32.0.21\n\n"
  May 13 18:42:02.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-75, will wait for the garbage collector to delete the pods @ 05/13/23 18:42:02.432
  May 13 18:42:02.496: INFO: Deleting ReplicationController externalsvc took: 9.727246ms
  May 13 18:42:02.598: INFO: Terminating ReplicationController externalsvc pods took: 101.238057ms
  E0513 18:42:02.720576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:03.720959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:42:04.215: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-75" for this suite. @ 05/13/23 18:42:04.222
• [7.033 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 05/13/23 18:42:04.228
  May 13 18:42:04.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename disruption @ 05/13/23 18:42:04.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:42:04.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:42:04.243
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/13/23 18:42:04.246
  STEP: Waiting for the pdb to be processed @ 05/13/23 18:42:04.248
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/13/23 18:42:04.257
  STEP: Waiting for all pods to be running @ 05/13/23 18:42:04.257
  May 13 18:42:04.259: INFO: pods: 0 < 3
  E0513 18:42:04.721317      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:05.721661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/13/23 18:42:06.264
  STEP: Updating the pdb to allow a pod to be evicted @ 05/13/23 18:42:06.277
  STEP: Waiting for the pdb to be processed @ 05/13/23 18:42:06.283
  E0513 18:42:06.722856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:07.724039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/13/23 18:42:08.291
  STEP: Waiting for all pods to be running @ 05/13/23 18:42:08.291
  STEP: Waiting for the pdb to observed all healthy pods @ 05/13/23 18:42:08.296
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/13/23 18:42:08.332
  STEP: Waiting for the pdb to be processed @ 05/13/23 18:42:08.349
  STEP: Waiting for all pods to be running @ 05/13/23 18:42:08.353
  May 13 18:42:08.354: INFO: running pods: 2 < 3
  E0513 18:42:08.724856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:09.725333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/13/23 18:42:10.363
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/13/23 18:42:10.371
  STEP: Waiting for the pdb to be deleted @ 05/13/23 18:42:10.375
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/13/23 18:42:10.376
  STEP: Waiting for all pods to be running @ 05/13/23 18:42:10.376
  May 13 18:42:10.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4081" for this suite. @ 05/13/23 18:42:10.386
• [6.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/13/23 18:42:10.395
  May 13 18:42:10.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename watch @ 05/13/23 18:42:10.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:42:10.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:42:10.408
  STEP: creating a watch on configmaps with label A @ 05/13/23 18:42:10.41
  STEP: creating a watch on configmaps with label B @ 05/13/23 18:42:10.411
  STEP: creating a watch on configmaps with label A or B @ 05/13/23 18:42:10.412
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/13/23 18:42:10.413
  May 13 18:42:10.416: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3977  2305d634-fdbf-4953-ae0d-f0bef0ae179d 18977 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 18:42:10.416: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3977  2305d634-fdbf-4953-ae0d-f0bef0ae179d 18977 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/13/23 18:42:10.416
  May 13 18:42:10.419: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3977  2305d634-fdbf-4953-ae0d-f0bef0ae179d 18978 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 18:42:10.419: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3977  2305d634-fdbf-4953-ae0d-f0bef0ae179d 18978 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/13/23 18:42:10.419
  May 13 18:42:10.422: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3977  2305d634-fdbf-4953-ae0d-f0bef0ae179d 18979 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 18:42:10.422: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3977  2305d634-fdbf-4953-ae0d-f0bef0ae179d 18979 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/13/23 18:42:10.423
  May 13 18:42:10.424: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3977  2305d634-fdbf-4953-ae0d-f0bef0ae179d 18980 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 18:42:10.424: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3977  2305d634-fdbf-4953-ae0d-f0bef0ae179d 18980 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/13/23 18:42:10.424
  May 13 18:42:10.426: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3977  565c90a6-2b5a-4130-8d53-b6eb2ea40891 18981 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 18:42:10.426: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3977  565c90a6-2b5a-4130-8d53-b6eb2ea40891 18981 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0513 18:42:10.725859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:11.727112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:12.728283      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:13.728366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:14.731361      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:15.732315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:16.732911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:17.733414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:18.733948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:19.737738      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/13/23 18:42:20.426
  May 13 18:42:20.434: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3977  565c90a6-2b5a-4130-8d53-b6eb2ea40891 19073 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 18:42:20.434: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3977  565c90a6-2b5a-4130-8d53-b6eb2ea40891 19073 0 2023-05-13 18:42:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-13 18:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0513 18:42:20.738436      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:21.738720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:22.739251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:23.750493      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:24.751675      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:25.752647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:26.752989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:27.753993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:28.754166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:29.754961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:42:30.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3977" for this suite. @ 05/13/23 18:42:30.437
• [20.046 seconds]
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/13/23 18:42:30.441
  May 13 18:42:30.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename proxy @ 05/13/23 18:42:30.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:42:30.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:42:30.45
  STEP: starting an echo server on multiple ports @ 05/13/23 18:42:30.458
  STEP: creating replication controller proxy-service-fpm5n in namespace proxy-4882 @ 05/13/23 18:42:30.459
  I0513 18:42:30.467319      21 runners.go:194] Created replication controller with name: proxy-service-fpm5n, namespace: proxy-4882, replica count: 1
  E0513 18:42:30.755190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:42:31.518276      21 runners.go:194] proxy-service-fpm5n Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0513 18:42:31.756493      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:42:32.519791      21 runners.go:194] proxy-service-fpm5n Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0513 18:42:32.757562      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:42:33.522431      21 runners.go:194] proxy-service-fpm5n Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 13 18:42:33.525: INFO: setup took 3.07220516s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/13/23 18:42:33.525
  May 13 18:42:33.531: INFO: (0) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 5.765045ms)
  May 13 18:42:33.531: INFO: (0) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 6.347396ms)
  May 13 18:42:33.532: INFO: (0) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 6.427156ms)
  May 13 18:42:33.532: INFO: (0) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 6.409616ms)
  May 13 18:42:33.532: INFO: (0) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 6.251902ms)
  May 13 18:42:33.532: INFO: (0) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 6.975854ms)
  May 13 18:42:33.533: INFO: (0) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 7.917085ms)
  May 13 18:42:33.534: INFO: (0) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 9.067974ms)
  May 13 18:42:33.535: INFO: (0) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 9.020089ms)
  May 13 18:42:33.535: INFO: (0) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 9.109781ms)
  May 13 18:42:33.535: INFO: (0) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 9.498726ms)
  May 13 18:42:33.535: INFO: (0) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 9.102726ms)
  May 13 18:42:33.535: INFO: (0) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 9.901228ms)
  May 13 18:42:33.535: INFO: (0) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 9.49286ms)
  May 13 18:42:33.537: INFO: (0) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 11.289624ms)
  May 13 18:42:33.537: INFO: (0) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 11.673422ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 2.55356ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 1.972542ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.02586ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 2.005153ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 2.025015ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 2.6279ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.287085ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 3.219266ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 2.272615ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 2.272975ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.238465ms)
  May 13 18:42:33.541: INFO: (1) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 2.645934ms)
  May 13 18:42:33.542: INFO: (1) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 2.909042ms)
  May 13 18:42:33.542: INFO: (1) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 2.930577ms)
  May 13 18:42:33.543: INFO: (1) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 4.909802ms)
  May 13 18:42:33.543: INFO: (1) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 3.035522ms)
  May 13 18:42:33.546: INFO: (2) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 2.574743ms)
  May 13 18:42:33.546: INFO: (2) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.0928ms)
  May 13 18:42:33.546: INFO: (2) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 3.249003ms)
  May 13 18:42:33.547: INFO: (2) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 3.644558ms)
  May 13 18:42:33.546: INFO: (2) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 3.073467ms)
  May 13 18:42:33.546: INFO: (2) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 2.96311ms)
  May 13 18:42:33.546: INFO: (2) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 3.126214ms)
  May 13 18:42:33.547: INFO: (2) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 3.464121ms)
  May 13 18:42:33.546: INFO: (2) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 3.205397ms)
  May 13 18:42:33.547: INFO: (2) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 3.003786ms)
  May 13 18:42:33.547: INFO: (2) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 3.658888ms)
  May 13 18:42:33.548: INFO: (2) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.923543ms)
  May 13 18:42:33.548: INFO: (2) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 4.010142ms)
  May 13 18:42:33.548: INFO: (2) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 4.434524ms)
  May 13 18:42:33.548: INFO: (2) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 3.258249ms)
  May 13 18:42:33.549: INFO: (2) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 6.46753ms)
  May 13 18:42:33.551: INFO: (3) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 1.157659ms)
  May 13 18:42:33.552: INFO: (3) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.038012ms)
  May 13 18:42:33.553: INFO: (3) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 2.684754ms)
  May 13 18:42:33.552: INFO: (3) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 2.496178ms)
  May 13 18:42:33.552: INFO: (3) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 2.284987ms)
  May 13 18:42:33.552: INFO: (3) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.307297ms)
  May 13 18:42:33.553: INFO: (3) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 2.783217ms)
  May 13 18:42:33.555: INFO: (3) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 4.563132ms)
  May 13 18:42:33.555: INFO: (3) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 4.574621ms)
  May 13 18:42:33.555: INFO: (3) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 4.497296ms)
  May 13 18:42:33.555: INFO: (3) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 4.598729ms)
  May 13 18:42:33.555: INFO: (3) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 4.840825ms)
  May 13 18:42:33.555: INFO: (3) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 4.843168ms)
  May 13 18:42:33.556: INFO: (3) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 5.618458ms)
  May 13 18:42:33.556: INFO: (3) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 6.084795ms)
  May 13 18:42:33.556: INFO: (3) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 6.155556ms)
  May 13 18:42:33.559: INFO: (4) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 2.987678ms)
  May 13 18:42:33.559: INFO: (4) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 2.609311ms)
  May 13 18:42:33.559: INFO: (4) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.705636ms)
  May 13 18:42:33.559: INFO: (4) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 2.704056ms)
  May 13 18:42:33.560: INFO: (4) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 3.72121ms)
  May 13 18:42:33.560: INFO: (4) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 3.783159ms)
  May 13 18:42:33.560: INFO: (4) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 3.707194ms)
  May 13 18:42:33.560: INFO: (4) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.848108ms)
  May 13 18:42:33.560: INFO: (4) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 3.795954ms)
  May 13 18:42:33.560: INFO: (4) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 3.874989ms)
  May 13 18:42:33.560: INFO: (4) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 3.800841ms)
  May 13 18:42:33.560: INFO: (4) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 3.972921ms)
  May 13 18:42:33.560: INFO: (4) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 4.181543ms)
  May 13 18:42:33.561: INFO: (4) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 4.317611ms)
  May 13 18:42:33.561: INFO: (4) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 4.164462ms)
  May 13 18:42:33.561: INFO: (4) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 4.166876ms)
  May 13 18:42:33.565: INFO: (5) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 3.929574ms)
  May 13 18:42:33.565: INFO: (5) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 4.214102ms)
  May 13 18:42:33.565: INFO: (5) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 4.070264ms)
  May 13 18:42:33.565: INFO: (5) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 4.060997ms)
  May 13 18:42:33.565: INFO: (5) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 4.004214ms)
  May 13 18:42:33.565: INFO: (5) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 4.102565ms)
  May 13 18:42:33.565: INFO: (5) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 4.208284ms)
  May 13 18:42:33.565: INFO: (5) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 4.364868ms)
  May 13 18:42:33.565: INFO: (5) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 4.081593ms)
  May 13 18:42:33.566: INFO: (5) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 4.284886ms)
  May 13 18:42:33.566: INFO: (5) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 4.331597ms)
  May 13 18:42:33.566: INFO: (5) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 4.214215ms)
  May 13 18:42:33.566: INFO: (5) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 4.241281ms)
  May 13 18:42:33.566: INFO: (5) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 4.361966ms)
  May 13 18:42:33.566: INFO: (5) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 4.211514ms)
  May 13 18:42:33.566: INFO: (5) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 4.346352ms)
  May 13 18:42:33.569: INFO: (6) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 2.814556ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.722855ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 6.176621ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.703325ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 4.464229ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 4.529665ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 4.479737ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 4.514087ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 4.483326ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 4.497514ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 4.527049ms)
  May 13 18:42:33.573: INFO: (6) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 4.551254ms)
  May 13 18:42:33.569: INFO: (6) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 3.342181ms)
  May 13 18:42:33.569: INFO: (6) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 2.846364ms)
  May 13 18:42:33.569: INFO: (6) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.922831ms)
  May 13 18:42:33.569: INFO: (6) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.917549ms)
  May 13 18:42:33.580: INFO: (7) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.632383ms)
  May 13 18:42:33.580: INFO: (7) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.538323ms)
  May 13 18:42:33.580: INFO: (7) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 2.509766ms)
  May 13 18:42:33.580: INFO: (7) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 3.885139ms)
  May 13 18:42:33.580: INFO: (7) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 2.59052ms)
  May 13 18:42:33.580: INFO: (7) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 3.9444ms)
  May 13 18:42:33.580: INFO: (7) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 3.422937ms)
  May 13 18:42:33.580: INFO: (7) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 3.886545ms)
  May 13 18:42:33.581: INFO: (7) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 3.80826ms)
  May 13 18:42:33.581: INFO: (7) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 5.35355ms)
  May 13 18:42:33.581: INFO: (7) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 4.043849ms)
  May 13 18:42:33.581: INFO: (7) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 5.516321ms)
  May 13 18:42:33.581: INFO: (7) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 5.34498ms)
  May 13 18:42:33.581: INFO: (7) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 4.045778ms)
  May 13 18:42:33.582: INFO: (7) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 5.946312ms)
  May 13 18:42:33.582: INFO: (7) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 4.321222ms)
  May 13 18:42:33.583: INFO: (8) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 1.429589ms)
  May 13 18:42:33.586: INFO: (8) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 3.400633ms)
  May 13 18:42:33.586: INFO: (8) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 4.247808ms)
  May 13 18:42:33.586: INFO: (8) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 4.177694ms)
  May 13 18:42:33.586: INFO: (8) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 4.175195ms)
  May 13 18:42:33.586: INFO: (8) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.443578ms)
  May 13 18:42:33.586: INFO: (8) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 4.167938ms)
  May 13 18:42:33.586: INFO: (8) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 4.218053ms)
  May 13 18:42:33.586: INFO: (8) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 4.211427ms)
  May 13 18:42:33.586: INFO: (8) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 4.275614ms)
  May 13 18:42:33.587: INFO: (8) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 5.117576ms)
  May 13 18:42:33.589: INFO: (8) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 6.387132ms)
  May 13 18:42:33.589: INFO: (8) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 6.809226ms)
  May 13 18:42:33.589: INFO: (8) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 6.451659ms)
  May 13 18:42:33.589: INFO: (8) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 6.052083ms)
  May 13 18:42:33.589: INFO: (8) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 6.039049ms)
  May 13 18:42:33.592: INFO: (9) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 2.636129ms)
  May 13 18:42:33.592: INFO: (9) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 3.127541ms)
  May 13 18:42:33.592: INFO: (9) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 3.240404ms)
  May 13 18:42:33.592: INFO: (9) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 3.221044ms)
  May 13 18:42:33.593: INFO: (9) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 3.850701ms)
  May 13 18:42:33.593: INFO: (9) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 4.079154ms)
  May 13 18:42:33.593: INFO: (9) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 3.957849ms)
  May 13 18:42:33.593: INFO: (9) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 4.012265ms)
  May 13 18:42:33.593: INFO: (9) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 4.038466ms)
  May 13 18:42:33.594: INFO: (9) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 4.942067ms)
  May 13 18:42:33.594: INFO: (9) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 5.157159ms)
  May 13 18:42:33.594: INFO: (9) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 5.159929ms)
  May 13 18:42:33.595: INFO: (9) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 5.769443ms)
  May 13 18:42:33.595: INFO: (9) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 5.810643ms)
  May 13 18:42:33.595: INFO: (9) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 5.77991ms)
  May 13 18:42:33.595: INFO: (9) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 5.93275ms)
  May 13 18:42:33.598: INFO: (10) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 2.749392ms)
  May 13 18:42:33.598: INFO: (10) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 2.93189ms)
  May 13 18:42:33.598: INFO: (10) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.768911ms)
  May 13 18:42:33.598: INFO: (10) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 3.158981ms)
  May 13 18:42:33.598: INFO: (10) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 2.752628ms)
  May 13 18:42:33.598: INFO: (10) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 2.822304ms)
  May 13 18:42:33.598: INFO: (10) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 2.912692ms)
  May 13 18:42:33.598: INFO: (10) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.762293ms)
  May 13 18:42:33.598: INFO: (10) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.050773ms)
  May 13 18:42:33.598: INFO: (10) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 2.996384ms)
  May 13 18:42:33.600: INFO: (10) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 4.977144ms)
  May 13 18:42:33.600: INFO: (10) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 4.857123ms)
  May 13 18:42:33.600: INFO: (10) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 5.340632ms)
  May 13 18:42:33.600: INFO: (10) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 5.46096ms)
  May 13 18:42:33.600: INFO: (10) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 5.584294ms)
  May 13 18:42:33.601: INFO: (10) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 5.437628ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 4.124954ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 3.129538ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.248785ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 4.269418ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 4.184233ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 3.687382ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 3.211017ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 3.199122ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 3.290586ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 3.405011ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 3.421488ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 4.424088ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 3.342139ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 3.492844ms)
  May 13 18:42:33.605: INFO: (11) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 3.464835ms)
  May 13 18:42:33.606: INFO: (11) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 4.733023ms)
  May 13 18:42:33.610: INFO: (12) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 4.122883ms)
  May 13 18:42:33.610: INFO: (12) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.56689ms)
  May 13 18:42:33.610: INFO: (12) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 3.791956ms)
  May 13 18:42:33.610: INFO: (12) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 3.584509ms)
  May 13 18:42:33.610: INFO: (12) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 3.638663ms)
  May 13 18:42:33.610: INFO: (12) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 3.590898ms)
  May 13 18:42:33.610: INFO: (12) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.563409ms)
  May 13 18:42:33.610: INFO: (12) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 3.537676ms)
  May 13 18:42:33.611: INFO: (12) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 3.695663ms)
  May 13 18:42:33.611: INFO: (12) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 3.6954ms)
  May 13 18:42:33.611: INFO: (12) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 3.633726ms)
  May 13 18:42:33.611: INFO: (12) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 3.802946ms)
  May 13 18:42:33.611: INFO: (12) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 4.225576ms)
  May 13 18:42:33.610: INFO: (12) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 3.559646ms)
  May 13 18:42:33.612: INFO: (12) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 4.920552ms)
  May 13 18:42:33.613: INFO: (12) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 5.652854ms)
  May 13 18:42:33.617: INFO: (13) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 3.532177ms)
  May 13 18:42:33.618: INFO: (13) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 4.512783ms)
  May 13 18:42:33.618: INFO: (13) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 4.815783ms)
  May 13 18:42:33.618: INFO: (13) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 4.468732ms)
  May 13 18:42:33.618: INFO: (13) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 4.994684ms)
  May 13 18:42:33.618: INFO: (13) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 4.78543ms)
  May 13 18:42:33.619: INFO: (13) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 5.834136ms)
  May 13 18:42:33.619: INFO: (13) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 5.725838ms)
  May 13 18:42:33.619: INFO: (13) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 5.778732ms)
  May 13 18:42:33.619: INFO: (13) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 6.02454ms)
  May 13 18:42:33.619: INFO: (13) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 5.626045ms)
  May 13 18:42:33.619: INFO: (13) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 5.863305ms)
  May 13 18:42:33.619: INFO: (13) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 6.011561ms)
  May 13 18:42:33.619: INFO: (13) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 6.254363ms)
  May 13 18:42:33.619: INFO: (13) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 5.672616ms)
  May 13 18:42:33.620: INFO: (13) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 7.314383ms)
  May 13 18:42:33.623: INFO: (14) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 2.690774ms)
  May 13 18:42:33.623: INFO: (14) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 2.940877ms)
  May 13 18:42:33.624: INFO: (14) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 3.00939ms)
  May 13 18:42:33.624: INFO: (14) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 3.510568ms)
  May 13 18:42:33.625: INFO: (14) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 4.07722ms)
  May 13 18:42:33.625: INFO: (14) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 4.793488ms)
  May 13 18:42:33.625: INFO: (14) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 3.911164ms)
  May 13 18:42:33.625: INFO: (14) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 5.028423ms)
  May 13 18:42:33.625: INFO: (14) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 4.992667ms)
  May 13 18:42:33.625: INFO: (14) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 4.839929ms)
  May 13 18:42:33.625: INFO: (14) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 4.823065ms)
  May 13 18:42:33.625: INFO: (14) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 4.88703ms)
  May 13 18:42:33.626: INFO: (14) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 5.081445ms)
  May 13 18:42:33.626: INFO: (14) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 5.321596ms)
  May 13 18:42:33.626: INFO: (14) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 5.80746ms)
  May 13 18:42:33.627: INFO: (14) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 6.318079ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 3.871912ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 3.840859ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 3.921888ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 3.874634ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.931579ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 3.900262ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 4.16182ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 4.010155ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 3.937844ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 4.048599ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 4.022501ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 4.033599ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 4.097481ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 4.130254ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 3.998415ms)
  May 13 18:42:33.631: INFO: (15) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 3.994221ms)
  May 13 18:42:33.635: INFO: (16) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 3.464361ms)
  May 13 18:42:33.635: INFO: (16) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 3.559311ms)
  May 13 18:42:33.635: INFO: (16) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 3.597773ms)
  May 13 18:42:33.635: INFO: (16) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 3.747765ms)
  May 13 18:42:33.635: INFO: (16) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 3.588938ms)
  May 13 18:42:33.635: INFO: (16) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 2.597238ms)
  May 13 18:42:33.635: INFO: (16) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 2.576313ms)
  May 13 18:42:33.635: INFO: (16) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.636702ms)
  May 13 18:42:33.636: INFO: (16) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 2.362023ms)
  May 13 18:42:33.637: INFO: (16) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 5.137255ms)
  May 13 18:42:33.637: INFO: (16) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 5.154868ms)
  May 13 18:42:33.637: INFO: (16) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 4.754006ms)
  May 13 18:42:33.637: INFO: (16) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 4.184301ms)
  May 13 18:42:33.638: INFO: (16) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 5.986577ms)
  May 13 18:42:33.638: INFO: (16) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 5.941544ms)
  May 13 18:42:33.638: INFO: (16) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 6.086379ms)
  May 13 18:42:33.640: INFO: (17) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 2.274865ms)
  May 13 18:42:33.646: INFO: (17) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 7.653567ms)
  May 13 18:42:33.648: INFO: (17) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 9.784761ms)
  May 13 18:42:33.648: INFO: (17) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 9.594502ms)
  May 13 18:42:33.648: INFO: (17) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 10.57886ms)
  May 13 18:42:33.650: INFO: (17) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 11.693439ms)
  May 13 18:42:33.650: INFO: (17) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 11.742135ms)
  May 13 18:42:33.651: INFO: (17) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 12.407129ms)
  May 13 18:42:33.651: INFO: (17) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 12.383498ms)
  May 13 18:42:33.651: INFO: (17) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 13.396171ms)
  May 13 18:42:33.651: INFO: (17) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 12.744269ms)
  May 13 18:42:33.651: INFO: (17) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 12.919155ms)
  May 13 18:42:33.652: INFO: (17) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 13.778717ms)
  May 13 18:42:33.652: INFO: (17) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 13.8296ms)
  May 13 18:42:33.654: INFO: (17) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 15.390755ms)
  May 13 18:42:33.654: INFO: (17) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 15.448377ms)
  May 13 18:42:33.660: INFO: (18) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 3.928359ms)
  May 13 18:42:33.660: INFO: (18) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 4.714909ms)
  May 13 18:42:33.660: INFO: (18) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 4.736492ms)
  May 13 18:42:33.660: INFO: (18) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 3.912089ms)
  May 13 18:42:33.660: INFO: (18) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 3.933872ms)
  May 13 18:42:33.660: INFO: (18) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 3.887498ms)
  May 13 18:42:33.660: INFO: (18) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 3.910988ms)
  May 13 18:42:33.660: INFO: (18) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 4.737701ms)
  May 13 18:42:33.660: INFO: (18) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 4.296811ms)
  May 13 18:42:33.661: INFO: (18) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 5.282063ms)
  May 13 18:42:33.661: INFO: (18) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 5.336529ms)
  May 13 18:42:33.661: INFO: (18) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 5.482ms)
  May 13 18:42:33.661: INFO: (18) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 6.547435ms)
  May 13 18:42:33.661: INFO: (18) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 5.489566ms)
  May 13 18:42:33.661: INFO: (18) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 5.484636ms)
  May 13 18:42:33.661: INFO: (18) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 6.256898ms)
  May 13 18:42:33.670: INFO: (19) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:443/proxy/tlsrewritem... (200; 7.509085ms)
  May 13 18:42:33.670: INFO: (19) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">test<... (200; 8.110382ms)
  May 13 18:42:33.670: INFO: (19) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:460/proxy/: tls baz (200; 7.898689ms)
  May 13 18:42:33.670: INFO: (19) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 8.106439ms)
  May 13 18:42:33.670: INFO: (19) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 8.516667ms)
  May 13 18:42:33.670: INFO: (19) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:160/proxy/: foo (200; 8.155857ms)
  May 13 18:42:33.670: INFO: (19) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm/proxy/rewriteme">test</a> (200; 8.445215ms)
  May 13 18:42:33.670: INFO: (19) /api/v1/namespaces/proxy-4882/pods/proxy-service-fpm5n-rbrwm:162/proxy/: bar (200; 8.365598ms)
  May 13 18:42:33.670: INFO: (19) /api/v1/namespaces/proxy-4882/pods/https:proxy-service-fpm5n-rbrwm:462/proxy/: tls qux (200; 8.538399ms)
  May 13 18:42:33.670: INFO: (19) /api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/: <a href="/api/v1/namespaces/proxy-4882/pods/http:proxy-service-fpm5n-rbrwm:1080/proxy/rewriteme">... (200; 7.592741ms)
  May 13 18:42:33.672: INFO: (19) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname1/proxy/: tls baz (200; 9.973206ms)
  May 13 18:42:33.672: INFO: (19) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname2/proxy/: bar (200; 9.835928ms)
  May 13 18:42:33.672: INFO: (19) /api/v1/namespaces/proxy-4882/services/http:proxy-service-fpm5n:portname1/proxy/: foo (200; 10.010406ms)
  May 13 18:42:33.672: INFO: (19) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname1/proxy/: foo (200; 10.229778ms)
  May 13 18:42:33.672: INFO: (19) /api/v1/namespaces/proxy-4882/services/https:proxy-service-fpm5n:tlsportname2/proxy/: tls qux (200; 10.045418ms)
  May 13 18:42:33.672: INFO: (19) /api/v1/namespaces/proxy-4882/services/proxy-service-fpm5n:portname2/proxy/: bar (200; 9.972346ms)
  May 13 18:42:33.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-fpm5n in namespace proxy-4882, will wait for the garbage collector to delete the pods @ 05/13/23 18:42:33.675
  May 13 18:42:33.734: INFO: Deleting ReplicationController proxy-service-fpm5n took: 5.006138ms
  E0513 18:42:33.758251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:42:33.838: INFO: Terminating ReplicationController proxy-service-fpm5n pods took: 103.838674ms
  E0513 18:42:34.758383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:35.759205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-4882" for this suite. @ 05/13/23 18:42:36.039
• [5.604 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 05/13/23 18:42:36.047
  May 13 18:42:36.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename job @ 05/13/23 18:42:36.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:42:36.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:42:36.056
  STEP: Creating Indexed job @ 05/13/23 18:42:36.057
  STEP: Ensuring job reaches completions @ 05/13/23 18:42:36.06
  E0513 18:42:36.759434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:37.761938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:38.762711      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:39.765571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:40.766307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:41.766834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 05/13/23 18:42:42.063
  May 13 18:42:42.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1410" for this suite. @ 05/13/23 18:42:42.067
• [6.022 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 05/13/23 18:42:42.07
  May 13 18:42:42.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 18:42:42.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:42:42.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:42:42.082
  STEP: validating api versions @ 05/13/23 18:42:42.084
  May 13 18:42:42.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-5780 api-versions'
  May 13 18:42:42.135: INFO: stderr: ""
  May 13 18:42:42.135: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1alpha1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.apiserver.k8s.io/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nresource.k8s.io/v1alpha2\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  May 13 18:42:42.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5780" for this suite. @ 05/13/23 18:42:42.138
• [0.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 05/13/23 18:42:42.142
  May 13 18:42:42.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 18:42:42.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:42:42.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:42:42.15
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/13/23 18:42:42.152
  E0513 18:42:42.766960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:43.767832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:42:44.159
  May 13 18:42:44.161: INFO: Trying to get logs from node worker00 pod pod-d6381e5f-a29c-4894-a967-38f243ea9a5c container test-container: <nil>
  STEP: delete the pod @ 05/13/23 18:42:44.163
  May 13 18:42:44.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8101" for this suite. @ 05/13/23 18:42:44.174
• [2.036 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 05/13/23 18:42:44.177
  May 13 18:42:44.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 18:42:44.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:42:44.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:42:44.185
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 18:42:44.186
  E0513 18:42:44.768411      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:45.770331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:42:46.196
  May 13 18:42:46.198: INFO: Trying to get logs from node worker00 pod downwardapi-volume-e3995284-58f5-4153-a61b-ad3a4881c74c container client-container: <nil>
  STEP: delete the pod @ 05/13/23 18:42:46.201
  May 13 18:42:46.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8511" for this suite. @ 05/13/23 18:42:46.21
• [2.036 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 05/13/23 18:42:46.213
  May 13 18:42:46.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pod-network-test @ 05/13/23 18:42:46.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:42:46.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:42:46.226
  STEP: Performing setup for networking test in namespace pod-network-test-2528 @ 05/13/23 18:42:46.228
  STEP: creating a selector @ 05/13/23 18:42:46.228
  STEP: Creating the service pods in kubernetes @ 05/13/23 18:42:46.228
  May 13 18:42:46.228: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0513 18:42:46.770160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:47.770961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:48.772659      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:49.773927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:50.775057      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:51.775500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:52.775662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:53.776251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:54.777410      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:55.777471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:56.778997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:57.782015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/13/23 18:42:58.288
  E0513 18:42:58.783880      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:42:59.784311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:43:00.314: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 13 18:43:00.314: INFO: Breadth first check of 10.200.131.161 on host 192.168.58.100...
  May 13 18:43:00.318: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.131.142:9080/dial?request=hostname&protocol=udp&host=10.200.131.161&port=8081&tries=1'] Namespace:pod-network-test-2528 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:43:00.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:43:00.320: INFO: ExecWithOptions: Clientset creation
  May 13 18:43:00.320: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/pod-network-test-2528/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.200.131.142%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.200.131.161%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 13 18:43:00.379: INFO: Waiting for responses: map[]
  May 13 18:43:00.379: INFO: reached 10.200.131.161 after 0/1 tries
  May 13 18:43:00.379: INFO: Breadth first check of 10.200.5.35 on host 192.168.58.101...
  May 13 18:43:00.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.131.142:9080/dial?request=hostname&protocol=udp&host=10.200.5.35&port=8081&tries=1'] Namespace:pod-network-test-2528 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:43:00.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:43:00.381: INFO: ExecWithOptions: Clientset creation
  May 13 18:43:00.381: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/pod-network-test-2528/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.200.131.142%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.200.5.35%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 13 18:43:00.408: INFO: Waiting for responses: map[]
  May 13 18:43:00.408: INFO: reached 10.200.5.35 after 0/1 tries
  May 13 18:43:00.408: INFO: Going to retry 0 out of 2 pods....
  May 13 18:43:00.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-2528" for this suite. @ 05/13/23 18:43:00.412
• [14.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 05/13/23 18:43:00.415
  May 13 18:43:00.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename var-expansion @ 05/13/23 18:43:00.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:43:00.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:43:00.424
  E0513 18:43:00.785240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:01.785589      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:43:02.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 18:43:02.439: INFO: Deleting pod "var-expansion-dc8c8098-5df6-44a0-8246-076454daf185" in namespace "var-expansion-7242"
  May 13 18:43:02.452: INFO: Wait up to 5m0s for pod "var-expansion-dc8c8098-5df6-44a0-8246-076454daf185" to be fully deleted
  E0513 18:43:02.785828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:03.785892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-7242" for this suite. @ 05/13/23 18:43:04.475
• [4.064 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 05/13/23 18:43:04.48
  May 13 18:43:04.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename subpath @ 05/13/23 18:43:04.482
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:43:04.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:43:04.491
  STEP: Setting up data @ 05/13/23 18:43:04.492
  STEP: Creating pod pod-subpath-test-projected-6885 @ 05/13/23 18:43:04.498
  STEP: Creating a pod to test atomic-volume-subpath @ 05/13/23 18:43:04.498
  E0513 18:43:04.787482      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:05.788000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:06.788895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:07.789535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:08.790426      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:09.791164      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:10.791382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:11.791709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:12.792197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:13.792833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:14.796452      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:15.798587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:16.798248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:17.798967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:18.799062      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:19.799270      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:20.801823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:21.801961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:22.802642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:23.804208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:24.804875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:25.805187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:43:26.554
  May 13 18:43:26.558: INFO: Trying to get logs from node worker00 pod pod-subpath-test-projected-6885 container test-container-subpath-projected-6885: <nil>
  STEP: delete the pod @ 05/13/23 18:43:26.563
  STEP: Deleting pod pod-subpath-test-projected-6885 @ 05/13/23 18:43:26.571
  May 13 18:43:26.572: INFO: Deleting pod "pod-subpath-test-projected-6885" in namespace "subpath-4469"
  May 13 18:43:26.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4469" for this suite. @ 05/13/23 18:43:26.575
• [22.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 05/13/23 18:43:26.579
  May 13 18:43:26.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename security-context-test @ 05/13/23 18:43:26.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:43:26.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:43:26.592
  E0513 18:43:26.805550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:27.806279      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:28.806998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:29.807353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:30.807267      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:31.807680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:43:32.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6089" for this suite. @ 05/13/23 18:43:32.668
• [6.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 05/13/23 18:43:32.683
  May 13 18:43:32.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/13/23 18:43:32.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:43:32.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:43:32.706
  May 13 18:43:32.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:43:32.808528      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:43:33.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7854" for this suite. @ 05/13/23 18:43:33.742
• [1.072 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 05/13/23 18:43:33.758
  May 13 18:43:33.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename var-expansion @ 05/13/23 18:43:33.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:43:33.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:43:33.779
  STEP: creating the pod @ 05/13/23 18:43:33.781
  STEP: waiting for pod running @ 05/13/23 18:43:33.788
  E0513 18:43:33.809203      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:34.810088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 05/13/23 18:43:35.797
  May 13 18:43:35.802: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1438 PodName:var-expansion-b417da42-b91b-4b23-8f4a-12327cf51fde ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:43:35.802: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:43:35.804: INFO: ExecWithOptions: Clientset creation
  May 13 18:43:35.804: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/var-expansion-1438/pods/var-expansion-b417da42-b91b-4b23-8f4a-12327cf51fde/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  E0513 18:43:35.810088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: test for file in mounted path @ 05/13/23 18:43:35.849
  May 13 18:43:35.852: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1438 PodName:var-expansion-b417da42-b91b-4b23-8f4a-12327cf51fde ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:43:35.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:43:35.852: INFO: ExecWithOptions: Clientset creation
  May 13 18:43:35.853: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/var-expansion-1438/pods/var-expansion-b417da42-b91b-4b23-8f4a-12327cf51fde/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/13/23 18:43:35.906
  May 13 18:43:36.424: INFO: Successfully updated pod "var-expansion-b417da42-b91b-4b23-8f4a-12327cf51fde"
  STEP: waiting for annotated pod running @ 05/13/23 18:43:36.424
  STEP: deleting the pod gracefully @ 05/13/23 18:43:36.427
  May 13 18:43:36.427: INFO: Deleting pod "var-expansion-b417da42-b91b-4b23-8f4a-12327cf51fde" in namespace "var-expansion-1438"
  May 13 18:43:36.431: INFO: Wait up to 5m0s for pod "var-expansion-b417da42-b91b-4b23-8f4a-12327cf51fde" to be fully deleted
  E0513 18:43:36.811106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:37.812699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:38.813579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:39.814611      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:40.815296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:41.815388      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:42.816531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:43.816867      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:44.816933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:45.819199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:46.821296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:47.821687      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:48.822967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:49.825518      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:50.828450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:51.828966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:52.829995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:53.831459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:54.832098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:55.833004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:56.834767      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:57.837480      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:58.838028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:43:59.838836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:00.839325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:01.846657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:02.846103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:03.846592      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:04.847265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:05.849106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:06.851430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:07.852119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:08.853305      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:09.854532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:44:10.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1438" for this suite. @ 05/13/23 18:44:10.525
• [36.772 seconds]
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 05/13/23 18:44:10.53
  May 13 18:44:10.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename svcaccounts @ 05/13/23 18:44:10.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:44:10.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:44:10.539
  E0513 18:44:10.855176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:11.855732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 05/13/23 18:44:12.556
  May 13 18:44:12.556: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6233 pod-service-account-747544af-a40f-4bf9-b65e-a4fd1ebae29d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 05/13/23 18:44:12.632
  May 13 18:44:12.632: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6233 pod-service-account-747544af-a40f-4bf9-b65e-a4fd1ebae29d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 05/13/23 18:44:12.722
  May 13 18:44:12.722: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6233 pod-service-account-747544af-a40f-4bf9-b65e-a4fd1ebae29d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  May 13 18:44:12.806: INFO: Got root ca configmap in namespace "svcaccounts-6233"
  May 13 18:44:12.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6233" for this suite. @ 05/13/23 18:44:12.811
• [2.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/13/23 18:44:12.817
  May 13 18:44:12.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 18:44:12.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:44:12.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:44:12.825
  May 13 18:44:12.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: creating the pod @ 05/13/23 18:44:12.827
  STEP: submitting the pod to kubernetes @ 05/13/23 18:44:12.827
  E0513 18:44:12.857890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:13.858911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:14.858919      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:44:14.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-150" for this suite. @ 05/13/23 18:44:14.896
• [2.082 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 05/13/23 18:44:14.899
  May 13 18:44:14.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/13/23 18:44:14.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:44:14.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:44:14.906
  May 13 18:44:14.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:44:15.859697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:16.860399      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:17.860984      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:18.870293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:19.862798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:20.863982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:44:21.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-203" for this suite. @ 05/13/23 18:44:21.161
• [6.268 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 05/13/23 18:44:21.169
  May 13 18:44:21.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 18:44:21.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:44:21.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:44:21.177
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 18:44:21.178
  E0513 18:44:21.868265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:22.868652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:23.870053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:24.870245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:44:25.198
  May 13 18:44:25.202: INFO: Trying to get logs from node worker00 pod downwardapi-volume-f91a145f-94fa-4bbd-b7e5-c89179543cfa container client-container: <nil>
  STEP: delete the pod @ 05/13/23 18:44:25.205
  May 13 18:44:25.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8159" for this suite. @ 05/13/23 18:44:25.215
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/13/23 18:44:25.218
  May 13 18:44:25.219: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename containers @ 05/13/23 18:44:25.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:44:25.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:44:25.226
  E0513 18:44:25.870865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:26.870842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:44:27.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2915" for this suite. @ 05/13/23 18:44:27.241
• [2.026 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 05/13/23 18:44:27.246
  May 13 18:44:27.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 18:44:27.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:44:27.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:44:27.254
  STEP: Setting up server cert @ 05/13/23 18:44:27.263
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 18:44:27.445
  STEP: Deploying the webhook pod @ 05/13/23 18:44:27.458
  STEP: Wait for the deployment to be ready @ 05/13/23 18:44:27.463
  May 13 18:44:27.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 18:44:27.870890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:28.873526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 18:44:29.477
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 18:44:29.498
  E0513 18:44:29.874509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:44:30.501: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/13/23 18:44:30.503
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/13/23 18:44:30.513
  May 13 18:44:30.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:44:30.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6380" for this suite. @ 05/13/23 18:44:30.54
  STEP: Destroying namespace "webhook-markers-3239" for this suite. @ 05/13/23 18:44:30.544
• [3.302 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 05/13/23 18:44:30.549
  May 13 18:44:30.550: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 18:44:30.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:44:30.556
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:44:30.559
  STEP: Setting up server cert @ 05/13/23 18:44:30.569
  E0513 18:44:30.874724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 18:44:30.878
  STEP: Deploying the webhook pod @ 05/13/23 18:44:30.881
  STEP: Wait for the deployment to be ready @ 05/13/23 18:44:30.885
  May 13 18:44:30.892: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 18:44:31.875601      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:32.876808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 18:44:32.895
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 18:44:32.91
  E0513 18:44:33.878295      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:44:33.911: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/13/23 18:44:33.913
  STEP: create a pod that should be updated by the webhook @ 05/13/23 18:44:33.938
  May 13 18:44:33.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6230" for this suite. @ 05/13/23 18:44:33.967
  STEP: Destroying namespace "webhook-markers-6005" for this suite. @ 05/13/23 18:44:33.971
• [3.452 seconds]
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 05/13/23 18:44:34.001
  May 13 18:44:34.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-webhook @ 05/13/23 18:44:34.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:44:34.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:44:34.016
  STEP: Setting up server cert @ 05/13/23 18:44:34.018
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/13/23 18:44:34.213
  STEP: Deploying the custom resource conversion webhook pod @ 05/13/23 18:44:34.226
  STEP: Wait for the deployment to be ready @ 05/13/23 18:44:34.245
  May 13 18:44:34.247: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0513 18:44:34.879174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:35.881042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:44:36.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 18, 44, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 18, 44, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 18, 44, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 18, 44, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 18:44:36.881622      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:37.882912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 18:44:38.264
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 18:44:38.285
  E0513 18:44:38.882849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:44:39.286: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 13 18:44:39.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:44:39.884039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:40.884646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/13/23 18:44:41.836
  STEP: v2 custom resource should be converted @ 05/13/23 18:44:41.839
  May 13 18:44:41.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0513 18:44:41.885971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-8463" for this suite. @ 05/13/23 18:44:42.375
• [8.377 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/13/23 18:44:42.382
  May 13 18:44:42.382: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-runtime @ 05/13/23 18:44:42.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:44:42.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:44:42.4
  STEP: create the container @ 05/13/23 18:44:42.401
  W0513 18:44:42.404948      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/13/23 18:44:42.405
  E0513 18:44:42.888234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:43.888525      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:44.888720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/13/23 18:44:45.427
  STEP: the container should be terminated @ 05/13/23 18:44:45.43
  STEP: the termination message should be set @ 05/13/23 18:44:45.43
  May 13 18:44:45.430: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/13/23 18:44:45.43
  May 13 18:44:45.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-37" for this suite. @ 05/13/23 18:44:45.446
• [3.066 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 05/13/23 18:44:45.448
  May 13 18:44:45.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 18:44:45.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:44:45.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:44:45.455
  E0513 18:44:45.889217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:46.889673      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:47.890648      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:48.890785      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:49.891953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:50.892503      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:51.894885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:52.894991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:53.895777      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:54.896752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:55.897759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:56.898694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:57.899311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:58.899583      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:44:59.900692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:00.902164      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:01.902498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/13/23 18:45:02.462
  E0513 18:45:02.902834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:03.903082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:04.903360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:05.903576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:06.906680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/13/23 18:45:07.471
  STEP: Ensuring resource quota status is calculated @ 05/13/23 18:45:07.486
  E0513 18:45:07.907331      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:08.907667      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 05/13/23 18:45:09.488
  STEP: Ensuring resource quota status captures configMap creation @ 05/13/23 18:45:09.494
  E0513 18:45:09.909758      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:10.910792      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 05/13/23 18:45:11.498
  STEP: Ensuring resource quota status released usage @ 05/13/23 18:45:11.504
  E0513 18:45:11.911273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:12.911858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:45:13.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8937" for this suite. @ 05/13/23 18:45:13.512
• [28.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 05/13/23 18:45:13.532
  May 13 18:45:13.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 18:45:13.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:45:13.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:45:13.546
  STEP: Creating a ResourceQuota @ 05/13/23 18:45:13.548
  STEP: Getting a ResourceQuota @ 05/13/23 18:45:13.554
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/13/23 18:45:13.556
  STEP: Patching the ResourceQuota @ 05/13/23 18:45:13.563
  STEP: Deleting a Collection of ResourceQuotas @ 05/13/23 18:45:13.569
  STEP: Verifying the deleted ResourceQuota @ 05/13/23 18:45:13.578
  May 13 18:45:13.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5211" for this suite. @ 05/13/23 18:45:13.582
• [0.054 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 05/13/23 18:45:13.587
  May 13 18:45:13.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 18:45:13.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:45:13.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:45:13.597
  STEP: Setting up server cert @ 05/13/23 18:45:13.61
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 18:45:13.761
  STEP: Deploying the webhook pod @ 05/13/23 18:45:13.765
  STEP: Wait for the deployment to be ready @ 05/13/23 18:45:13.77
  May 13 18:45:13.776: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 18:45:13.912807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:14.914099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 18:45:15.789
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 18:45:15.816
  E0513 18:45:15.914763      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:45:16.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 13 18:45:16.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:45:16.916970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2680-crds.webhook.example.com via the AdmissionRegistration API @ 05/13/23 18:45:17.342
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/13/23 18:45:17.36
  E0513 18:45:17.917841      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:18.918100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:45:19.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0513 18:45:19.918833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-9583" for this suite. @ 05/13/23 18:45:19.919
  STEP: Destroying namespace "webhook-markers-1728" for this suite. @ 05/13/23 18:45:19.926
• [6.343 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 05/13/23 18:45:19.931
  May 13 18:45:19.931: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:45:19.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:45:19.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:45:19.948
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-2fca5539-350f-4784-aa3a-4acd65b7bd39 @ 05/13/23 18:45:19.951
  STEP: Creating the pod @ 05/13/23 18:45:19.955
  E0513 18:45:20.920127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:21.920942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-2fca5539-350f-4784-aa3a-4acd65b7bd39 @ 05/13/23 18:45:21.973
  STEP: waiting to observe update in volume @ 05/13/23 18:45:21.987
  E0513 18:45:22.922102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:23.922795      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:24.923472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:25.924330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:26.924803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:27.926025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:28.926962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:29.927789      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:30.927961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:31.928509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:32.928529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:33.929511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:34.929745      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:35.930893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:36.931374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:37.931522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:38.931937      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:39.932913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:40.933431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:41.933803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:42.934556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:43.935234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:44.936231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:45.936535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:46.938574      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:47.938953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:48.939453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:49.944189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:50.945630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:51.946351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:52.946548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:53.946887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:54.949545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:55.949978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:56.952054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:57.952338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:58.953655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:45:59.954502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:00.955566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:01.955722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:02.956368      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:03.957554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:04.957949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:05.958849      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:06.960002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:07.960609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:08.960666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:09.961269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:10.961602      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:11.962083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:12.962678      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:13.964403      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:14.966579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:15.967259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:16.967451      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:17.968937      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:18.969221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:19.969304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:20.969713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:21.970985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:22.972161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:23.973119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:24.974605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:25.976504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:26.976674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:27.977087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:28.977832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:29.978176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:30.978498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:31.979132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:32.979578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:33.981309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:34.992620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:35.993700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:46:36.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3628" for this suite. @ 05/13/23 18:46:36.347
• [76.421 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 05/13/23 18:46:36.352
  May 13 18:46:36.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 18:46:36.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:46:36.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:46:36.366
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 18:46:36.368
  E0513 18:46:36.994494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:37.995011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:46:38.38
  May 13 18:46:38.384: INFO: Trying to get logs from node worker00 pod downwardapi-volume-0c232844-a67c-4c14-84c8-be88b1fd8a94 container client-container: <nil>
  STEP: delete the pod @ 05/13/23 18:46:38.387
  May 13 18:46:38.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1553" for this suite. @ 05/13/23 18:46:38.397
• [2.049 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 05/13/23 18:46:38.403
  May 13 18:46:38.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:46:38.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:46:38.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:46:38.413
  May 13 18:46:38.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1838" for this suite. @ 05/13/23 18:46:38.417
• [0.015 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 05/13/23 18:46:38.419
  May 13 18:46:38.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename limitrange @ 05/13/23 18:46:38.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:46:38.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:46:38.427
  STEP: Creating a LimitRange @ 05/13/23 18:46:38.428
  STEP: Setting up watch @ 05/13/23 18:46:38.428
  STEP: Submitting a LimitRange @ 05/13/23 18:46:38.53
  STEP: Verifying LimitRange creation was observed @ 05/13/23 18:46:38.538
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/13/23 18:46:38.539
  May 13 18:46:38.542: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 13 18:46:38.542: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/13/23 18:46:38.542
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/13/23 18:46:38.547
  May 13 18:46:38.551: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 13 18:46:38.551: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/13/23 18:46:38.552
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/13/23 18:46:38.555
  May 13 18:46:38.558: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  May 13 18:46:38.559: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/13/23 18:46:38.559
  STEP: Failing to create a Pod with more than max resources @ 05/13/23 18:46:38.563
  STEP: Updating a LimitRange @ 05/13/23 18:46:38.564
  STEP: Verifying LimitRange updating is effective @ 05/13/23 18:46:38.567
  E0513 18:46:38.995882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:39.996244      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 05/13/23 18:46:40.57
  STEP: Failing to create a Pod with more than max resources @ 05/13/23 18:46:40.574
  STEP: Deleting a LimitRange @ 05/13/23 18:46:40.576
  STEP: Verifying the LimitRange was deleted @ 05/13/23 18:46:40.583
  E0513 18:46:40.997524      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:41.997901      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:42.998418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:43.999264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:45.000372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:46:45.588: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/13/23 18:46:45.588
  May 13 18:46:45.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-9853" for this suite. @ 05/13/23 18:46:45.605
• [7.191 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 05/13/23 18:46:45.611
  May 13 18:46:45.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/13/23 18:46:45.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:46:45.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:46:45.623
  May 13 18:46:45.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:46:46.000399      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/13/23 18:46:46.827
  May 13 18:46:46.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 --namespace=crd-publish-openapi-5261 create -f -'
  E0513 18:46:47.001587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:46:47.193: INFO: stderr: ""
  May 13 18:46:47.193: INFO: stdout: "e2e-test-crd-publish-openapi-7740-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 13 18:46:47.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 --namespace=crd-publish-openapi-5261 delete e2e-test-crd-publish-openapi-7740-crds test-foo'
  May 13 18:46:47.233: INFO: stderr: ""
  May 13 18:46:47.233: INFO: stdout: "e2e-test-crd-publish-openapi-7740-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  May 13 18:46:47.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 --namespace=crd-publish-openapi-5261 apply -f -'
  May 13 18:46:47.385: INFO: stderr: ""
  May 13 18:46:47.385: INFO: stdout: "e2e-test-crd-publish-openapi-7740-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 13 18:46:47.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 --namespace=crd-publish-openapi-5261 delete e2e-test-crd-publish-openapi-7740-crds test-foo'
  May 13 18:46:47.434: INFO: stderr: ""
  May 13 18:46:47.434: INFO: stdout: "e2e-test-crd-publish-openapi-7740-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/13/23 18:46:47.434
  May 13 18:46:47.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 --namespace=crd-publish-openapi-5261 create -f -'
  May 13 18:46:47.572: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/13/23 18:46:47.572
  May 13 18:46:47.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 --namespace=crd-publish-openapi-5261 create -f -'
  May 13 18:46:47.705: INFO: rc: 1
  May 13 18:46:47.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 --namespace=crd-publish-openapi-5261 apply -f -'
  May 13 18:46:47.832: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/13/23 18:46:47.832
  May 13 18:46:47.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 --namespace=crd-publish-openapi-5261 create -f -'
  May 13 18:46:47.949: INFO: rc: 1
  May 13 18:46:47.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 --namespace=crd-publish-openapi-5261 apply -f -'
  E0513 18:46:48.001792      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:46:48.084: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/13/23 18:46:48.084
  May 13 18:46:48.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 explain e2e-test-crd-publish-openapi-7740-crds'
  May 13 18:46:48.212: INFO: stderr: ""
  May 13 18:46:48.212: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7740-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/13/23 18:46:48.213
  May 13 18:46:48.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 explain e2e-test-crd-publish-openapi-7740-crds.metadata'
  May 13 18:46:48.363: INFO: stderr: ""
  May 13 18:46:48.363: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7740-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  May 13 18:46:48.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 explain e2e-test-crd-publish-openapi-7740-crds.spec'
  May 13 18:46:48.511: INFO: stderr: ""
  May 13 18:46:48.511: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7740-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  May 13 18:46:48.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 explain e2e-test-crd-publish-openapi-7740-crds.spec.bars'
  May 13 18:46:48.649: INFO: stderr: ""
  May 13 18:46:48.649: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-7740-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/13/23 18:46:48.649
  May 13 18:46:48.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5261 explain e2e-test-crd-publish-openapi-7740-crds.spec.bars2'
  May 13 18:46:48.778: INFO: rc: 1
  E0513 18:46:49.002046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:50.002737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:46:50.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5261" for this suite. @ 05/13/23 18:46:50.524
• [4.917 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 05/13/23 18:46:50.529
  May 13 18:46:50.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 18:46:50.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:46:50.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:46:50.54
  STEP: Creating configMap with name configmap-test-volume-map-fa8af85c-bd2a-4908-b474-74de2b8e7830 @ 05/13/23 18:46:50.542
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:46:50.547
  E0513 18:46:51.003481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:52.003891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:46:52.559
  May 13 18:46:52.563: INFO: Trying to get logs from node worker00 pod pod-configmaps-7c53dbf8-a1d2-4d4a-97c4-84ac134301c5 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 18:46:52.567
  May 13 18:46:52.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3309" for this suite. @ 05/13/23 18:46:52.577
• [2.051 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 05/13/23 18:46:52.58
  May 13 18:46:52.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename namespaces @ 05/13/23 18:46:52.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:46:52.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:46:52.596
  STEP: creating a Namespace @ 05/13/23 18:46:52.598
  STEP: patching the Namespace @ 05/13/23 18:46:52.607
  STEP: get the Namespace and ensuring it has the label @ 05/13/23 18:46:52.611
  May 13 18:46:52.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5805" for this suite. @ 05/13/23 18:46:52.614
  STEP: Destroying namespace "nspatchtest-871aeb39-4885-4e08-b299-80db1332af35-6556" for this suite. @ 05/13/23 18:46:52.616
• [0.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 05/13/23 18:46:52.619
  May 13 18:46:52.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:46:52.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:46:52.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:46:52.626
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 18:46:52.627
  E0513 18:46:53.003929      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:54.004307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:55.005532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:56.006416      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:46:56.641
  May 13 18:46:56.646: INFO: Trying to get logs from node worker00 pod downwardapi-volume-21a5b59e-c35e-4220-9c6a-1e72898248f6 container client-container: <nil>
  STEP: delete the pod @ 05/13/23 18:46:56.653
  May 13 18:46:56.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8412" for this suite. @ 05/13/23 18:46:56.663
• [4.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/13/23 18:46:56.666
  May 13 18:46:56.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename hostport @ 05/13/23 18:46:56.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:46:56.675
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:46:56.677
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/13/23 18:46:56.681
  E0513 18:46:57.007230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:46:58.008041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.58.100 on the node which pod1 resides and expect scheduled @ 05/13/23 18:46:58.696
  E0513 18:46:59.009071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:00.009600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:01.009896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:02.010831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.58.100 but use UDP protocol on the node which pod2 resides @ 05/13/23 18:47:02.722
  E0513 18:47:03.011654      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:04.011883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:05.012854      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:06.013258      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/13/23 18:47:06.767
  May 13 18:47:06.767: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.58.100 http://127.0.0.1:54323/hostname] Namespace:hostport-4082 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:47:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:47:06.769: INFO: ExecWithOptions: Clientset creation
  May 13 18:47:06.769: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/hostport-4082/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.58.100+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.58.100, port: 54323 @ 05/13/23 18:47:06.824
  May 13 18:47:06.824: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.58.100:54323/hostname] Namespace:hostport-4082 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:47:06.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:47:06.824: INFO: ExecWithOptions: Clientset creation
  May 13 18:47:06.824: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/hostport-4082/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.58.100%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.58.100, port: 54323 UDP @ 05/13/23 18:47:06.875
  May 13 18:47:06.875: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.58.100 54323] Namespace:hostport-4082 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:47:06.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:47:06.875: INFO: ExecWithOptions: Clientset creation
  May 13 18:47:06.875: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/hostport-4082/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.58.100+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0513 18:47:07.014131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:08.014471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:09.014846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:10.015163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:11.015287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:47:11.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-4082" for this suite. @ 05/13/23 18:47:11.913
• [15.263 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 05/13/23 18:47:11.933
  May 13 18:47:11.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 18:47:11.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:47:11.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:47:11.946
  STEP: creating the pod @ 05/13/23 18:47:11.947
  May 13 18:47:11.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-578 create -f -'
  E0513 18:47:12.015336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:47:12.344: INFO: stderr: ""
  May 13 18:47:12.344: INFO: stdout: "pod/pause created\n"
  E0513 18:47:13.016091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:14.022636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/13/23 18:47:14.351
  May 13 18:47:14.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-578 label pods pause testing-label=testing-label-value'
  May 13 18:47:14.401: INFO: stderr: ""
  May 13 18:47:14.401: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/13/23 18:47:14.401
  May 13 18:47:14.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-578 get pod pause -L testing-label'
  May 13 18:47:14.440: INFO: stderr: ""
  May 13 18:47:14.440: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/13/23 18:47:14.44
  May 13 18:47:14.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-578 label pods pause testing-label-'
  May 13 18:47:14.486: INFO: stderr: ""
  May 13 18:47:14.487: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/13/23 18:47:14.487
  May 13 18:47:14.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-578 get pod pause -L testing-label'
  May 13 18:47:14.524: INFO: stderr: ""
  May 13 18:47:14.524: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 05/13/23 18:47:14.524
  May 13 18:47:14.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-578 delete --grace-period=0 --force -f -'
  May 13 18:47:14.575: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 13 18:47:14.575: INFO: stdout: "pod \"pause\" force deleted\n"
  May 13 18:47:14.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-578 get rc,svc -l name=pause --no-headers'
  May 13 18:47:14.634: INFO: stderr: "No resources found in kubectl-578 namespace.\n"
  May 13 18:47:14.634: INFO: stdout: ""
  May 13 18:47:14.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-578 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 13 18:47:14.676: INFO: stderr: ""
  May 13 18:47:14.676: INFO: stdout: ""
  May 13 18:47:14.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-578" for this suite. @ 05/13/23 18:47:14.678
• [2.748 seconds]
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 05/13/23 18:47:14.681
  May 13 18:47:14.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename gc @ 05/13/23 18:47:14.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:47:14.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:47:14.693
  STEP: create the rc @ 05/13/23 18:47:14.699
  W0513 18:47:14.702132      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0513 18:47:15.023357      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:16.024388      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:17.024974      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:18.032908      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:19.033997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:20.045750      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/13/23 18:47:20.745
  STEP: wait for the rc to be deleted @ 05/13/23 18:47:20.759
  E0513 18:47:21.046178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:47:21.811: INFO: 80 pods remaining
  May 13 18:47:21.811: INFO: 80 pods has nil DeletionTimestamp
  May 13 18:47:21.811: INFO: 
  E0513 18:47:22.047196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:47:22.816: INFO: 72 pods remaining
  May 13 18:47:22.816: INFO: 71 pods has nil DeletionTimestamp
  May 13 18:47:22.816: INFO: 
  E0513 18:47:23.048351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:47:23.797: INFO: 60 pods remaining
  May 13 18:47:23.797: INFO: 60 pods has nil DeletionTimestamp
  May 13 18:47:23.797: INFO: 
  E0513 18:47:24.053413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:47:24.819: INFO: 40 pods remaining
  May 13 18:47:24.819: INFO: 40 pods has nil DeletionTimestamp
  May 13 18:47:24.820: INFO: 
  E0513 18:47:25.054787      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:47:25.975: INFO: 32 pods remaining
  May 13 18:47:25.975: INFO: 29 pods has nil DeletionTimestamp
  May 13 18:47:25.975: INFO: 
  E0513 18:47:26.055163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:47:26.795: INFO: 20 pods remaining
  May 13 18:47:26.795: INFO: 20 pods has nil DeletionTimestamp
  May 13 18:47:26.795: INFO: 
  E0513 18:47:27.055674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/13/23 18:47:27.763
  E0513 18:47:28.056594      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:29.057578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:30.059338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:31.059347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:32.060694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:33.061025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:34.062103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:35.062797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:36.063666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:37.064501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:38.065422      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:39.068404      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:40.070707      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:41.071074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:42.072011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:43.073166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:44.075892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:45.076538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:46.076352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:47.078886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:48.078727      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:49.080517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:50.082203      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:51.082969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:52.083073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:53.083254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:54.085060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:55.087456      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:56.088081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:57.089172      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:58.090763      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:47:59.090784      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:00.091229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:01.092819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:02.094148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:03.096844      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:04.096665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:05.097149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:06.097411      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:07.099146      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:08.099811      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:09.100021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:10.101327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:11.101573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:12.102626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:13.103157      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:14.103520      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:15.104001      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:16.107477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:17.107523      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:18.108525      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:19.109551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:20.110606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:21.110772      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:22.111478      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:23.114620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:24.115028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:25.115698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:26.115988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:27.115888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:48:27.930: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
  May 13 18:48:27.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2986" for this suite. @ 05/13/23 18:48:27.932
• [73.265 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 05/13/23 18:48:27.948
  May 13 18:48:27.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename cronjob @ 05/13/23 18:48:27.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:48:27.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:48:27.956
  STEP: Creating a cronjob @ 05/13/23 18:48:27.958
  STEP: creating @ 05/13/23 18:48:27.958
  STEP: getting @ 05/13/23 18:48:27.962
  STEP: listing @ 05/13/23 18:48:27.963
  STEP: watching @ 05/13/23 18:48:27.964
  May 13 18:48:27.964: INFO: starting watch
  STEP: cluster-wide listing @ 05/13/23 18:48:27.966
  STEP: cluster-wide watching @ 05/13/23 18:48:27.967
  May 13 18:48:27.967: INFO: starting watch
  STEP: patching @ 05/13/23 18:48:27.967
  STEP: updating @ 05/13/23 18:48:27.971
  May 13 18:48:27.975: INFO: waiting for watch events with expected annotations
  May 13 18:48:27.975: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/13/23 18:48:27.975
  STEP: updating /status @ 05/13/23 18:48:27.979
  STEP: get /status @ 05/13/23 18:48:27.983
  STEP: deleting @ 05/13/23 18:48:27.984
  STEP: deleting a collection @ 05/13/23 18:48:27.991
  May 13 18:48:27.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9497" for this suite. @ 05/13/23 18:48:27.995
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/13/23 18:48:28
  May 13 18:48:28.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename cronjob @ 05/13/23 18:48:28.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:48:28.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:48:28.01
  STEP: Creating a cronjob @ 05/13/23 18:48:28.011
  STEP: Ensuring more than one job is running at a time @ 05/13/23 18:48:28.013
  E0513 18:48:28.116557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:29.117113      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:30.118475      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:31.121009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:32.121640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:33.122215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:34.122863      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:35.123845      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:36.124872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:37.125919      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:38.126017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:39.126584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:40.126790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:41.127170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:42.127443      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:43.127651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:44.128786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:45.129803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:46.130060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:47.131128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:48.131887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:49.133375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:50.136380      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:51.137198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:52.138689      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:53.139549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:54.139841      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:55.140764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:56.142293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:57.143405      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:58.146619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:48:59.148239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:00.149421      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:01.149774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:02.151271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:03.151570      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:04.153681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:05.153865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:06.154272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:07.155734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:08.157061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:09.157069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:10.157558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:11.158813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:12.159660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:13.160042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:14.160933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:15.163913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:16.165065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:17.165237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:18.167099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:19.167140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:20.167281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:21.168201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:22.169963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:23.171259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:24.175253      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:25.175839      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:26.176337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:27.178741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:28.178736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:29.178833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:30.179987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:31.181130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:32.182319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:33.183315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:34.183692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:35.184611      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:36.185582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:37.185726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:38.186364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:39.187101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:40.188600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:41.189760      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:42.190495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:43.191773      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:44.192704      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:45.193025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:46.193734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:47.194504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:48.195536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:49.196766      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:50.197823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:51.197918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:52.198724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:53.200692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:54.203988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:55.205143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:56.205498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:57.206789      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:58.206852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:49:59.207736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:00.208830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:01.208894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/13/23 18:50:02.02
  STEP: Removing cronjob @ 05/13/23 18:50:02.025
  May 13 18:50:02.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1023" for this suite. @ 05/13/23 18:50:02.038
• [94.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 05/13/23 18:50:02.043
  May 13 18:50:02.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 18:50:02.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:50:02.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:50:02.058
  STEP: Creating a ResourceQuota @ 05/13/23 18:50:02.064
  STEP: Getting a ResourceQuota @ 05/13/23 18:50:02.067
  STEP: Updating a ResourceQuota @ 05/13/23 18:50:02.069
  STEP: Verifying a ResourceQuota was modified @ 05/13/23 18:50:02.072
  STEP: Deleting a ResourceQuota @ 05/13/23 18:50:02.073
  STEP: Verifying the deleted ResourceQuota @ 05/13/23 18:50:02.075
  May 13 18:50:02.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6802" for this suite. @ 05/13/23 18:50:02.077
• [0.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 05/13/23 18:50:02.083
  May 13 18:50:02.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 18:50:02.084
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:50:02.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:50:02.097
  STEP: Discovering how many secrets are in namespace by default @ 05/13/23 18:50:02.099
  E0513 18:50:02.209642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:03.210472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:04.210595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:05.214751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:06.215458      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/13/23 18:50:07.1
  E0513 18:50:07.215632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:08.216719      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:09.216662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:10.217830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:11.218809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/13/23 18:50:12.107
  STEP: Ensuring resource quota status is calculated @ 05/13/23 18:50:12.113
  E0513 18:50:12.222904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:13.223936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 05/13/23 18:50:14.119
  STEP: Ensuring resource quota status captures secret creation @ 05/13/23 18:50:14.135
  E0513 18:50:14.225818      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:15.226076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 05/13/23 18:50:16.138
  STEP: Ensuring resource quota status released usage @ 05/13/23 18:50:16.141
  E0513 18:50:16.228454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:17.229288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:50:18.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6605" for this suite. @ 05/13/23 18:50:18.152
• [16.087 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 05/13/23 18:50:18.171
  May 13 18:50:18.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename subpath @ 05/13/23 18:50:18.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:50:18.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:50:18.184
  STEP: Setting up data @ 05/13/23 18:50:18.186
  STEP: Creating pod pod-subpath-test-configmap-w749 @ 05/13/23 18:50:18.19
  STEP: Creating a pod to test atomic-volume-subpath @ 05/13/23 18:50:18.19
  E0513 18:50:18.229959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:19.231053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:20.231807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:21.232597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:22.232912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:23.233533      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:24.233823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:25.234483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:26.234582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:27.234703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:28.235682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:29.236189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:30.237145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:31.237676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:32.238099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:33.238645      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:34.239505      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:35.242054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:36.242702      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:37.244118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:38.244795      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:39.247019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:40.248105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:41.248833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:42.250063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:50:42.262
  May 13 18:50:42.265: INFO: Trying to get logs from node worker00 pod pod-subpath-test-configmap-w749 container test-container-subpath-configmap-w749: <nil>
  STEP: delete the pod @ 05/13/23 18:50:42.271
  STEP: Deleting pod pod-subpath-test-configmap-w749 @ 05/13/23 18:50:42.291
  May 13 18:50:42.291: INFO: Deleting pod "pod-subpath-test-configmap-w749" in namespace "subpath-4145"
  May 13 18:50:42.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4145" for this suite. @ 05/13/23 18:50:42.294
• [24.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 05/13/23 18:50:42.299
  May 13 18:50:42.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:50:42.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:50:42.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:50:42.308
  STEP: Creating configMap with name cm-test-opt-del-c242e909-be90-4d9d-b975-23bb9fe2fac7 @ 05/13/23 18:50:42.31
  STEP: Creating configMap with name cm-test-opt-upd-74bd459a-7263-4df2-93b1-2d24b1aa51c0 @ 05/13/23 18:50:42.312
  STEP: Creating the pod @ 05/13/23 18:50:42.313
  E0513 18:50:43.254304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:44.258653      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-c242e909-be90-4d9d-b975-23bb9fe2fac7 @ 05/13/23 18:50:44.341
  STEP: Updating configmap cm-test-opt-upd-74bd459a-7263-4df2-93b1-2d24b1aa51c0 @ 05/13/23 18:50:44.345
  STEP: Creating configMap with name cm-test-opt-create-45a6aa9a-3d09-4b7b-b813-83c9155c481c @ 05/13/23 18:50:44.349
  STEP: waiting to observe update in volume @ 05/13/23 18:50:44.351
  E0513 18:50:45.258481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:46.259506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:47.259893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:48.260448      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:50:48.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6993" for this suite. @ 05/13/23 18:50:48.376
• [6.092 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 05/13/23 18:50:48.391
  May 13 18:50:48.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 18:50:48.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:50:48.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:50:48.4
  STEP: Creating Pod @ 05/13/23 18:50:48.402
  E0513 18:50:49.261380      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:50.263694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 05/13/23 18:50:50.412
  May 13 18:50:50.412: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2206 PodName:pod-sharedvolume-d0e30f09-30e1-46c8-8a6e-ab0f7b21e46b ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 18:50:50.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 18:50:50.413: INFO: ExecWithOptions: Clientset creation
  May 13 18:50:50.413: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/emptydir-2206/pods/pod-sharedvolume-d0e30f09-30e1-46c8-8a6e-ab0f7b21e46b/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  May 13 18:50:50.472: INFO: Exec stderr: ""
  May 13 18:50:50.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2206" for this suite. @ 05/13/23 18:50:50.475
• [2.097 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 05/13/23 18:50:50.488
  May 13 18:50:50.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sched-preemption @ 05/13/23 18:50:50.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:50:50.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:50:50.499
  May 13 18:50:50.505: INFO: Waiting up to 1m0s for all nodes to be ready
  E0513 18:50:51.263373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:52.265664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:53.265713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:54.266724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:55.267117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:56.268266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:57.268470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:58.269487      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:50:59.270031      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:00.270223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:01.270549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:02.271258      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:03.281144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:04.285232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:05.285028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:06.285875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:07.287397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:08.288476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:09.289319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:10.289910      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:11.290200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:12.291333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:13.291454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:14.291818      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:15.292627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:16.292862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:17.293440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:18.296406      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:19.297178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:20.297780      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:21.299042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:22.299237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:23.299925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:24.311040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:25.312277      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:26.312340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:27.314418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:28.315053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:29.316644      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:30.317237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:31.319077      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:32.319757      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:33.320381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:34.320830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:35.321263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:36.322652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:37.322767      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:38.324187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:39.325636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:40.325963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:41.326238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:42.326341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:43.329127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:44.329856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:45.330732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:46.331464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:47.335242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:48.338108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:49.338562      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:50.338902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:51:50.535: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/13/23 18:51:50.538
  May 13 18:51:50.566: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 13 18:51:50.574: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 13 18:51:50.587: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 13 18:51:50.591: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/13/23 18:51:50.591
  E0513 18:51:51.339407      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:52.340487      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/13/23 18:51:52.607
  E0513 18:51:53.341290      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:54.341448      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:55.341513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:56.343218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:51:56.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-2355" for this suite. @ 05/13/23 18:51:56.685
• [66.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 05/13/23 18:51:56.689
  May 13 18:51:56.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:51:56.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:51:56.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:51:56.699
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 18:51:56.703
  E0513 18:51:57.344138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:58.345247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:51:59.345335      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:00.348906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:52:00.729
  May 13 18:52:00.733: INFO: Trying to get logs from node worker00 pod downwardapi-volume-d8796b93-1109-4d60-b0a7-3bc2884b2540 container client-container: <nil>
  STEP: delete the pod @ 05/13/23 18:52:00.74
  May 13 18:52:00.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8867" for this suite. @ 05/13/23 18:52:00.766
• [4.079 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 05/13/23 18:52:00.768
  May 13 18:52:00.769: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 18:52:00.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:00.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:00.777
  STEP: Counting existing ResourceQuota @ 05/13/23 18:52:00.778
  E0513 18:52:01.348349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:02.349140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:03.350196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:04.350506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:05.351449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/13/23 18:52:05.781
  STEP: Ensuring resource quota status is calculated @ 05/13/23 18:52:05.792
  E0513 18:52:06.351628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:07.351947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 05/13/23 18:52:07.799
  STEP: Creating a NodePort Service @ 05/13/23 18:52:07.828
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/13/23 18:52:07.842
  STEP: Ensuring resource quota status captures service creation @ 05/13/23 18:52:07.849
  E0513 18:52:08.352321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:09.352778      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 05/13/23 18:52:09.855
  STEP: Ensuring resource quota status released usage @ 05/13/23 18:52:09.877
  E0513 18:52:10.353294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:11.353954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:52:11.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5427" for this suite. @ 05/13/23 18:52:11.881
• [11.118 seconds]
------------------------------
SS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/13/23 18:52:11.887
  May 13 18:52:11.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename certificates @ 05/13/23 18:52:11.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:11.897
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:11.899
  E0513 18:52:12.354496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 05/13/23 18:52:12.37
  STEP: getting /apis/certificates.k8s.io @ 05/13/23 18:52:12.372
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/13/23 18:52:12.372
  STEP: creating @ 05/13/23 18:52:12.373
  STEP: getting @ 05/13/23 18:52:12.385
  STEP: listing @ 05/13/23 18:52:12.387
  STEP: watching @ 05/13/23 18:52:12.389
  May 13 18:52:12.389: INFO: starting watch
  STEP: patching @ 05/13/23 18:52:12.39
  STEP: updating @ 05/13/23 18:52:12.393
  May 13 18:52:12.396: INFO: waiting for watch events with expected annotations
  May 13 18:52:12.396: INFO: saw patched and updated annotations
  STEP: getting /approval @ 05/13/23 18:52:12.396
  STEP: patching /approval @ 05/13/23 18:52:12.398
  STEP: updating /approval @ 05/13/23 18:52:12.4
  STEP: getting /status @ 05/13/23 18:52:12.403
  STEP: patching /status @ 05/13/23 18:52:12.404
  STEP: updating /status @ 05/13/23 18:52:12.407
  STEP: deleting @ 05/13/23 18:52:12.41
  STEP: deleting a collection @ 05/13/23 18:52:12.414
  May 13 18:52:12.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-9040" for this suite. @ 05/13/23 18:52:12.419
• [0.536 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 05/13/23 18:52:12.425
  May 13 18:52:12.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 18:52:12.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:12.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:12.436
  STEP: Creating configMap configmap-1953/configmap-test-45b90bfe-e6d2-45ef-901a-b0be5493d6b0 @ 05/13/23 18:52:12.438
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:52:12.441
  E0513 18:52:13.355197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:14.355861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:52:14.451
  May 13 18:52:14.453: INFO: Trying to get logs from node worker00 pod pod-configmaps-ee531de6-b02b-4835-9e18-9c5472bacf2d container env-test: <nil>
  STEP: delete the pod @ 05/13/23 18:52:14.457
  May 13 18:52:14.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1953" for this suite. @ 05/13/23 18:52:14.468
• [2.047 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 05/13/23 18:52:14.472
  May 13 18:52:14.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 18:52:14.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:14.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:14.485
  STEP: validating cluster-info @ 05/13/23 18:52:14.487
  May 13 18:52:14.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3702 cluster-info'
  May 13 18:52:14.533: INFO: stderr: ""
  May 13 18:52:14.533: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  May 13 18:52:14.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3702" for this suite. @ 05/13/23 18:52:14.535
• [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 05/13/23 18:52:14.539
  May 13 18:52:14.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename gc @ 05/13/23 18:52:14.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:14.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:14.553
  May 13 18:52:14.576: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"0e595441-475b-4508-a342-30f63f415a5f", Controller:(*bool)(0xc0040bad66), BlockOwnerDeletion:(*bool)(0xc0040bad67)}}
  May 13 18:52:14.582: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"baaffc01-91e0-4758-87d5-4cff2506a79f", Controller:(*bool)(0xc0054c6fbe), BlockOwnerDeletion:(*bool)(0xc0054c6fbf)}}
  May 13 18:52:14.586: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"734a762d-49c5-47e3-98a1-c01b8468c9bd", Controller:(*bool)(0xc0040baf8e), BlockOwnerDeletion:(*bool)(0xc0040baf8f)}}
  E0513 18:52:15.356883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:16.360917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:17.361648      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:18.364359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:19.365407      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:52:19.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8995" for this suite. @ 05/13/23 18:52:19.595
• [5.060 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 05/13/23 18:52:19.6
  May 13 18:52:19.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/13/23 18:52:19.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:19.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:19.616
  May 13 18:52:19.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:52:20.366382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/13/23 18:52:20.837
  May 13 18:52:20.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-2111 --namespace=crd-publish-openapi-2111 create -f -'
  May 13 18:52:21.189: INFO: stderr: ""
  May 13 18:52:21.189: INFO: stdout: "e2e-test-crd-publish-openapi-3974-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 13 18:52:21.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-2111 --namespace=crd-publish-openapi-2111 delete e2e-test-crd-publish-openapi-3974-crds test-cr'
  May 13 18:52:21.236: INFO: stderr: ""
  May 13 18:52:21.236: INFO: stdout: "e2e-test-crd-publish-openapi-3974-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  May 13 18:52:21.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-2111 --namespace=crd-publish-openapi-2111 apply -f -'
  E0513 18:52:21.367908      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:52:21.386: INFO: stderr: ""
  May 13 18:52:21.387: INFO: stdout: "e2e-test-crd-publish-openapi-3974-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 13 18:52:21.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-2111 --namespace=crd-publish-openapi-2111 delete e2e-test-crd-publish-openapi-3974-crds test-cr'
  May 13 18:52:21.428: INFO: stderr: ""
  May 13 18:52:21.428: INFO: stdout: "e2e-test-crd-publish-openapi-3974-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/13/23 18:52:21.428
  May 13 18:52:21.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-2111 explain e2e-test-crd-publish-openapi-3974-crds'
  May 13 18:52:21.571: INFO: stderr: ""
  May 13 18:52:21.571: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-3974-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0513 18:52:22.370989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:52:22.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2111" for this suite. @ 05/13/23 18:52:22.732
• [3.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 05/13/23 18:52:22.749
  May 13 18:52:22.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename field-validation @ 05/13/23 18:52:22.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:22.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:22.759
  May 13 18:52:22.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  W0513 18:52:22.761031      21 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc006260470 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0513 18:52:23.373306      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:24.374609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0513 18:52:25.306270      21 warnings.go:70] unknown field "alpha"
  W0513 18:52:25.306314      21 warnings.go:70] unknown field "beta"
  W0513 18:52:25.306330      21 warnings.go:70] unknown field "delta"
  W0513 18:52:25.306336      21 warnings.go:70] unknown field "epsilon"
  W0513 18:52:25.306340      21 warnings.go:70] unknown field "gamma"
  May 13 18:52:25.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6025" for this suite. @ 05/13/23 18:52:25.319
• [2.574 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/13/23 18:52:25.324
  May 13 18:52:25.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubelet-test @ 05/13/23 18:52:25.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:25.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:25.335
  E0513 18:52:25.374786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:26.374830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:52:27.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8721" for this suite. @ 05/13/23 18:52:27.373
  E0513 18:52:27.375080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
• [2.053 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 05/13/23 18:52:27.38
  May 13 18:52:27.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename var-expansion @ 05/13/23 18:52:27.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:27.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:27.389
  STEP: Creating a pod to test substitution in container's command @ 05/13/23 18:52:27.39
  E0513 18:52:28.375561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:29.375695      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:30.376522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:31.377218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:52:31.402
  May 13 18:52:31.408: INFO: Trying to get logs from node worker00 pod var-expansion-231ca207-4d45-4031-a71b-bd91c5cc8a38 container dapi-container: <nil>
  STEP: delete the pod @ 05/13/23 18:52:31.414
  May 13 18:52:31.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4948" for this suite. @ 05/13/23 18:52:31.427
• [4.050 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 05/13/23 18:52:31.431
  May 13 18:52:31.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename runtimeclass @ 05/13/23 18:52:31.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:31.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:31.44
  STEP: Deleting RuntimeClass runtimeclass-7593-delete-me @ 05/13/23 18:52:31.445
  STEP: Waiting for the RuntimeClass to disappear @ 05/13/23 18:52:31.451
  May 13 18:52:31.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7593" for this suite. @ 05/13/23 18:52:31.459
• [0.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 05/13/23 18:52:31.464
  May 13 18:52:31.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename statefulset @ 05/13/23 18:52:31.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:31.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:31.475
  STEP: Creating service test in namespace statefulset-3452 @ 05/13/23 18:52:31.476
  May 13 18:52:31.488: INFO: Found 0 stateful pods, waiting for 1
  E0513 18:52:32.377431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:33.377614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:34.377685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:35.378560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:36.379069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:37.379213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:38.379357      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:39.382724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:40.382728      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:41.384688      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:52:41.496: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/13/23 18:52:41.503
  W0513 18:52:41.511889      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 13 18:52:41.514: INFO: Found 1 stateful pods, waiting for 2
  E0513 18:52:42.384732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:43.389506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:44.389301      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:45.390960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:46.391718      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:47.392207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:48.392472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:49.392852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:50.392979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:51.393475      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:52:51.529: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 13 18:52:51.529: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/13/23 18:52:51.535
  STEP: Delete all of the StatefulSets @ 05/13/23 18:52:51.537
  STEP: Verify that StatefulSets have been deleted @ 05/13/23 18:52:51.541
  May 13 18:52:51.542: INFO: Deleting all statefulset in ns statefulset-3452
  May 13 18:52:51.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3452" for this suite. @ 05/13/23 18:52:51.559
• [20.104 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 05/13/23 18:52:51.568
  May 13 18:52:51.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:52:51.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:52:51.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:52:51.586
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-8955 @ 05/13/23 18:52:51.587
  STEP: changing the ExternalName service to type=ClusterIP @ 05/13/23 18:52:51.592
  STEP: creating replication controller externalname-service in namespace services-8955 @ 05/13/23 18:52:51.606
  I0513 18:52:51.616843      21 runners.go:194] Created replication controller with name: externalname-service, namespace: services-8955, replica count: 2
  E0513 18:52:52.393697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:53.396778      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:54.397300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:52:54.669258      21 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 13 18:52:54.669: INFO: Creating new exec pod
  E0513 18:52:55.397798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:56.398968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:52:57.399182      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:52:57.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-8955 exec execpodb9sd5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 13 18:52:57.782: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 13 18:52:57.782: INFO: stdout: ""
  E0513 18:52:58.399572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:52:58.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-8955 exec execpodb9sd5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 13 18:52:58.894: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 13 18:52:58.894: INFO: stdout: ""
  E0513 18:52:59.400391      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:52:59.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-8955 exec execpodb9sd5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 13 18:52:59.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 13 18:52:59.890: INFO: stdout: ""
  E0513 18:53:00.400136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:00.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-8955 exec execpodb9sd5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 13 18:53:00.879: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 13 18:53:00.879: INFO: stdout: "externalname-service-g9mgp"
  May 13 18:53:00.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-8955 exec execpodb9sd5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.166 80'
  May 13 18:53:00.979: INFO: stderr: "+ nc -v -t -w 2 10.32.0.166 80\n+ echo hostName\nConnection to 10.32.0.166 80 port [tcp/http] succeeded!\n"
  May 13 18:53:00.979: INFO: stdout: ""
  E0513 18:53:01.402760      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:01.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-8955 exec execpodb9sd5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.166 80'
  May 13 18:53:02.057: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.166 80\nConnection to 10.32.0.166 80 port [tcp/http] succeeded!\n"
  May 13 18:53:02.057: INFO: stdout: ""
  E0513 18:53:02.403605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:02.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-8955 exec execpodb9sd5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.166 80'
  May 13 18:53:03.085: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.166 80\nConnection to 10.32.0.166 80 port [tcp/http] succeeded!\n"
  May 13 18:53:03.085: INFO: stdout: "externalname-service-x6gr5"
  May 13 18:53:03.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 18:53:03.087: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-8955" for this suite. @ 05/13/23 18:53:03.116
• [11.552 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/13/23 18:53:03.121
  May 13 18:53:03.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename deployment @ 05/13/23 18:53:03.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:53:03.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:53:03.131
  May 13 18:53:03.132: INFO: Creating deployment "webserver-deployment"
  May 13 18:53:03.134: INFO: Waiting for observed generation 1
  E0513 18:53:03.405193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:04.405758      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:05.144: INFO: Waiting for all required pods to come up
  May 13 18:53:05.152: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/13/23 18:53:05.152
  May 13 18:53:05.153: INFO: Waiting for deployment "webserver-deployment" to complete
  May 13 18:53:05.166: INFO: Updating deployment "webserver-deployment" with a non-existent image
  May 13 18:53:05.180: INFO: Updating deployment webserver-deployment
  May 13 18:53:05.180: INFO: Waiting for observed generation 2
  E0513 18:53:05.407936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:06.408371      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:07.187: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  May 13 18:53:07.193: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  May 13 18:53:07.197: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 13 18:53:07.203: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  May 13 18:53:07.203: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  May 13 18:53:07.204: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 13 18:53:07.207: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  May 13 18:53:07.207: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  May 13 18:53:07.221: INFO: Updating deployment webserver-deployment
  May 13 18:53:07.221: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  May 13 18:53:07.227: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  May 13 18:53:07.230: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  May 13 18:53:07.243: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-8079  2f901767-dc48-4996-af7f-54a10d33d92b 25915 3 2023-05-13 18:53:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-13 18:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 18:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0004dd718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-05-13 18:53:05 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-13 18:53:07 +0000 UTC,LastTransitionTime:2023-05-13 18:53:07 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  May 13 18:53:07.252: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-8079  3313a362-3bff-4e64-8a9b-aeb953be9186 25911 3 2023-05-13 18:53:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 2f901767-dc48-4996-af7f-54a10d33d92b 0xc001d1d9e7 0xc001d1d9e8}] [] [{kube-controller-manager Update apps/v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-13 18:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f901767-dc48-4996-af7f-54a10d33d92b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d1da88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 13 18:53:07.252: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  May 13 18:53:07.252: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-8079  e913860f-38cf-4ff5-9907-39431de0e706 25910 3 2023-05-13 18:53:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 2f901767-dc48-4996-af7f-54a10d33d92b 0xc001d1d8e7 0xc001d1d8e8}] [] [{kube-controller-manager Update apps/v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-13 18:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f901767-dc48-4996-af7f-54a10d33d92b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d1d978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  May 13 18:53:07.261: INFO: Pod "webserver-deployment-67bd4bf6dc-56g57" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-56g57 webserver-deployment-67bd4bf6dc- deployment-8079  d6837c9b-677b-40df-b639-f1bb6aef338e 25814 0 2023-05-13 18:53:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:2d2ee268020bafa362b3f17ba709872d7df30691711bac545bb04afe5ddc8266 cni.projectcalico.org/podIP:10.200.5.37/32 cni.projectcalico.org/podIPs:10.200.5.37/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc0004dddb7 0xc0004dddb8}] [] [{calico Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.5.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4bld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4bld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.101,PodIP:10.200.5.37,StartTime:2023-05-13 18:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 18:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6201beee0059d065338eeac936794a9074a5bf7e1bdae104d19b08ef52f52292,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.37,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.265: INFO: Pod "webserver-deployment-67bd4bf6dc-6mlrc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6mlrc webserver-deployment-67bd4bf6dc- deployment-8079  e73fd3ae-e9e7-4d37-b275-c99dc770af6c 25806 0 2023-05-13 18:53:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:bc15a86a2de5e2a4eca85410c6cf5e289d29ca8c5acf3045624d4b05a00d3dd3 cni.projectcalico.org/podIP:10.200.131.140/32 cni.projectcalico.org/podIPs:10.200.131.140/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc0004ddfb0 0xc0004ddfb1}] [] [{kube-controller-manager Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-13 18:53:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-13 18:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rnrlx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rnrlx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:10.200.131.140,StartTime:2023-05-13 18:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 18:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://17bdb9b2d5abb25c250e549925d8d7049dde11fe4aea40401c9f50f45703a48b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.140,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.270: INFO: Pod "webserver-deployment-67bd4bf6dc-7hb4g" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7hb4g webserver-deployment-67bd4bf6dc- deployment-8079  0a0faa54-b426-4584-9ecf-110b58423309 25925 0 2023-05-13 18:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc003a661b7 0xc003a661b8}] [] [{kube-controller-manager Update v1 2023-05-13 18:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-566hk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-566hk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.278: INFO: Pod "webserver-deployment-67bd4bf6dc-7mlrc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7mlrc webserver-deployment-67bd4bf6dc- deployment-8079  96a0c050-ceac-4746-8ab9-38f5827741a1 25802 0 2023-05-13 18:53:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:b869024fc807844276cc6582fc3d74965b2bed98a132b1379ed1957473ff18d4 cni.projectcalico.org/podIP:10.200.5.63/32 cni.projectcalico.org/podIPs:10.200.5.63/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc003a66340 0xc003a66341}] [] [{kube-controller-manager Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-13 18:53:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-13 18:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.5.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h22gr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h22gr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.101,PodIP:10.200.5.63,StartTime:2023-05-13 18:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 18:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a63a17d5f8580626fcdcbfd67b5a3c8fb30d0ee77a54fb1e75a17eb949b4f3d3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.63,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.279: INFO: Pod "webserver-deployment-67bd4bf6dc-7tv92" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7tv92 webserver-deployment-67bd4bf6dc- deployment-8079  cc221a3e-3471-4dbf-97f9-9fb91cc60a7e 25920 0 2023-05-13 18:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc003a66540 0xc003a66541}] [] [{kube-controller-manager Update v1 2023-05-13 18:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rl8ld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rl8ld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.279: INFO: Pod "webserver-deployment-67bd4bf6dc-kx4vx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-kx4vx webserver-deployment-67bd4bf6dc- deployment-8079  ccd99b24-92a5-49f6-b97f-746445ad5911 25810 0 2023-05-13 18:53:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:de2b3e3636cab995f67cafccc5b31f26838148ec8bdb4ad6176f455e3e57c785 cni.projectcalico.org/podIP:10.200.131.132/32 cni.projectcalico.org/podIPs:10.200.131.132/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc003a666a0 0xc003a666a1}] [] [{calico Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-56kmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-56kmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:10.200.131.132,StartTime:2023-05-13 18:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 18:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2d8ec1faca6bde3d35407a740c6dcdfadc26eab191e2dae3e82a6bb7432f1a14,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.132,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.280: INFO: Pod "webserver-deployment-67bd4bf6dc-pk46q" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pk46q webserver-deployment-67bd4bf6dc- deployment-8079  27c9b093-51e7-438c-8b9a-89aa8eac2458 25809 0 2023-05-13 18:53:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:60599b6a52f970a41b7be8dd32b419bc5869661c707a2dc71fcaf922587e9ab7 cni.projectcalico.org/podIP:10.200.5.28/32 cni.projectcalico.org/podIPs:10.200.5.28/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc003a668c7 0xc003a668c8}] [] [{calico Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.5.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jnngq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jnngq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.101,PodIP:10.200.5.28,StartTime:2023-05-13 18:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 18:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f18c96df93face4f4ebd90f7be6378594ee8dddc8ebf9727c8838619f5e47d29,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.28,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.280: INFO: Pod "webserver-deployment-67bd4bf6dc-sddcn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sddcn webserver-deployment-67bd4bf6dc- deployment-8079  bcf42194-ab6c-4b4a-9d92-86f6b547a111 25926 0 2023-05-13 18:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc003a66ac0 0xc003a66ac1}] [] [{kube-controller-manager Update v1 2023-05-13 18:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z54vl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z54vl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.281: INFO: Pod "webserver-deployment-67bd4bf6dc-sxkdp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sxkdp webserver-deployment-67bd4bf6dc- deployment-8079  5a580cbe-e2a8-48a1-b57b-df98ca9b90de 25821 0 2023-05-13 18:53:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:75650d3a62dfe29dba7c47acbb552d068f86332c6f0a5c2cc8e507936d63d5c7 cni.projectcalico.org/podIP:10.200.5.17/32 cni.projectcalico.org/podIPs:10.200.5.17/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc003a66c50 0xc003a66c51}] [] [{calico Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.5.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lh5mw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lh5mw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.101,PodIP:10.200.5.17,StartTime:2023-05-13 18:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 18:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://42a7b7065203f0d36495fd23d7d638d003f39d53dfdc8463cb3a139ace1f4e65,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.17,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.281: INFO: Pod "webserver-deployment-67bd4bf6dc-thxt6" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-thxt6 webserver-deployment-67bd4bf6dc- deployment-8079  5c3fe887-e5d4-410e-a917-95f6552efa53 25819 0 2023-05-13 18:53:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:ca0d038e7f11b1bf6e132c7b689d1c18c58009d0fcea75783f6ad54d41ae2d4d cni.projectcalico.org/podIP:10.200.131.184/32 cni.projectcalico.org/podIPs:10.200.131.184/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc003a66e50 0xc003a66e51}] [] [{calico Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lxcxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lxcxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:10.200.131.184,StartTime:2023-05-13 18:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 18:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://91553793d44cab0ec2954a2a7e23243862bc67f3fe35cebce6eef5074ca5a767,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.184,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.289: INFO: Pod "webserver-deployment-67bd4bf6dc-z47d9" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z47d9 webserver-deployment-67bd4bf6dc- deployment-8079  6f16ac8f-0fea-4b92-bd90-a2fd3afbee8b 25807 0 2023-05-13 18:53:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:4ff4565c1520278102f4bd7ecad75392dae811d1f7497bd41221ef3e44fbbfe1 cni.projectcalico.org/podIP:10.200.5.39/32 cni.projectcalico.org/podIPs:10.200.5.39/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc e913860f-38cf-4ff5-9907-39431de0e706 0xc003a67077 0xc003a67078}] [] [{calico Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e913860f-38cf-4ff5-9907-39431de0e706\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.5.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w6qvs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w6qvs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.101,PodIP:10.200.5.39,StartTime:2023-05-13 18:53:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 18:53:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2a2e6cb9cca1937af616d470927a6a5f6a9ca8fc073f7e2e416a5b46e0572073,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.39,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.290: INFO: Pod "webserver-deployment-7b75d79cf5-7hxc2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7hxc2 webserver-deployment-7b75d79cf5- deployment-8079  e38ab3eb-87e0-4d8e-b7d3-aa740ab2b478 25879 0 2023-05-13 18:53:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:a52971246f6bc5e1f9f9798e4a2c4aa8a85d52adbe1377f51ca2ff1cb7397d16 cni.projectcalico.org/podIP:10.200.131.177/32 cni.projectcalico.org/podIPs:10.200.131.177/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3313a362-3bff-4e64-8a9b-aeb953be9186 0xc003a67290 0xc003a67291}] [] [{calico Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3313a362-3bff-4e64-8a9b-aeb953be9186\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9vpnc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9vpnc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:,StartTime:2023-05-13 18:53:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.292: INFO: Pod "webserver-deployment-7b75d79cf5-cncls" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-cncls webserver-deployment-7b75d79cf5- deployment-8079  f35be9c5-ea89-4e4a-b925-ec1b5dc8db45 25881 0 2023-05-13 18:53:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:4e60b779bed74f6e7d74d9cd7648162f3499deda83aa0a876b9c567cfec9661e cni.projectcalico.org/podIP:10.200.5.57/32 cni.projectcalico.org/podIPs:10.200.5.57/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3313a362-3bff-4e64-8a9b-aeb953be9186 0xc003a674c7 0xc003a674c8}] [] [{calico Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3313a362-3bff-4e64-8a9b-aeb953be9186\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kjmhx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kjmhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.101,PodIP:,StartTime:2023-05-13 18:53:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.293: INFO: Pod "webserver-deployment-7b75d79cf5-dtg8p" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-dtg8p webserver-deployment-7b75d79cf5- deployment-8079  a7b9c81b-0f50-4fd0-80aa-67449192a8e3 25919 0 2023-05-13 18:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3313a362-3bff-4e64-8a9b-aeb953be9186 0xc003a676c7 0xc003a676c8}] [] [{kube-controller-manager Update v1 2023-05-13 18:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3313a362-3bff-4e64-8a9b-aeb953be9186\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nddf5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nddf5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.294: INFO: Pod "webserver-deployment-7b75d79cf5-hgmsv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hgmsv webserver-deployment-7b75d79cf5- deployment-8079  3a6476c0-18fc-4704-b129-d73c1427dd73 25921 0 2023-05-13 18:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3313a362-3bff-4e64-8a9b-aeb953be9186 0xc003a67840 0xc003a67841}] [] [{kube-controller-manager Update v1 2023-05-13 18:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3313a362-3bff-4e64-8a9b-aeb953be9186\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cnh9r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cnh9r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.305: INFO: Pod "webserver-deployment-7b75d79cf5-s5mz2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-s5mz2 webserver-deployment-7b75d79cf5- deployment-8079  b20da2bb-00d2-4d4a-9e2e-b444ed7b327a 25885 0 2023-05-13 18:53:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:92117640c12596113c156236e6c4704fa24ed4e60ed36b7a417cbbb9787dbf44 cni.projectcalico.org/podIP:10.200.5.19/32 cni.projectcalico.org/podIPs:10.200.5.19/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3313a362-3bff-4e64-8a9b-aeb953be9186 0xc003a679a7 0xc003a679a8}] [] [{calico Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3313a362-3bff-4e64-8a9b-aeb953be9186\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9wckj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9wckj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.101,PodIP:,StartTime:2023-05-13 18:53:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.307: INFO: Pod "webserver-deployment-7b75d79cf5-tskkv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tskkv webserver-deployment-7b75d79cf5- deployment-8079  ce9d1345-14b3-4912-afb4-c12a8a8d5701 25884 0 2023-05-13 18:53:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:cc36d254bbd6cf963bcdd3c630c282e4aaddc527caa733ae04e52642b37d7000 cni.projectcalico.org/podIP:10.200.131.139/32 cni.projectcalico.org/podIPs:10.200.131.139/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3313a362-3bff-4e64-8a9b-aeb953be9186 0xc003a67ba7 0xc003a67ba8}] [] [{calico Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3313a362-3bff-4e64-8a9b-aeb953be9186\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lgcq8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lgcq8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:,StartTime:2023-05-13 18:53:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.307: INFO: Pod "webserver-deployment-7b75d79cf5-vd65h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vd65h webserver-deployment-7b75d79cf5- deployment-8079  e3848975-336d-41f7-b5ad-b899c47c63b1 25890 0 2023-05-13 18:53:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:01a2cb18e6f7fb9df4955db91a13c0dd624c22b2df0ab22485aa1be9571352c0 cni.projectcalico.org/podIP:10.200.5.48/32 cni.projectcalico.org/podIPs:10.200.5.48/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3313a362-3bff-4e64-8a9b-aeb953be9186 0xc003a67dc7 0xc003a67dc8}] [] [{calico Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3313a362-3bff-4e64-8a9b-aeb953be9186\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 18:53:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fxbpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fxbpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 18:53:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.101,PodIP:,StartTime:2023-05-13 18:53:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.307: INFO: Pod "webserver-deployment-7b75d79cf5-zxd2t" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zxd2t webserver-deployment-7b75d79cf5- deployment-8079  8e20e2ef-a70a-47a5-8e73-bdc7b930c4a1 25922 0 2023-05-13 18:53:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 3313a362-3bff-4e64-8a9b-aeb953be9186 0xc003a67fc7 0xc003a67fc8}] [] [{kube-controller-manager Update v1 2023-05-13 18:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3313a362-3bff-4e64-8a9b-aeb953be9186\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-965s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-965s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 18:53:07.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8079" for this suite. @ 05/13/23 18:53:07.314
• [4.209 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 05/13/23 18:53:07.33
  May 13 18:53:07.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:53:07.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:53:07.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:53:07.351
  STEP: creating service in namespace services-5352 @ 05/13/23 18:53:07.353
  STEP: creating service affinity-nodeport-transition in namespace services-5352 @ 05/13/23 18:53:07.353
  STEP: creating replication controller affinity-nodeport-transition in namespace services-5352 @ 05/13/23 18:53:07.393
  I0513 18:53:07.402292      21 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-5352, replica count: 3
  E0513 18:53:07.408401      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:08.409137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:09.409496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:10.410817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:53:10.455750      21 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 13 18:53:10.468: INFO: Creating new exec pod
  E0513 18:53:11.411955      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:12.412127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:13.415803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:13.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5352 exec execpod-affinitycktc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  May 13 18:53:13.751: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  May 13 18:53:13.751: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:53:13.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5352 exec execpod-affinitycktc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.109 80'
  May 13 18:53:13.862: INFO: stderr: "+ nc -v -t -w 2 10.32.0.109 80\n+ echo hostName\nConnection to 10.32.0.109 80 port [tcp/http] succeeded!\n"
  May 13 18:53:13.862: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:53:13.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5352 exec execpod-affinitycktc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.100 31916'
  May 13 18:53:13.982: INFO: stderr: "+ nc -v -t -w+  2 192.168.58.100 31916echo\n hostName\nConnection to 192.168.58.100 31916 port [tcp/*] succeeded!\n"
  May 13 18:53:13.982: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:53:13.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5352 exec execpod-affinitycktc9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.101 31916'
  May 13 18:53:14.108: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.101 31916\nConnection to 192.168.58.101 31916 port [tcp/*] succeeded!\n"
  May 13 18:53:14.108: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 18:53:14.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5352 exec execpod-affinitycktc9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.58.100:31916/ ; done'
  May 13 18:53:14.271: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n"
  May 13 18:53:14.271: INFO: stdout: "\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-s9m2d\naffinity-nodeport-transition-s9m2d\naffinity-nodeport-transition-s9m2d\naffinity-nodeport-transition-s9m2d\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-s9m2d\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-8xm2p\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-s9m2d\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-8xm2p"
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-s9m2d
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-s9m2d
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-s9m2d
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-s9m2d
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-s9m2d
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-8xm2p
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-s9m2d
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.271: INFO: Received response from host: affinity-nodeport-transition-8xm2p
  May 13 18:53:14.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5352 exec execpod-affinitycktc9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.58.100:31916/ ; done'
  E0513 18:53:14.416624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:14.418: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.58.100:31916/\n"
  May 13 18:53:14.418: INFO: stdout: "\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96\naffinity-nodeport-transition-r5d96"
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Received response from host: affinity-nodeport-transition-r5d96
  May 13 18:53:14.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 18:53:14.422: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5352, will wait for the garbage collector to delete the pods @ 05/13/23 18:53:14.429
  May 13 18:53:14.485: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.992634ms
  May 13 18:53:14.588: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 103.062305ms
  E0513 18:53:15.416720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:16.420396      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5352" for this suite. @ 05/13/23 18:53:16.607
• [9.280 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 05/13/23 18:53:16.611
  May 13 18:53:16.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:53:16.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:53:16.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:53:16.621
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5920 @ 05/13/23 18:53:16.623
  STEP: changing the ExternalName service to type=NodePort @ 05/13/23 18:53:16.625
  STEP: creating replication controller externalname-service in namespace services-5920 @ 05/13/23 18:53:16.63
  I0513 18:53:16.634511      21 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5920, replica count: 2
  E0513 18:53:17.419782      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:18.420880      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:19.423007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 18:53:19.684960      21 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 13 18:53:19.685: INFO: Creating new exec pod
  E0513 18:53:20.423157      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:21.423658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:22.425794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:22.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5920 exec execpod68wxs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 13 18:53:22.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 13 18:53:22.811: INFO: stdout: "externalname-service-nqj5x"
  May 13 18:53:22.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5920 exec execpod68wxs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.103 80'
  May 13 18:53:22.889: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.103 80\nConnection to 10.32.0.103 80 port [tcp/http] succeeded!\n"
  May 13 18:53:22.889: INFO: stdout: ""
  E0513 18:53:23.426883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:23.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5920 exec execpod68wxs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.103 80'
  May 13 18:53:23.987: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.103 80\nConnection to 10.32.0.103 80 port [tcp/http] succeeded!\n"
  May 13 18:53:23.987: INFO: stdout: ""
  E0513 18:53:24.427121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:24.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5920 exec execpod68wxs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.103 80'
  May 13 18:53:24.966: INFO: stderr: "+ + nc -v -t -w 2echo 10.32.0.103 hostName 80\n\nConnection to 10.32.0.103 80 port [tcp/http] succeeded!\n"
  May 13 18:53:24.966: INFO: stdout: ""
  E0513 18:53:25.427084      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:25.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5920 exec execpod68wxs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.103 80'
  May 13 18:53:25.978: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.103 80\nConnection to 10.32.0.103 80 port [tcp/http] succeeded!\n"
  May 13 18:53:25.978: INFO: stdout: "externalname-service-9chcn"
  May 13 18:53:25.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5920 exec execpod68wxs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.100 30938'
  May 13 18:53:26.081: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.100 30938\nConnection to 192.168.58.100 30938 port [tcp/*] succeeded!\n"
  May 13 18:53:26.081: INFO: stdout: ""
  E0513 18:53:26.428497      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:27.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5920 exec execpod68wxs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.100 30938'
  May 13 18:53:27.172: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.100 30938\nConnection to 192.168.58.100 30938 port [tcp/*] succeeded!\n"
  May 13 18:53:27.172: INFO: stdout: ""
  E0513 18:53:27.428985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:28.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5920 exec execpod68wxs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.100 30938'
  May 13 18:53:28.163: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.100 30938\nConnection to 192.168.58.100 30938 port [tcp/*] succeeded!\n"
  May 13 18:53:28.163: INFO: stdout: "externalname-service-9chcn"
  May 13 18:53:28.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-5920 exec execpod68wxs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.58.101 30938'
  May 13 18:53:28.250: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.58.101 30938\nConnection to 192.168.58.101 30938 port [tcp/*] succeeded!\n"
  May 13 18:53:28.250: INFO: stdout: "externalname-service-9chcn"
  May 13 18:53:28.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 18:53:28.252: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-5920" for this suite. @ 05/13/23 18:53:28.266
• [11.660 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 05/13/23 18:53:28.271
  May 13 18:53:28.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 18:53:28.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:53:28.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:53:28.281
  STEP: creating a replication controller @ 05/13/23 18:53:28.282
  May 13 18:53:28.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 create -f -'
  E0513 18:53:28.429071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:28.740: INFO: stderr: ""
  May 13 18:53:28.740: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/13/23 18:53:28.74
  May 13 18:53:28.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 13 18:53:28.840: INFO: stderr: ""
  May 13 18:53:28.840: INFO: stdout: "update-demo-nautilus-zrfvg update-demo-nautilus-zx5jz "
  May 13 18:53:28.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zrfvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 18:53:28.917: INFO: stderr: ""
  May 13 18:53:28.917: INFO: stdout: ""
  May 13 18:53:28.918: INFO: update-demo-nautilus-zrfvg is created but not running
  E0513 18:53:29.429585      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:30.430651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:31.431747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:32.432039      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:33.432678      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:33.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 13 18:53:33.978: INFO: stderr: ""
  May 13 18:53:33.978: INFO: stdout: "update-demo-nautilus-zrfvg update-demo-nautilus-zx5jz "
  May 13 18:53:33.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zrfvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 18:53:34.025: INFO: stderr: ""
  May 13 18:53:34.025: INFO: stdout: ""
  May 13 18:53:34.025: INFO: update-demo-nautilus-zrfvg is created but not running
  E0513 18:53:34.433616      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:35.434768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:36.435120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:37.435395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:38.435610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:39.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 13 18:53:39.106: INFO: stderr: ""
  May 13 18:53:39.106: INFO: stdout: "update-demo-nautilus-zrfvg update-demo-nautilus-zx5jz "
  May 13 18:53:39.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zrfvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 18:53:39.186: INFO: stderr: ""
  May 13 18:53:39.186: INFO: stdout: ""
  May 13 18:53:39.186: INFO: update-demo-nautilus-zrfvg is created but not running
  E0513 18:53:39.436641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:40.437960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:41.440110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:42.440342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:43.441206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:44.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 13 18:53:44.247: INFO: stderr: ""
  May 13 18:53:44.247: INFO: stdout: "update-demo-nautilus-zrfvg update-demo-nautilus-zx5jz "
  May 13 18:53:44.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zrfvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 18:53:44.309: INFO: stderr: ""
  May 13 18:53:44.310: INFO: stdout: "true"
  May 13 18:53:44.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zrfvg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 13 18:53:44.388: INFO: stderr: ""
  May 13 18:53:44.388: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 13 18:53:44.388: INFO: validating pod update-demo-nautilus-zrfvg
  May 13 18:53:44.395: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 13 18:53:44.395: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 13 18:53:44.395: INFO: update-demo-nautilus-zrfvg is verified up and running
  May 13 18:53:44.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zx5jz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0513 18:53:44.441337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:44.471: INFO: stderr: ""
  May 13 18:53:44.471: INFO: stdout: "true"
  May 13 18:53:44.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zx5jz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 13 18:53:44.542: INFO: stderr: ""
  May 13 18:53:44.542: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 13 18:53:44.542: INFO: validating pod update-demo-nautilus-zx5jz
  May 13 18:53:44.547: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 13 18:53:44.547: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 13 18:53:44.547: INFO: update-demo-nautilus-zx5jz is verified up and running
  STEP: scaling down the replication controller @ 05/13/23 18:53:44.547
  May 13 18:53:44.550: INFO: scanned /root for discovery docs: <nil>
  May 13 18:53:44.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0513 18:53:45.441418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:45.661: INFO: stderr: ""
  May 13 18:53:45.661: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/13/23 18:53:45.661
  May 13 18:53:45.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 13 18:53:45.738: INFO: stderr: ""
  May 13 18:53:45.738: INFO: stdout: "update-demo-nautilus-zx5jz "
  May 13 18:53:45.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zx5jz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 18:53:45.811: INFO: stderr: ""
  May 13 18:53:45.811: INFO: stdout: "true"
  May 13 18:53:45.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zx5jz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 13 18:53:45.880: INFO: stderr: ""
  May 13 18:53:45.880: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 13 18:53:45.880: INFO: validating pod update-demo-nautilus-zx5jz
  May 13 18:53:45.885: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 13 18:53:45.885: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 13 18:53:45.885: INFO: update-demo-nautilus-zx5jz is verified up and running
  STEP: scaling up the replication controller @ 05/13/23 18:53:45.885
  May 13 18:53:45.891: INFO: scanned /root for discovery docs: <nil>
  May 13 18:53:45.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0513 18:53:46.442151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:46.990: INFO: stderr: ""
  May 13 18:53:46.990: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/13/23 18:53:46.99
  May 13 18:53:46.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 13 18:53:47.074: INFO: stderr: ""
  May 13 18:53:47.074: INFO: stdout: "update-demo-nautilus-2w85s update-demo-nautilus-zx5jz "
  May 13 18:53:47.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-2w85s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 18:53:47.193: INFO: stderr: ""
  May 13 18:53:47.193: INFO: stdout: ""
  May 13 18:53:47.193: INFO: update-demo-nautilus-2w85s is created but not running
  E0513 18:53:47.442634      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:48.442825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:49.443330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:50.443745      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:51.450020      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:52.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 13 18:53:52.244: INFO: stderr: ""
  May 13 18:53:52.244: INFO: stdout: "update-demo-nautilus-2w85s update-demo-nautilus-zx5jz "
  May 13 18:53:52.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-2w85s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 18:53:52.300: INFO: stderr: ""
  May 13 18:53:52.300: INFO: stdout: "true"
  May 13 18:53:52.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-2w85s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 13 18:53:52.360: INFO: stderr: ""
  May 13 18:53:52.360: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 13 18:53:52.360: INFO: validating pod update-demo-nautilus-2w85s
  May 13 18:53:52.367: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 13 18:53:52.367: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 13 18:53:52.367: INFO: update-demo-nautilus-2w85s is verified up and running
  May 13 18:53:52.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zx5jz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 18:53:52.427: INFO: stderr: ""
  May 13 18:53:52.427: INFO: stdout: "true"
  May 13 18:53:52.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods update-demo-nautilus-zx5jz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0513 18:53:52.451008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:53:52.491: INFO: stderr: ""
  May 13 18:53:52.491: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 13 18:53:52.491: INFO: validating pod update-demo-nautilus-zx5jz
  May 13 18:53:52.494: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 13 18:53:52.496: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 13 18:53:52.496: INFO: update-demo-nautilus-zx5jz is verified up and running
  STEP: using delete to clean up resources @ 05/13/23 18:53:52.496
  May 13 18:53:52.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 delete --grace-period=0 --force -f -'
  May 13 18:53:52.592: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 13 18:53:52.592: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 13 18:53:52.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get rc,svc -l name=update-demo --no-headers'
  May 13 18:53:52.706: INFO: stderr: "No resources found in kubectl-3456 namespace.\n"
  May 13 18:53:52.706: INFO: stdout: ""
  May 13 18:53:52.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3456 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 13 18:53:52.907: INFO: stderr: ""
  May 13 18:53:52.907: INFO: stdout: ""
  May 13 18:53:52.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3456" for this suite. @ 05/13/23 18:53:52.91
• [24.646 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 05/13/23 18:53:52.917
  May 13 18:53:52.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 18:53:52.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:53:52.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:53:52.938
  STEP: creating an Endpoint @ 05/13/23 18:53:52.945
  STEP: waiting for available Endpoint @ 05/13/23 18:53:52.952
  STEP: listing all Endpoints @ 05/13/23 18:53:52.953
  STEP: updating the Endpoint @ 05/13/23 18:53:52.956
  STEP: fetching the Endpoint @ 05/13/23 18:53:52.962
  STEP: patching the Endpoint @ 05/13/23 18:53:52.967
  STEP: fetching the Endpoint @ 05/13/23 18:53:52.978
  STEP: deleting the Endpoint by Collection @ 05/13/23 18:53:52.98
  STEP: waiting for Endpoint deletion @ 05/13/23 18:53:52.986
  STEP: fetching the Endpoint @ 05/13/23 18:53:52.988
  May 13 18:53:52.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7668" for this suite. @ 05/13/23 18:53:53.001
• [0.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/13/23 18:53:53.01
  May 13 18:53:53.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:53:53.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:53:53.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:53:53.024
  STEP: Creating projection with secret that has name projected-secret-test-map-b697baf8-d0f8-4e40-b7a6-5bc8c71c17aa @ 05/13/23 18:53:53.027
  STEP: Creating a pod to test consume secrets @ 05/13/23 18:53:53.031
  E0513 18:53:53.452040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:54.454981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:53:55.063
  May 13 18:53:55.069: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-f38bfe5c-b8b1-4abf-98db-3929605f42c7 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 18:53:55.074
  May 13 18:53:55.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1332" for this suite. @ 05/13/23 18:53:55.094
• [2.087 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 05/13/23 18:53:55.105
  May 13 18:53:55.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 18:53:55.105
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:53:55.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:53:55.117
  May 13 18:53:55.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1581 version'
  May 13 18:53:55.149: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  May 13 18:53:55.149: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:14:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  May 13 18:53:55.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1581" for this suite. @ 05/13/23 18:53:55.153
• [0.050 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 05/13/23 18:53:55.155
  May 13 18:53:55.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 18:53:55.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:53:55.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:53:55.166
  STEP: Creating a ResourceQuota with best effort scope @ 05/13/23 18:53:55.168
  STEP: Ensuring ResourceQuota status is calculated @ 05/13/23 18:53:55.17
  E0513 18:53:55.457124      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:56.458181      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 05/13/23 18:53:57.172
  STEP: Ensuring ResourceQuota status is calculated @ 05/13/23 18:53:57.174
  E0513 18:53:57.458556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:53:58.459251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 05/13/23 18:53:59.177
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/13/23 18:53:59.197
  E0513 18:53:59.459581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:00.460545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/13/23 18:54:01.202
  E0513 18:54:01.460850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:02.461404      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/13/23 18:54:03.208
  STEP: Ensuring resource quota status released the pod usage @ 05/13/23 18:54:03.234
  E0513 18:54:03.462673      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:04.462738      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 05/13/23 18:54:05.24
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/13/23 18:54:05.264
  E0513 18:54:05.463665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:06.464114      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/13/23 18:54:07.27
  E0513 18:54:07.464469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:08.465335      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/13/23 18:54:09.273
  STEP: Ensuring resource quota status released the pod usage @ 05/13/23 18:54:09.292
  E0513 18:54:09.465584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:10.467134      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:54:11.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5814" for this suite. @ 05/13/23 18:54:11.299
• [16.148 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 05/13/23 18:54:11.304
  May 13 18:54:11.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 18:54:11.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:54:11.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:54:11.317
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/13/23 18:54:11.318
  E0513 18:54:11.467400      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:12.468490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:13.469514      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:14.469966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:54:15.341
  May 13 18:54:15.345: INFO: Trying to get logs from node worker00 pod pod-c0568543-4161-499d-9520-b496f3fe8c81 container test-container: <nil>
  STEP: delete the pod @ 05/13/23 18:54:15.349
  May 13 18:54:15.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5476" for this suite. @ 05/13/23 18:54:15.373
• [4.073 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 05/13/23 18:54:15.379
  May 13 18:54:15.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 18:54:15.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:54:15.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:54:15.388
  STEP: Creating configMap with name projected-configmap-test-volume-map-1fed6cc7-d5e6-4e5c-a53a-e22d74056cb3 @ 05/13/23 18:54:15.39
  STEP: Creating a pod to test consume configMaps @ 05/13/23 18:54:15.391
  E0513 18:54:15.471363      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:16.471650      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:17.471684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:18.472325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:54:19.407
  May 13 18:54:19.411: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-a96bf8b4-822e-4134-8093-ca4bf25b5b50 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 18:54:19.418
  May 13 18:54:19.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9292" for this suite. @ 05/13/23 18:54:19.448
• [4.072 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/13/23 18:54:19.452
  May 13 18:54:19.452: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename events @ 05/13/23 18:54:19.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:54:19.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:54:19.46
  STEP: Create set of events @ 05/13/23 18:54:19.462
  STEP: get a list of Events with a label in the current namespace @ 05/13/23 18:54:19.469
  STEP: delete a list of events @ 05/13/23 18:54:19.472
  May 13 18:54:19.472: INFO: requesting DeleteCollection of events
  E0513 18:54:19.472547      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check that the list of events matches the requested quantity @ 05/13/23 18:54:19.477
  May 13 18:54:19.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5800" for this suite. @ 05/13/23 18:54:19.48
• [0.030 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 05/13/23 18:54:19.482
  May 13 18:54:19.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 18:54:19.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:54:19.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:54:19.49
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/13/23 18:54:19.491
  E0513 18:54:20.473189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:21.473613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:22.474568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:23.475246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:54:23.507
  May 13 18:54:23.511: INFO: Trying to get logs from node worker00 pod pod-d994af26-de44-444d-9d29-559f9bd23ca0 container test-container: <nil>
  STEP: delete the pod @ 05/13/23 18:54:23.516
  May 13 18:54:23.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2856" for this suite. @ 05/13/23 18:54:23.524
• [4.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 05/13/23 18:54:23.529
  May 13 18:54:23.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replicaset @ 05/13/23 18:54:23.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:54:23.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:54:23.54
  STEP: Create a Replicaset @ 05/13/23 18:54:23.543
  STEP: Verify that the required pods have come up. @ 05/13/23 18:54:23.546
  May 13 18:54:23.548: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0513 18:54:24.475959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:25.484434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:26.485171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:27.486141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:28.486554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:54:28.550: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/13/23 18:54:28.55
  STEP: Getting /status @ 05/13/23 18:54:28.55
  May 13 18:54:28.553: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/13/23 18:54:28.553
  May 13 18:54:28.559: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/13/23 18:54:28.559
  May 13 18:54:28.561: INFO: Observed &ReplicaSet event: ADDED
  May 13 18:54:28.561: INFO: Observed &ReplicaSet event: MODIFIED
  May 13 18:54:28.561: INFO: Observed &ReplicaSet event: MODIFIED
  May 13 18:54:28.561: INFO: Observed &ReplicaSet event: MODIFIED
  May 13 18:54:28.561: INFO: Found replicaset test-rs in namespace replicaset-1523 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 13 18:54:28.561: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/13/23 18:54:28.562
  May 13 18:54:28.562: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 13 18:54:28.565: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/13/23 18:54:28.565
  May 13 18:54:28.566: INFO: Observed &ReplicaSet event: ADDED
  May 13 18:54:28.566: INFO: Observed &ReplicaSet event: MODIFIED
  May 13 18:54:28.567: INFO: Observed &ReplicaSet event: MODIFIED
  May 13 18:54:28.567: INFO: Observed &ReplicaSet event: MODIFIED
  May 13 18:54:28.567: INFO: Observed replicaset test-rs in namespace replicaset-1523 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 13 18:54:28.568: INFO: Observed &ReplicaSet event: MODIFIED
  May 13 18:54:28.568: INFO: Found replicaset test-rs in namespace replicaset-1523 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May 13 18:54:28.568: INFO: Replicaset test-rs has a patched status
  May 13 18:54:28.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1523" for this suite. @ 05/13/23 18:54:28.57
• [5.044 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/13/23 18:54:28.574
  May 13 18:54:28.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 18:54:28.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:54:28.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:54:28.583
  STEP: Creating secret with name secret-test-map-450852f1-bc09-45ae-aa48-e22e60b94498 @ 05/13/23 18:54:28.585
  STEP: Creating a pod to test consume secrets @ 05/13/23 18:54:28.587
  E0513 18:54:29.487373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:30.487440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:31.487820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:32.488755      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:54:32.602
  May 13 18:54:32.607: INFO: Trying to get logs from node worker00 pod pod-secrets-3e3db9e0-6478-4f00-bab2-a107c38455b0 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 18:54:32.612
  May 13 18:54:32.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4716" for this suite. @ 05/13/23 18:54:32.625
• [4.055 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/13/23 18:54:32.629
  May 13 18:54:32.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-runtime @ 05/13/23 18:54:32.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:54:32.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:54:32.639
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/13/23 18:54:32.647
  E0513 18:54:33.489025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:34.489347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:35.489463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:36.489752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:37.490058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:38.497289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:39.498509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:40.498926      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:41.499583      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:42.500844      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:43.500808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:44.501438      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:45.501372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:46.501751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:47.502533      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:48.503757      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:49.508512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:50.508723      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:51.509560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/13/23 18:54:51.729
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/13/23 18:54:51.734
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/13/23 18:54:51.738
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 05/13/23 18:54:51.739
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/13/23 18:54:51.765
  E0513 18:54:52.510538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:53.510991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:54.513771      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/13/23 18:54:54.782
  E0513 18:54:55.514827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/13/23 18:54:55.791
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/13/23 18:54:55.798
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 05/13/23 18:54:55.798
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/13/23 18:54:55.827
  E0513 18:54:56.515188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/13/23 18:54:56.834
  E0513 18:54:57.515568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:54:58.515809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/13/23 18:54:58.845
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/13/23 18:54:58.848
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 05/13/23 18:54:58.848
  May 13 18:54:58.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2340" for this suite. @ 05/13/23 18:54:58.867
• [26.241 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 05/13/23 18:54:58.87
  May 13 18:54:58.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 18:54:58.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:54:58.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:54:58.88
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/13/23 18:54:58.881
  May 13 18:54:58.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-4629 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  May 13 18:54:58.929: INFO: stderr: ""
  May 13 18:54:58.929: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/13/23 18:54:58.929
  May 13 18:54:58.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-4629 delete pods e2e-test-httpd-pod'
  E0513 18:54:59.516102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:00.516596      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:55:01.043: INFO: stderr: ""
  May 13 18:55:01.043: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 13 18:55:01.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4629" for this suite. @ 05/13/23 18:55:01.045
• [2.178 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 05/13/23 18:55:01.048
  May 13 18:55:01.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename disruption @ 05/13/23 18:55:01.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:55:01.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:55:01.056
  STEP: Creating a kubernetes client @ 05/13/23 18:55:01.057
  May 13 18:55:01.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename disruption-2 @ 05/13/23 18:55:01.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:55:01.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:55:01.065
  STEP: Waiting for the pdb to be processed @ 05/13/23 18:55:01.068
  E0513 18:55:01.517106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:02.517587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 05/13/23 18:55:03.082
  E0513 18:55:03.518516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:04.519140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 05/13/23 18:55:05.105
  E0513 18:55:05.519452      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:06.521461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 05/13/23 18:55:07.11
  STEP: listing a collection of PDBs in namespace disruption-7379 @ 05/13/23 18:55:07.111
  STEP: deleting a collection of PDBs @ 05/13/23 18:55:07.112
  STEP: Waiting for the PDB collection to be deleted @ 05/13/23 18:55:07.126
  May 13 18:55:07.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 18:55:07.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-4875" for this suite. @ 05/13/23 18:55:07.131
  STEP: Destroying namespace "disruption-7379" for this suite. @ 05/13/23 18:55:07.133
• [6.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 05/13/23 18:55:07.139
  May 13 18:55:07.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 18:55:07.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:55:07.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:55:07.148
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/13/23 18:55:07.15
  E0513 18:55:07.522500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:08.527248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:09.527189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:10.528770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:55:11.163
  May 13 18:55:11.166: INFO: Trying to get logs from node worker00 pod pod-6e4409d0-3eab-4779-abb8-e59773ef5553 container test-container: <nil>
  STEP: delete the pod @ 05/13/23 18:55:11.17
  May 13 18:55:11.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7301" for this suite. @ 05/13/23 18:55:11.193
• [4.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 05/13/23 18:55:11.198
  May 13 18:55:11.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 18:55:11.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:55:11.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:55:11.21
  STEP: creating a ConfigMap @ 05/13/23 18:55:11.211
  STEP: fetching the ConfigMap @ 05/13/23 18:55:11.212
  STEP: patching the ConfigMap @ 05/13/23 18:55:11.213
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/13/23 18:55:11.215
  STEP: deleting the ConfigMap by collection with a label selector @ 05/13/23 18:55:11.216
  STEP: listing all ConfigMaps in test namespace @ 05/13/23 18:55:11.219
  May 13 18:55:11.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4909" for this suite. @ 05/13/23 18:55:11.221
• [0.028 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/13/23 18:55:11.226
  May 13 18:55:11.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-watch @ 05/13/23 18:55:11.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:55:11.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:55:11.235
  May 13 18:55:11.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:55:11.529711      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:12.529978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:13.530323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 05/13/23 18:55:13.78
  May 13 18:55:13.785: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-13T18:55:13Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-13T18:55:13Z]] name:name1 resourceVersion:27594 uid:7f27b551-9f6b-439e-9bda-2793981dff37] num:map[num1:9223372036854775807 num2:1000000]]}
  E0513 18:55:14.530901      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:15.531934      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:16.532323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:17.533027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:18.534330      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:19.534593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:20.534560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:21.537016      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:22.537506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:23.538190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 05/13/23 18:55:23.786
  May 13 18:55:23.793: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-13T18:55:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-13T18:55:23Z]] name:name2 resourceVersion:27645 uid:15b91ac8-74cc-4183-8da7-9bd90ae56f15] num:map[num1:9223372036854775807 num2:1000000]]}
  E0513 18:55:24.539197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:25.539323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:26.540017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:27.542978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:28.543605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:29.544143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:30.544629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:31.544832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:32.546579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:33.546902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 05/13/23 18:55:33.794
  May 13 18:55:33.808: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-13T18:55:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-13T18:55:33Z]] name:name1 resourceVersion:27681 uid:7f27b551-9f6b-439e-9bda-2793981dff37] num:map[num1:9223372036854775807 num2:1000000]]}
  E0513 18:55:34.547445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:35.548512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:36.549096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:37.549957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:38.551266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:39.551446      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:40.552488      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:41.553734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:42.554373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:43.554797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 05/13/23 18:55:43.808
  May 13 18:55:43.812: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-13T18:55:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-13T18:55:43Z]] name:name2 resourceVersion:27718 uid:15b91ac8-74cc-4183-8da7-9bd90ae56f15] num:map[num1:9223372036854775807 num2:1000000]]}
  E0513 18:55:44.556599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:45.559532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:46.559488      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:47.560123      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:48.560383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:49.561200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:50.563017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:51.563206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:52.563913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:53.564053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 05/13/23 18:55:53.813
  May 13 18:55:53.823: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-13T18:55:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-13T18:55:33Z]] name:name1 resourceVersion:27754 uid:7f27b551-9f6b-439e-9bda-2793981dff37] num:map[num1:9223372036854775807 num2:1000000]]}
  E0513 18:55:54.564510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:55.564737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:56.567022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:57.568144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:58.568320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:55:59.569005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:00.569891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:01.570691      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:02.571107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:03.571336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 05/13/23 18:56:03.824
  May 13 18:56:03.829: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-13T18:55:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-13T18:55:43Z]] name:name2 resourceVersion:27790 uid:15b91ac8-74cc-4183-8da7-9bd90ae56f15] num:map[num1:9223372036854775807 num2:1000000]]}
  E0513 18:56:04.572342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:05.575178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:06.575491      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:07.577030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:08.578035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:09.578102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:10.578248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:11.578415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:12.580247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:13.582514      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:56:14.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-7113" for this suite. @ 05/13/23 18:56:14.341
• [63.127 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 05/13/23 18:56:14.355
  May 13 18:56:14.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename field-validation @ 05/13/23 18:56:14.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:56:14.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:56:14.372
  May 13 18:56:14.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 18:56:14.583804      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:15.585198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:16.587677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0513 18:56:16.897518      21 warnings.go:70] unknown field "alpha"
  W0513 18:56:16.897533      21 warnings.go:70] unknown field "beta"
  W0513 18:56:16.897535      21 warnings.go:70] unknown field "delta"
  W0513 18:56:16.897537      21 warnings.go:70] unknown field "epsilon"
  W0513 18:56:16.897539      21 warnings.go:70] unknown field "gamma"
  May 13 18:56:16.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6389" for this suite. @ 05/13/23 18:56:16.91
• [2.559 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 05/13/23 18:56:16.913
  May 13 18:56:16.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replication-controller @ 05/13/23 18:56:16.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:56:16.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:56:16.925
  May 13 18:56:16.927: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0513 18:56:17.589171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/13/23 18:56:17.934
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/13/23 18:56:17.937
  E0513 18:56:18.589365      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/13/23 18:56:18.944
  May 13 18:56:18.951: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/13/23 18:56:18.951
  E0513 18:56:19.590146      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:56:19.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2430" for this suite. @ 05/13/23 18:56:19.963
• [3.054 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 05/13/23 18:56:19.967
  May 13 18:56:19.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 18:56:19.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:56:19.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:56:19.98
  STEP: Setting up server cert @ 05/13/23 18:56:19.992
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 18:56:20.112
  STEP: Deploying the webhook pod @ 05/13/23 18:56:20.117
  STEP: Wait for the deployment to be ready @ 05/13/23 18:56:20.124
  May 13 18:56:20.128: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0513 18:56:20.591086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:21.591672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 18:56:22.134
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 18:56:22.139
  E0513 18:56:22.592168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:56:23.139: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/13/23 18:56:23.144
  STEP: create a pod @ 05/13/23 18:56:23.17
  E0513 18:56:23.592938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:24.593221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/13/23 18:56:25.178
  May 13 18:56:25.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=webhook-4050 attach --namespace=webhook-4050 to-be-attached-pod -i -c=container1'
  May 13 18:56:25.265: INFO: rc: 1
  May 13 18:56:25.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4050" for this suite. @ 05/13/23 18:56:25.31
  STEP: Destroying namespace "webhook-markers-9937" for this suite. @ 05/13/23 18:56:25.317
• [5.356 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 05/13/23 18:56:25.323
  May 13 18:56:25.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename job @ 05/13/23 18:56:25.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:56:25.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:56:25.343
  STEP: Creating a job @ 05/13/23 18:56:25.345
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/13/23 18:56:25.356
  E0513 18:56:25.594136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:26.594615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/13/23 18:56:27.359
  STEP: updating /status @ 05/13/23 18:56:27.374
  STEP: get /status @ 05/13/23 18:56:27.399
  May 13 18:56:27.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2658" for this suite. @ 05/13/23 18:56:27.402
• [2.083 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 05/13/23 18:56:27.408
  May 13 18:56:27.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sched-preemption @ 05/13/23 18:56:27.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:56:27.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:56:27.424
  May 13 18:56:27.437: INFO: Waiting up to 1m0s for all nodes to be ready
  E0513 18:56:27.597053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:28.597833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:29.598087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:30.598998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:31.599555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:32.599577      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:33.601064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:34.602135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:35.604055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:36.605181      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:37.606428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:38.606643      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:39.607560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:40.608351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:41.608612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:42.612529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:43.613120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:44.613418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:45.613664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:46.615550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:47.616747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:48.616850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:49.617038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:50.618240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:51.619420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:52.620191      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:53.621626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:54.621913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:55.622287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:56.623275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:57.623518      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:58.623733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:56:59.628072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:00.628391      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:01.629191      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:02.628927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:03.629460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:04.630724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:05.632831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:06.633426      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:07.634216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:08.639508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:09.639490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:10.641799      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:11.642441      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:12.642700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:13.643543      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:14.643987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:15.644768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:16.646213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:17.646948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:18.647659      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:19.647604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:20.647948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:21.649013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:22.649270      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:23.650806      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:24.651613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:25.651950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:26.654013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:57:27.463: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/13/23 18:57:27.465
  May 13 18:57:27.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/13/23 18:57:27.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:57:27.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:57:27.48
  STEP: Finding an available node @ 05/13/23 18:57:27.482
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/13/23 18:57:27.482
  E0513 18:57:27.657809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:28.657943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/13/23 18:57:29.491
  May 13 18:57:29.500: INFO: found a healthy node: worker00
  E0513 18:57:29.658275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:30.658947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:31.659479      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:32.660462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:33.660868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:34.662570      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:57:35.584: INFO: pods created so far: [1 1 1]
  May 13 18:57:35.584: INFO: length of pods created so far: 3
  E0513 18:57:35.663080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:36.664354      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:57:37.593: INFO: pods created so far: [2 2 1]
  E0513 18:57:37.665699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:38.665858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:39.666564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:40.666672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:41.666784      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:42.667709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:43.668948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:57:44.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 18:57:44.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-123" for this suite. @ 05/13/23 18:57:44.667
  E0513 18:57:44.669139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "sched-preemption-4654" for this suite. @ 05/13/23 18:57:44.673
• [77.274 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 05/13/23 18:57:44.681
  May 13 18:57:44.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 18:57:44.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:57:44.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:57:44.691
  STEP: Creating a pod to test downward api env vars @ 05/13/23 18:57:44.692
  E0513 18:57:45.670665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:46.670952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:47.671253      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:48.671817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 18:57:48.705
  May 13 18:57:48.706: INFO: Trying to get logs from node worker00 pod downward-api-d05b3937-49b0-4c4e-8eca-4009f46c81ea container dapi-container: <nil>
  STEP: delete the pod @ 05/13/23 18:57:48.71
  May 13 18:57:48.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8726" for this suite. @ 05/13/23 18:57:48.72
• [4.041 seconds]
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 05/13/23 18:57:48.724
  May 13 18:57:48.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-probe @ 05/13/23 18:57:48.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 18:57:48.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 18:57:48.733
  STEP: Creating pod test-grpc-94952134-08bb-4731-abb5-160dcdb11a96 in namespace container-probe-586 @ 05/13/23 18:57:48.735
  E0513 18:57:49.672727      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:50.673038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:51.673304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:52.674070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 18:57:52.756: INFO: Started pod test-grpc-94952134-08bb-4731-abb5-160dcdb11a96 in namespace container-probe-586
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/13/23 18:57:52.756
  May 13 18:57:52.757: INFO: Initial restart count of pod test-grpc-94952134-08bb-4731-abb5-160dcdb11a96 is 0
  E0513 18:57:53.674242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:54.674727      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:55.675323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:56.675921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:57.676383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:58.676533      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:57:59.676687      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:00.676741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:01.676874      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:02.676991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:03.677612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:04.678170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:05.678369      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:06.678994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:07.679699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:08.679813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:09.680630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:10.680885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:11.680966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:12.682676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:13.690755      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:14.692113      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:15.692226      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:16.693004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:17.693190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:18.693408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:19.694151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:20.694432      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:21.694790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:22.695257      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:23.695617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:24.696102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:25.696433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:26.696726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:27.696921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:28.697015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:29.697326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:30.697444      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:31.698981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:32.699521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:33.699959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:34.707110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:35.707180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:36.707245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:37.707346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:38.708235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:39.708852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:40.708994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:41.709817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:42.710246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:43.710822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:44.711103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:45.714421      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:46.715131      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:47.715266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:48.715731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:49.716761      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:50.717589      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:51.719856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:52.720229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:53.721125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:54.721352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:55.721859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:56.722112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:57.722334      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:58.722554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:58:59.723098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:00.723994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:01.728769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:02.730413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:03.730528      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:04.731586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:05.731678      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:06.732531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:07.732939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:08.734883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:09.735571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:10.737397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:11.738298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:12.738707      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:13.738815      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:14.742541      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:15.742684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:16.743276      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:17.745508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:18.745518      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:19.745732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:20.746041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:21.746516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:22.746698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:23.747397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:24.747286      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:25.747309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:26.749049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:27.749296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:28.749885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:29.750951      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:30.751861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:31.752337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:32.753323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:33.753613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:34.753962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:35.756555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:36.756741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:37.758788      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:38.760867      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:39.761522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:40.762047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:41.763109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:42.764264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:43.764672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:44.765654      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:45.766509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:46.766603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:47.767148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:48.767227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:49.767820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:50.769115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:51.770247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:52.770455      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:53.770652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:54.774855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:55.775613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:56.775951      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:57.776003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:58.786280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 18:59:59.786439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:00.787067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:01.788463      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:02.788764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:03.789034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:04.789281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:05.792869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:06.792706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:07.794269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:08.794474      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:09.794527      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:10.797086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:11.797914      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:12.798558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:13.799434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:14.800640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:15.800719      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:16.800899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:17.801007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:18.801414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:19.802499      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:20.802592      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:21.803546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:22.803828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:23.804290      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:24.804470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:25.804832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:26.806543      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:27.806998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:28.807197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:29.808765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:30.809310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:31.810775      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:32.811172      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:33.811250      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:34.811652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:35.812310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:36.812597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:37.814685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:38.814848      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:39.815565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:40.815802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:41.817841      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:42.817956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:43.820432      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:44.824379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:45.825244      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:46.827557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:47.827824      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:48.828573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:49.829272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:50.829397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:51.829770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:52.831383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:53.831494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:54.832064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:55.833112      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:56.834091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:57.834912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:58.835355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:00:59.835573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:00.835664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:01.836303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:02.836639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:03.836846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:04.836873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:05.837152      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:06.838069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:07.838987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:08.841560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:09.846686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:10.847696      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:11.848532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:12.849102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:13.849223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:14.849325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:15.851173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:16.851662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:17.852616      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:18.853220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:19.853480      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:20.853565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:21.867813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:22.868753      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:23.874737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:24.870339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:25.871026      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:26.871098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:27.872096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:28.872198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:29.872460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:30.872761      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:31.873677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:32.874986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:33.875061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:34.876558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:35.876842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:36.879448      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:37.880296      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:38.880606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:39.881339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:40.881661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:41.881968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:42.882369      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:43.882962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:44.883318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:45.884018      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:46.884025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:47.884234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:48.884566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:49.889376      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:50.889114      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:51.891210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:52.891427      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:01:53.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 19:01:53.206
  STEP: Destroying namespace "container-probe-586" for this suite. @ 05/13/23 19:01:53.219
• [244.504 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 05/13/23 19:01:53.228
  May 13 19:01:53.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 19:01:53.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:01:53.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:01:53.247
  May 13 19:01:53.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2595" for this suite. @ 05/13/23 19:01:53.286
• [0.061 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 05/13/23 19:01:53.289
  May 13 19:01:53.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename subpath @ 05/13/23 19:01:53.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:01:53.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:01:53.3
  STEP: Setting up data @ 05/13/23 19:01:53.302
  STEP: Creating pod pod-subpath-test-secret-555h @ 05/13/23 19:01:53.308
  STEP: Creating a pod to test atomic-volume-subpath @ 05/13/23 19:01:53.308
  E0513 19:01:53.891661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:54.894847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:55.895262      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:56.895785      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:57.896358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:58.897402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:01:59.898271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:00.898962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:01.904261      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:02.904799      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:03.905853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:04.905996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:05.907668      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:06.908325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:07.908499      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:08.909710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:09.909870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:10.911480      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:11.912280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:12.913698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:13.914208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:14.914629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:15.915827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:16.916137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:02:17.355
  May 13 19:02:17.356: INFO: Trying to get logs from node worker00 pod pod-subpath-test-secret-555h container test-container-subpath-secret-555h: <nil>
  STEP: delete the pod @ 05/13/23 19:02:17.359
  STEP: Deleting pod pod-subpath-test-secret-555h @ 05/13/23 19:02:17.379
  May 13 19:02:17.379: INFO: Deleting pod "pod-subpath-test-secret-555h" in namespace "subpath-9042"
  May 13 19:02:17.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9042" for this suite. @ 05/13/23 19:02:17.382
• [24.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 05/13/23 19:02:17.386
  May 13 19:02:17.386: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/13/23 19:02:17.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:02:17.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:02:17.396
  May 13 19:02:17.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:02:17.916690      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/13/23 19:02:18.608
  May 13 19:02:18.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-9150 --namespace=crd-publish-openapi-9150 create -f -'
  E0513 19:02:18.917013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:02:19.184: INFO: stderr: ""
  May 13 19:02:19.184: INFO: stdout: "e2e-test-crd-publish-openapi-7567-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 13 19:02:19.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-9150 --namespace=crd-publish-openapi-9150 delete e2e-test-crd-publish-openapi-7567-crds test-cr'
  May 13 19:02:19.256: INFO: stderr: ""
  May 13 19:02:19.256: INFO: stdout: "e2e-test-crd-publish-openapi-7567-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  May 13 19:02:19.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-9150 --namespace=crd-publish-openapi-9150 apply -f -'
  May 13 19:02:19.538: INFO: stderr: ""
  May 13 19:02:19.538: INFO: stdout: "e2e-test-crd-publish-openapi-7567-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 13 19:02:19.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-9150 --namespace=crd-publish-openapi-9150 delete e2e-test-crd-publish-openapi-7567-crds test-cr'
  May 13 19:02:19.614: INFO: stderr: ""
  May 13 19:02:19.614: INFO: stdout: "e2e-test-crd-publish-openapi-7567-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/13/23 19:02:19.614
  May 13 19:02:19.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-9150 explain e2e-test-crd-publish-openapi-7567-crds'
  May 13 19:02:19.828: INFO: stderr: ""
  May 13 19:02:19.828: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-7567-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0513 19:02:19.916927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:20.917104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:02:21.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9150" for this suite. @ 05/13/23 19:02:21.031
• [3.659 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 05/13/23 19:02:21.046
  May 13 19:02:21.046: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename field-validation @ 05/13/23 19:02:21.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:02:21.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:02:21.056
  May 13 19:02:21.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:02:21.917478      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:22.917835      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:02:23.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-3609" for this suite. @ 05/13/23 19:02:23.602
• [2.559 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 05/13/23 19:02:23.606
  May 13 19:02:23.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 19:02:23.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:02:23.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:02:23.619
  STEP: create deployment with httpd image @ 05/13/23 19:02:23.621
  May 13 19:02:23.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1236 create -f -'
  E0513 19:02:23.918655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:02:24.111: INFO: stderr: ""
  May 13 19:02:24.111: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/13/23 19:02:24.111
  May 13 19:02:24.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1236 diff -f -'
  May 13 19:02:24.330: INFO: rc: 1
  May 13 19:02:24.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1236 delete -f -'
  May 13 19:02:24.387: INFO: stderr: ""
  May 13 19:02:24.387: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  May 13 19:02:24.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1236" for this suite. @ 05/13/23 19:02:24.392
• [0.790 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 05/13/23 19:02:24.396
  May 13 19:02:24.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename dns @ 05/13/23 19:02:24.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:02:24.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:02:24.409
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/13/23 19:02:24.411
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/13/23 19:02:24.411
  STEP: creating a pod to probe DNS @ 05/13/23 19:02:24.411
  STEP: submitting the pod to kubernetes @ 05/13/23 19:02:24.411
  E0513 19:02:24.918710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:25.919651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/13/23 19:02:26.424
  STEP: looking for the results for each expected name from probers @ 05/13/23 19:02:26.426
  May 13 19:02:26.434: INFO: DNS probes using dns-6522/dns-test-b4b0a47d-7a7e-49fb-a79a-01018de6492a succeeded

  May 13 19:02:26.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 19:02:26.436
  STEP: Destroying namespace "dns-6522" for this suite. @ 05/13/23 19:02:26.456
• [2.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 05/13/23 19:02:26.464
  May 13 19:02:26.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 19:02:26.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:02:26.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:02:26.477
  STEP: Creating configMap configmap-5781/configmap-test-dae26ca5-35fb-45f4-a0f4-990d87f7ba72 @ 05/13/23 19:02:26.479
  STEP: Creating a pod to test consume configMaps @ 05/13/23 19:02:26.481
  E0513 19:02:26.919747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:27.919994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:28.920921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:29.923662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:02:30.492
  May 13 19:02:30.495: INFO: Trying to get logs from node worker00 pod pod-configmaps-48535c9b-5923-43d4-8c7d-32147c157823 container env-test: <nil>
  STEP: delete the pod @ 05/13/23 19:02:30.5
  May 13 19:02:30.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5781" for this suite. @ 05/13/23 19:02:30.524
• [4.065 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 05/13/23 19:02:30.532
  May 13 19:02:30.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/13/23 19:02:30.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:02:30.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:02:30.551
  STEP: create the container to handle the HTTPGet hook request. @ 05/13/23 19:02:30.556
  E0513 19:02:30.924677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:31.924876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/13/23 19:02:32.569
  E0513 19:02:32.925575      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:33.926378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 05/13/23 19:02:34.588
  E0513 19:02:34.927611      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:35.928639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:36.928728      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:37.932398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 05/13/23 19:02:38.608
  May 13 19:02:38.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8911" for this suite. @ 05/13/23 19:02:38.614
• [8.086 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 05/13/23 19:02:38.617
  May 13 19:02:38.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename job @ 05/13/23 19:02:38.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:02:38.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:02:38.626
  STEP: Creating a job @ 05/13/23 19:02:38.628
  STEP: Ensuring active pods == parallelism @ 05/13/23 19:02:38.631
  E0513 19:02:38.933107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:39.934073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 05/13/23 19:02:40.634
  E0513 19:02:40.934227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:02:41.146: INFO: Successfully updated pod "adopt-release-d5nr7"
  STEP: Checking that the Job readopts the Pod @ 05/13/23 19:02:41.146
  E0513 19:02:42.044271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:43.045044      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 05/13/23 19:02:43.155
  May 13 19:02:43.660: INFO: Successfully updated pod "adopt-release-d5nr7"
  STEP: Checking that the Job releases the Pod @ 05/13/23 19:02:43.66
  E0513 19:02:44.045638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:45.046521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:02:45.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2156" for this suite. @ 05/13/23 19:02:45.667
• [7.054 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 05/13/23 19:02:45.674
  May 13 19:02:45.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename var-expansion @ 05/13/23 19:02:45.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:02:45.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:02:45.694
  STEP: creating the pod with failed condition @ 05/13/23 19:02:45.695
  E0513 19:02:46.046547      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:47.046925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:48.047109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:49.049775      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:50.050188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:51.050846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:52.051966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:53.051998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:54.053351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:55.053489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:56.053803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:57.054887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:58.055991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:02:59.056370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:00.057040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:01.057513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:02.059010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:03.059205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:04.059325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:05.059847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:06.060064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:07.061942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:08.062043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:09.062183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:10.064521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:11.065349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:12.065576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:13.066554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:14.066852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:15.069773      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:16.070393      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:17.071328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:18.072762      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:19.072959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:20.073303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:21.073594      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:22.075194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:23.075387      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:24.076773      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:25.080511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:26.080624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:27.081812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:28.082263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:29.082549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:30.083315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:31.084300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:32.085147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:33.085299      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:34.085749      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:35.085730      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:36.086088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:37.086540      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:38.087396      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:39.087961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:40.090556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:41.091188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:42.091252      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:43.091350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:44.094515      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:45.094983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:46.095650      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:47.095651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:48.095855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:49.096578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:50.096664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:51.097851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:52.099084      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:53.100276      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:54.100437      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:55.100955      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:56.101550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:57.102467      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:58.102574      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:03:59.102828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:00.102925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:01.103062      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:02.103239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:03.103566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:04.104626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:05.104829      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:06.105074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:07.109645      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:08.110576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:09.110846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:10.111868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:11.113252      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:12.116359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:13.116959      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:14.118032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:15.118248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:16.118742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:17.119032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:18.119224      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:19.123340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:20.123934      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:21.124671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:22.125453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:23.125553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:24.125961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:25.126470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:26.127394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:27.129590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:28.130678      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:29.131643      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:30.131657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:31.134524      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:32.136000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:33.136752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:34.136805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:35.137123      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:36.137437      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:37.138327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:38.138509      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:39.138718      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:40.139625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:41.139763      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:42.139814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:43.140260      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:44.141072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:45.141542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 05/13/23 19:04:45.705
  E0513 19:04:46.142625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:04:46.225: INFO: Successfully updated pod "var-expansion-21cee5dd-46ad-4ac3-be38-e7178ea40ee0"
  STEP: waiting for pod running @ 05/13/23 19:04:46.225
  E0513 19:04:47.143613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:48.143928      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 05/13/23 19:04:48.232
  May 13 19:04:48.232: INFO: Deleting pod "var-expansion-21cee5dd-46ad-4ac3-be38-e7178ea40ee0" in namespace "var-expansion-5146"
  May 13 19:04:48.246: INFO: Wait up to 5m0s for pod "var-expansion-21cee5dd-46ad-4ac3-be38-e7178ea40ee0" to be fully deleted
  E0513 19:04:49.144201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:50.144650      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:51.145311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:52.146599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:53.146897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:54.147143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:55.147932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:56.148052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:57.148247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:58.148438      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:04:59.148677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:00.149321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:01.150341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:02.152230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:03.153128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:04.153161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:05.155493      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:06.155444      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:07.156476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:08.156676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:09.156855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:10.157239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:11.157814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:12.157835      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:13.217488      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:14.217527      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:15.218730      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:16.219572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:17.220443      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:18.221484      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:19.221482      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:20.222822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:20.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5146" for this suite. @ 05/13/23 19:05:20.293
• [154.633 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 05/13/23 19:05:20.309
  May 13 19:05:20.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename podtemplate @ 05/13/23 19:05:20.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:05:20.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:05:20.321
  STEP: Create set of pod templates @ 05/13/23 19:05:20.323
  May 13 19:05:20.325: INFO: created test-podtemplate-1
  May 13 19:05:20.327: INFO: created test-podtemplate-2
  May 13 19:05:20.329: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/13/23 19:05:20.329
  STEP: delete collection of pod templates @ 05/13/23 19:05:20.331
  May 13 19:05:20.331: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 05/13/23 19:05:20.336
  May 13 19:05:20.336: INFO: requesting list of pod templates to confirm quantity
  May 13 19:05:20.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8672" for this suite. @ 05/13/23 19:05:20.341
• [0.036 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/13/23 19:05:20.346
  May 13 19:05:20.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename tables @ 05/13/23 19:05:20.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:05:20.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:05:20.357
  May 13 19:05:20.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-767" for this suite. @ 05/13/23 19:05:20.363
• [0.020 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 05/13/23 19:05:20.37
  May 13 19:05:20.370: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename controllerrevisions @ 05/13/23 19:05:20.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:05:20.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:05:20.379
  STEP: Creating DaemonSet "e2e-wkgrd-daemon-set" @ 05/13/23 19:05:20.387
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/13/23 19:05:20.391
  May 13 19:05:20.396: INFO: Number of nodes with available pods controlled by daemonset e2e-wkgrd-daemon-set: 0
  May 13 19:05:20.396: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:05:21.224486      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:21.401: INFO: Number of nodes with available pods controlled by daemonset e2e-wkgrd-daemon-set: 1
  May 13 19:05:21.401: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:05:22.225070      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:22.403: INFO: Number of nodes with available pods controlled by daemonset e2e-wkgrd-daemon-set: 2
  May 13 19:05:22.403: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-wkgrd-daemon-set
  STEP: Confirm DaemonSet "e2e-wkgrd-daemon-set" successfully created with "daemonset-name=e2e-wkgrd-daemon-set" label @ 05/13/23 19:05:22.406
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-wkgrd-daemon-set" @ 05/13/23 19:05:22.411
  May 13 19:05:22.412: INFO: Located ControllerRevision: "e2e-wkgrd-daemon-set-54cbf747bd"
  STEP: Patching ControllerRevision "e2e-wkgrd-daemon-set-54cbf747bd" @ 05/13/23 19:05:22.413
  May 13 19:05:22.426: INFO: e2e-wkgrd-daemon-set-54cbf747bd has been patched
  STEP: Create a new ControllerRevision @ 05/13/23 19:05:22.427
  May 13 19:05:22.431: INFO: Created ControllerRevision: e2e-wkgrd-daemon-set-7c45d467c8
  STEP: Confirm that there are two ControllerRevisions @ 05/13/23 19:05:22.431
  May 13 19:05:22.431: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 13 19:05:22.434: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-wkgrd-daemon-set-54cbf747bd" @ 05/13/23 19:05:22.434
  STEP: Confirm that there is only one ControllerRevision @ 05/13/23 19:05:22.44
  May 13 19:05:22.440: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 13 19:05:22.442: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-wkgrd-daemon-set-7c45d467c8" @ 05/13/23 19:05:22.444
  May 13 19:05:22.448: INFO: e2e-wkgrd-daemon-set-7c45d467c8 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/13/23 19:05:22.448
  W0513 19:05:22.451390      21 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/13/23 19:05:22.451
  May 13 19:05:22.451: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0513 19:05:23.226206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:23.453: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 13 19:05:23.455: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-wkgrd-daemon-set-7c45d467c8=updated" @ 05/13/23 19:05:23.455
  STEP: Confirm that there is only one ControllerRevision @ 05/13/23 19:05:23.461
  May 13 19:05:23.461: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 13 19:05:23.464: INFO: Found 1 ControllerRevisions
  May 13 19:05:23.466: INFO: ControllerRevision "e2e-wkgrd-daemon-set-79588654f9" has revision 3
  STEP: Deleting DaemonSet "e2e-wkgrd-daemon-set" @ 05/13/23 19:05:23.471
  STEP: deleting DaemonSet.extensions e2e-wkgrd-daemon-set in namespace controllerrevisions-701, will wait for the garbage collector to delete the pods @ 05/13/23 19:05:23.471
  May 13 19:05:23.529: INFO: Deleting DaemonSet.extensions e2e-wkgrd-daemon-set took: 5.028033ms
  May 13 19:05:23.630: INFO: Terminating DaemonSet.extensions e2e-wkgrd-daemon-set pods took: 100.719617ms
  E0513 19:05:24.226911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:25.132: INFO: Number of nodes with available pods controlled by daemonset e2e-wkgrd-daemon-set: 0
  May 13 19:05:25.132: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-wkgrd-daemon-set
  May 13 19:05:25.134: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30779"},"items":null}

  May 13 19:05:25.137: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30779"},"items":null}

  May 13 19:05:25.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-701" for this suite. @ 05/13/23 19:05:25.144
• [4.778 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 05/13/23 19:05:25.149
  May 13 19:05:25.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 19:05:25.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:05:25.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:05:25.159
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:05:25.16
  E0513 19:05:25.227378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:26.230511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:27.230600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:28.230923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:05:29.17
  May 13 19:05:29.171: INFO: Trying to get logs from node worker00 pod downwardapi-volume-f5263745-7031-4713-b877-e03b4ed577ee container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:05:29.175
  May 13 19:05:29.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6979" for this suite. @ 05/13/23 19:05:29.196
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 05/13/23 19:05:29.204
  May 13 19:05:29.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:05:29.204
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:05:29.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:05:29.216
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:05:29.219
  E0513 19:05:29.231453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:30.231942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:31.232678      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:05:31.234
  May 13 19:05:31.237: INFO: Trying to get logs from node worker00 pod downwardapi-volume-92e1ebff-fed7-453b-ad8f-d4616e13d7b4 container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:05:31.24
  May 13 19:05:31.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2223" for this suite. @ 05/13/23 19:05:31.249
• [2.048 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 05/13/23 19:05:31.255
  May 13 19:05:31.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename disruption @ 05/13/23 19:05:31.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:05:31.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:05:31.266
  STEP: creating the pdb @ 05/13/23 19:05:31.268
  STEP: Waiting for the pdb to be processed @ 05/13/23 19:05:31.271
  E0513 19:05:32.233496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:33.233758      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 05/13/23 19:05:33.275
  STEP: Waiting for the pdb to be processed @ 05/13/23 19:05:33.28
  STEP: patching the pdb @ 05/13/23 19:05:33.285
  STEP: Waiting for the pdb to be processed @ 05/13/23 19:05:33.29
  E0513 19:05:34.234313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:35.234819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 05/13/23 19:05:35.297
  May 13 19:05:35.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6423" for this suite. @ 05/13/23 19:05:35.302
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 05/13/23 19:05:35.305
  May 13 19:05:35.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename runtimeclass @ 05/13/23 19:05:35.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:05:35.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:05:35.317
  STEP: getting /apis @ 05/13/23 19:05:35.32
  STEP: getting /apis/node.k8s.io @ 05/13/23 19:05:35.323
  STEP: getting /apis/node.k8s.io/v1 @ 05/13/23 19:05:35.324
  STEP: creating @ 05/13/23 19:05:35.325
  STEP: watching @ 05/13/23 19:05:35.333
  May 13 19:05:35.333: INFO: starting watch
  STEP: getting @ 05/13/23 19:05:35.338
  STEP: listing @ 05/13/23 19:05:35.34
  STEP: patching @ 05/13/23 19:05:35.341
  STEP: updating @ 05/13/23 19:05:35.344
  May 13 19:05:35.346: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 05/13/23 19:05:35.346
  STEP: deleting a collection @ 05/13/23 19:05:35.352
  May 13 19:05:35.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-706" for this suite. @ 05/13/23 19:05:35.361
• [0.061 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 05/13/23 19:05:35.366
  May 13 19:05:35.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 19:05:35.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:05:35.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:05:35.379
  May 13 19:05:35.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6445 create -f -'
  May 13 19:05:35.613: INFO: stderr: ""
  May 13 19:05:35.613: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  May 13 19:05:35.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6445 create -f -'
  May 13 19:05:35.831: INFO: stderr: ""
  May 13 19:05:35.831: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/13/23 19:05:35.831
  E0513 19:05:36.235799      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:36.835: INFO: Selector matched 1 pods for map[app:agnhost]
  May 13 19:05:36.835: INFO: Found 0 / 1
  E0513 19:05:37.257140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:37.835: INFO: Selector matched 1 pods for map[app:agnhost]
  May 13 19:05:37.835: INFO: Found 1 / 1
  May 13 19:05:37.835: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 13 19:05:37.837: INFO: Selector matched 1 pods for map[app:agnhost]
  May 13 19:05:37.837: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 13 19:05:37.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6445 describe pod agnhost-primary-b42l2'
  May 13 19:05:37.893: INFO: stderr: ""
  May 13 19:05:37.893: INFO: stdout: "Name:             agnhost-primary-b42l2\nNamespace:        kubectl-6445\nPriority:         0\nService Account:  default\nNode:             worker00/192.168.58.100\nStart Time:       Sat, 13 May 2023 19:05:35 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: f5bf661bc7c43dca65e77fb9c74231dc3d599020b38c2db2d6273ea9e2f09de3\n                  cni.projectcalico.org/podIP: 10.200.131.164/32\n                  cni.projectcalico.org/podIPs: 10.200.131.164/32\nStatus:           Running\nIP:               10.200.131.164\nIPs:\n  IP:           10.200.131.164\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6e80f567e927886da8b13e9f7f59b87a3edf3cabeda6171b89b080c4954d6cb8\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 13 May 2023 19:05:36 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-d2k5r (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-d2k5r:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6445/agnhost-primary-b42l2 to worker00\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  May 13 19:05:37.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6445 describe rc agnhost-primary'
  May 13 19:05:37.952: INFO: stderr: ""
  May 13 19:05:37.952: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6445\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-b42l2\n"
  May 13 19:05:37.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6445 describe service agnhost-primary'
  May 13 19:05:38.003: INFO: stderr: ""
  May 13 19:05:38.003: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6445\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.32.0.99\nIPs:               10.32.0.99\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.200.131.164:6379\nSession Affinity:  None\nEvents:            <none>\n"
  May 13 19:05:38.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6445 describe node worker00'
  May 13 19:05:38.086: INFO: stderr: ""
  May 13 19:05:38.086: INFO: stdout: "Name:               worker00\nRoles:              controller,worker\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=worker00\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/controller=true\n                    node-role.kubernetes.io/worker=true\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cephfs.csi.ceph.com\":\"worker00\",\"rbd.csi.ceph.com\":\"worker00\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.58.100/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.200.131.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 13 May 2023 18:05:39 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  worker00\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 13 May 2023 19:05:28 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 13 May 2023 18:09:16 +0000   Sat, 13 May 2023 18:09:16 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 13 May 2023 19:04:21 +0000   Sat, 13 May 2023 18:05:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 13 May 2023 19:04:21 +0000   Sat, 13 May 2023 18:05:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 13 May 2023 19:04:21 +0000   Sat, 13 May 2023 18:05:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 13 May 2023 19:04:21 +0000   Sat, 13 May 2023 18:06:13 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.58.100\n  Hostname:    worker00\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    31811408Ki\n  example.com/fakecpu:  1k\n  hugepages-2Mi:        0\n  memory:               4019456Ki\n  pods:                 110\nAllocatable:\n  cpu:                  4\n  ephemeral-storage:    29317393565\n  example.com/fakecpu:  1k\n  hugepages-2Mi:        0\n  memory:               3917056Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 4c20f6b6ea184be689fb0a68204a6800\n  System UUID:                9904c0ca-0765-2041-89fe-2abd4fa384a1\n  Boot ID:                    efc0de99-e01d-44f7-8745-865d010924d8\n  Kernel Version:             5.15.0-30-generic\n  OS Image:                   Ubuntu 22.04 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.1\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nPodCIDR:                      10.200.0.0/24\nPodCIDRs:                     10.200.0.0/24\nNon-terminated Pods:          (15 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 etcd-worker00                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         58m\n  kube-system                 gobetween-worker00                                         100m (2%)     0 (0%)      0 (0%)           0 (0%)         58m\n  kube-system                 kube-apiserver-worker00                                    250m (6%)     0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                 kube-controller-manager-worker00                           200m (5%)     0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                 kube-proxy-worker00                                        200m (5%)     0 (0%)      0 (0%)           0 (0%)         59m\n  kube-system                 kube-scheduler-worker00                                    100m (2%)     0 (0%)      0 (0%)           0 (0%)         58m\n  kubectl-6445                agnhost-primary-b42l2                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  networking                  calico-node-xsr47                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         59m\n  networking                  metallb-speaker-v7flx                                      100m (2%)     100m (2%)   100Mi (2%)       100Mi (2%)     46m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  sonobuoy                    sonobuoy-e2e-job-dc09634ed87e4d2d                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0619921c091243c5-6tddn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  storage                     ceph-csi-cephfs-nodeplugin-rhhl2                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  storage                     ceph-csi-rbd-nodeplugin-t44kh                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  storage                     ceph-mon-worker00-79cd8cd599-5skt9                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests     Limits\n  --------             --------     ------\n  cpu                  1200m (30%)  100m (2%)\n  memory               100Mi (2%)   100Mi (2%)\n  ephemeral-storage    0 (0%)       0 (0%)\n  hugepages-2Mi        0 (0%)       0 (0%)\n  example.com/fakecpu  0            0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 59m                kube-proxy       \n  Normal   Starting                 60m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      60m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  60m (x8 over 60m)  kubelet          Node worker00 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    60m (x7 over 60m)  kubelet          Node worker00 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     60m (x7 over 60m)  kubelet          Node worker00 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  60m                kubelet          Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           59m                node-controller  Node worker00 event: Registered Node worker00 in Controller\n"
  May 13 19:05:38.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6445 describe namespace kubectl-6445'
  May 13 19:05:38.135: INFO: stderr: ""
  May 13 19:05:38.135: INFO: stdout: "Name:         kubectl-6445\nLabels:       e2e-framework=kubectl\n              e2e-run=f9beaaef-e856-41aa-8a17-4bd32496d1c5\n              kubernetes.io/metadata.name=kubectl-6445\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  May 13 19:05:38.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6445" for this suite. @ 05/13/23 19:05:38.137
• [2.775 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/13/23 19:05:38.142
  May 13 19:05:38.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename deployment @ 05/13/23 19:05:38.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:05:38.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:05:38.155
  May 13 19:05:38.163: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0513 19:05:38.257446      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:39.257621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:40.257758      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:41.262825      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:42.263473      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:43.172: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/13/23 19:05:43.172
  May 13 19:05:43.172: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0513 19:05:43.264440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:44.264598      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:45.176: INFO: Creating deployment "test-rollover-deployment"
  May 13 19:05:45.191: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0513 19:05:45.265971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:46.267234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:47.194: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  May 13 19:05:47.197: INFO: Ensure that both replica sets have 1 created replica
  May 13 19:05:47.199: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  May 13 19:05:47.204: INFO: Updating deployment test-rollover-deployment
  May 13 19:05:47.204: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0513 19:05:47.267799      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:48.267927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:49.209: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  May 13 19:05:49.214: INFO: Make sure deployment "test-rollover-deployment" is complete
  May 13 19:05:49.218: INFO: all replica sets need to contain the pod-template-hash label
  May 13 19:05:49.218: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 5, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:05:49.268393      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:50.269211      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:51.221: INFO: all replica sets need to contain the pod-template-hash label
  May 13 19:05:51.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 5, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:05:51.269288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:52.269666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:53.222: INFO: all replica sets need to contain the pod-template-hash label
  May 13 19:05:53.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 5, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:05:53.270055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:54.270025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:55.222: INFO: all replica sets need to contain the pod-template-hash label
  May 13 19:05:55.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 5, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:05:55.271034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:56.271980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:57.228: INFO: all replica sets need to contain the pod-template-hash label
  May 13 19:05:57.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 5, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 5, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:05:57.273323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:05:58.274239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:05:59.225: INFO: 
  May 13 19:05:59.225: INFO: Ensure that both old replica sets have no replicas
  May 13 19:05:59.233: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-2775  c052a997-8044-4e55-becb-47cce993d6dd 31165 2 2023-05-13 19:05:45 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-13 19:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007f9ca38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-13 19:05:45 +0000 UTC,LastTransitionTime:2023-05-13 19:05:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-05-13 19:05:58 +0000 UTC,LastTransitionTime:2023-05-13 19:05:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 13 19:05:59.235: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-2775  d31e4e4d-4dc6-4fcc-a4c6-c1eceecfa13d 31155 2 2023-05-13 19:05:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c052a997-8044-4e55-becb-47cce993d6dd 0xc007f9cec7 0xc007f9cec8}] [] [{kube-controller-manager Update apps/v1 2023-05-13 19:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c052a997-8044-4e55-becb-47cce993d6dd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:05:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007f9cf88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 13 19:05:59.235: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  May 13 19:05:59.235: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2775  d577e839-7d58-4437-a254-23d3a596c303 31163 2 2023-05-13 19:05:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c052a997-8044-4e55-becb-47cce993d6dd 0xc007f9cd97 0xc007f9cd98}] [] [{e2e.test Update apps/v1 2023-05-13 19:05:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:05:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c052a997-8044-4e55-becb-47cce993d6dd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:05:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007f9ce58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 13 19:05:59.235: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-2775  8ecaf5d4-6fc0-427c-8417-9d40982b9262 31097 2 2023-05-13 19:05:45 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c052a997-8044-4e55-becb-47cce993d6dd 0xc007f9cff7 0xc007f9cff8}] [] [{kube-controller-manager Update apps/v1 2023-05-13 19:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c052a997-8044-4e55-becb-47cce993d6dd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:05:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007f9d0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 13 19:05:59.238: INFO: Pod "test-rollover-deployment-57777854c9-97cz7" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-97cz7 test-rollover-deployment-57777854c9- deployment-2775  ec6fa748-ab59-41e6-b0d8-b9d8f041f6c3 31112 0 2023-05-13 19:05:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[cni.projectcalico.org/containerID:5c186db415852905039060703b6910de4cefc46fb0491607f5e4040cd6f33785 cni.projectcalico.org/podIP:10.200.131.170/32 cni.projectcalico.org/podIPs:10.200.131.170/32] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 d31e4e4d-4dc6-4fcc-a4c6-c1eceecfa13d 0xc007099b37 0xc007099b38}] [] [{calico Update v1 2023-05-13 19:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 19:05:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d31e4e4d-4dc6-4fcc-a4c6-c1eceecfa13d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 19:05:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.170\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmtdw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmtdw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:05:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:05:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:05:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:10.200.131.170,StartTime:2023-05-13 19:05:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 19:05:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://81c775c8db9cda06cb7c95029899ac385dd2cfb3abee1a7b71b48553359a73be,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.170,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 19:05:59.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2775" for this suite. @ 05/13/23 19:05:59.241
• [21.103 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 05/13/23 19:05:59.247
  May 13 19:05:59.247: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename svcaccounts @ 05/13/23 19:05:59.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:05:59.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:05:59.263
  May 13 19:05:59.268: INFO: Got root ca configmap in namespace "svcaccounts-6674"
  May 13 19:05:59.273: INFO: Deleted root ca configmap in namespace "svcaccounts-6674"
  E0513 19:05:59.275436      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 05/13/23 19:05:59.774
  May 13 19:05:59.775: INFO: Recreated root ca configmap in namespace "svcaccounts-6674"
  May 13 19:05:59.778: INFO: Updated root ca configmap in namespace "svcaccounts-6674"
  E0513 19:06:00.275756      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for the root ca configmap reconciled @ 05/13/23 19:06:00.278
  May 13 19:06:00.281: INFO: Reconciled root ca configmap in namespace "svcaccounts-6674"
  May 13 19:06:00.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6674" for this suite. @ 05/13/23 19:06:00.283
• [1.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 05/13/23 19:06:00.296
  May 13 19:06:00.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 19:06:00.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:00.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:00.311
  STEP: Setting up server cert @ 05/13/23 19:06:00.325
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 19:06:00.525
  STEP: Deploying the webhook pod @ 05/13/23 19:06:00.539
  STEP: Wait for the deployment to be ready @ 05/13/23 19:06:00.548
  May 13 19:06:00.553: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 19:06:01.276587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:02.277030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 19:06:02.56
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 19:06:02.577
  E0513 19:06:03.277911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:06:03.577: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/13/23 19:06:03.623
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/13/23 19:06:03.65
  STEP: Deleting the collection of validation webhooks @ 05/13/23 19:06:03.674
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/13/23 19:06:03.704
  May 13 19:06:03.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3212" for this suite. @ 05/13/23 19:06:03.748
  STEP: Destroying namespace "webhook-markers-358" for this suite. @ 05/13/23 19:06:03.752
• [3.461 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 05/13/23 19:06:03.759
  May 13 19:06:03.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/13/23 19:06:03.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:03.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:03.78
  STEP: fetching the /apis discovery document @ 05/13/23 19:06:03.782
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/13/23 19:06:03.783
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/13/23 19:06:03.783
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/13/23 19:06:03.783
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/13/23 19:06:03.784
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/13/23 19:06:03.784
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/13/23 19:06:03.784
  May 13 19:06:03.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6182" for this suite. @ 05/13/23 19:06:03.787
• [0.032 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 05/13/23 19:06:03.793
  May 13 19:06:03.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 19:06:03.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:03.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:03.802
  STEP: creating a replication controller @ 05/13/23 19:06:03.805
  May 13 19:06:03.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 create -f -'
  May 13 19:06:04.018: INFO: stderr: ""
  May 13 19:06:04.018: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/13/23 19:06:04.018
  May 13 19:06:04.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 13 19:06:04.088: INFO: stderr: ""
  May 13 19:06:04.088: INFO: stdout: "update-demo-nautilus-k9lmt update-demo-nautilus-wp4hn "
  May 13 19:06:04.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 get pods update-demo-nautilus-k9lmt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 19:06:04.147: INFO: stderr: ""
  May 13 19:06:04.147: INFO: stdout: ""
  May 13 19:06:04.147: INFO: update-demo-nautilus-k9lmt is created but not running
  E0513 19:06:04.279017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:05.279791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:06.280541      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:07.280838      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:08.282699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:06:09.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 13 19:06:09.212: INFO: stderr: ""
  May 13 19:06:09.212: INFO: stdout: "update-demo-nautilus-k9lmt update-demo-nautilus-wp4hn "
  May 13 19:06:09.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 get pods update-demo-nautilus-k9lmt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 19:06:09.282: INFO: stderr: ""
  May 13 19:06:09.282: INFO: stdout: "true"
  May 13 19:06:09.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 get pods update-demo-nautilus-k9lmt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0513 19:06:09.283106      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:06:09.343: INFO: stderr: ""
  May 13 19:06:09.343: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 13 19:06:09.343: INFO: validating pod update-demo-nautilus-k9lmt
  May 13 19:06:09.347: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 13 19:06:09.347: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 13 19:06:09.347: INFO: update-demo-nautilus-k9lmt is verified up and running
  May 13 19:06:09.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 get pods update-demo-nautilus-wp4hn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 13 19:06:09.419: INFO: stderr: ""
  May 13 19:06:09.419: INFO: stdout: "true"
  May 13 19:06:09.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 get pods update-demo-nautilus-wp4hn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 13 19:06:09.497: INFO: stderr: ""
  May 13 19:06:09.497: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 13 19:06:09.497: INFO: validating pod update-demo-nautilus-wp4hn
  May 13 19:06:09.501: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 13 19:06:09.501: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 13 19:06:09.501: INFO: update-demo-nautilus-wp4hn is verified up and running
  STEP: using delete to clean up resources @ 05/13/23 19:06:09.501
  May 13 19:06:09.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 delete --grace-period=0 --force -f -'
  May 13 19:06:09.569: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 13 19:06:09.569: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 13 19:06:09.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 get rc,svc -l name=update-demo --no-headers'
  May 13 19:06:09.669: INFO: stderr: "No resources found in kubectl-1338 namespace.\n"
  May 13 19:06:09.669: INFO: stdout: ""
  May 13 19:06:09.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-1338 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 13 19:06:09.754: INFO: stderr: ""
  May 13 19:06:09.754: INFO: stdout: ""
  May 13 19:06:09.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1338" for this suite. @ 05/13/23 19:06:09.76
• [5.975 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 05/13/23 19:06:09.767
  May 13 19:06:09.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 19:06:09.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:09.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:09.783
  STEP: Creating configMap with name configmap-test-volume-a1130f0b-6310-4c04-9792-601240e4de8c @ 05/13/23 19:06:09.786
  STEP: Creating a pod to test consume configMaps @ 05/13/23 19:06:09.789
  E0513 19:06:10.283612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:11.284840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:12.285190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:13.285812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:06:13.811
  May 13 19:06:13.814: INFO: Trying to get logs from node worker00 pod pod-configmaps-bbcb8f41-8018-41a8-aad3-cdeb3334ae57 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 19:06:13.819
  May 13 19:06:13.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7848" for this suite. @ 05/13/23 19:06:13.839
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/13/23 19:06:13.843
  May 13 19:06:13.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/13/23 19:06:13.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:13.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:13.855
  E0513 19:06:14.287803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:15.287871      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:06:15.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 05/13/23 19:06:15.874
  STEP: Cleaning up the configmap @ 05/13/23 19:06:15.877
  STEP: Cleaning up the pod @ 05/13/23 19:06:15.879
  STEP: Destroying namespace "emptydir-wrapper-5645" for this suite. @ 05/13/23 19:06:15.883
• [2.048 seconds]
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 05/13/23 19:06:15.891
  May 13 19:06:15.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename runtimeclass @ 05/13/23 19:06:15.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:15.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:15.903
  E0513 19:06:16.288371      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:17.288551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:06:17.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4714" for this suite. @ 05/13/23 19:06:17.924
• [2.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 05/13/23 19:06:17.927
  May 13 19:06:17.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:06:17.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:17.936
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:17.937
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:06:17.939
  E0513 19:06:18.288728      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:19.289254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:20.292056      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:21.293352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:06:21.971
  May 13 19:06:21.973: INFO: Trying to get logs from node worker00 pod downwardapi-volume-9321c019-2d88-48ca-ba62-06b52b4f56e5 container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:06:21.976
  May 13 19:06:21.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-816" for this suite. @ 05/13/23 19:06:21.999
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 05/13/23 19:06:22.005
  May 13 19:06:22.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 19:06:22.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:22.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:22.02
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:06:22.023
  E0513 19:06:22.293264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:23.293485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:24.294546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:25.295495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:06:26.038
  May 13 19:06:26.039: INFO: Trying to get logs from node worker00 pod downwardapi-volume-faa981e3-69a9-4e31-ac90-0577ac9b8b6e container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:06:26.042
  May 13 19:06:26.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9260" for this suite. @ 05/13/23 19:06:26.051
• [4.049 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 05/13/23 19:06:26.055
  May 13 19:06:26.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 19:06:26.055
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:26.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:26.067
  STEP: Setting up server cert @ 05/13/23 19:06:26.083
  E0513 19:06:26.301098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 19:06:26.411
  STEP: Deploying the webhook pod @ 05/13/23 19:06:26.425
  STEP: Wait for the deployment to be ready @ 05/13/23 19:06:26.433
  May 13 19:06:26.435: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0513 19:06:27.301394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:28.301580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 19:06:28.443
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 19:06:28.461
  E0513 19:06:29.302163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:06:29.462: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/13/23 19:06:29.5
  STEP: Creating a configMap that should be mutated @ 05/13/23 19:06:29.506
  STEP: Deleting the collection of validation webhooks @ 05/13/23 19:06:29.521
  STEP: Creating a configMap that should not be mutated @ 05/13/23 19:06:29.546
  May 13 19:06:29.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1144" for this suite. @ 05/13/23 19:06:29.571
  STEP: Destroying namespace "webhook-markers-3381" for this suite. @ 05/13/23 19:06:29.575
• [3.525 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 05/13/23 19:06:29.58
  May 13 19:06:29.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 19:06:29.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:29.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:29.598
  STEP: Setting up server cert @ 05/13/23 19:06:29.61
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 19:06:29.866
  STEP: Deploying the webhook pod @ 05/13/23 19:06:29.869
  STEP: Wait for the deployment to be ready @ 05/13/23 19:06:29.876
  May 13 19:06:29.882: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 19:06:30.302739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:31.302765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 19:06:31.887
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 19:06:31.892
  E0513 19:06:32.303866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:06:32.894: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/13/23 19:06:32.896
  STEP: create a namespace for the webhook @ 05/13/23 19:06:32.907
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/13/23 19:06:32.918
  May 13 19:06:32.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5100" for this suite. @ 05/13/23 19:06:32.953
  STEP: Destroying namespace "webhook-markers-638" for this suite. @ 05/13/23 19:06:32.971
  STEP: Destroying namespace "fail-closed-namespace-9285" for this suite. @ 05/13/23 19:06:32.978
• [3.401 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 05/13/23 19:06:32.987
  May 13 19:06:32.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 19:06:32.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:32.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:33
  STEP: Counting existing ResourceQuota @ 05/13/23 19:06:33.002
  E0513 19:06:33.307550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:34.307664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:35.307891      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:36.309662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:37.310321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/13/23 19:06:38.003
  STEP: Ensuring resource quota status is calculated @ 05/13/23 19:06:38.009
  E0513 19:06:38.311029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:39.311217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:06:40.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-796" for this suite. @ 05/13/23 19:06:40.013
• [7.029 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 05/13/23 19:06:40.016
  May 13 19:06:40.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename discovery @ 05/13/23 19:06:40.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:40.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:40.025
  STEP: Setting up server cert @ 05/13/23 19:06:40.026
  E0513 19:06:40.312187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:06:40.450: INFO: Checking APIGroup: apiregistration.k8s.io
  May 13 19:06:40.451: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  May 13 19:06:40.451: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  May 13 19:06:40.451: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  May 13 19:06:40.451: INFO: Checking APIGroup: apps
  May 13 19:06:40.452: INFO: PreferredVersion.GroupVersion: apps/v1
  May 13 19:06:40.452: INFO: Versions found [{apps/v1 v1}]
  May 13 19:06:40.452: INFO: apps/v1 matches apps/v1
  May 13 19:06:40.452: INFO: Checking APIGroup: events.k8s.io
  May 13 19:06:40.453: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  May 13 19:06:40.453: INFO: Versions found [{events.k8s.io/v1 v1}]
  May 13 19:06:40.453: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  May 13 19:06:40.453: INFO: Checking APIGroup: authentication.k8s.io
  May 13 19:06:40.453: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  May 13 19:06:40.453: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1} {authentication.k8s.io/v1alpha1 v1alpha1}]
  May 13 19:06:40.453: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  May 13 19:06:40.453: INFO: Checking APIGroup: authorization.k8s.io
  May 13 19:06:40.455: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  May 13 19:06:40.455: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  May 13 19:06:40.455: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  May 13 19:06:40.455: INFO: Checking APIGroup: autoscaling
  May 13 19:06:40.456: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  May 13 19:06:40.456: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  May 13 19:06:40.456: INFO: autoscaling/v2 matches autoscaling/v2
  May 13 19:06:40.456: INFO: Checking APIGroup: batch
  May 13 19:06:40.457: INFO: PreferredVersion.GroupVersion: batch/v1
  May 13 19:06:40.457: INFO: Versions found [{batch/v1 v1}]
  May 13 19:06:40.457: INFO: batch/v1 matches batch/v1
  May 13 19:06:40.457: INFO: Checking APIGroup: certificates.k8s.io
  May 13 19:06:40.457: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  May 13 19:06:40.457: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  May 13 19:06:40.457: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  May 13 19:06:40.457: INFO: Checking APIGroup: networking.k8s.io
  May 13 19:06:40.458: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  May 13 19:06:40.458: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
  May 13 19:06:40.458: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  May 13 19:06:40.458: INFO: Checking APIGroup: policy
  May 13 19:06:40.459: INFO: PreferredVersion.GroupVersion: policy/v1
  May 13 19:06:40.459: INFO: Versions found [{policy/v1 v1}]
  May 13 19:06:40.459: INFO: policy/v1 matches policy/v1
  May 13 19:06:40.459: INFO: Checking APIGroup: rbac.authorization.k8s.io
  May 13 19:06:40.460: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  May 13 19:06:40.460: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  May 13 19:06:40.460: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  May 13 19:06:40.460: INFO: Checking APIGroup: storage.k8s.io
  May 13 19:06:40.461: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  May 13 19:06:40.461: INFO: Versions found [{storage.k8s.io/v1 v1}]
  May 13 19:06:40.461: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  May 13 19:06:40.461: INFO: Checking APIGroup: admissionregistration.k8s.io
  May 13 19:06:40.462: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  May 13 19:06:40.462: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1alpha1 v1alpha1}]
  May 13 19:06:40.462: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  May 13 19:06:40.462: INFO: Checking APIGroup: apiextensions.k8s.io
  May 13 19:06:40.463: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  May 13 19:06:40.463: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  May 13 19:06:40.463: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  May 13 19:06:40.463: INFO: Checking APIGroup: scheduling.k8s.io
  May 13 19:06:40.463: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  May 13 19:06:40.463: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  May 13 19:06:40.463: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  May 13 19:06:40.463: INFO: Checking APIGroup: coordination.k8s.io
  May 13 19:06:40.464: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  May 13 19:06:40.464: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  May 13 19:06:40.464: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  May 13 19:06:40.464: INFO: Checking APIGroup: node.k8s.io
  May 13 19:06:40.465: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  May 13 19:06:40.465: INFO: Versions found [{node.k8s.io/v1 v1}]
  May 13 19:06:40.465: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  May 13 19:06:40.465: INFO: Checking APIGroup: discovery.k8s.io
  May 13 19:06:40.466: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  May 13 19:06:40.466: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  May 13 19:06:40.466: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  May 13 19:06:40.466: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  May 13 19:06:40.467: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  May 13 19:06:40.467: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  May 13 19:06:40.467: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  May 13 19:06:40.467: INFO: Checking APIGroup: internal.apiserver.k8s.io
  May 13 19:06:40.468: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
  May 13 19:06:40.468: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
  May 13 19:06:40.468: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
  May 13 19:06:40.468: INFO: Checking APIGroup: resource.k8s.io
  May 13 19:06:40.469: INFO: PreferredVersion.GroupVersion: resource.k8s.io/v1alpha2
  May 13 19:06:40.469: INFO: Versions found [{resource.k8s.io/v1alpha2 v1alpha2}]
  May 13 19:06:40.469: INFO: resource.k8s.io/v1alpha2 matches resource.k8s.io/v1alpha2
  May 13 19:06:40.469: INFO: Checking APIGroup: crd.projectcalico.org
  May 13 19:06:40.469: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
  May 13 19:06:40.469: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
  May 13 19:06:40.469: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
  May 13 19:06:40.469: INFO: Checking APIGroup: snapshot.storage.k8s.io
  May 13 19:06:40.470: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
  May 13 19:06:40.470: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
  May 13 19:06:40.470: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
  May 13 19:06:40.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-3638" for this suite. @ 05/13/23 19:06:40.472
• [0.463 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 05/13/23 19:06:40.481
  May 13 19:06:40.481: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pod-network-test @ 05/13/23 19:06:40.482
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:06:40.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:06:40.492
  STEP: Performing setup for networking test in namespace pod-network-test-3964 @ 05/13/23 19:06:40.493
  STEP: creating a selector @ 05/13/23 19:06:40.493
  STEP: Creating the service pods in kubernetes @ 05/13/23 19:06:40.494
  May 13 19:06:40.494: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0513 19:06:41.313440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:42.313767      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:43.314468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:44.315308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:45.315446      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:46.316538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:47.317507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:48.318095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:49.320787      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:50.322545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:51.323025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:52.323682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:53.324914      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:54.326663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:55.326786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:56.327886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:57.328030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:58.328191      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:06:59.329355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:00.329517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:01.329609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:02.331016      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/13/23 19:07:02.545
  E0513 19:07:03.333026      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:04.334136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:04.572: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 13 19:07:04.572: INFO: Going to poll 10.200.131.161 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  May 13 19:07:04.574: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.131.161:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3964 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:07:04.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:07:04.575: INFO: ExecWithOptions: Clientset creation
  May 13 19:07:04.575: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/pod-network-test-3964/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.200.131.161%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 13 19:07:04.641: INFO: Found all 1 expected endpoints: [netserver-0]
  May 13 19:07:04.641: INFO: Going to poll 10.200.5.23 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  May 13 19:07:04.643: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.5.23:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3964 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:07:04.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:07:04.643: INFO: ExecWithOptions: Clientset creation
  May 13 19:07:04.644: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/pod-network-test-3964/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.200.5.23%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 13 19:07:04.682: INFO: Found all 1 expected endpoints: [netserver-1]
  May 13 19:07:04.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3964" for this suite. @ 05/13/23 19:07:04.683
• [24.206 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 05/13/23 19:07:04.687
  May 13 19:07:04.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename endpointslice @ 05/13/23 19:07:04.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:07:04.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:07:04.699
  May 13 19:07:04.703: INFO: Endpoints addresses: [192.168.58.100] , ports: [6443]
  May 13 19:07:04.703: INFO: EndpointSlices addresses: [192.168.58.100] , ports: [6443]
  May 13 19:07:04.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6745" for this suite. @ 05/13/23 19:07:04.704
• [0.021 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/13/23 19:07:04.71
  May 13 19:07:04.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 19:07:04.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:07:04.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:07:04.718
  STEP: Creating secret with name secret-test-map-be5197de-2a33-4787-9ada-a629f092c61c @ 05/13/23 19:07:04.72
  STEP: Creating a pod to test consume secrets @ 05/13/23 19:07:04.722
  E0513 19:07:05.335494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:06.336620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:07.337067      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:08.337221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:07:08.74
  May 13 19:07:08.742: INFO: Trying to get logs from node worker00 pod pod-secrets-0fa7b2ee-2fbf-41fb-be21-e38443397654 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 19:07:08.745
  May 13 19:07:08.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5851" for this suite. @ 05/13/23 19:07:08.769
• [4.065 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 05/13/23 19:07:08.778
  May 13 19:07:08.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 19:07:08.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:07:08.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:07:08.792
  STEP: creating service in namespace services-522 @ 05/13/23 19:07:08.797
  STEP: creating service affinity-clusterip-transition in namespace services-522 @ 05/13/23 19:07:08.797
  STEP: creating replication controller affinity-clusterip-transition in namespace services-522 @ 05/13/23 19:07:08.812
  I0513 19:07:08.816648      21 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-522, replica count: 3
  E0513 19:07:09.338555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:10.338612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:11.339271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0513 19:07:11.868995      21 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 13 19:07:11.873: INFO: Creating new exec pod
  E0513 19:07:12.342842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:13.344475      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:14.344646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:14.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-522 exec execpod-affinityv8jpr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  May 13 19:07:15.002: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  May 13 19:07:15.002: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 19:07:15.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-522 exec execpod-affinityv8jpr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.72 80'
  May 13 19:07:15.089: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.72 80\nConnection to 10.32.0.72 80 port [tcp/http] succeeded!\n"
  May 13 19:07:15.089: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 19:07:15.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-522 exec execpod-affinityv8jpr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.32.0.72:80/ ; done'
  May 13 19:07:15.308: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n"
  May 13 19:07:15.308: INFO: stdout: "\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-qld29\naffinity-clusterip-transition-nkfxh\naffinity-clusterip-transition-qld29\naffinity-clusterip-transition-qld29\naffinity-clusterip-transition-nkfxh\naffinity-clusterip-transition-nkfxh\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-qld29\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-nkfxh\naffinity-clusterip-transition-nkfxh\naffinity-clusterip-transition-qld29\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-nkfxh\naffinity-clusterip-transition-tfqxz"
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-qld29
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-nkfxh
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-qld29
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-qld29
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-nkfxh
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-nkfxh
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-qld29
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-nkfxh
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-nkfxh
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-qld29
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-nkfxh
  May 13 19:07:15.308: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-522 exec execpod-affinityv8jpr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.32.0.72:80/ ; done'
  E0513 19:07:15.346554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:15.488: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.32.0.72:80/\n"
  May 13 19:07:15.488: INFO: stdout: "\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz\naffinity-clusterip-transition-tfqxz"
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Received response from host: affinity-clusterip-transition-tfqxz
  May 13 19:07:15.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 19:07:15.491: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-522, will wait for the garbage collector to delete the pods @ 05/13/23 19:07:15.503
  May 13 19:07:15.561: INFO: Deleting ReplicationController affinity-clusterip-transition took: 4.089065ms
  May 13 19:07:15.661: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.552515ms
  E0513 19:07:16.347175      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:17.347722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-522" for this suite. @ 05/13/23 19:07:17.675
• [8.901 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 05/13/23 19:07:17.68
  May 13 19:07:17.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/13/23 19:07:17.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:07:17.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:07:17.694
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/13/23 19:07:17.696
  May 13 19:07:17.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:07:18.348477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:19.349998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:20.349865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:21.351293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:22.351609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/13/23 19:07:22.749
  May 13 19:07:22.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:07:23.352170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:23.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:07:24.352506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:25.353091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:26.353831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:27.356468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:28.357005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:28.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6099" for this suite. @ 05/13/23 19:07:28.916
• [11.239 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/13/23 19:07:28.92
  May 13 19:07:28.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 19:07:28.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:07:28.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:07:28.934
  STEP: creating the pod @ 05/13/23 19:07:28.936
  STEP: submitting the pod to kubernetes @ 05/13/23 19:07:28.936
  W0513 19:07:28.941772      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0513 19:07:29.358956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:30.361847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 05/13/23 19:07:30.948
  STEP: updating the pod @ 05/13/23 19:07:30.951
  E0513 19:07:31.362542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:31.457: INFO: Successfully updated pod "pod-update-activedeadlineseconds-82934f25-207e-4cb9-9901-e21dfca6f98a"
  E0513 19:07:32.362850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:33.364361      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:34.364604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:35.365160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:35.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9397" for this suite. @ 05/13/23 19:07:35.476
• [6.561 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/13/23 19:07:35.482
  May 13 19:07:35.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubelet-test @ 05/13/23 19:07:35.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:07:35.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:07:35.496
  E0513 19:07:36.366217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:37.367198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:37.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9997" for this suite. @ 05/13/23 19:07:37.512
• [2.043 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 05/13/23 19:07:37.526
  May 13 19:07:37.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/13/23 19:07:37.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:07:37.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:07:37.54
  May 13 19:07:37.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:07:38.367631      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/13/23 19:07:38.734
  May 13 19:07:38.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5631 --namespace=crd-publish-openapi-5631 create -f -'
  May 13 19:07:39.267: INFO: stderr: ""
  May 13 19:07:39.267: INFO: stdout: "e2e-test-crd-publish-openapi-3617-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 13 19:07:39.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5631 --namespace=crd-publish-openapi-5631 delete e2e-test-crd-publish-openapi-3617-crds test-cr'
  May 13 19:07:39.343: INFO: stderr: ""
  May 13 19:07:39.343: INFO: stdout: "e2e-test-crd-publish-openapi-3617-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  May 13 19:07:39.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5631 --namespace=crd-publish-openapi-5631 apply -f -'
  E0513 19:07:39.368248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:39.554: INFO: stderr: ""
  May 13 19:07:39.554: INFO: stdout: "e2e-test-crd-publish-openapi-3617-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 13 19:07:39.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5631 --namespace=crd-publish-openapi-5631 delete e2e-test-crd-publish-openapi-3617-crds test-cr'
  May 13 19:07:39.621: INFO: stderr: ""
  May 13 19:07:39.621: INFO: stdout: "e2e-test-crd-publish-openapi-3617-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/13/23 19:07:39.621
  May 13 19:07:39.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=crd-publish-openapi-5631 explain e2e-test-crd-publish-openapi-3617-crds'
  May 13 19:07:39.818: INFO: stderr: ""
  May 13 19:07:39.818: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-3617-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0513 19:07:40.369246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:41.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5631" for this suite. @ 05/13/23 19:07:41.081
• [3.561 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 05/13/23 19:07:41.087
  May 13 19:07:41.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-probe @ 05/13/23 19:07:41.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:07:41.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:07:41.1
  STEP: Creating pod liveness-ee70da93-40df-47d3-81a3-99b5658cdf6a in namespace container-probe-6005 @ 05/13/23 19:07:41.102
  E0513 19:07:41.370311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:42.370542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:07:43.112: INFO: Started pod liveness-ee70da93-40df-47d3-81a3-99b5658cdf6a in namespace container-probe-6005
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/13/23 19:07:43.112
  May 13 19:07:43.114: INFO: Initial restart count of pod liveness-ee70da93-40df-47d3-81a3-99b5658cdf6a is 0
  E0513 19:07:43.372609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:44.373664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:45.375207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:46.375911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:47.376713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:48.377085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:49.378549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:50.378654      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:51.378759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:52.379069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:53.379170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:54.380265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:55.380341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:56.380733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:57.381233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:58.382125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:07:59.390290      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:00.390586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:01.390733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:02.391018      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:08:03.149: INFO: Restart count of pod container-probe-6005/liveness-ee70da93-40df-47d3-81a3-99b5658cdf6a is now 1 (20.034778183s elapsed)
  E0513 19:08:03.391141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:04.391895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:05.392378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:06.393954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:07.394489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:08.394797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:09.395475      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:10.396334      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:11.397014      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:12.397453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:13.398746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:14.398624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:15.400111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:16.402794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:17.403119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:18.403366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:19.404994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:20.405216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:21.406193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:22.406960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:08:23.181: INFO: Restart count of pod container-probe-6005/liveness-ee70da93-40df-47d3-81a3-99b5658cdf6a is now 2 (40.066901328s elapsed)
  E0513 19:08:23.407636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:24.413011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:25.414009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:26.415026      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:27.415962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:28.416149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:29.416232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:30.416552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:31.417510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:32.417759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:33.418238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:34.418523      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:35.419159      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:36.420186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:37.421557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:38.421713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:39.422572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:40.422821      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:41.423677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:42.423782      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:08:43.219: INFO: Restart count of pod container-probe-6005/liveness-ee70da93-40df-47d3-81a3-99b5658cdf6a is now 3 (1m0.105193885s elapsed)
  E0513 19:08:43.424851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:44.424721      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:45.429398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:46.430435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:47.431485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:48.431918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:49.431966      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:50.432542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:51.432766      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:52.432927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:53.433698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:54.433787      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:55.434609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:56.434827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:57.435349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:58.435507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:08:59.436566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:00.437568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:01.438284      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:02.438377      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:09:03.276: INFO: Restart count of pod container-probe-6005/liveness-ee70da93-40df-47d3-81a3-99b5658cdf6a is now 4 (1m20.162246392s elapsed)
  E0513 19:09:03.439434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:04.439702      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:05.440446      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:06.441272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:07.442042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:08.447683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:09.448529      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:10.449624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:11.451747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:12.452309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:13.453508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:14.457254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:15.457693      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:16.458402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:17.460253      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:18.461028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:19.461752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:20.462209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:21.463133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:22.463700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:23.464307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:24.465735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:25.465834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:26.467522      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:27.467951      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:28.468896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:29.469620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:30.470093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:31.471176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:32.471512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:33.472311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:34.472344      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:35.472815      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:36.473259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:37.474902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:38.475483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:39.476584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:40.477147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:41.478143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:42.478265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:43.479285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:44.479515      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:45.479559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:46.479693      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:47.479752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:48.479850      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:49.480103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:50.480415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:51.485015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:52.484432      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:53.486370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:54.487153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:55.487564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:56.487858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:57.488162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:58.490002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:09:59.490620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:00.490888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:01.491783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:02.492154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:03.493581      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:04.494256      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:05.494872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:06.495859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:07.496337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:08.497149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:09.498316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:10.498751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:11.499727      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:12.499822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:13.499832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:14.499928      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:10:15.407: INFO: Restart count of pod container-probe-6005/liveness-ee70da93-40df-47d3-81a3-99b5658cdf6a is now 5 (2m32.293115755s elapsed)
  May 13 19:10:15.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 19:10:15.41
  STEP: Destroying namespace "container-probe-6005" for this suite. @ 05/13/23 19:10:15.418
• [154.342 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/13/23 19:10:15.43
  May 13 19:10:15.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 19:10:15.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:10:15.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:10:15.44
  E0513 19:10:15.500203      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:16.500618      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:17.501192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:18.501402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:19.501542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:20.501632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:10:21.472
  May 13 19:10:21.474: INFO: Trying to get logs from node worker00 pod client-envvars-910f882d-b6ac-4210-bb28-e0026a4e4704 container env3cont: <nil>
  STEP: delete the pod @ 05/13/23 19:10:21.478
  May 13 19:10:21.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1654" for this suite. @ 05/13/23 19:10:21.489
• [6.062 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 05/13/23 19:10:21.493
  May 13 19:10:21.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replication-controller @ 05/13/23 19:10:21.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:10:21.5
  E0513 19:10:21.502568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:10:21.502
  STEP: Creating replication controller my-hostname-basic-04f82b67-fdfd-45c9-959f-a6fc835fab2c @ 05/13/23 19:10:21.504
  May 13 19:10:21.508: INFO: Pod name my-hostname-basic-04f82b67-fdfd-45c9-959f-a6fc835fab2c: Found 0 pods out of 1
  E0513 19:10:22.503469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:23.503753      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:24.504228      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:25.504605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:26.511531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:10:26.516: INFO: Pod name my-hostname-basic-04f82b67-fdfd-45c9-959f-a6fc835fab2c: Found 1 pods out of 1
  May 13 19:10:26.516: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-04f82b67-fdfd-45c9-959f-a6fc835fab2c" are running
  May 13 19:10:26.518: INFO: Pod "my-hostname-basic-04f82b67-fdfd-45c9-959f-a6fc835fab2c-chhlr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-13 19:10:21 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-13 19:10:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-13 19:10:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-13 19:10:21 +0000 UTC Reason: Message:}])
  May 13 19:10:26.518: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/13/23 19:10:26.519
  May 13 19:10:26.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3098" for this suite. @ 05/13/23 19:10:26.528
• [5.042 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 05/13/23 19:10:26.537
  May 13 19:10:26.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename cronjob @ 05/13/23 19:10:26.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:10:26.548
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:10:26.549
  STEP: Creating a ReplaceConcurrent cronjob @ 05/13/23 19:10:26.551
  STEP: Ensuring a job is scheduled @ 05/13/23 19:10:26.553
  E0513 19:10:27.506435      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:28.506465      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:29.507364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:30.507924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:31.508988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:32.509464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:33.510116      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:34.510965      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:35.511464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:36.512006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:37.517837      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:38.518552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:39.519619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:40.519922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:41.520052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:42.520395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:43.522022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:44.521411      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:45.522015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:46.523238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:47.523615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:48.523817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:49.523881      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:50.524161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:51.524245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:52.524437      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:53.524494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:54.524568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:55.524627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:56.525469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:57.526472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:58.530905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:10:59.531520      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:00.531548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/13/23 19:11:00.556
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/13/23 19:11:00.558
  STEP: Ensuring the job is replaced with a new one @ 05/13/23 19:11:00.563
  E0513 19:11:01.532332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:02.532975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:03.533414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:04.533770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:05.533957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:06.534458      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:07.536337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:08.537464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:09.537774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:10.539321      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:11.542285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:12.544753      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:13.546319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:14.546479      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:15.546513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:16.552199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:17.553508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:18.554288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:19.554476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:20.554666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:21.554807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:22.555046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:23.559554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:24.560016      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:25.560420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:26.560477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:27.561117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:28.562002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:29.562689      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:30.563823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:31.564867      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:32.565371      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:33.565769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:34.566291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:35.566544      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:36.566855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:37.566921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:38.570994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:39.571764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:40.572926      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:41.573038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:42.573725      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:43.574320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:44.575662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:45.578409      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:46.579933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:47.580420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:48.580506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:49.581339      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:50.581550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:51.584599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:52.584730      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:53.585005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:54.585266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:55.586347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:56.589421      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:57.590338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:58.591555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:11:59.592770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/13/23 19:12:00.565
  May 13 19:12:00.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2481" for this suite. @ 05/13/23 19:12:00.571
• [94.040 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/13/23 19:12:00.578
  May 13 19:12:00.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename watch @ 05/13/23 19:12:00.579
  E0513 19:12:00.593489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:12:00.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:12:00.605
  STEP: creating a watch on configmaps with a certain label @ 05/13/23 19:12:00.608
  STEP: creating a new configmap @ 05/13/23 19:12:00.609
  STEP: modifying the configmap once @ 05/13/23 19:12:00.612
  STEP: changing the label value of the configmap @ 05/13/23 19:12:00.617
  STEP: Expecting to observe a delete notification for the watched object @ 05/13/23 19:12:00.621
  May 13 19:12:00.621: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7689  2a667254-d725-44c4-b908-d0ea83a66917 33604 0 2023-05-13 19:12:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-13 19:12:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 19:12:00.622: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7689  2a667254-d725-44c4-b908-d0ea83a66917 33605 0 2023-05-13 19:12:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-13 19:12:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 19:12:00.622: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7689  2a667254-d725-44c4-b908-d0ea83a66917 33606 0 2023-05-13 19:12:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-13 19:12:00 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/13/23 19:12:00.622
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/13/23 19:12:00.633
  E0513 19:12:01.595820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:02.596155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:03.596304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:04.597279      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:05.598199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:06.598378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:07.598872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:08.599787      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:09.600604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:10.601562      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 05/13/23 19:12:10.64
  STEP: modifying the configmap a third time @ 05/13/23 19:12:10.654
  STEP: deleting the configmap @ 05/13/23 19:12:10.658
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/13/23 19:12:10.661
  May 13 19:12:10.661: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7689  2a667254-d725-44c4-b908-d0ea83a66917 33659 0 2023-05-13 19:12:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-13 19:12:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 19:12:10.661: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7689  2a667254-d725-44c4-b908-d0ea83a66917 33660 0 2023-05-13 19:12:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-13 19:12:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 19:12:10.662: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7689  2a667254-d725-44c4-b908-d0ea83a66917 33661 0 2023-05-13 19:12:00 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-13 19:12:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 19:12:10.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7689" for this suite. @ 05/13/23 19:12:10.663
• [10.088 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 05/13/23 19:12:10.667
  May 13 19:12:10.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 19:12:10.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:12:10.68
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:12:10.681
  STEP: creating Agnhost RC @ 05/13/23 19:12:10.683
  May 13 19:12:10.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3157 create -f -'
  May 13 19:12:11.186: INFO: stderr: ""
  May 13 19:12:11.186: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/13/23 19:12:11.186
  E0513 19:12:11.602151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:12.189: INFO: Selector matched 1 pods for map[app:agnhost]
  May 13 19:12:12.189: INFO: Found 1 / 1
  May 13 19:12:12.189: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/13/23 19:12:12.189
  May 13 19:12:12.192: INFO: Selector matched 1 pods for map[app:agnhost]
  May 13 19:12:12.192: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 13 19:12:12.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-3157 patch pod agnhost-primary-lmv9x -p {"metadata":{"annotations":{"x":"y"}}}'
  May 13 19:12:12.244: INFO: stderr: ""
  May 13 19:12:12.244: INFO: stdout: "pod/agnhost-primary-lmv9x patched\n"
  STEP: checking annotations @ 05/13/23 19:12:12.244
  May 13 19:12:12.246: INFO: Selector matched 1 pods for map[app:agnhost]
  May 13 19:12:12.246: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 13 19:12:12.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3157" for this suite. @ 05/13/23 19:12:12.248
• [1.585 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 05/13/23 19:12:12.253
  May 13 19:12:12.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 19:12:12.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:12:12.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:12:12.265
  STEP: Creating secret with name secret-test-d690463a-7431-4ab6-bcc8-e183daa53856 @ 05/13/23 19:12:12.266
  STEP: Creating a pod to test consume secrets @ 05/13/23 19:12:12.269
  E0513 19:12:12.602323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:13.603307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:14.604263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:15.604954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:12:16.28
  May 13 19:12:16.281: INFO: Trying to get logs from node worker00 pod pod-secrets-a9a399e9-e1e1-4046-a28b-c1bfb3b856c2 container secret-env-test: <nil>
  STEP: delete the pod @ 05/13/23 19:12:16.284
  May 13 19:12:16.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4358" for this suite. @ 05/13/23 19:12:16.292
• [4.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/13/23 19:12:16.296
  May 13 19:12:16.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename proxy @ 05/13/23 19:12:16.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:12:16.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:12:16.31
  May 13 19:12:16.313: INFO: Creating pod...
  E0513 19:12:16.606614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:17.607703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:18.324: INFO: Creating service...
  May 13 19:12:18.333: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/pods/agnhost/proxy?method=DELETE
  May 13 19:12:18.338: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 13 19:12:18.338: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/pods/agnhost/proxy?method=OPTIONS
  May 13 19:12:18.341: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 13 19:12:18.341: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/pods/agnhost/proxy?method=PATCH
  May 13 19:12:18.342: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 13 19:12:18.342: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/pods/agnhost/proxy?method=POST
  May 13 19:12:18.344: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 13 19:12:18.344: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/pods/agnhost/proxy?method=PUT
  May 13 19:12:18.345: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 13 19:12:18.345: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/services/e2e-proxy-test-service/proxy?method=DELETE
  May 13 19:12:18.347: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 13 19:12:18.347: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/services/e2e-proxy-test-service/proxy?method=OPTIONS
  May 13 19:12:18.349: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 13 19:12:18.349: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/services/e2e-proxy-test-service/proxy?method=PATCH
  May 13 19:12:18.352: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 13 19:12:18.352: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/services/e2e-proxy-test-service/proxy?method=POST
  May 13 19:12:18.356: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 13 19:12:18.356: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/services/e2e-proxy-test-service/proxy?method=PUT
  May 13 19:12:18.359: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 13 19:12:18.359: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/pods/agnhost/proxy?method=GET
  May 13 19:12:18.360: INFO: http.Client request:GET StatusCode:301
  May 13 19:12:18.360: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/services/e2e-proxy-test-service/proxy?method=GET
  May 13 19:12:18.362: INFO: http.Client request:GET StatusCode:301
  May 13 19:12:18.362: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/pods/agnhost/proxy?method=HEAD
  May 13 19:12:18.363: INFO: http.Client request:HEAD StatusCode:301
  May 13 19:12:18.363: INFO: Starting http.Client for https://10.32.0.1:443/api/v1/namespaces/proxy-470/services/e2e-proxy-test-service/proxy?method=HEAD
  May 13 19:12:18.364: INFO: http.Client request:HEAD StatusCode:301
  May 13 19:12:18.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-470" for this suite. @ 05/13/23 19:12:18.366
• [2.072 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 05/13/23 19:12:18.369
  May 13 19:12:18.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 19:12:18.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:12:18.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:12:18.382
  STEP: Creating the pod @ 05/13/23 19:12:18.383
  E0513 19:12:18.608520      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:19.608808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:20.609674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:20.915: INFO: Successfully updated pod "annotationupdate4a4fb6d1-f07d-4dd0-88e6-6ae304142e4e"
  E0513 19:12:21.610133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:22.610449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:23.610633      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:24.610886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:24.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6792" for this suite. @ 05/13/23 19:12:24.93
• [6.564 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 05/13/23 19:12:24.934
  May 13 19:12:24.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename aggregator @ 05/13/23 19:12:24.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:12:24.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:12:24.946
  May 13 19:12:24.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Registering the sample API server. @ 05/13/23 19:12:24.948
  May 13 19:12:25.168: INFO: Found ClusterRoles; assuming RBAC is enabled.
  May 13 19:12:25.185: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0513 19:12:25.610994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:26.611238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:27.219: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:27.611874      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:28.611986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:29.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:29.614440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:30.614714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:31.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:31.614896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:32.615051      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:33.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:33.615838      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:34.618584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:35.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:35.619383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:36.620208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:37.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:37.620605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:38.620735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:39.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:39.620939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:40.621466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:41.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:41.622489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:42.623000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:43.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:43.623169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:44.623895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:45.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:45.624953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:46.625257      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:47.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:47.625418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:48.625765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:49.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:49.627028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:50.628647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:51.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:51.628762      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:52.628999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:53.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0513 19:12:53.629836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:54.631329      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:55.345: INFO: Waited 109.052467ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/13/23 19:12:55.368
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/13/23 19:12:55.37
  STEP: List APIServices @ 05/13/23 19:12:55.374
  May 13 19:12:55.377: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/13/23 19:12:55.377
  May 13 19:12:55.391: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/13/23 19:12:55.391
  May 13 19:12:55.403: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.May, 13, 19, 12, 55, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/13/23 19:12:55.403
  May 13 19:12:55.405: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-05-13 19:12:55 +0000 UTC Passed all checks passed}
  May 13 19:12:55.405: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 13 19:12:55.405: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/13/23 19:12:55.405
  May 13 19:12:55.415: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-273826139" @ 05/13/23 19:12:55.415
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/13/23 19:12:55.43
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/13/23 19:12:55.436
  STEP: Patch APIService Status @ 05/13/23 19:12:55.438
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/13/23 19:12:55.442
  May 13 19:12:55.446: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-05-13 19:12:55 +0000 UTC Passed all checks passed}
  May 13 19:12:55.446: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 13 19:12:55.446: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  May 13 19:12:55.446: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 05/13/23 19:12:55.446
  STEP: Confirm that the generated APIService has been deleted @ 05/13/23 19:12:55.452
  May 13 19:12:55.452: INFO: Requesting list of APIServices to confirm quantity
  May 13 19:12:55.457: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  May 13 19:12:55.457: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  May 13 19:12:55.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-3209" for this suite. @ 05/13/23 19:12:55.532
• [30.602 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 05/13/23 19:12:55.541
  May 13 19:12:55.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 19:12:55.541
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:12:55.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:12:55.564
  STEP: Creating the pod @ 05/13/23 19:12:55.567
  E0513 19:12:55.631423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:56.632287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:57.633988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:12:58.088: INFO: Successfully updated pod "labelsupdated134730b-6789-4e88-9c50-563ce655ada1"
  E0513 19:12:58.635028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:12:59.635305      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:00.635824      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:01.636222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:13:02.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1514" for this suite. @ 05/13/23 19:13:02.105
• [6.567 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 05/13/23 19:13:02.109
  May 13 19:13:02.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename job @ 05/13/23 19:13:02.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:02.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:02.118
  STEP: Creating a job @ 05/13/23 19:13:02.119
  STEP: Ensuring job reaches completions @ 05/13/23 19:13:02.122
  E0513 19:13:02.636431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:03.636727      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:04.636798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:05.637316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:06.639249      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:07.639300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:08.639495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:09.640566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:10.641256      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:11.641840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:13:12.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4875" for this suite. @ 05/13/23 19:13:12.128
• [10.034 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/13/23 19:13:12.143
  May 13 19:13:12.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:13:12.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:12.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:12.159
  STEP: Creating projection with secret that has name projected-secret-test-map-f9184359-0d9d-49be-a213-dc5894006d95 @ 05/13/23 19:13:12.162
  STEP: Creating a pod to test consume secrets @ 05/13/23 19:13:12.166
  E0513 19:13:12.642272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:13.642548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:14.643169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:15.644480      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:13:16.178
  May 13 19:13:16.180: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-c1fb069c-a743-4a40-8dfc-7609d0e7f5b0 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 19:13:16.182
  May 13 19:13:16.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5692" for this suite. @ 05/13/23 19:13:16.191
• [4.051 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 05/13/23 19:13:16.198
  May 13 19:13:16.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 19:13:16.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:16.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:16.208
  STEP: Creating a pod to test downward api env vars @ 05/13/23 19:13:16.209
  E0513 19:13:16.644552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:17.644755      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:18.645923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:19.646213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:13:20.228
  May 13 19:13:20.229: INFO: Trying to get logs from node worker00 pod downward-api-2278f76f-b38f-4223-86c2-1aaa49d3cff2 container dapi-container: <nil>
  STEP: delete the pod @ 05/13/23 19:13:20.232
  May 13 19:13:20.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4143" for this suite. @ 05/13/23 19:13:20.251
• [4.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 05/13/23 19:13:20.255
  May 13 19:13:20.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 19:13:20.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:20.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:20.265
  STEP: creating a collection of services @ 05/13/23 19:13:20.267
  May 13 19:13:20.267: INFO: Creating e2e-svc-a-g4pz5
  May 13 19:13:20.273: INFO: Creating e2e-svc-b-kbx76
  May 13 19:13:20.281: INFO: Creating e2e-svc-c-zm5wl
  STEP: deleting service collection @ 05/13/23 19:13:20.289
  May 13 19:13:20.304: INFO: Collection of services has been deleted
  May 13 19:13:20.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6214" for this suite. @ 05/13/23 19:13:20.305
• [0.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 05/13/23 19:13:20.309
  May 13 19:13:20.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename svcaccounts @ 05/13/23 19:13:20.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:20.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:20.321
  STEP: Creating ServiceAccount "e2e-sa-vz8fb"  @ 05/13/23 19:13:20.323
  May 13 19:13:20.327: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-vz8fb"  @ 05/13/23 19:13:20.327
  May 13 19:13:20.331: INFO: AutomountServiceAccountToken: true
  May 13 19:13:20.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4301" for this suite. @ 05/13/23 19:13:20.334
• [0.028 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 05/13/23 19:13:20.341
  May 13 19:13:20.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 19:13:20.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:20.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:20.352
  STEP: Setting up server cert @ 05/13/23 19:13:20.365
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 19:13:20.551
  STEP: Deploying the webhook pod @ 05/13/23 19:13:20.555
  STEP: Wait for the deployment to be ready @ 05/13/23 19:13:20.562
  May 13 19:13:20.567: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 19:13:20.647019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:21.648239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 19:13:22.571
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 19:13:22.576
  E0513 19:13:22.649168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:13:23.581: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/13/23 19:13:23.583
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/13/23 19:13:23.604
  STEP: Creating a dummy validating-webhook-configuration object @ 05/13/23 19:13:23.613
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/13/23 19:13:23.618
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/13/23 19:13:23.621
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/13/23 19:13:23.625
  May 13 19:13:23.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0513 19:13:23.649229      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-9326" for this suite. @ 05/13/23 19:13:23.653
  STEP: Destroying namespace "webhook-markers-202" for this suite. @ 05/13/23 19:13:23.66
• [3.325 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 05/13/23 19:13:23.667
  May 13 19:13:23.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:13:23.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:23.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:23.68
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:13:23.681
  E0513 19:13:24.649674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:25.649953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:13:25.689
  May 13 19:13:25.692: INFO: Trying to get logs from node worker00 pod downwardapi-volume-fc092105-028b-40c8-b1b1-14f7e5ffb469 container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:13:25.697
  May 13 19:13:25.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8392" for this suite. @ 05/13/23 19:13:25.711
• [2.049 seconds]
------------------------------
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 05/13/23 19:13:25.716
  May 13 19:13:25.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sysctl @ 05/13/23 19:13:25.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:25.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:25.726
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/13/23 19:13:25.728
  May 13 19:13:25.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-864" for this suite. @ 05/13/23 19:13:25.732
• [0.019 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/13/23 19:13:25.736
  May 13 19:13:25.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 19:13:25.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:25.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:25.746
  STEP: Create a pod @ 05/13/23 19:13:25.748
  E0513 19:13:26.650014      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:27.650450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/13/23 19:13:27.763
  May 13 19:13:27.779: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  May 13 19:13:27.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2393" for this suite. @ 05/13/23 19:13:27.782
• [2.051 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 05/13/23 19:13:27.789
  May 13 19:13:27.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename podtemplate @ 05/13/23 19:13:27.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:27.811
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:27.813
  STEP: Create a pod template @ 05/13/23 19:13:27.815
  STEP: Replace a pod template @ 05/13/23 19:13:27.818
  May 13 19:13:27.822: INFO: Found updated podtemplate annotation: "true"

  May 13 19:13:27.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7284" for this suite. @ 05/13/23 19:13:27.823
• [0.037 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 05/13/23 19:13:27.826
  May 13 19:13:27.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 19:13:27.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:27.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:27.836
  STEP: Starting the proxy @ 05/13/23 19:13:27.838
  May 13 19:13:27.838: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-7447 proxy --unix-socket=/tmp/kubectl-proxy-unix4046791516/test'
  STEP: retrieving proxy /api/ output @ 05/13/23 19:13:27.869
  May 13 19:13:27.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7447" for this suite. @ 05/13/23 19:13:27.873
• [0.050 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 05/13/23 19:13:27.876
  May 13 19:13:27.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 19:13:27.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:27.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:27.889
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:13:27.891
  E0513 19:13:28.651077      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:29.651171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:30.651684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:31.652963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:13:31.903
  May 13 19:13:31.905: INFO: Trying to get logs from node worker00 pod downwardapi-volume-36ed529f-d93f-4ff8-8f09-861891bf0cdb container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:13:31.909
  May 13 19:13:31.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3063" for this suite. @ 05/13/23 19:13:31.93
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 05/13/23 19:13:31.937
  May 13 19:13:31.937: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 19:13:31.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:31.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:31.951
  STEP: Creating configMap that has name configmap-test-emptyKey-dfc4d7d6-96c9-4534-af82-212af0502d78 @ 05/13/23 19:13:31.953
  May 13 19:13:31.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5832" for this suite. @ 05/13/23 19:13:31.957
• [0.025 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 05/13/23 19:13:31.962
  May 13 19:13:31.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename field-validation @ 05/13/23 19:13:31.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:31.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:31.974
  STEP: apply creating a deployment @ 05/13/23 19:13:31.976
  May 13 19:13:31.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6730" for this suite. @ 05/13/23 19:13:31.982
• [0.022 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 05/13/23 19:13:31.985
  May 13 19:13:31.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-probe @ 05/13/23 19:13:31.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:13:32.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:13:32.075
  STEP: Creating pod liveness-aa43e607-743a-4d42-92e8-c5d0d1c8954d in namespace container-probe-5429 @ 05/13/23 19:13:32.077
  E0513 19:13:32.653455      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:33.654285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:13:34.088: INFO: Started pod liveness-aa43e607-743a-4d42-92e8-c5d0d1c8954d in namespace container-probe-5429
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/13/23 19:13:34.088
  May 13 19:13:34.091: INFO: Initial restart count of pod liveness-aa43e607-743a-4d42-92e8-c5d0d1c8954d is 0
  E0513 19:13:34.654022      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:35.654251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:36.654814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:37.654938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:38.655941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:39.656006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:40.657823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:41.657652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:42.658109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:43.658175      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:44.658549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:45.658892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:46.659081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:47.659569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:48.659747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:49.660591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:50.660973      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:51.661978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:52.662387      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:53.662486      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:54.662815      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:55.663647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:56.664208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:57.665254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:58.666097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:13:59.666223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:00.666466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:01.671873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:02.672239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:03.672515      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:04.673643      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:05.673586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:06.674521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:07.675517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:08.677150      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:09.677675      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:10.678242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:11.687180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:12.686996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:13.688102      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:14.689121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:15.689918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:16.690094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:17.690188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:18.699478      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:19.699692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:20.701381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:21.701793      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:22.702504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:23.706903      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:24.706922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:25.707528      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:26.707604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:27.707827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:28.708364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:29.708555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:30.708744      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:31.709626      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:32.710948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:33.711100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:34.713215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:35.713170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:36.714212      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:37.714437      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:38.715283      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:39.718114      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:40.718532      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:41.720892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:42.721003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:43.723627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:44.725899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:45.726055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:46.726738      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:47.726938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:48.727562      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:49.727676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:50.727921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:51.728975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:52.729164      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:53.729373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:54.730201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:55.730412      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:56.730454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:57.730665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:58.730740      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:14:59.731917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:00.732961      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:01.734340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:02.737260      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:03.737575      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:04.740783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:05.740873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:06.741872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:07.742813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:08.743387      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:09.744579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:10.745055      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:11.746075      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:12.747064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:13.747896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:14.748281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:15.748566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:16.748741      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:17.749590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:18.750517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:19.751653      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:20.751937      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:21.753215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:22.754719      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:23.755796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:24.755864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:25.757882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:26.758900      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:27.759720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:28.759935      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:29.762255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:30.763458      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:31.764441      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:32.764817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:33.765809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:34.766906      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:35.767076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:36.768017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:37.769196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:38.770271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:39.771117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:40.771372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:41.771753      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:42.772450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:43.772882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:44.774007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:45.774301      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:46.775807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:47.776208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:48.776431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:49.776893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:50.778250      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:51.778396      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:52.779249      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:53.779746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:54.779802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:55.780736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:56.782628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:57.785002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:58.785149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:15:59.785447      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:00.786019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:01.786566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:02.787145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:03.790341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:04.790563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:05.791404      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:06.793084      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:07.793642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:08.794680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:09.794714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:10.795492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:11.796396      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:12.797569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:13.798025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:14.800192      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:15.800313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:16.801018      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:17.802684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:18.803254      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:19.803569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:20.803829      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:21.804002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:22.804195      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:23.806313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:24.807672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:25.808647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:26.809051      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:27.809109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:28.809291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:29.809708      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:30.810482      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:31.810649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:32.810874      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:33.813624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:34.814259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:35.814514      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:36.814686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:37.815647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:38.817110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:39.817190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:40.821179      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:41.821818      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:42.822756      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:43.823315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:44.823898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:45.824366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:46.824946      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:47.825220      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:48.825396      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:49.826104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:50.827263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:51.828282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:52.828557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:53.828893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:54.829670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:55.829936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:56.832421      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:57.832657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:58.834097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:16:59.834381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:00.834604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:01.834897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:02.835816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:03.837038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:04.839217      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:05.837970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:06.838165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:07.838323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:08.838987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:09.840082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:10.840199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:11.841525      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:12.841595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:13.844567      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:14.848612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:15.849143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:16.849503      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:17.849811      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:18.850840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:19.852378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:20.853306      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:21.853644      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:22.855341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:23.855551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:24.856533      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:25.856454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:26.859114      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:27.859271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:28.862715      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:29.865127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:30.865488      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:31.866374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:32.868042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:33.868753      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:17:34.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 19:17:34.52
  STEP: Destroying namespace "container-probe-5429" for this suite. @ 05/13/23 19:17:34.549
• [242.573 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 05/13/23 19:17:34.559
  May 13 19:17:34.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sched-preemption @ 05/13/23 19:17:34.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:17:34.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:17:34.578
  May 13 19:17:34.592: INFO: Waiting up to 1m0s for all nodes to be ready
  E0513 19:17:34.870385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:35.871081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:36.871882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:37.872207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:38.872720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:39.872943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:40.873808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:41.874103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:42.874557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:43.877995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:44.878318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:45.878269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:46.879536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:47.880660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:48.880842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:49.882287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:50.882972      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:51.883496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:52.883971      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:53.884573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:54.885752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:55.886516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:56.888528      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:57.888864      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:58.889962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:17:59.890265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:00.890925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:01.891275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:02.892096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:03.892370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:04.892617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:05.893805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:06.894763      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:07.894908      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:08.895508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:09.896700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:10.897257      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:11.897555      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:12.899129      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:13.899060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:14.901198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:15.901455      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:16.901783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:17.902378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:18.902649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:19.906619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:20.907610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:21.908205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:22.908378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:23.908500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:24.910443      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:25.911701      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:26.912742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:27.913273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:28.913452      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:29.913670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:30.914525      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:31.915603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:32.916791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:33.917198      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:18:34.629: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/13/23 19:18:34.632
  May 13 19:18:34.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/13/23 19:18:34.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:18:34.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:18:34.656
  May 13 19:18:34.679: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  May 13 19:18:34.685: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  May 13 19:18:34.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 13 19:18:34.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-4232" for this suite. @ 05/13/23 19:18:34.769
  STEP: Destroying namespace "sched-preemption-6827" for this suite. @ 05/13/23 19:18:34.775
• [60.225 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 05/13/23 19:18:34.787
  May 13 19:18:34.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename field-validation @ 05/13/23 19:18:34.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:18:34.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:18:34.811
  May 13 19:18:34.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:18:34.918662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:35.919958      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:36.920584      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0513 19:18:37.360169      21 warnings.go:70] unknown field "alpha"
  W0513 19:18:37.360183      21 warnings.go:70] unknown field "beta"
  W0513 19:18:37.360186      21 warnings.go:70] unknown field "delta"
  W0513 19:18:37.360188      21 warnings.go:70] unknown field "epsilon"
  W0513 19:18:37.360194      21 warnings.go:70] unknown field "gamma"
  May 13 19:18:37.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7932" for this suite. @ 05/13/23 19:18:37.373
• [2.589 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/13/23 19:18:37.377
  May 13 19:18:37.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename cronjob @ 05/13/23 19:18:37.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:18:37.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:18:37.385
  STEP: Creating a suspended cronjob @ 05/13/23 19:18:37.386
  STEP: Ensuring no jobs are scheduled @ 05/13/23 19:18:37.389
  E0513 19:18:37.921801      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:38.921981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:39.922619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:40.923953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:41.924979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:42.925117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:43.929559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:44.931642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:45.932817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:46.933389      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:47.933700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:48.934674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:49.934608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:50.936025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:51.935583      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:52.935668      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:53.938713      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:54.938998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:55.939949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:56.940579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:57.941600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:58.941485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:18:59.941697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:00.942382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:01.942579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:02.942788      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:03.943568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:04.943819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:05.945147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:06.945760      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:07.946685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:08.947224      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:09.947963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:10.948143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:11.949434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:12.950256      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:13.950300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:14.950525      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:15.950748      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:16.950858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:17.950957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:18.952119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:19.952494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:20.954073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:21.954133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:22.954805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:23.957783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:24.958128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:25.959185      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:26.960645      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:27.960812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:28.961352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:29.962246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:30.962454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:31.962606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:32.963447      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:33.963604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:34.963934      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:35.964133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:36.964754      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:37.965833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:38.966641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:39.966912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:40.968248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:41.969094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:42.970752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:43.971345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:44.973717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:45.973948      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:46.974197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:47.975567      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:48.975987      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:49.978551      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:50.981185      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:51.981348      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:52.983524      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:53.983774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:54.984068      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:55.984414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:56.985034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:57.985447      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:58.986129      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:19:59.986226      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:00.988319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:01.988661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:02.989111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:03.993179      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:04.993231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:05.993726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:06.994431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:07.995117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:08.996031      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:09.996145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:10.996662      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:11.996993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:12.997763      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:13.999530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:15.000395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:16.000408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:17.000930      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:18.001504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:19.002575      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:20.005188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:21.005837      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:22.010808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:23.011496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:24.011429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:25.011632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:26.012744      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:27.013292      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:28.013745      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:29.013800      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:30.013932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:31.015221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:32.016041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:33.016169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:34.018641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:35.018684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:36.019152      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:37.019839      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:38.021047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:39.021248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:40.023059      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:41.028438      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:42.029470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:43.029946      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:44.030495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:45.032943      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:46.033535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:47.033745      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:48.035569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:49.035796      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:50.036674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:51.037082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:52.038377      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:53.038740      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:54.044411      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:55.044683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:56.044912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:57.045178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:58.046313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:20:59.047212      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:00.047667      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:01.052303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:02.052493      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:03.052554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:04.053072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:05.054826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:06.054985      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:07.055629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:08.056836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:09.059158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:10.059875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:11.060029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:12.060764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:13.060803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:14.061239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:15.061308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:16.064143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:17.076733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:18.077350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:19.080062      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:20.081040      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:21.081568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:22.082271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:23.082310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:24.082501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:25.082977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:26.084168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:27.084375      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:28.084997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:29.087605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:30.087969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:31.088550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:32.088701      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:33.090151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:34.090663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:35.091447      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:36.091717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:37.093007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:38.093469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:39.093878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:40.094294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:41.094656      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:42.095138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:43.095691      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:44.095949      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:45.096991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:46.097132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:47.097619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:48.098578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:49.098817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:50.098615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:51.098926      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:52.100024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:53.100354      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:54.101275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:55.103830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:56.104411      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:57.105372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:58.108683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:21:59.109274      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:00.109670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:01.109878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:02.111609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:03.112620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:04.113858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:05.114205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:06.115263      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:07.116424      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:08.116872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:09.117289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:10.119183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:11.119838      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:12.120499      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:13.120890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:14.121161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:15.122325      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:16.123739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:17.124179      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:18.124271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:19.124436      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:20.124610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:21.127027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:22.127208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:23.127927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:24.128300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:25.128964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:26.129069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:27.129567      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:28.129739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:29.132001      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:30.131253      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:31.131423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:32.133064      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:33.135061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:34.140640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:35.140727      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:36.141535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:37.142826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:38.143469      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:39.144470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:40.144884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:41.145196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:42.145879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:43.146918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:44.146840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:45.147041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:46.147855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:47.148313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:48.149190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:49.149455      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:50.150818      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:51.151886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:52.152418      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:53.152659      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:54.153144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:55.153414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:56.153506      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:57.155187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:58.155069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:22:59.155889      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:00.155259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:01.155570      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:02.155564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:03.156134      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:04.157852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:05.158204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:06.161751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:07.162611      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:08.164218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:09.164988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:10.165903      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:11.168047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:12.168212      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:13.171911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:14.172328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:15.173286      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:16.174498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:17.174347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:18.176434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:19.176241      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:20.177153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:21.177847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:22.180904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:23.182593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:24.183646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:25.184923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:26.186317      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:27.186458      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:28.187479      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:29.188443      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:30.188660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:31.189981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:32.190078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:33.190347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:34.190851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:35.190921      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:36.193277      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:37.193564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/13/23 19:23:37.4
  STEP: Removing cronjob @ 05/13/23 19:23:37.404
  May 13 19:23:37.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3055" for this suite. @ 05/13/23 19:23:37.417
• [300.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 05/13/23 19:23:37.431
  May 13 19:23:37.431: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename namespaces @ 05/13/23 19:23:37.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:23:37.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:23:37.454
  STEP: Updating Namespace "namespaces-2566" @ 05/13/23 19:23:37.457
  May 13 19:23:37.468: INFO: Namespace "namespaces-2566" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"f9beaaef-e856-41aa-8a17-4bd32496d1c5", "kubernetes.io/metadata.name":"namespaces-2566", "namespaces-2566":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  May 13 19:23:37.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2566" for this suite. @ 05/13/23 19:23:37.474
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 05/13/23 19:23:37.483
  May 13 19:23:37.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 19:23:37.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:23:37.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:23:37.525
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:23:37.529
  E0513 19:23:38.194709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:39.194826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:40.199459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:41.200063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:23:41.559
  May 13 19:23:41.563: INFO: Trying to get logs from node worker00 pod downwardapi-volume-a5a8abb0-f9e9-4404-abb7-499f6e76f9ee container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:23:41.579
  May 13 19:23:41.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-279" for this suite. @ 05/13/23 19:23:41.605
• [4.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 05/13/23 19:23:41.624
  May 13 19:23:41.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 19:23:41.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:23:41.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:23:41.65
  STEP: Creating resourceQuota "e2e-rq-status-fjwn9" @ 05/13/23 19:23:41.658
  May 13 19:23:41.680: INFO: Resource quota "e2e-rq-status-fjwn9" reports spec: hard cpu limit of 500m
  May 13 19:23:41.681: INFO: Resource quota "e2e-rq-status-fjwn9" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-fjwn9" /status @ 05/13/23 19:23:41.681
  STEP: Confirm /status for "e2e-rq-status-fjwn9" resourceQuota via watch @ 05/13/23 19:23:41.699
  May 13 19:23:41.704: INFO: observed resourceQuota "e2e-rq-status-fjwn9" in namespace "resourcequota-9079" with hard status: v1.ResourceList(nil)
  May 13 19:23:41.706: INFO: Found resourceQuota "e2e-rq-status-fjwn9" in namespace "resourcequota-9079" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 13 19:23:41.706: INFO: ResourceQuota "e2e-rq-status-fjwn9" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/13/23 19:23:41.711
  May 13 19:23:41.722: INFO: Resource quota "e2e-rq-status-fjwn9" reports spec: hard cpu limit of 1
  May 13 19:23:41.722: INFO: Resource quota "e2e-rq-status-fjwn9" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-fjwn9" /status @ 05/13/23 19:23:41.722
  STEP: Confirm /status for "e2e-rq-status-fjwn9" resourceQuota via watch @ 05/13/23 19:23:41.764
  May 13 19:23:41.768: INFO: observed resourceQuota "e2e-rq-status-fjwn9" in namespace "resourcequota-9079" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 13 19:23:41.768: INFO: Found resourceQuota "e2e-rq-status-fjwn9" in namespace "resourcequota-9079" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  May 13 19:23:41.768: INFO: ResourceQuota "e2e-rq-status-fjwn9" /status was patched
  STEP: Get "e2e-rq-status-fjwn9" /status @ 05/13/23 19:23:41.769
  May 13 19:23:41.783: INFO: Resourcequota "e2e-rq-status-fjwn9" reports status: hard cpu of 1
  May 13 19:23:41.784: INFO: Resourcequota "e2e-rq-status-fjwn9" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-fjwn9" /status before checking Spec is unchanged @ 05/13/23 19:23:41.789
  May 13 19:23:41.798: INFO: Resourcequota "e2e-rq-status-fjwn9" reports status: hard cpu of 2
  May 13 19:23:41.798: INFO: Resourcequota "e2e-rq-status-fjwn9" reports status: hard memory of 2Gi
  May 13 19:23:41.802: INFO: Found resourceQuota "e2e-rq-status-fjwn9" in namespace "resourcequota-9079" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0513 19:23:42.201196      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:43.202769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:44.203298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:45.203663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:46.203829      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:47.204148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:48.204514      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:49.205429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:50.209798      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:51.210276      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:52.210852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:53.210925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:54.210980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:55.211144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:56.211430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:57.212602      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:58.213717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:23:59.217924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:00.218482      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:01.219144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:02.219250      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:03.219847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:04.220043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:05.220809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:06.220858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:07.221592      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:08.222777      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:09.222827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:10.222875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:11.223188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:12.223309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:13.223848      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:14.224926      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:15.224997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:16.225995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:17.226121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:18.227439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:19.228121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:20.229471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:21.230769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:22.231144      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:23.233812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:24.235328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:25.236199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:26.238472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:27.239690      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:28.239797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:29.240884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:30.241653      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:31.242295      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:32.246236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:33.246305      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:34.246631      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:35.246806      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:36.247414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:37.247758      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:38.248025      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:39.249245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:40.249353      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:41.250328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:42.251072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:43.251797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:44.252637      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:45.253019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:46.255477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:47.256490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:48.257793      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:49.259425      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:50.259774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:51.264489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:52.264940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:53.264933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:54.265957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:55.266078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:56.266560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:57.266946      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:58.270897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:24:59.270894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:00.271203      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:01.271476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:02.272490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:03.272769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:04.273851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:05.273999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:06.274359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:07.274531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:08.275542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:09.276288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:10.276546      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:11.277009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:12.284016      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:13.283995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:14.284671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:15.286513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:16.287478      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:17.287492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:18.288003      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:19.288183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:20.289187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:21.289593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:22.289721      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:23.290288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:24.290782      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:25.293428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:26.294313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:27.295092      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:28.295910      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:29.296160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:30.297712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:31.298015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:32.298247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:33.299174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:34.302829      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:35.304468      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:36.304593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:37.305100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:38.305386      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:39.305865      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:40.306136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:41.306234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:42.307048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:43.307258      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:44.307897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:45.307984      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:46.308338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:47.308428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:48.309459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:49.309947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:50.310271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:51.310773      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:52.311148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:53.312384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:54.312832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:55.313316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:56.313883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:57.314869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:58.314946      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:25:59.315570      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:00.316307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:01.317160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:26:01.808: INFO: ResourceQuota "e2e-rq-status-fjwn9" Spec was unchanged and /status reset
  May 13 19:26:01.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9079" for this suite. @ 05/13/23 19:26:01.811
• [140.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 05/13/23 19:26:01.826
  May 13 19:26:01.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename statefulset @ 05/13/23 19:26:01.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:26:01.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:26:01.836
  STEP: Creating service test in namespace statefulset-1240 @ 05/13/23 19:26:01.838
  STEP: Looking for a node to schedule stateful set and pod @ 05/13/23 19:26:01.84
  STEP: Creating pod with conflicting port in namespace statefulset-1240 @ 05/13/23 19:26:01.843
  STEP: Waiting until pod test-pod will start running in namespace statefulset-1240 @ 05/13/23 19:26:01.853
  E0513 19:26:02.317194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:03.317623      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-1240 @ 05/13/23 19:26:03.861
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1240 @ 05/13/23 19:26:03.875
  May 13 19:26:03.890: INFO: Observed stateful pod in namespace: statefulset-1240, name: ss-0, uid: 16b3479f-27a2-418a-85b4-e38f3394518c, status phase: Pending. Waiting for statefulset controller to delete.
  May 13 19:26:03.897: INFO: Observed stateful pod in namespace: statefulset-1240, name: ss-0, uid: 16b3479f-27a2-418a-85b4-e38f3394518c, status phase: Failed. Waiting for statefulset controller to delete.
  May 13 19:26:03.901: INFO: Observed stateful pod in namespace: statefulset-1240, name: ss-0, uid: 16b3479f-27a2-418a-85b4-e38f3394518c, status phase: Failed. Waiting for statefulset controller to delete.
  May 13 19:26:03.903: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1240
  STEP: Removing pod with conflicting port in namespace statefulset-1240 @ 05/13/23 19:26:03.903
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1240 and will be in running state @ 05/13/23 19:26:03.911
  E0513 19:26:04.317924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:05.318826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:26:05.918: INFO: Deleting all statefulset in ns statefulset-1240
  May 13 19:26:05.920: INFO: Scaling statefulset ss to 0
  E0513 19:26:06.319374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:07.319474      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:08.320624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:09.321281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:10.322269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:11.324423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:12.325099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:13.326173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:14.327096      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:15.334616      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:26:15.931: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 19:26:15.935: INFO: Deleting statefulset ss
  May 13 19:26:15.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1240" for this suite. @ 05/13/23 19:26:15.953
• [14.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 05/13/23 19:26:15.964
  May 13 19:26:15.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename gc @ 05/13/23 19:26:15.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:26:15.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:26:15.995
  STEP: create the rc1 @ 05/13/23 19:26:16.006
  STEP: create the rc2 @ 05/13/23 19:26:16.014
  E0513 19:26:16.335954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:17.337264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:18.342118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:19.342809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:20.345559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:21.346725      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/13/23 19:26:22.032
  E0513 19:26:22.388095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/13/23 19:26:23.065
  STEP: wait for the rc to be deleted @ 05/13/23 19:26:23.08
  E0513 19:26:23.397834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:24.398334      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:25.398647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:26.399242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:27.400550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:26:28.138: INFO: 70 pods remaining
  May 13 19:26:28.138: INFO: 70 pods has nil DeletionTimestamp
  May 13 19:26:28.138: INFO: 
  E0513 19:26:28.401762      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:29.403682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:30.408410      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:31.408881      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:32.411994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/13/23 19:26:33.096
  E0513 19:26:33.414171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:34.416724      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:35.417855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:36.418430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:37.418765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:38.421620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:39.421524      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:40.422510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:41.422968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:42.423163      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:43.423318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:44.423912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:45.424054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:46.425103      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:47.427125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:48.427100      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:49.428373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:50.429502      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:51.430012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:52.430690      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:53.432373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:54.433337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:55.433922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:56.434812      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:57.434955      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:58.435632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:26:59.441345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:00.444423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:01.445200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:02.452242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:03.453082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:04.455395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:05.455656      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:06.456019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:07.456990      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:08.461856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:09.462734      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:10.463097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:11.463367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:12.464259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:13.465206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:14.466997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:15.467458      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:16.467851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:17.468923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:18.469918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:19.470126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:20.470120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:21.471191      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:22.473914      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:23.473982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:24.474245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:25.474566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:26.474999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:27.478587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:28.479386      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:29.479705      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:30.479766      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:31.480573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:32.480621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:27:33.209: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
  May 13 19:27:33.209: INFO: Deleting pod "simpletest-rc-to-be-deleted-28sgp" in namespace "gc-2076"
  May 13 19:27:33.216: INFO: Deleting pod "simpletest-rc-to-be-deleted-2tfgl" in namespace "gc-2076"
  May 13 19:27:33.227: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xldc" in namespace "gc-2076"
  May 13 19:27:33.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-47zjc" in namespace "gc-2076"
  May 13 19:27:33.254: INFO: Deleting pod "simpletest-rc-to-be-deleted-4g4wf" in namespace "gc-2076"
  May 13 19:27:33.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-4j5qh" in namespace "gc-2076"
  May 13 19:27:33.293: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lns5" in namespace "gc-2076"
  May 13 19:27:33.333: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mngq" in namespace "gc-2076"
  May 13 19:27:33.349: INFO: Deleting pod "simpletest-rc-to-be-deleted-5256g" in namespace "gc-2076"
  May 13 19:27:33.372: INFO: Deleting pod "simpletest-rc-to-be-deleted-55jj6" in namespace "gc-2076"
  May 13 19:27:33.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jn7j" in namespace "gc-2076"
  May 13 19:27:33.432: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wfqg" in namespace "gc-2076"
  May 13 19:27:33.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wn4j" in namespace "gc-2076"
  E0513 19:27:33.485188      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:27:33.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xrrw" in namespace "gc-2076"
  May 13 19:27:33.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zw68" in namespace "gc-2076"
  May 13 19:27:33.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-69nd5" in namespace "gc-2076"
  May 13 19:27:33.679: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mcsz" in namespace "gc-2076"
  May 13 19:27:33.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-6v2gp" in namespace "gc-2076"
  May 13 19:27:33.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zklb" in namespace "gc-2076"
  May 13 19:27:33.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bnbv" in namespace "gc-2076"
  May 13 19:27:33.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-7mvcd" in namespace "gc-2076"
  May 13 19:27:33.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qblf" in namespace "gc-2076"
  May 13 19:27:33.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nzc8" in namespace "gc-2076"
  May 13 19:27:33.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-8v6c2" in namespace "gc-2076"
  May 13 19:27:33.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-9c25d" in namespace "gc-2076"
  May 13 19:27:33.963: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sqkv" in namespace "gc-2076"
  May 13 19:27:33.992: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhvvw" in namespace "gc-2076"
  May 13 19:27:34.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-bj25k" in namespace "gc-2076"
  May 13 19:27:34.066: INFO: Deleting pod "simpletest-rc-to-be-deleted-blq78" in namespace "gc-2076"
  May 13 19:27:34.097: INFO: Deleting pod "simpletest-rc-to-be-deleted-blqhf" in namespace "gc-2076"
  May 13 19:27:34.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-bt56t" in namespace "gc-2076"
  May 13 19:27:34.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-cj2rc" in namespace "gc-2076"
  May 13 19:27:34.185: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnlf7" in namespace "gc-2076"
  May 13 19:27:34.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctcfw" in namespace "gc-2076"
  May 13 19:27:34.225: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhhhj" in namespace "gc-2076"
  May 13 19:27:34.274: INFO: Deleting pod "simpletest-rc-to-be-deleted-dps52" in namespace "gc-2076"
  May 13 19:27:34.294: INFO: Deleting pod "simpletest-rc-to-be-deleted-f24q5" in namespace "gc-2076"
  May 13 19:27:34.336: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft82t" in namespace "gc-2076"
  May 13 19:27:34.375: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftq7b" in namespace "gc-2076"
  May 13 19:27:34.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2bhk" in namespace "gc-2076"
  May 13 19:27:34.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtb8g" in namespace "gc-2076"
  May 13 19:27:34.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtvb9" in namespace "gc-2076"
  E0513 19:27:34.488122      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:27:34.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwbmw" in namespace "gc-2076"
  May 13 19:27:34.518: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2z6q" in namespace "gc-2076"
  May 13 19:27:34.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-hm627" in namespace "gc-2076"
  May 13 19:27:34.574: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmhps" in namespace "gc-2076"
  May 13 19:27:34.590: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnpf9" in namespace "gc-2076"
  May 13 19:27:34.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqjgw" in namespace "gc-2076"
  May 13 19:27:34.628: INFO: Deleting pod "simpletest-rc-to-be-deleted-htqgw" in namespace "gc-2076"
  May 13 19:27:34.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-jb26v" in namespace "gc-2076"
  May 13 19:27:34.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2076" for this suite. @ 05/13/23 19:27:34.714
• [78.757 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 05/13/23 19:27:34.721
  May 13 19:27:34.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 19:27:34.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:27:34.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:27:34.745
  STEP: creating service multi-endpoint-test in namespace services-6539 @ 05/13/23 19:27:34.753
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6539 to expose endpoints map[] @ 05/13/23 19:27:34.765
  May 13 19:27:34.782: INFO: successfully validated that service multi-endpoint-test in namespace services-6539 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-6539 @ 05/13/23 19:27:34.783
  E0513 19:27:35.488779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:36.490745      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:37.493404      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:38.493731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6539 to expose endpoints map[pod1:[100]] @ 05/13/23 19:27:38.813
  May 13 19:27:38.816: INFO: successfully validated that service multi-endpoint-test in namespace services-6539 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-6539 @ 05/13/23 19:27:38.817
  E0513 19:27:39.494700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:40.497282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6539 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/13/23 19:27:40.827
  May 13 19:27:40.832: INFO: successfully validated that service multi-endpoint-test in namespace services-6539 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/13/23 19:27:40.832
  May 13 19:27:40.832: INFO: Creating new exec pod
  E0513 19:27:41.497940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:42.498005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:43.498767      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:27:43.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-6539 exec execpodgtcqq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  May 13 19:27:43.947: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  May 13 19:27:43.947: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 19:27:43.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-6539 exec execpodgtcqq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.192 80'
  May 13 19:27:44.031: INFO: stderr: "+ echo+  hostNamenc\n -v -t -w 2 10.32.0.192 80\nConnection to 10.32.0.192 80 port [tcp/http] succeeded!\n"
  May 13 19:27:44.031: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 19:27:44.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-6539 exec execpodgtcqq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  May 13 19:27:44.128: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  May 13 19:27:44.128: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 13 19:27:44.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=services-6539 exec execpodgtcqq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.32.0.192 81'
  May 13 19:27:44.226: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.32.0.192 81\nConnection to 10.32.0.192 81 port [tcp/*] succeeded!\n"
  May 13 19:27:44.226: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-6539 @ 05/13/23 19:27:44.226
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6539 to expose endpoints map[pod2:[101]] @ 05/13/23 19:27:44.244
  May 13 19:27:44.265: INFO: successfully validated that service multi-endpoint-test in namespace services-6539 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-6539 @ 05/13/23 19:27:44.265
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6539 to expose endpoints map[] @ 05/13/23 19:27:44.278
  E0513 19:27:44.498815      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:27:45.293: INFO: successfully validated that service multi-endpoint-test in namespace services-6539 exposes endpoints map[]
  May 13 19:27:45.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6539" for this suite. @ 05/13/23 19:27:45.31
• [10.593 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 05/13/23 19:27:45.315
  May 13 19:27:45.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 19:27:45.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:27:45.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:27:45.326
  STEP: Setting up server cert @ 05/13/23 19:27:45.343
  E0513 19:27:45.499903      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 19:27:45.915
  STEP: Deploying the webhook pod @ 05/13/23 19:27:45.919
  STEP: Wait for the deployment to be ready @ 05/13/23 19:27:45.927
  May 13 19:27:45.934: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 19:27:46.500168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:47.500429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 19:27:47.939
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 19:27:47.947
  E0513 19:27:48.501837      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:27:48.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/13/23 19:27:48.949
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/13/23 19:27:48.959
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/13/23 19:27:48.963
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/13/23 19:27:48.969
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/13/23 19:27:48.973
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/13/23 19:27:48.979
  May 13 19:27:48.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7196" for this suite. @ 05/13/23 19:27:49.006
  STEP: Destroying namespace "webhook-markers-385" for this suite. @ 05/13/23 19:27:49.011
• [3.704 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 05/13/23 19:27:49.021
  May 13 19:27:49.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 19:27:49.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:27:49.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:27:49.048
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/13/23 19:27:49.05
  May 13 19:27:49.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6751 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 13 19:27:49.107: INFO: stderr: ""
  May 13 19:27:49.107: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/13/23 19:27:49.107
  E0513 19:27:49.503027      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:50.503074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:51.503696      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:52.504559      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:53.507012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/13/23 19:27:54.158
  May 13 19:27:54.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6751 get pod e2e-test-httpd-pod -o json'
  May 13 19:27:54.201: INFO: stderr: ""
  May 13 19:27:54.201: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"7aa2f3ac8c2a1674cd6de0cd71e28293d09c7f79cdc6675b89f3a1310bbac44f\",\n            \"cni.projectcalico.org/podIP\": \"10.200.131.180/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.200.131.180/32\"\n        },\n        \"creationTimestamp\": \"2023-05-13T19:27:49Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6751\",\n        \"resourceVersion\": \"40467\",\n        \"uid\": \"6b258ac0-4ffb-4124-bf82-5c1e6ec7b172\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-nl8xm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker00\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-nl8xm\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-13T19:27:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-13T19:27:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-13T19:27:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-13T19:27:49Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ecb505b0de50fa10957ee860b61a1c49da96d3b02b008ef60ad2b6b3af064e59\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-13T19:27:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.58.100\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.200.131.180\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.200.131.180\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-13T19:27:49Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/13/23 19:27:54.201
  May 13 19:27:54.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6751 replace -f -'
  E0513 19:27:54.507157      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:27:54.723: INFO: stderr: ""
  May 13 19:27:54.723: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 05/13/23 19:27:54.723
  May 13 19:27:54.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-6751 delete pods e2e-test-httpd-pod'
  E0513 19:27:55.507525      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:27:56.400: INFO: stderr: ""
  May 13 19:27:56.400: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 13 19:27:56.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6751" for this suite. @ 05/13/23 19:27:56.403
• [7.385 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 05/13/23 19:27:56.406
  May 13 19:27:56.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 19:27:56.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:27:56.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:27:56.421
  STEP: Creating configMap with name configmap-test-volume-176f7de5-f2c2-4260-90c0-4753c38a962e @ 05/13/23 19:27:56.425
  STEP: Creating a pod to test consume configMaps @ 05/13/23 19:27:56.429
  E0513 19:27:56.507956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:57.508885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:58.509216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:27:59.510427      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:28:00.44
  May 13 19:28:00.441: INFO: Trying to get logs from node worker00 pod pod-configmaps-eb651dbc-3e21-41ec-a530-94dce16eb0f0 container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 19:28:00.444
  May 13 19:28:00.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5055" for this suite. @ 05/13/23 19:28:00.456
• [4.054 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 05/13/23 19:28:00.46
  May 13 19:28:00.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sched-preemption @ 05/13/23 19:28:00.461
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:28:00.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:28:00.472
  May 13 19:28:00.480: INFO: Waiting up to 1m0s for all nodes to be ready
  E0513 19:28:00.511591      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:01.512218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:02.513688      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:03.514593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:04.514828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:05.515238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:06.516218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:07.518336      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:08.518852      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:09.519477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:10.520565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:11.521563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:12.521687      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:13.522513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:14.523145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:15.524061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:16.524208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:17.526355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:18.534248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:19.535356      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:20.536896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:21.538673      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:22.538998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:23.540665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:24.540861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:25.541045      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:26.542315      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:27.542471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:28.545528      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:29.546703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:30.547853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:31.548978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:32.549280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:33.550243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:34.551449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:35.551498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:36.552494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:37.552856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:38.554184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:39.554453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:40.554511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:41.555411      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:42.556366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:43.556969      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:44.557015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:45.558108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:46.561660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:47.563002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:48.563179      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:49.565561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:50.566038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:51.566917      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:52.568685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:53.569964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:54.571967      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:55.572844      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:56.573108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:57.574193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:58.575184      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:28:59.576307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:29:00.516: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/13/23 19:29:00.528
  May 13 19:29:00.549: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 13 19:29:00.561: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  E0513 19:29:00.576183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:29:00.587: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 13 19:29:00.597: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/13/23 19:29:00.597
  E0513 19:29:01.576235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:02.576425      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/13/23 19:29:02.608
  E0513 19:29:03.577013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:04.578209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:05.578423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:06.579350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:29:06.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-6286" for this suite. @ 05/13/23 19:29:06.65
• [66.193 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 05/13/23 19:29:06.654
  May 13 19:29:06.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename daemonsets @ 05/13/23 19:29:06.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:29:06.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:29:06.669
  STEP: Creating simple DaemonSet "daemon-set" @ 05/13/23 19:29:06.679
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/13/23 19:29:06.682
  May 13 19:29:06.685: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:29:06.685: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:29:07.579492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:29:07.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:29:07.690: INFO: Node worker01 is running 0 daemon pod, expected 1
  E0513 19:29:08.583274      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:29:08.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 13 19:29:08.693: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/13/23 19:29:08.695
  May 13 19:29:08.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:29:08.715: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:29:09.583574      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:29:09.720: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:29:09.720: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:29:10.583534      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:29:10.719: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:29:10.719: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:29:11.585074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:29:11.722: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 13 19:29:11.722: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/13/23 19:29:11.724
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7861, will wait for the garbage collector to delete the pods @ 05/13/23 19:29:11.724
  May 13 19:29:11.785: INFO: Deleting DaemonSet.extensions daemon-set took: 2.956638ms
  May 13 19:29:11.885: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.149678ms
  E0513 19:29:12.585173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:13.586054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:14.587489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:29:14.688: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:29:14.688: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 13 19:29:14.689: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41083"},"items":null}

  May 13 19:29:14.690: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41083"},"items":null}

  May 13 19:29:14.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7861" for this suite. @ 05/13/23 19:29:14.698
• [8.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/13/23 19:29:14.703
  May 13 19:29:14.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:29:14.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:29:14.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:29:14.714
  STEP: Creating projection with secret that has name projected-secret-test-a636c006-7e1a-4b7f-a431-fb048ec4635f @ 05/13/23 19:29:14.717
  STEP: Creating a pod to test consume secrets @ 05/13/23 19:29:14.72
  E0513 19:29:15.588056      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:16.588810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:17.589536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:18.626442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:29:18.73
  May 13 19:29:18.733: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-ee2f21ad-edf4-41fb-8b60-b894c6757b1d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 19:29:18.741
  May 13 19:29:18.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4150" for this suite. @ 05/13/23 19:29:18.75
• [4.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 05/13/23 19:29:18.756
  May 13 19:29:18.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename services @ 05/13/23 19:29:18.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:29:18.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:29:18.769
  STEP: fetching services @ 05/13/23 19:29:18.772
  May 13 19:29:18.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8186" for this suite. @ 05/13/23 19:29:18.777
• [0.025 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 05/13/23 19:29:18.783
  May 13 19:29:18.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 19:29:18.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:29:18.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:29:18.8
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/13/23 19:29:18.807
  E0513 19:29:19.629723      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:20.629625      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:29:20.817
  May 13 19:29:20.818: INFO: Trying to get logs from node worker00 pod pod-8c58e16e-35cc-4cad-b4ac-fd647c943c1f container test-container: <nil>
  STEP: delete the pod @ 05/13/23 19:29:20.821
  May 13 19:29:20.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3812" for this suite. @ 05/13/23 19:29:20.835
• [2.055 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 05/13/23 19:29:20.838
  May 13 19:29:20.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename endpointslice @ 05/13/23 19:29:20.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:29:20.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:29:20.848
  E0513 19:29:21.629748      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:22.631015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:23.631356      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:24.632358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:25.632623      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 05/13/23 19:29:25.889
  E0513 19:29:26.632940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:27.633608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:28.633800      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:29.637118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:30.637567      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 05/13/23 19:29:30.894
  E0513 19:29:31.637998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:32.638483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:33.639001      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:34.639922      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:35.640345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/13/23 19:29:35.908
  E0513 19:29:36.640439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:37.641593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:38.641991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:39.641988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:40.643008      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 05/13/23 19:29:40.914
  May 13 19:29:40.935: INFO: EndpointSlice for Service endpointslice-431/example-named-port not found
  E0513 19:29:41.643629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:42.643697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:43.644629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:44.645457      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:45.647316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:46.647924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:47.648221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:48.648358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:49.649154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:50.651083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:29:50.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-431" for this suite. @ 05/13/23 19:29:50.943
• [30.119 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/13/23 19:29:50.959
  May 13 19:29:50.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename containers @ 05/13/23 19:29:50.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:29:50.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:29:50.972
  STEP: Creating a pod to test override arguments @ 05/13/23 19:29:50.975
  E0513 19:29:51.651649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:52.651991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:29:52.987
  May 13 19:29:52.989: INFO: Trying to get logs from node worker00 pod client-containers-196e90e4-85eb-4be0-8fc9-6b9d0dc182b1 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 19:29:52.993
  May 13 19:29:53.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4381" for this suite. @ 05/13/23 19:29:53.004
• [2.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 05/13/23 19:29:53.01
  May 13 19:29:53.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename svcaccounts @ 05/13/23 19:29:53.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:29:53.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:29:53.021
  STEP: Creating a pod to test service account token:  @ 05/13/23 19:29:53.022
  E0513 19:29:53.652117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:54.652169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:29:55.041
  May 13 19:29:55.043: INFO: Trying to get logs from node worker00 pod test-pod-588c80ec-6b9b-4de3-af64-b279686007f0 container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 19:29:55.046
  May 13 19:29:55.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9732" for this suite. @ 05/13/23 19:29:55.063
• [2.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/13/23 19:29:55.07
  May 13 19:29:55.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 19:29:55.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:29:55.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:29:55.085
  STEP: Creating secret with name secret-test-6dfb89ce-6082-431b-9daa-539e866144cf @ 05/13/23 19:29:55.097
  STEP: Creating a pod to test consume secrets @ 05/13/23 19:29:55.1
  E0513 19:29:55.652979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:56.653205      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:29:57.109
  May 13 19:29:57.111: INFO: Trying to get logs from node worker00 pod pod-secrets-9f94013d-c628-4528-b8f6-a5ac7ef62fb3 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 19:29:57.114
  May 13 19:29:57.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3647" for this suite. @ 05/13/23 19:29:57.134
  STEP: Destroying namespace "secret-namespace-3787" for this suite. @ 05/13/23 19:29:57.137
• [2.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 05/13/23 19:29:57.142
  May 13 19:29:57.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/13/23 19:29:57.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:29:57.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:29:57.154
  STEP: creating a target pod @ 05/13/23 19:29:57.155
  E0513 19:29:57.653207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:29:58.653543      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 05/13/23 19:29:59.168
  E0513 19:29:59.654007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:00.655680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 05/13/23 19:30:01.227
  May 13 19:30:01.227: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4280 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:30:01.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:30:01.228: INFO: ExecWithOptions: Clientset creation
  May 13 19:30:01.228: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/ephemeral-containers-test-4280/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  May 13 19:30:01.271: INFO: Exec stderr: ""
  May 13 19:30:01.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-4280" for this suite. @ 05/13/23 19:30:01.277
• [4.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 05/13/23 19:30:01.285
  May 13 19:30:01.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename var-expansion @ 05/13/23 19:30:01.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:01.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:01.296
  STEP: Creating a pod to test substitution in volume subpath @ 05/13/23 19:30:01.299
  E0513 19:30:01.656902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:02.658842      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:03.659898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:04.660629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:30:05.318
  May 13 19:30:05.319: INFO: Trying to get logs from node worker00 pod var-expansion-62a9906b-1c17-4172-a401-70487b795da2 container dapi-container: <nil>
  STEP: delete the pod @ 05/13/23 19:30:05.322
  May 13 19:30:05.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7289" for this suite. @ 05/13/23 19:30:05.342
• [4.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 05/13/23 19:30:05.345
  May 13 19:30:05.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename events @ 05/13/23 19:30:05.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:05.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:05.355
  STEP: Create set of events @ 05/13/23 19:30:05.358
  May 13 19:30:05.362: INFO: created test-event-1
  May 13 19:30:05.365: INFO: created test-event-2
  May 13 19:30:05.367: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/13/23 19:30:05.367
  STEP: delete collection of events @ 05/13/23 19:30:05.37
  May 13 19:30:05.370: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/13/23 19:30:05.377
  May 13 19:30:05.377: INFO: requesting list of events to confirm quantity
  May 13 19:30:05.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8897" for this suite. @ 05/13/23 19:30:05.38
• [0.039 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 05/13/23 19:30:05.387
  May 13 19:30:05.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename daemonsets @ 05/13/23 19:30:05.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:05.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:05.397
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/13/23 19:30:05.407
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/13/23 19:30:05.409
  May 13 19:30:05.413: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:30:05.413: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:30:05.661622      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:06.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:30:06.416: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:30:06.662216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:07.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 13 19:30:07.417: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/13/23 19:30:07.42
  May 13 19:30:07.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:30:07.437: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:30:07.662700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:08.443: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:30:08.443: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:30:08.662717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:09.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 13 19:30:09.442: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/13/23 19:30:09.442
  STEP: Deleting DaemonSet "daemon-set" @ 05/13/23 19:30:09.445
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3581, will wait for the garbage collector to delete the pods @ 05/13/23 19:30:09.445
  May 13 19:30:09.509: INFO: Deleting DaemonSet.extensions daemon-set took: 12.583727ms
  May 13 19:30:09.610: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.85236ms
  E0513 19:30:09.663209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:10.663882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:11.663870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:11.916: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:30:11.916: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 13 19:30:11.918: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41724"},"items":null}

  May 13 19:30:11.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41724"},"items":null}

  May 13 19:30:11.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3581" for this suite. @ 05/13/23 19:30:11.924
• [6.540 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 05/13/23 19:30:11.932
  May 13 19:30:11.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-probe @ 05/13/23 19:30:11.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:11.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:11.948
  STEP: Creating pod liveness-28082acf-b8a8-47c7-8708-89bd7fba1128 in namespace container-probe-5675 @ 05/13/23 19:30:11.95
  E0513 19:30:12.664859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:13.665054      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:13.959: INFO: Started pod liveness-28082acf-b8a8-47c7-8708-89bd7fba1128 in namespace container-probe-5675
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/13/23 19:30:13.959
  May 13 19:30:13.961: INFO: Initial restart count of pod liveness-28082acf-b8a8-47c7-8708-89bd7fba1128 is 0
  E0513 19:30:14.665726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:15.667326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:16.667481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:17.667821      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:18.668752      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:19.669034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:20.669306      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:21.670408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:22.670844      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:23.671420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:24.671460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:25.673271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:26.673578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:27.673935      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:28.677932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:29.678458      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:30.678556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:31.679558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:32.680915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:33.680944      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:33.990: INFO: Restart count of pod container-probe-5675/liveness-28082acf-b8a8-47c7-8708-89bd7fba1128 is now 1 (20.028669718s elapsed)
  May 13 19:30:33.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 19:30:33.992
  STEP: Destroying namespace "container-probe-5675" for this suite. @ 05/13/23 19:30:34.002
• [22.074 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/13/23 19:30:34.006
  May 13 19:30:34.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename deployment @ 05/13/23 19:30:34.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:34.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:34.021
  STEP: creating a Deployment @ 05/13/23 19:30:34.024
  STEP: waiting for Deployment to be created @ 05/13/23 19:30:34.027
  STEP: waiting for all Replicas to be Ready @ 05/13/23 19:30:34.028
  May 13 19:30:34.028: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 13 19:30:34.028: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 13 19:30:34.041: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 13 19:30:34.041: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 13 19:30:34.061: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 13 19:30:34.061: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 13 19:30:34.072: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 13 19:30:34.072: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0513 19:30:34.682573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:34.938: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 13 19:30:34.939: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 13 19:30:35.071: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/13/23 19:30:35.071
  W0513 19:30:35.076094      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 13 19:30:35.077: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/13/23 19:30:35.077
  May 13 19:30:35.079: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0
  May 13 19:30:35.079: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0
  May 13 19:30:35.079: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0
  May 13 19:30:35.079: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0
  May 13 19:30:35.079: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0
  May 13 19:30:35.079: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0
  May 13 19:30:35.079: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0
  May 13 19:30:35.079: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 0
  May 13 19:30:35.079: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  May 13 19:30:35.079: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  May 13 19:30:35.080: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:35.080: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:35.080: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:35.080: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:35.087: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:35.087: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:35.095: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:35.095: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:35.106: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  May 13 19:30:35.107: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  May 13 19:30:35.113: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  May 13 19:30:35.114: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  E0513 19:30:35.683863      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:35.948: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:35.948: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:35.962: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  STEP: listing Deployments @ 05/13/23 19:30:35.962
  May 13 19:30:35.965: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/13/23 19:30:35.965
  May 13 19:30:35.979: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/13/23 19:30:35.979
  May 13 19:30:36.004: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 13 19:30:36.004: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 13 19:30:36.006: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 13 19:30:36.012: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 13 19:30:36.018: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0513 19:30:36.684957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:36.979: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 13 19:30:36.988: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 13 19:30:36.996: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 13 19:30:37.016: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 13 19:30:37.026: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0513 19:30:37.686095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:38.115: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/13/23 19:30:38.135
  STEP: fetching the DeploymentStatus @ 05/13/23 19:30:38.145
  May 13 19:30:38.150: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  May 13 19:30:38.150: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  May 13 19:30:38.150: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  May 13 19:30:38.150: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  May 13 19:30:38.150: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 1
  May 13 19:30:38.150: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:38.150: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:38.151: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:38.151: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:38.151: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 2
  May 13 19:30:38.151: INFO: observed Deployment test-deployment in namespace deployment-9662 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/13/23 19:30:38.151
  May 13 19:30:38.159: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.160: INFO: observed event type MODIFIED
  May 13 19:30:38.169: INFO: Log out all the ReplicaSets if there is no deployment created
  May 13 19:30:38.176: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-9662  142aef41-2d48-4c72-b48c-3aadd75030ea 41935 3 2023-05-13 19:30:34 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 0b9d9f3f-9f43-4b17-b7a2-9b35db419580 0xc0063c26d7 0xc0063c26d8}] [] [{kube-controller-manager Update apps/v1 2023-05-13 19:30:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b9d9f3f-9f43-4b17-b7a2-9b35db419580\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:30:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0063c2760 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 13 19:30:38.180: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-9662  a93fa968-4ecf-436d-b7ad-c7e5d3c132fe 42048 4 2023-05-13 19:30:35 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 0b9d9f3f-9f43-4b17-b7a2-9b35db419580 0xc0063c27c7 0xc0063c27c8}] [] [{kube-controller-manager Update apps/v1 2023-05-13 19:30:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b9d9f3f-9f43-4b17-b7a2-9b35db419580\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:30:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0063c2860 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 13 19:30:38.190: INFO: pod: "test-deployment-5b5dcbcd95-8jlxb":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-8jlxb test-deployment-5b5dcbcd95- deployment-9662  893bf7d5-db7b-412f-9e96-fbed4649952e 42042 0 2023-05-13 19:30:35 +0000 UTC 2023-05-13 19:30:39 +0000 UTC 0xc0063c2c08 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:81de19dd6aad71f4761e6f2ee406f0e794c760d8341cba104dd283455b2e0f9c cni.projectcalico.org/podIP:10.200.131.136/32 cni.projectcalico.org/podIPs:10.200.131.136/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 a93fa968-4ecf-436d-b7ad-c7e5d3c132fe 0xc0063c2c37 0xc0063c2c38}] [] [{calico Update v1 2023-05-13 19:30:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-13 19:30:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a93fa968-4ecf-436d-b7ad-c7e5d3c132fe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-13 19:30:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x2jbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x2jbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:10.200.131.136,StartTime:2023-05-13 19:30:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 19:30:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.9,ImageID:k8s.gcr.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://58d4ad7dc2e6378e365f709f2152ee4e08449c0a01a66da6426ec19c43327296,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.136,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 13 19:30:38.190: INFO: pod: "test-deployment-5b5dcbcd95-bppsh":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-bppsh test-deployment-5b5dcbcd95- deployment-9662  a7bf5240-488c-4cb4-8645-cbbb18a36477 42007 0 2023-05-13 19:30:35 +0000 UTC 2023-05-13 19:30:37 +0000 UTC 0xc0063c2e20 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:35cf46bbbbfe44590486db08fa3f4f7b9c93aed4abaef84eb998cb0601808cfb cni.projectcalico.org/podIP:10.200.5.53/32 cni.projectcalico.org/podIPs:10.200.5.53/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 a93fa968-4ecf-436d-b7ad-c7e5d3c132fe 0xc0063c2e77 0xc0063c2e78}] [] [{kube-controller-manager Update v1 2023-05-13 19:30:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a93fa968-4ecf-436d-b7ad-c7e5d3c132fe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-13 19:30:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-13 19:30:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.5.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l46jn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l46jn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.101,PodIP:10.200.5.53,StartTime:2023-05-13 19:30:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 19:30:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.9,ImageID:k8s.gcr.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://5dd99f94d0f532b05fa2a620e4c5dbcb41cf4674cb406398d331bce8873cd31c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.53,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 13 19:30:38.191: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-9662  dd2343fc-51ae-4d1d-8002-0c75f67aaf57 42040 2 2023-05-13 19:30:35 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 0b9d9f3f-9f43-4b17-b7a2-9b35db419580 0xc0063c28c7 0xc0063c28c8}] [] [{kube-controller-manager Update apps/v1 2023-05-13 19:30:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b9d9f3f-9f43-4b17-b7a2-9b35db419580\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:30:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0063c2950 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  May 13 19:30:38.199: INFO: pod: "test-deployment-6fc78d85c6-fzmch":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-fzmch test-deployment-6fc78d85c6- deployment-9662  7c7d309a-50c7-4521-8582-df0f4266f8a2 41984 0 2023-05-13 19:30:35 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:07722969fd496b0ec3de4e91dfc82642fbced340c9ff36671c7178b77d30f4f1 cni.projectcalico.org/podIP:10.200.131.146/32 cni.projectcalico.org/podIPs:10.200.131.146/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 dd2343fc-51ae-4d1d-8002-0c75f67aaf57 0xc0058dc6a7 0xc0058dc6a8}] [] [{kube-controller-manager Update v1 2023-05-13 19:30:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dd2343fc-51ae-4d1d-8002-0c75f67aaf57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-13 19:30:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-13 19:30:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qv48g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qv48g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:10.200.131.146,StartTime:2023-05-13 19:30:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 19:30:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c905211dc10ebc6a88a3c09e89cb5110d45177e199db070bbb066dc93c52aab8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.146,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 13 19:30:38.200: INFO: pod: "test-deployment-6fc78d85c6-pf9f5":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-pf9f5 test-deployment-6fc78d85c6- deployment-9662  a5aec77e-e98a-4993-ab0d-07e3a1d920d5 42054 0 2023-05-13 19:30:36 +0000 UTC 2023-05-13 19:30:39 +0000 UTC 0xc0058dc970 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:abda1d0ac6698f1bdd5c547e129055ca108986f6d5674ff20b5953c54052efaa cni.projectcalico.org/podIP:10.200.5.54/32 cni.projectcalico.org/podIPs:10.200.5.54/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 dd2343fc-51ae-4d1d-8002-0c75f67aaf57 0xc0058dc9c7 0xc0058dc9c8}] [] [{kube-controller-manager Update v1 2023-05-13 19:30:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dd2343fc-51ae-4d1d-8002-0c75f67aaf57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-13 19:30:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-13 19:30:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.5.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5868v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5868v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:30:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.101,PodIP:10.200.5.54,StartTime:2023-05-13 19:30:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 19:30:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cf274b4c77bd262b354dd37f53a66459123f0dc56b69a6570f0d0270129ce8e8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.54,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 13 19:30:38.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9662" for this suite. @ 05/13/23 19:30:38.204
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 05/13/23 19:30:38.21
  May 13 19:30:38.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename endpointslice @ 05/13/23 19:30:38.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:38.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:38.227
  E0513 19:30:38.686494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:39.686754      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:40.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7021" for this suite. @ 05/13/23 19:30:40.269
• [2.063 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 05/13/23 19:30:40.272
  May 13 19:30:40.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 19:30:40.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:40.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:40.286
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/13/23 19:30:40.289
  E0513 19:30:40.687709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:41.688925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:30:42.299
  May 13 19:30:42.301: INFO: Trying to get logs from node worker00 pod pod-3b859697-54b4-439f-9ba4-092df98ea022 container test-container: <nil>
  STEP: delete the pod @ 05/13/23 19:30:42.305
  May 13 19:30:42.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1464" for this suite. @ 05/13/23 19:30:42.314
• [2.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 05/13/23 19:30:42.318
  May 13 19:30:42.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/13/23 19:30:42.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:42.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:42.335
  STEP: create the container to handle the HTTPGet hook request. @ 05/13/23 19:30:42.341
  E0513 19:30:42.689119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:43.689356      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/13/23 19:30:44.353
  E0513 19:30:44.690957      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:45.691126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 05/13/23 19:30:46.367
  E0513 19:30:46.691641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:47.691822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 05/13/23 19:30:48.386
  May 13 19:30:48.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-750" for this suite. @ 05/13/23 19:30:48.397
• [6.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 05/13/23 19:30:48.403
  May 13 19:30:48.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 19:30:48.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:48.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:48.415
  STEP: Setting up server cert @ 05/13/23 19:30:48.428
  E0513 19:30:48.692445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 19:30:48.741
  STEP: Deploying the webhook pod @ 05/13/23 19:30:48.747
  STEP: Wait for the deployment to be ready @ 05/13/23 19:30:48.757
  May 13 19:30:48.765: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 19:30:49.692582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:50.692745      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 19:30:50.772
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 19:30:50.791
  E0513 19:30:51.692869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:51.810: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 13 19:30:51.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-921-crds.webhook.example.com via the AdmissionRegistration API @ 05/13/23 19:30:52.319
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/13/23 19:30:52.337
  E0513 19:30:52.694390      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:53.694557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:54.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0513 19:30:54.694657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-84" for this suite. @ 05/13/23 19:30:54.903
  STEP: Destroying namespace "webhook-markers-3431" for this suite. @ 05/13/23 19:30:54.907
• [6.508 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 05/13/23 19:30:54.912
  May 13 19:30:54.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename runtimeclass @ 05/13/23 19:30:54.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:54.923
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:54.927
  E0513 19:30:55.695571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:56.696030      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:30:56.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3082" for this suite. @ 05/13/23 19:30:56.975
• [2.069 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 05/13/23 19:30:56.98
  May 13 19:30:56.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 19:30:56.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:30:56.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:30:56.994
  STEP: Creating configMap with name configmap-test-volume-map-b3d54927-2c47-4abb-b4ea-77172ace81e4 @ 05/13/23 19:30:56.996
  STEP: Creating a pod to test consume configMaps @ 05/13/23 19:30:56.999
  E0513 19:30:57.696158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:58.696459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:30:59.696854      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:00.697898      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:31:01.017
  May 13 19:31:01.018: INFO: Trying to get logs from node worker00 pod pod-configmaps-e0dcb971-443b-43f5-8fc8-4831d0c0db0f container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 19:31:01.021
  May 13 19:31:01.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9649" for this suite. @ 05/13/23 19:31:01.034
• [4.057 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 05/13/23 19:31:01.038
  May 13 19:31:01.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/13/23 19:31:01.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:31:01.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:31:01.049
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/13/23 19:31:01.051
  May 13 19:31:01.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:31:01.698182      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:31:02.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:31:02.698802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:03.699547      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:04.701453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:05.701444      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:06.701907      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:31:07.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-117" for this suite. @ 05/13/23 19:31:07.271
• [6.236 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 05/13/23 19:31:07.274
  May 13 19:31:07.274: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 19:31:07.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:31:07.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:31:07.285
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/13/23 19:31:07.287
  E0513 19:31:07.702312      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:08.702378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:31:09.298
  May 13 19:31:09.300: INFO: Trying to get logs from node worker00 pod pod-47e05732-e576-4c98-97ea-b357c49eeda1 container test-container: <nil>
  STEP: delete the pod @ 05/13/23 19:31:09.303
  May 13 19:31:09.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-968" for this suite. @ 05/13/23 19:31:09.316
• [2.046 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 05/13/23 19:31:09.321
  May 13 19:31:09.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename subpath @ 05/13/23 19:31:09.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:31:09.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:31:09.333
  STEP: Setting up data @ 05/13/23 19:31:09.335
  STEP: Creating pod pod-subpath-test-downwardapi-b6vv @ 05/13/23 19:31:09.34
  STEP: Creating a pod to test atomic-volume-subpath @ 05/13/23 19:31:09.34
  E0513 19:31:09.703035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:10.703277      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:11.703566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:12.703611      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:13.703630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:14.706616      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:15.706832      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:16.708568      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:17.708976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:18.709285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:19.710810      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:20.711553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:21.713199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:22.715945      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:23.716273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:24.716664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:25.718193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:26.719269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:27.719628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:28.720028      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:29.720238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:30.720489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:31:31.387
  May 13 19:31:31.388: INFO: Trying to get logs from node worker00 pod pod-subpath-test-downwardapi-b6vv container test-container-subpath-downwardapi-b6vv: <nil>
  STEP: delete the pod @ 05/13/23 19:31:31.392
  STEP: Deleting pod pod-subpath-test-downwardapi-b6vv @ 05/13/23 19:31:31.403
  May 13 19:31:31.403: INFO: Deleting pod "pod-subpath-test-downwardapi-b6vv" in namespace "subpath-3201"
  May 13 19:31:31.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3201" for this suite. @ 05/13/23 19:31:31.408
• [22.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/13/23 19:31:31.413
  May 13 19:31:31.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename containers @ 05/13/23 19:31:31.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:31:31.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:31:31.428
  STEP: Creating a pod to test override command @ 05/13/23 19:31:31.43
  E0513 19:31:31.721726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:32.722658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:31:33.444
  May 13 19:31:33.447: INFO: Trying to get logs from node worker00 pod client-containers-8c17a700-9ae1-49e7-9c8d-af409e23deef container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 19:31:33.45
  May 13 19:31:33.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5757" for this suite. @ 05/13/23 19:31:33.462
• [2.051 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 05/13/23 19:31:33.465
  May 13 19:31:33.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename init-container @ 05/13/23 19:31:33.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:31:33.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:31:33.477
  STEP: creating the pod @ 05/13/23 19:31:33.479
  May 13 19:31:33.479: INFO: PodSpec: initContainers in spec.initContainers
  E0513 19:31:33.724033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:34.725236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:35.725486      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:36.726373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:31:37.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5487" for this suite. @ 05/13/23 19:31:37.107
• [3.647 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 05/13/23 19:31:37.112
  May 13 19:31:37.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:31:37.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:31:37.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:31:37.122
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:31:37.124
  E0513 19:31:37.726557      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:38.727445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:39.728261      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:40.728430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:31:41.135
  May 13 19:31:41.137: INFO: Trying to get logs from node worker00 pod downwardapi-volume-2de8fc01-73b4-4693-80d3-0fa67a84bae8 container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:31:41.14
  May 13 19:31:41.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9577" for this suite. @ 05/13/23 19:31:41.165
• [4.056 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/13/23 19:31:41.169
  May 13 19:31:41.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-runtime @ 05/13/23 19:31:41.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:31:41.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:31:41.18
  STEP: create the container @ 05/13/23 19:31:41.182
  W0513 19:31:41.186539      21 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/13/23 19:31:41.186
  E0513 19:31:41.729240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:42.730374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:43.731245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/13/23 19:31:44.2
  STEP: the container should be terminated @ 05/13/23 19:31:44.202
  STEP: the termination message should be set @ 05/13/23 19:31:44.203
  May 13 19:31:44.203: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 05/13/23 19:31:44.203
  May 13 19:31:44.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5269" for this suite. @ 05/13/23 19:31:44.213
• [3.047 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 05/13/23 19:31:44.216
  May 13 19:31:44.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl @ 05/13/23 19:31:44.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:31:44.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:31:44.227
  STEP: creating Agnhost RC @ 05/13/23 19:31:44.229
  May 13 19:31:44.229: INFO: namespace kubectl-9266
  May 13 19:31:44.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-9266 create -f -'
  May 13 19:31:44.416: INFO: stderr: ""
  May 13 19:31:44.416: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/13/23 19:31:44.416
  E0513 19:31:44.731431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:31:45.418: INFO: Selector matched 1 pods for map[app:agnhost]
  May 13 19:31:45.418: INFO: Found 1 / 1
  May 13 19:31:45.418: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 13 19:31:45.420: INFO: Selector matched 1 pods for map[app:agnhost]
  May 13 19:31:45.420: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 13 19:31:45.420: INFO: wait on agnhost-primary startup in kubectl-9266 
  May 13 19:31:45.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-9266 logs agnhost-primary-fwchh agnhost-primary'
  May 13 19:31:45.464: INFO: stderr: ""
  May 13 19:31:45.464: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 05/13/23 19:31:45.464
  May 13 19:31:45.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-9266 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  May 13 19:31:45.513: INFO: stderr: ""
  May 13 19:31:45.513: INFO: stdout: "service/rm2 exposed\n"
  May 13 19:31:45.519: INFO: Service rm2 in namespace kubectl-9266 found.
  E0513 19:31:45.731735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:46.733580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 05/13/23 19:31:47.523
  May 13 19:31:47.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-9266 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  May 13 19:31:47.588: INFO: stderr: ""
  May 13 19:31:47.588: INFO: stdout: "service/rm3 exposed\n"
  May 13 19:31:47.591: INFO: Service rm3 in namespace kubectl-9266 found.
  E0513 19:31:47.733784      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:48.734116      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:31:49.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9266" for this suite. @ 05/13/23 19:31:49.599
• [5.400 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 05/13/23 19:31:49.619
  May 13 19:31:49.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename var-expansion @ 05/13/23 19:31:49.621
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:31:49.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:31:49.637
  STEP: Creating a pod to test substitution in container's args @ 05/13/23 19:31:49.64
  E0513 19:31:49.734999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:50.735868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:51.736387      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:52.736517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:31:53.656
  May 13 19:31:53.659: INFO: Trying to get logs from node worker00 pod var-expansion-56290d19-dd03-45d5-98fd-cd12d9ddc1b0 container dapi-container: <nil>
  STEP: delete the pod @ 05/13/23 19:31:53.663
  May 13 19:31:53.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7273" for this suite. @ 05/13/23 19:31:53.683
• [4.067 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 05/13/23 19:31:53.686
  May 13 19:31:53.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename statefulset @ 05/13/23 19:31:53.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:31:53.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:31:53.702
  STEP: Creating service test in namespace statefulset-5414 @ 05/13/23 19:31:53.704
  STEP: Creating a new StatefulSet @ 05/13/23 19:31:53.709
  May 13 19:31:53.716: INFO: Found 0 stateful pods, waiting for 3
  E0513 19:31:53.737072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:54.737765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:55.739148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:56.739775      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:57.742470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:58.743127      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:31:59.743624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:00.743826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:01.744856      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:02.745266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:32:03.737: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 13 19:32:03.737: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 13 19:32:03.737: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  May 13 19:32:03.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-5414 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0513 19:32:03.745494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:32:03.852: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 13 19:32:03.852: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 13 19:32:03.852: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0513 19:32:04.745640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:05.747242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:06.747930      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:07.748408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:08.748552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:09.748816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:10.749979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:11.750582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:12.752117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:13.752765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/13/23 19:32:13.861
  May 13 19:32:13.885: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/13/23 19:32:13.885
  E0513 19:32:14.752866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:15.753024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:16.753245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:17.755593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:18.755476      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:19.755515      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:20.755655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:21.755802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:22.757014      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:23.760239      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 05/13/23 19:32:23.893
  May 13 19:32:23.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-5414 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 13 19:32:24.011: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 13 19:32:24.011: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 13 19:32:24.011: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0513 19:32:24.760691      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:25.762138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:26.763641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:27.765066      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:28.765197      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:29.765913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:30.766037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:31.766579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:32.766731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:33.766773      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 05/13/23 19:32:34.022
  May 13 19:32:34.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-5414 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 13 19:32:34.127: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 13 19:32:34.127: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 13 19:32:34.127: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0513 19:32:34.766960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:35.767269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:36.768232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:37.771808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:38.772553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:39.772861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:40.773696      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:41.775058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:42.775179      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:43.775630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:32:44.167: INFO: Updating stateful set ss2
  E0513 19:32:44.778156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:45.778310      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:46.779293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:47.779695      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:48.779872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:49.780744      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:50.780826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:51.781760      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:52.782323      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:53.785382      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 05/13/23 19:32:54.176
  May 13 19:32:54.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=statefulset-5414 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 13 19:32:54.285: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 13 19:32:54.285: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 13 19:32:54.285: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0513 19:32:54.785596      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:55.786379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:56.787343      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:57.787470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:58.787630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:32:59.788078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:00.791036      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:01.898970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:02.899071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:03.899681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:33:04.295: INFO: Deleting all statefulset in ns statefulset-5414
  May 13 19:33:04.297: INFO: Scaling statefulset ss2 to 0
  E0513 19:33:04.899982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:05.900456      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:06.900932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:07.901349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:08.901699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:09.903006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:10.903690      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:11.904398      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:12.905542      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:13.905706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:33:14.316: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 19:33:14.318: INFO: Deleting statefulset ss2
  May 13 19:33:14.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5414" for this suite. @ 05/13/23 19:33:14.339
• [80.657 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 05/13/23 19:33:14.345
  May 13 19:33:14.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 19:33:14.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:33:14.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:33:14.358
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/13/23 19:33:14.36
  E0513 19:33:14.906667      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:15.907714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:16.907924      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:17.909620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:33:18.372
  May 13 19:33:18.374: INFO: Trying to get logs from node worker00 pod pod-ca8d448c-8f26-46bb-8184-851ea8a02011 container test-container: <nil>
  STEP: delete the pod @ 05/13/23 19:33:18.393
  May 13 19:33:18.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-328" for this suite. @ 05/13/23 19:33:18.402
• [4.061 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/13/23 19:33:18.406
  May 13 19:33:18.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:33:18.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:33:18.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:33:18.416
  STEP: Creating secret with name projected-secret-test-f2ab49b6-8ff8-439f-8599-a027d7e14e0c @ 05/13/23 19:33:18.417
  STEP: Creating a pod to test consume secrets @ 05/13/23 19:33:18.42
  E0513 19:33:18.910601      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:19.911489      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:20.911671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:21.912632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:33:22.436
  May 13 19:33:22.438: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-680d7fd3-668a-488a-84d8-e4940ff37e6d container secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 19:33:22.441
  May 13 19:33:22.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9173" for this suite. @ 05/13/23 19:33:22.461
• [4.060 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 05/13/23 19:33:22.467
  May 13 19:33:22.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 19:33:22.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:33:22.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:33:22.479
  STEP: creating secret secrets-8522/secret-test-477efd8b-24bb-42b6-bcce-5eb54569f9ad @ 05/13/23 19:33:22.481
  STEP: Creating a pod to test consume secrets @ 05/13/23 19:33:22.484
  E0513 19:33:22.913347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:23.913975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:24.915012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:25.915240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:33:26.497
  May 13 19:33:26.499: INFO: Trying to get logs from node worker00 pod pod-configmaps-8e2181d8-127e-4b29-b387-e3f869b72150 container env-test: <nil>
  STEP: delete the pod @ 05/13/23 19:33:26.504
  May 13 19:33:26.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8522" for this suite. @ 05/13/23 19:33:26.519
• [4.057 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 05/13/23 19:33:26.526
  May 13 19:33:26.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename daemonsets @ 05/13/23 19:33:26.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:33:26.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:33:26.547
  STEP: Creating simple DaemonSet "daemon-set" @ 05/13/23 19:33:26.561
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/13/23 19:33:26.564
  May 13 19:33:26.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:33:26.569: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:33:26.915612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:33:27.572: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:33:27.572: INFO: Node worker01 is running 0 daemon pod, expected 1
  E0513 19:33:27.917010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:33:28.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 13 19:33:28.575: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/13/23 19:33:28.578
  STEP: DeleteCollection of the DaemonSets @ 05/13/23 19:33:28.581
  STEP: Verify that ReplicaSets have been deleted @ 05/13/23 19:33:28.587
  May 13 19:33:28.597: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43876"},"items":null}

  May 13 19:33:28.601: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43876"},"items":[{"metadata":{"name":"daemon-set-2ltgl","generateName":"daemon-set-","namespace":"daemonsets-3129","uid":"eec7b3d9-11f6-4b12-9dc5-8e15fe7c0c45","resourceVersion":"43852","creationTimestamp":"2023-05-13T19:33:26Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"35cfe37f6b77e791aa9023d29ab98c6d91b9c8eb6422e2d5192544bafb150272","cni.projectcalico.org/podIP":"10.200.131.167/32","cni.projectcalico.org/podIPs":"10.200.131.167/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"109b954b-01fb-44a2-9b1e-ff861c443578","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-13T19:33:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"109b954b-01fb-44a2-9b1e-ff861c443578\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-13T19:33:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-13T19:33:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nn6hf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nn6hf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker00","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker00"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-13T19:33:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-13T19:33:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-13T19:33:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-13T19:33:26Z"}],"hostIP":"192.168.58.100","podIP":"10.200.131.167","podIPs":[{"ip":"10.200.131.167"}],"startTime":"2023-05-13T19:33:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-13T19:33:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://de6545f8a01d36c6a4b5250bdd9612762404e18dc4d8d0fe90cdfa23fecaf5e6","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-4b5qc","generateName":"daemon-set-","namespace":"daemonsets-3129","uid":"f849c0c1-a236-4052-a6b6-db85e0ea63e7","resourceVersion":"43863","creationTimestamp":"2023-05-13T19:33:26Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7a2e3565f35e11298dc633e96f48e89164943d7052952d4b6327006e01089de3","cni.projectcalico.org/podIP":"10.200.5.62/32","cni.projectcalico.org/podIPs":"10.200.5.62/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"109b954b-01fb-44a2-9b1e-ff861c443578","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-13T19:33:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"109b954b-01fb-44a2-9b1e-ff861c443578\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-13T19:33:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-13T19:33:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.5.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xtfqp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xtfqp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"worker01","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["worker01"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-13T19:33:26Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-13T19:33:27Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-13T19:33:27Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-13T19:33:26Z"}],"hostIP":"192.168.58.101","podIP":"10.200.5.62","podIPs":[{"ip":"10.200.5.62"}],"startTime":"2023-05-13T19:33:26Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-13T19:33:27Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://2cb9f87d427997b2906a170cfdac06e263c43c5728ea5dfaa4b9700447c7ff02","started":true}],"qosClass":"BestEffort"}}]}

  May 13 19:33:28.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3129" for this suite. @ 05/13/23 19:33:28.614
• [2.092 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 05/13/23 19:33:28.619
  May 13 19:33:28.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename var-expansion @ 05/13/23 19:33:28.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:33:28.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:33:28.63
  STEP: Creating a pod to test env composition @ 05/13/23 19:33:28.631
  E0513 19:33:28.917392      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:29.917527      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:33:30.676
  May 13 19:33:30.677: INFO: Trying to get logs from node worker00 pod var-expansion-7812a615-3c02-4b52-bc35-42d6af58f9ec container dapi-container: <nil>
  STEP: delete the pod @ 05/13/23 19:33:30.68
  May 13 19:33:30.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6754" for this suite. @ 05/13/23 19:33:30.69
• [2.074 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 05/13/23 19:33:30.693
  May 13 19:33:30.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:33:30.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:33:30.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:33:30.705
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:33:30.707
  E0513 19:33:30.917913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:31.918823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:33:32.716
  May 13 19:33:32.718: INFO: Trying to get logs from node worker00 pod downwardapi-volume-e983e086-cc67-4309-a798-2a235e90c180 container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:33:32.722
  May 13 19:33:32.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7487" for this suite. @ 05/13/23 19:33:32.736
• [2.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 05/13/23 19:33:32.741
  May 13 19:33:32.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 19:33:32.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:33:32.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:33:32.751
  STEP: Setting up server cert @ 05/13/23 19:33:32.761
  E0513 19:33:32.919535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 19:33:33.032
  STEP: Deploying the webhook pod @ 05/13/23 19:33:33.038
  STEP: Wait for the deployment to be ready @ 05/13/23 19:33:33.045
  May 13 19:33:33.048: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0513 19:33:33.920746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:34.920774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 19:33:35.052
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 19:33:35.07
  E0513 19:33:35.921644      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:33:36.071: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/13/23 19:33:36.076
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/13/23 19:33:36.079
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/13/23 19:33:36.079
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/13/23 19:33:36.079
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/13/23 19:33:36.081
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/13/23 19:33:36.081
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/13/23 19:33:36.083
  May 13 19:33:36.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-328" for this suite. @ 05/13/23 19:33:36.122
  STEP: Destroying namespace "webhook-markers-7555" for this suite. @ 05/13/23 19:33:36.127
• [3.398 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 05/13/23 19:33:36.139
  May 13 19:33:36.139: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-probe @ 05/13/23 19:33:36.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:33:36.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:33:36.159
  E0513 19:33:36.928297      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:37.928730      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:38.929009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:39.929231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:40.929656      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:41.929747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:42.930175      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:43.932450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:44.933034      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:45.933646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:46.935190      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:47.935193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:48.935265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:49.935578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:50.936358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:51.936605      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:52.936656      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:53.937140      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:54.938063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:55.938808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:56.938918      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:57.939372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:58.939672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:33:59.940288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:00.941368      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:01.941592      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:02.942109      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:03.945161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:04.945360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:05.945769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:06.946819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:07.948081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:08.948176      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:09.949237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:10.950654      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:11.950782      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:12.950858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:13.954664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:14.955994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:15.956737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:16.957831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:17.958174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:18.959778      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:19.960816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:20.961705      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:21.962062      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:22.962416      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:23.964753      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:24.965076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:25.965212      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:26.966081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:27.967099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:28.967390      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:29.968360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:30.969493      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:31.970579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:32.971161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:33.971348      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:34.972287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:35.972677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:34:36.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-2327" for this suite. @ 05/13/23 19:34:36.182
• [60.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 05/13/23 19:34:36.186
  May 13 19:34:36.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replication-controller @ 05/13/23 19:34:36.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:34:36.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:34:36.207
  STEP: Creating ReplicationController "e2e-rc-jxppw" @ 05/13/23 19:34:36.211
  May 13 19:34:36.214: INFO: Get Replication Controller "e2e-rc-jxppw" to confirm replicas
  E0513 19:34:36.973288      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:34:37.222: INFO: Get Replication Controller "e2e-rc-jxppw" to confirm replicas
  May 13 19:34:37.224: INFO: Found 1 replicas for "e2e-rc-jxppw" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-jxppw" @ 05/13/23 19:34:37.225
  STEP: Updating a scale subresource @ 05/13/23 19:34:37.228
  STEP: Verifying replicas where modified for replication controller "e2e-rc-jxppw" @ 05/13/23 19:34:37.232
  May 13 19:34:37.232: INFO: Get Replication Controller "e2e-rc-jxppw" to confirm replicas
  E0513 19:34:37.974410      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:34:38.233: INFO: Get Replication Controller "e2e-rc-jxppw" to confirm replicas
  May 13 19:34:38.235: INFO: Found 2 replicas for "e2e-rc-jxppw" replication controller
  May 13 19:34:38.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9423" for this suite. @ 05/13/23 19:34:38.237
• [2.055 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 05/13/23 19:34:38.241
  May 13 19:34:38.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename security-context-test @ 05/13/23 19:34:38.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:34:38.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:34:38.253
  E0513 19:34:38.976620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:39.978082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:40.978332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:41.979433      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:34:42.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5753" for this suite. @ 05/13/23 19:34:42.272
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 05/13/23 19:34:42.291
  May 13 19:34:42.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename configmap @ 05/13/23 19:34:42.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:34:42.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:34:42.309
  STEP: Creating configMap with name cm-test-opt-del-77e98cd2-19e6-401d-a24c-ded74800683d @ 05/13/23 19:34:42.32
  STEP: Creating configMap with name cm-test-opt-upd-e586dafd-c14e-4655-b624-b7015432387a @ 05/13/23 19:34:42.323
  STEP: Creating the pod @ 05/13/23 19:34:42.326
  E0513 19:34:42.980926      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:43.983676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-77e98cd2-19e6-401d-a24c-ded74800683d @ 05/13/23 19:34:44.351
  STEP: Updating configmap cm-test-opt-upd-e586dafd-c14e-4655-b624-b7015432387a @ 05/13/23 19:34:44.355
  STEP: Creating configMap with name cm-test-opt-create-aac83a0c-da32-4b2e-b540-af2e5c39b395 @ 05/13/23 19:34:44.358
  STEP: waiting to observe update in volume @ 05/13/23 19:34:44.36
  E0513 19:34:44.988983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:45.990498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:34:46.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1804" for this suite. @ 05/13/23 19:34:46.376
• [4.092 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/13/23 19:34:46.385
  May 13 19:34:46.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 19:34:46.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:34:46.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:34:46.396
  STEP: Creating secret with name secret-test-04642aa4-53bd-47e6-8629-65ca6cdc7565 @ 05/13/23 19:34:46.398
  STEP: Creating a pod to test consume secrets @ 05/13/23 19:34:46.401
  E0513 19:34:46.993155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:47.994448      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:48.995089      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:49.995669      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:34:50.415
  May 13 19:34:50.417: INFO: Trying to get logs from node worker00 pod pod-secrets-e57a3654-c4d4-466a-b468-d65c15ae0d25 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 19:34:50.421
  May 13 19:34:50.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1610" for this suite. @ 05/13/23 19:34:50.434
• [4.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/13/23 19:34:50.437
  May 13 19:34:50.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename cronjob @ 05/13/23 19:34:50.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:34:50.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:34:50.449
  STEP: Creating a ForbidConcurrent cronjob @ 05/13/23 19:34:50.45
  STEP: Ensuring a job is scheduled @ 05/13/23 19:34:50.454
  E0513 19:34:50.997002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:51.997539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:52.997854      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:53.997964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:54.998442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:55.999150      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:56.999649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:57.999700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:34:59.000346      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:00.000523      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/13/23 19:35:00.456
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/13/23 19:35:00.457
  STEP: Ensuring no more jobs are scheduled @ 05/13/23 19:35:00.46
  E0513 19:35:01.000950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:02.001434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:03.001686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:04.002680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:05.003597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:06.004149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:07.009033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:08.009137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:09.009169      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:10.009340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:11.009686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:12.010000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:13.010857      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:14.011978      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:15.013367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:16.013485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:17.014233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:18.015036      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:19.015939      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:20.016866      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:21.016983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:22.018505      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:23.019554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:24.019661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:25.019795      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:26.020395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:27.020690      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:28.024136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:29.025227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:30.027005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:31.028204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:32.028893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:33.029459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:34.029548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:35.032162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:36.035414      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:37.035994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:38.036272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:39.037913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:40.039076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:41.039998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:42.040048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:43.040641      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:44.041136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:45.042242      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:46.042313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:47.042481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:48.043322      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:49.043946      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:50.044073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:51.044709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:52.045855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:53.046612      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:54.046897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:55.047148      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:56.047780      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:57.048994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:58.052285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:35:59.053975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:00.054125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:01.054516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:02.054633      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:03.055618      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:04.056072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:05.056293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:06.057206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:07.059407      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:08.059420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:09.060863      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:10.063673      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:11.063888      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:12.064136      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:13.064743      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:14.068126      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:15.068665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:16.069265      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:17.072164      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:18.073137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:19.074081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:20.074691      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:21.074766      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:22.075872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:23.077379      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:24.079029      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:25.082666      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:26.082805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:27.083380      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:28.083660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:29.084526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:30.085496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:31.086556      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:32.088599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:33.088360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:34.089735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:35.090692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:36.091432      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:37.091609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:38.092273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:39.092773      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:40.093000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:41.093271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:42.094578      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:43.095703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:44.095743      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:45.097565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:46.108757      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:47.099219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:48.099980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:49.100026      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:50.101073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:51.102094      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:52.102168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:53.102550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:54.102975      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:55.103406      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:56.103537      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:57.104453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:58.104516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:36:59.104712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:00.105533      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:01.105384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:02.105583      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:03.105804      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:04.107409      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:05.108456      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:06.110569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:07.111338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:08.112312      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:09.112359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:10.112772      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:11.113155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:12.116130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:13.117670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:14.118613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:15.118807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:16.119806      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:17.120007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:18.120073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:19.120257      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:20.120399      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:21.120807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:22.122608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:23.123000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:24.123273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:25.124599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:26.124718      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:27.125252      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:28.126968      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:29.131717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:30.132105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:31.134085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:32.134187      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:33.138368      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:34.138793      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:35.139600      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:36.140513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:37.140935      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:38.144716      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:39.145389      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:40.146658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:41.146935      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:42.147119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:43.147905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:44.149337      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:45.149693      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:46.150043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:47.150874      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:48.152508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:49.153381      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:50.154806      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:51.155426      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:52.155965      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:53.155980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:54.158670      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:55.159369      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:56.159878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:57.190032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:58.189859      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:37:59.190814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:00.190789      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:01.190857      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:02.193882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:03.194500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:04.194863      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:05.195010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:06.195597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:07.196599      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:08.196977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:09.199053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:10.199962      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:11.200534      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:12.202748      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:13.203807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:14.204895      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:15.205422      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:16.210535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:17.211233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:18.211858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:19.212089      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:20.212172      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:21.213264      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:22.214676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:23.214994      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:24.215078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:25.215566      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:26.215682      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:27.216271      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:28.217269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:29.217402      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:30.218609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:31.219082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:32.219660      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:33.219710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:34.219920      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:35.220010      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:36.220586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:37.221256      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:38.221282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:39.221586      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:40.221733      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:41.222007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:42.224334      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:43.224742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:44.224608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:45.225610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:46.226355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:47.226719      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:48.227056      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:49.227107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:50.227655      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:51.227707      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:52.227833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:53.228679      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:54.229341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:55.257189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:56.257739      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:57.260501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:58.260614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:38:59.261362      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:00.261526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:01.262565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:02.263473      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:03.263797      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:04.263887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:05.266539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:06.267280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:07.267774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:08.270701      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:09.271101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:10.271363      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:11.273166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:12.273799      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:13.274139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:14.274289      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:15.274887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:16.275795      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:17.277065      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:18.278517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:19.279069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:20.279151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:21.280677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:22.280751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:23.282466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:24.283052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:25.283221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:26.284101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:27.284698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:28.284759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:29.285058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:30.285409      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:31.286805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:32.288057      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:33.288407      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:34.289657      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:35.290060      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:36.292044      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:37.296275      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:38.296833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:39.297391      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:40.297801      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:41.298243      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:42.298413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:43.299514      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:44.300762      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:45.301044      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:46.302031      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:47.302539      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:48.302707      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:49.303236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:50.303430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:51.305359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:52.306608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:53.307408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:54.308009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:55.308309      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:56.308465      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:57.309074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:58.309056      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:39:59.309708      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:00.309814      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/13/23 19:40:00.463
  May 13 19:40:00.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4569" for this suite. @ 05/13/23 19:40:00.471
• [310.041 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/13/23 19:40:00.483
  May 13 19:40:00.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/13/23 19:40:00.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:40:00.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:40:00.507
  STEP: Setting up the test @ 05/13/23 19:40:00.51
  STEP: Creating hostNetwork=false pod @ 05/13/23 19:40:00.51
  E0513 19:40:01.310177      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:02.310459      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 05/13/23 19:40:02.524
  E0513 19:40:03.311788      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:04.311988      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 05/13/23 19:40:04.549
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/13/23 19:40:04.549
  May 13 19:40:04.549: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7824 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:40:04.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:40:04.550: INFO: ExecWithOptions: Clientset creation
  May 13 19:40:04.550: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7824/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 13 19:40:04.624: INFO: Exec stderr: ""
  May 13 19:40:04.624: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7824 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:40:04.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:40:04.625: INFO: ExecWithOptions: Clientset creation
  May 13 19:40:04.625: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7824/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 13 19:40:04.661: INFO: Exec stderr: ""
  May 13 19:40:04.661: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7824 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:40:04.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:40:04.662: INFO: ExecWithOptions: Clientset creation
  May 13 19:40:04.662: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7824/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 13 19:40:04.709: INFO: Exec stderr: ""
  May 13 19:40:04.709: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7824 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:40:04.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:40:04.709: INFO: ExecWithOptions: Clientset creation
  May 13 19:40:04.709: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7824/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 13 19:40:04.751: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/13/23 19:40:04.751
  May 13 19:40:04.751: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7824 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:40:04.751: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:40:04.751: INFO: ExecWithOptions: Clientset creation
  May 13 19:40:04.751: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7824/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 13 19:40:04.796: INFO: Exec stderr: ""
  May 13 19:40:04.796: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7824 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:40:04.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:40:04.797: INFO: ExecWithOptions: Clientset creation
  May 13 19:40:04.797: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7824/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 13 19:40:04.838: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/13/23 19:40:04.838
  May 13 19:40:04.838: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7824 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:40:04.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:40:04.838: INFO: ExecWithOptions: Clientset creation
  May 13 19:40:04.838: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7824/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 13 19:40:04.903: INFO: Exec stderr: ""
  May 13 19:40:04.903: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7824 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:40:04.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:40:04.904: INFO: ExecWithOptions: Clientset creation
  May 13 19:40:04.904: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7824/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 13 19:40:04.961: INFO: Exec stderr: ""
  May 13 19:40:04.961: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7824 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:40:04.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:40:04.963: INFO: ExecWithOptions: Clientset creation
  May 13 19:40:04.963: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7824/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 13 19:40:05.020: INFO: Exec stderr: ""
  May 13 19:40:05.020: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7824 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:40:05.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:40:05.021: INFO: ExecWithOptions: Clientset creation
  May 13 19:40:05.022: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7824/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 13 19:40:05.088: INFO: Exec stderr: ""
  May 13 19:40:05.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-7824" for this suite. @ 05/13/23 19:40:05.091
• [4.612 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 05/13/23 19:40:05.094
  May 13 19:40:05.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename disruption @ 05/13/23 19:40:05.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:40:05.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:40:05.108
  STEP: Waiting for the pdb to be processed @ 05/13/23 19:40:05.116
  E0513 19:40:05.314676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:06.315776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 05/13/23 19:40:07.167
  May 13 19:40:07.182: INFO: running pods: 0 < 3
  E0513 19:40:07.316863      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:08.317649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:40:09.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2885" for this suite. @ 05/13/23 19:40:09.189
• [4.107 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/13/23 19:40:09.204
  May 13 19:40:09.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename subjectreview @ 05/13/23 19:40:09.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:40:09.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:40:09.221
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-5965" @ 05/13/23 19:40:09.224
  May 13 19:40:09.228: INFO: saUsername: "system:serviceaccount:subjectreview-5965:e2e"
  May 13 19:40:09.228: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-5965"}
  May 13 19:40:09.228: INFO: saUID: "c2023327-7b63-4043-a333-8f6445ea8f03"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-5965:e2e" @ 05/13/23 19:40:09.228
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-5965:e2e" @ 05/13/23 19:40:09.228
  May 13 19:40:09.231: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-5965:e2e" api 'list' configmaps in "subjectreview-5965" namespace @ 05/13/23 19:40:09.231
  May 13 19:40:09.233: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-5965:e2e" @ 05/13/23 19:40:09.233
  May 13 19:40:09.235: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  May 13 19:40:09.235: INFO: LocalSubjectAccessReview has been verified
  May 13 19:40:09.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-5965" for this suite. @ 05/13/23 19:40:09.238
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 05/13/23 19:40:09.243
  May 13 19:40:09.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/13/23 19:40:09.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:40:09.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:40:09.258
  STEP: create the container to handle the HTTPGet hook request. @ 05/13/23 19:40:09.261
  E0513 19:40:09.318232      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:10.318248      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/13/23 19:40:11.29
  E0513 19:40:11.318824      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:12.318905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/13/23 19:40:13.311
  STEP: delete the pod with lifecycle hook @ 05/13/23 19:40:13.317
  E0513 19:40:13.319253      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:14.319350      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:15.321152      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:16.321777      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:17.391492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:40:17.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8394" for this suite. @ 05/13/23 19:40:17.396
• [8.156 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 05/13/23 19:40:17.401
  May 13 19:40:17.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubectl-logs @ 05/13/23 19:40:17.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:40:17.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:40:17.415
  STEP: creating an pod @ 05/13/23 19:40:17.419
  May 13 19:40:17.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-logs-6436 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  May 13 19:40:17.492: INFO: stderr: ""
  May 13 19:40:17.492: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/13/23 19:40:17.493
  May 13 19:40:17.493: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0513 19:40:18.392016      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:19.392141      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:40:19.502: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/13/23 19:40:19.502
  May 13 19:40:19.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-logs-6436 logs logs-generator logs-generator'
  May 13 19:40:19.617: INFO: stderr: ""
  May 13 19:40:19.617: INFO: stdout: "I0513 19:40:18.137718       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/bgl2 491\nI0513 19:40:18.337970       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/zl5 328\nI0513 19:40:18.538584       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/s49 570\nI0513 19:40:18.738057       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/8rg 562\nI0513 19:40:18.937925       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/2th 405\nI0513 19:40:19.140658       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/lfw 232\nI0513 19:40:19.338950       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/jqrq 353\nI0513 19:40:19.538631       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jzv2 306\n"
  STEP: limiting log lines @ 05/13/23 19:40:19.617
  May 13 19:40:19.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-logs-6436 logs logs-generator logs-generator --tail=1'
  May 13 19:40:19.756: INFO: stderr: ""
  May 13 19:40:19.756: INFO: stdout: "I0513 19:40:19.738474       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/2jc 340\n"
  May 13 19:40:19.756: INFO: got output "I0513 19:40:19.738474       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/2jc 340\n"
  STEP: limiting log bytes @ 05/13/23 19:40:19.756
  May 13 19:40:19.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-logs-6436 logs logs-generator logs-generator --limit-bytes=1'
  May 13 19:40:19.868: INFO: stderr: ""
  May 13 19:40:19.868: INFO: stdout: "I"
  May 13 19:40:19.868: INFO: got output "I"
  STEP: exposing timestamps @ 05/13/23 19:40:19.868
  May 13 19:40:19.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-logs-6436 logs logs-generator logs-generator --tail=1 --timestamps'
  May 13 19:40:19.965: INFO: stderr: ""
  May 13 19:40:19.965: INFO: stdout: "2023-05-13T19:40:19.938749465Z I0513 19:40:19.938538       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/gk5 536\n"
  May 13 19:40:19.965: INFO: got output "2023-05-13T19:40:19.938749465Z I0513 19:40:19.938538       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/gk5 536\n"
  STEP: restricting to a time range @ 05/13/23 19:40:19.965
  E0513 19:40:20.393549      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:21.393590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:22.393896      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:40:22.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-logs-6436 logs logs-generator logs-generator --since=1s'
  May 13 19:40:22.545: INFO: stderr: ""
  May 13 19:40:22.545: INFO: stdout: "I0513 19:40:21.545261       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/dhq 204\nI0513 19:40:21.737908       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/x6t5 330\nI0513 19:40:21.938002       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/x7s 289\nI0513 19:40:22.138329       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/cjq 425\nI0513 19:40:22.337863       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/nhpq 465\n"
  May 13 19:40:22.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-logs-6436 logs logs-generator logs-generator --since=24h'
  May 13 19:40:22.623: INFO: stderr: ""
  May 13 19:40:22.623: INFO: stdout: "I0513 19:40:18.137718       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/bgl2 491\nI0513 19:40:18.337970       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/zl5 328\nI0513 19:40:18.538584       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/s49 570\nI0513 19:40:18.738057       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/8rg 562\nI0513 19:40:18.937925       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/2th 405\nI0513 19:40:19.140658       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/lfw 232\nI0513 19:40:19.338950       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/jqrq 353\nI0513 19:40:19.538631       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jzv2 306\nI0513 19:40:19.738474       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/2jc 340\nI0513 19:40:19.938538       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/gk5 536\nI0513 19:40:20.140292       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/xwqr 566\nI0513 19:40:20.338758       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/x8k 316\nI0513 19:40:20.538444       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/jlz6 537\nI0513 19:40:20.737776       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/jl2x 444\nI0513 19:40:20.937783       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/2rs 434\nI0513 19:40:21.138130       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/wxlm 226\nI0513 19:40:21.338677       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/jvt 239\nI0513 19:40:21.545261       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/dhq 204\nI0513 19:40:21.737908       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/x6t5 330\nI0513 19:40:21.938002       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/x7s 289\nI0513 19:40:22.138329       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/cjq 425\nI0513 19:40:22.337863       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/nhpq 465\nI0513 19:40:22.537732       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/j2cr 507\n"
  May 13 19:40:22.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3569800363 --namespace=kubectl-logs-6436 delete pod logs-generator'
  E0513 19:40:23.393982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:40:23.683: INFO: stderr: ""
  May 13 19:40:23.683: INFO: stdout: "pod \"logs-generator\" deleted\n"
  May 13 19:40:23.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-6436" for this suite. @ 05/13/23 19:40:23.685
• [6.289 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 05/13/23 19:40:23.69
  May 13 19:40:23.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename daemonsets @ 05/13/23 19:40:23.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:40:23.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:40:23.705
  STEP: Creating simple DaemonSet "daemon-set" @ 05/13/23 19:40:23.717
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/13/23 19:40:23.72
  May 13 19:40:23.724: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:40:23.724: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:40:24.394903      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:40:24.728: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:40:24.728: INFO: Node worker01 is running 0 daemon pod, expected 1
  E0513 19:40:25.395233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:40:25.728: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 13 19:40:25.728: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Getting /status @ 05/13/23 19:40:25.729
  May 13 19:40:25.740: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/13/23 19:40:25.74
  May 13 19:40:25.745: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/13/23 19:40:25.745
  May 13 19:40:25.748: INFO: Observed &DaemonSet event: ADDED
  May 13 19:40:25.748: INFO: Observed &DaemonSet event: MODIFIED
  May 13 19:40:25.748: INFO: Observed &DaemonSet event: MODIFIED
  May 13 19:40:25.748: INFO: Observed &DaemonSet event: MODIFIED
  May 13 19:40:25.748: INFO: Found daemon set daemon-set in namespace daemonsets-327 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 13 19:40:25.748: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/13/23 19:40:25.748
  STEP: watching for the daemon set status to be patched @ 05/13/23 19:40:25.753
  May 13 19:40:25.755: INFO: Observed &DaemonSet event: ADDED
  May 13 19:40:25.755: INFO: Observed &DaemonSet event: MODIFIED
  May 13 19:40:25.755: INFO: Observed &DaemonSet event: MODIFIED
  May 13 19:40:25.755: INFO: Observed &DaemonSet event: MODIFIED
  May 13 19:40:25.755: INFO: Observed daemon set daemon-set in namespace daemonsets-327 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 13 19:40:25.755: INFO: Observed &DaemonSet event: MODIFIED
  May 13 19:40:25.756: INFO: Found daemon set daemon-set in namespace daemonsets-327 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  May 13 19:40:25.756: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/13/23 19:40:25.76
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-327, will wait for the garbage collector to delete the pods @ 05/13/23 19:40:25.76
  May 13 19:40:25.815: INFO: Deleting DaemonSet.extensions daemon-set took: 2.869589ms
  May 13 19:40:25.915: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.411738ms
  E0513 19:40:26.396628      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:27.397535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:40:28.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:40:28.118: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 13 19:40:28.120: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46089"},"items":null}

  May 13 19:40:28.122: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46089"},"items":null}

  May 13 19:40:28.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-327" for this suite. @ 05/13/23 19:40:28.129
• [4.443 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/13/23 19:40:28.133
  May 13 19:40:28.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename kubelet-test @ 05/13/23 19:40:28.134
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:40:28.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:40:28.148
  STEP: Waiting for pod completion @ 05/13/23 19:40:28.156
  E0513 19:40:28.398470      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:29.399004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:30.400101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:31.400177      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:40:32.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1134" for this suite. @ 05/13/23 19:40:32.17
• [4.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/13/23 19:40:32.188
  May 13 19:40:32.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sched-pred @ 05/13/23 19:40:32.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:40:32.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:40:32.199
  May 13 19:40:32.201: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 13 19:40:32.204: INFO: Waiting for terminating namespaces to be deleted...
  May 13 19:40:32.206: INFO: 
  Logging pods the apiserver thinks is on node worker00 before test
  May 13 19:40:32.211: INFO: etcd-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container etcd ready: true, restart count 0
  May 13 19:40:32.211: INFO: gobetween-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container gobetween ready: true, restart count 0
  May 13 19:40:32.211: INFO: kube-apiserver-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container kube-apiserver ready: true, restart count 0
  May 13 19:40:32.211: INFO: kube-controller-manager-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container kube-controller-manager ready: true, restart count 0
  May 13 19:40:32.211: INFO: kube-proxy-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container kube-proxy ready: true, restart count 0
  May 13 19:40:32.211: INFO: kube-scheduler-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container kube-scheduler ready: true, restart count 0
  May 13 19:40:32.211: INFO: agnhost-host-aliasesde9c978a-8bf9-4b96-804a-a11d44cafb49 from kubelet-test-1134 started at 2023-05-13 19:40:28 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container agnhost-container ready: false, restart count 0
  May 13 19:40:32.211: INFO: calico-node-xsr47 from networking started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container calico-node ready: true, restart count 0
  May 13 19:40:32.211: INFO: metallb-speaker-v7flx from networking started at 2023-05-13 18:18:58 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container speaker ready: true, restart count 0
  May 13 19:40:32.211: INFO: sonobuoy from sonobuoy started at 2023-05-13 18:11:13 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 13 19:40:32.211: INFO: sonobuoy-e2e-job-dc09634ed87e4d2d from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container e2e ready: true, restart count 0
  May 13 19:40:32.211: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 19:40:32.211: INFO: sonobuoy-systemd-logs-daemon-set-0619921c091243c5-6tddn from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 19:40:32.211: INFO: 	Container systemd-logs ready: true, restart count 0
  May 13 19:40:32.211: INFO: ceph-csi-cephfs-nodeplugin-rhhl2 from storage started at 2023-05-13 18:18:58 +0000 UTC (3 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:40:32.211: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:40:32.211: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:40:32.211: INFO: ceph-csi-rbd-nodeplugin-t44kh from storage started at 2023-05-13 18:18:58 +0000 UTC (3 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:40:32.211: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:40:32.211: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:40:32.211: INFO: ceph-mon-worker00-79cd8cd599-5skt9 from storage started at 2023-05-13 18:18:58 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.211: INFO: 	Container ceph-mon ready: true, restart count 0
  May 13 19:40:32.211: INFO: 
  Logging pods the apiserver thinks is on node worker01 before test
  May 13 19:40:32.220: INFO: coredns-5bf6b5d445-frtlk from kube-system started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.220: INFO: 	Container coredns ready: true, restart count 0
  May 13 19:40:32.221: INFO: coredns-5bf6b5d445-wv86k from kube-system started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.221: INFO: 	Container coredns ready: true, restart count 0
  May 13 19:40:32.221: INFO: gobetween-worker01 from kube-system started at 2023-05-13 18:05:13 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.221: INFO: 	Container gobetween ready: true, restart count 0
  May 13 19:40:32.221: INFO: kube-proxy-worker01 from kube-system started at 2023-05-13 18:05:13 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.221: INFO: 	Container kube-proxy ready: true, restart count 0
  May 13 19:40:32.221: INFO: kubernetes-dashboard-67c9dc7898-qhknx from kube-system started at 2023-05-13 18:06:09 +0000 UTC (2 container statuses recorded)
  May 13 19:40:32.222: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May 13 19:40:32.222: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May 13 19:40:32.222: INFO: calico-kube-controllers-849bffbf8c-mh9pf from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.222: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 13 19:40:32.222: INFO: calico-node-bsrzk from networking started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.222: INFO: 	Container calico-node ready: true, restart count 0
  May 13 19:40:32.222: INFO: calico-typha-7f5df955fb-2bg4l from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.222: INFO: 	Container calico-typha ready: true, restart count 0
  May 13 19:40:32.222: INFO: metallb-controller-64cc69b5c4-wd4dh from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.222: INFO: 	Container controller ready: true, restart count 0
  May 13 19:40:32.222: INFO: metallb-speaker-wntmv from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.222: INFO: 	Container speaker ready: true, restart count 0
  May 13 19:40:32.222: INFO: sonobuoy-systemd-logs-daemon-set-0619921c091243c5-wlgqk from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 19:40:32.222: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 19:40:32.222: INFO: 	Container systemd-logs ready: true, restart count 0
  May 13 19:40:32.222: INFO: ceph-csi-cephfs-nodeplugin-c7nqc from storage started at 2023-05-13 18:05:58 +0000 UTC (3 container statuses recorded)
  May 13 19:40:32.222: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:40:32.222: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:40:32.222: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:40:32.222: INFO: ceph-csi-cephfs-provisioner-b795fb565-6gcfc from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 19:40:32.222: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:40:32.222: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:40:32.222: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:40:32.222: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:40:32.223: INFO: ceph-csi-cephfs-provisioner-b795fb565-8656n from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 19:40:32.223: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:40:32.223: INFO: ceph-csi-cephfs-provisioner-b795fb565-m982k from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 19:40:32.223: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:40:32.223: INFO: ceph-csi-rbd-nodeplugin-xgqs8 from storage started at 2023-05-13 18:05:58 +0000 UTC (3 container statuses recorded)
  May 13 19:40:32.223: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:40:32.223: INFO: ceph-csi-rbd-provisioner-674549bd88-lkl8l from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 19:40:32.223: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:40:32.223: INFO: ceph-csi-rbd-provisioner-674549bd88-n9pkj from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 19:40:32.223: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:40:32.223: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:40:32.223: INFO: ceph-csi-rbd-provisioner-674549bd88-nk4kp from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 19:40:32.224: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 19:40:32.224: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:40:32.224: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:40:32.224: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 19:40:32.224: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:40:32.224: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:40:32.224: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:40:32.224: INFO: ceph-mds-worker01-9b6db8bb9-q6w24 from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.224: INFO: 	Container ceph-mds ready: true, restart count 0
  May 13 19:40:32.224: INFO: ceph-mgr-worker01-5bc7cddf9f-pmtvr from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.224: INFO: 	Container ceph-mgr ready: true, restart count 0
  May 13 19:40:32.224: INFO: ceph-osd-worker01-594bc8755f-r5bdz from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.224: INFO: 	Container ceph-osd ready: true, restart count 0
  May 13 19:40:32.224: INFO: ceph-rgw-worker01-64f8846fdb-jzw8c from storage started at 2023-05-13 18:05:59 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.224: INFO: 	Container ceph-rgw ready: true, restart count 0
  May 13 19:40:32.224: INFO: ceph-setup-q5j8d from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.224: INFO: 	Container ceph ready: false, restart count 0
  May 13 19:40:32.224: INFO: snapshot-controller-7dd49c567-9rfhc from storage started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.225: INFO: 	Container snapshot-controller ready: true, restart count 0
  May 13 19:40:32.225: INFO: snapshot-controller-7dd49c567-h5tj7 from storage started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:40:32.225: INFO: 	Container snapshot-controller ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/13/23 19:40:32.225
  E0513 19:40:32.400803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:33.402324      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/13/23 19:40:34.238
  STEP: Trying to apply a random label on the found node. @ 05/13/23 19:40:34.257
  STEP: verifying the node has the label kubernetes.io/e2e-60e4019d-364d-4ca1-a3bd-a26136daf92c 95 @ 05/13/23 19:40:34.267
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/13/23 19:40:34.272
  E0513 19:40:34.402613      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:35.403554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.58.100 on the node which pod4 resides and expect not scheduled @ 05/13/23 19:40:36.285
  E0513 19:40:36.403754      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:37.403950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:38.405041      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:39.405439      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:40.406533      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:41.406872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:42.407868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:43.408630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:44.408680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:45.409799      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:46.410627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:47.410940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:48.410983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:49.411596      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:50.412564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:51.414397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:52.416504      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:53.416512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:54.417638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:55.417843      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:56.418009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:57.418621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:58.419656      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:40:59.420151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:00.420649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:01.421367      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:02.421488      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:03.422805      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:04.423068      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:05.423145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:06.424072      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:07.424285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:08.424524      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:09.425193      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:10.426174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:11.426560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:12.427095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:13.431160      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:14.432449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:15.432633      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:16.433706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:17.434471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:18.436162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:19.436324      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:20.437304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:21.437689      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:22.438351      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:23.439496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:24.440508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:25.441366      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:26.442204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:27.442437      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:28.445883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:29.446374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:30.447834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:31.448225      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:32.449464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:33.449615      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:34.450098      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:35.450293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:36.450571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:37.453544      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:38.454304      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:39.454684      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:40.455723      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:41.456510      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:42.457247      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:43.457693      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:44.458617      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:45.459412      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:46.460221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:47.461352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:48.461783      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:49.462442      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:50.463950      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:51.464368      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:52.465005      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:53.465210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:54.466495      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:55.466923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:56.466860      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:57.467121      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:58.467848      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:41:59.469702      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:00.470091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:01.470720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:02.471132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:03.471809      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:04.471869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:05.472225      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:06.472365      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:07.472819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:08.475377      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:09.476281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:10.477869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:11.477789      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:12.479791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:13.492253      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:14.482588      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:15.482887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:16.483920      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:17.484516      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:18.484632      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:19.485009      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:20.485185      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:21.491700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:22.491669      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:23.491668      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:24.492240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:25.492846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:26.492711      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:27.493049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:28.493756      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:29.494143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:30.494700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:31.500986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:32.501191      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:33.502554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:34.502789      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:35.503149      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:36.504384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:37.504843      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:38.505153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:39.505808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:40.506282      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:41.508021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:42.508869      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:43.508986      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:44.509669      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:45.511802      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:46.511527      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:47.512093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:48.513181      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:49.513477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:50.513680      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:51.514084      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:52.514327      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:53.515080      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:54.515595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:55.516049      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:56.516564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:57.517420      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:58.517672      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:42:59.518481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:00.519428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:01.520520      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:02.520936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:03.521333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:04.522450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:05.523173      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:06.523267      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:07.523642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:08.524731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:09.537326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:10.536915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:11.538593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:12.539972      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:13.540511      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:14.540893      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:15.541446      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:16.542983      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:17.546465      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:18.545870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:19.546104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:20.546786      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:21.547152      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:22.547826      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:23.549692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:24.550307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:25.551498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:26.551698      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:27.553073      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:28.554292      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:29.554914      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:30.555085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:31.555639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:32.556307      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:33.556784      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:34.557372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:35.557415      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:36.557535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:37.558209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:38.558397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:39.558875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:40.559370      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:41.560424      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:42.560756      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:43.562543      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:44.562742      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:45.564410      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:46.565396      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:47.570053      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:48.570501      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:49.572303      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:50.572751      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:51.573457      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:52.573846      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:53.574024      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:54.575853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:55.578595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:56.579770      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:57.580294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:58.580493      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:43:59.580828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:00.581012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:01.582142      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:02.582517      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:03.583253      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:04.584531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:05.585601      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:06.587360      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:07.587646      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:08.588595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:09.588861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:10.588873      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:11.589977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:12.590494      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:13.590719      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:14.590862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:15.591563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:16.591614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:17.599592      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:18.599899      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:19.599996      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:20.600507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:21.600569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:22.601745      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:23.602115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:24.602316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:25.602499      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:26.602558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:27.602595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:28.604191      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:29.604285      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:30.604267      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:31.604817      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:32.604945      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:33.605461      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:34.605552      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:35.606618      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:36.607423      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:37.610937      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:38.611604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:39.612347      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:40.613004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:41.613621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:42.614262      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:43.614785      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:44.614823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:45.615837      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:46.615790      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:47.615953      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:48.617250      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:49.617548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:50.630652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:51.631358      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:52.632201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:53.632466      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:54.632835      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:55.633328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:56.634780      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:57.634890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:58.637947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:44:59.638090      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:00.639883      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:01.639892      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:02.641222      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:03.642074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:04.642170      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:05.643224      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:06.644523      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:07.645597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:08.646033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:09.646297      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:10.647311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:11.647536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:12.648604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:13.649843      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:14.649947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:15.650283      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:16.650538      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:17.650651      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:18.650872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:19.651284      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:20.651700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:21.652338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:22.652683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:23.652992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:24.657649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:25.657652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:26.658456      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:27.660487      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:28.663345      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:29.664115      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:30.667316      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:31.676081      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:32.678561      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:33.682757      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:34.683449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:35.684750      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-60e4019d-364d-4ca1-a3bd-a26136daf92c off the node worker00 @ 05/13/23 19:45:36.318
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-60e4019d-364d-4ca1-a3bd-a26136daf92c @ 05/13/23 19:45:36.338
  May 13 19:45:36.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6863" for this suite. @ 05/13/23 19:45:36.365
• [304.187 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 05/13/23 19:45:36.375
  May 13 19:45:36.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename events @ 05/13/23 19:45:36.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:45:36.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:45:36.441
  STEP: creating a test event @ 05/13/23 19:45:36.446
  STEP: listing all events in all namespaces @ 05/13/23 19:45:36.455
  STEP: patching the test event @ 05/13/23 19:45:36.463
  STEP: fetching the test event @ 05/13/23 19:45:36.471
  STEP: updating the test event @ 05/13/23 19:45:36.473
  STEP: getting the test event @ 05/13/23 19:45:36.485
  STEP: deleting the test event @ 05/13/23 19:45:36.49
  STEP: listing all events in all namespaces @ 05/13/23 19:45:36.501
  May 13 19:45:36.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-984" for this suite. @ 05/13/23 19:45:36.512
• [0.146 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 05/13/23 19:45:36.526
  May 13 19:45:36.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename field-validation @ 05/13/23 19:45:36.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:45:36.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:45:36.56
  May 13 19:45:36.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:45:36.685237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:37.692870      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:38.693281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0513 19:45:39.134361      21 warnings.go:70] unknown field "alpha"
  W0513 19:45:39.135085      21 warnings.go:70] unknown field "beta"
  W0513 19:45:39.135288      21 warnings.go:70] unknown field "delta"
  W0513 19:45:39.135419      21 warnings.go:70] unknown field "epsilon"
  W0513 19:45:39.135927      21 warnings.go:70] unknown field "gamma"
  May 13 19:45:39.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9423" for this suite. @ 05/13/23 19:45:39.194
• [2.682 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/13/23 19:45:39.215
  May 13 19:45:39.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename watch @ 05/13/23 19:45:39.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:45:39.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:45:39.255
  STEP: creating a watch on configmaps @ 05/13/23 19:45:39.262
  STEP: creating a new configmap @ 05/13/23 19:45:39.267
  STEP: modifying the configmap once @ 05/13/23 19:45:39.277
  STEP: closing the watch once it receives two notifications @ 05/13/23 19:45:39.291
  May 13 19:45:39.291: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4504  593c21d2-52c0-43ac-abf9-b79eae76fe32 47295 0 2023-05-13 19:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-13 19:45:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 19:45:39.292: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4504  593c21d2-52c0-43ac-abf9-b79eae76fe32 47296 0 2023-05-13 19:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-13 19:45:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/13/23 19:45:39.293
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/13/23 19:45:39.304
  STEP: deleting the configmap @ 05/13/23 19:45:39.305
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/13/23 19:45:39.313
  May 13 19:45:39.314: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4504  593c21d2-52c0-43ac-abf9-b79eae76fe32 47297 0 2023-05-13 19:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-13 19:45:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 19:45:39.314: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4504  593c21d2-52c0-43ac-abf9-b79eae76fe32 47298 0 2023-05-13 19:45:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-13 19:45:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 19:45:39.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4504" for this suite. @ 05/13/23 19:45:39.322
• [0.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 05/13/23 19:45:39.34
  May 13 19:45:39.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 19:45:39.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:45:39.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:45:39.369
  STEP: Creating a pod to test downward api env vars @ 05/13/23 19:45:39.376
  E0513 19:45:39.695083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:40.695932      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:41.696001      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:42.696280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:45:43.405
  May 13 19:45:43.407: INFO: Trying to get logs from node worker00 pod downward-api-cf31e2a5-e76a-43a5-b5c5-84ccef988b55 container dapi-container: <nil>
  STEP: delete the pod @ 05/13/23 19:45:43.415
  May 13 19:45:43.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4518" for this suite. @ 05/13/23 19:45:43.436
• [4.099 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 05/13/23 19:45:43.439
  May 13 19:45:43.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename daemonsets @ 05/13/23 19:45:43.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:45:43.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:45:43.456
  May 13 19:45:43.469: INFO: Create a RollingUpdate DaemonSet
  May 13 19:45:43.472: INFO: Check that daemon pods launch on every node of the cluster
  May 13 19:45:43.478: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:45:43.478: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:45:43.698341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:45:44.482: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:45:44.482: INFO: Node worker00 is running 0 daemon pod, expected 1
  E0513 19:45:44.699107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:45:45.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 13 19:45:45.484: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  May 13 19:45:45.484: INFO: Update the DaemonSet to trigger a rollout
  May 13 19:45:45.490: INFO: Updating DaemonSet daemon-set
  E0513 19:45:45.700015      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:46.700762      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:45:47.502: INFO: Roll back the DaemonSet before rollout is complete
  May 13 19:45:47.507: INFO: Updating DaemonSet daemon-set
  May 13 19:45:47.507: INFO: Make sure DaemonSet rollback is complete
  May 13 19:45:47.512: INFO: Wrong image for pod: daemon-set-vzk7c. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  May 13 19:45:47.512: INFO: Pod daemon-set-vzk7c is not available
  E0513 19:45:47.701085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:48.701174      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:49.701209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:50.701941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:51.702574      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:45:52.525: INFO: Pod daemon-set-tq7cn is not available
  STEP: Deleting DaemonSet "daemon-set" @ 05/13/23 19:45:52.531
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8864, will wait for the garbage collector to delete the pods @ 05/13/23 19:45:52.531
  May 13 19:45:52.590: INFO: Deleting DaemonSet.extensions daemon-set took: 4.59213ms
  May 13 19:45:52.690: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.303375ms
  E0513 19:45:52.703038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:53.703037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:45:54.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:45:54.393: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 13 19:45:54.395: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47535"},"items":null}

  May 13 19:45:54.397: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47535"},"items":null}

  May 13 19:45:54.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8864" for this suite. @ 05/13/23 19:45:54.403
• [10.967 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 05/13/23 19:45:54.409
  May 13 19:45:54.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replicaset @ 05/13/23 19:45:54.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:45:54.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:45:54.423
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/13/23 19:45:54.425
  May 13 19:45:54.431: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0513 19:45:54.703861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:55.706785      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:56.706571      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:57.707021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:45:58.707606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:45:59.437: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/13/23 19:45:59.437
  STEP: getting scale subresource @ 05/13/23 19:45:59.438
  STEP: updating a scale subresource @ 05/13/23 19:45:59.456
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/13/23 19:45:59.467
  STEP: Patch a scale subresource @ 05/13/23 19:45:59.477
  May 13 19:45:59.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-243" for this suite. @ 05/13/23 19:45:59.518
• [5.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 05/13/23 19:45:59.538
  May 13 19:45:59.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pod-network-test @ 05/13/23 19:45:59.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:45:59.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:45:59.593
  STEP: Performing setup for networking test in namespace pod-network-test-7354 @ 05/13/23 19:45:59.6
  STEP: creating a selector @ 05/13/23 19:45:59.6
  STEP: Creating the service pods in kubernetes @ 05/13/23 19:45:59.601
  May 13 19:45:59.601: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0513 19:45:59.708440      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:00.708794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:01.708820      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:02.709042      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:03.709128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:04.709417      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:05.709992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:06.710512      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:07.710720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:08.711759      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:09.714230      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:10.714481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:11.715976      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:12.716480      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:13.716837      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:14.720703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:15.721614      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:16.722209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:17.724490      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:18.725206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:19.726500      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:20.727082      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/13/23 19:46:21.724
  E0513 19:46:21.727663      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:22.731206      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:23.731138      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:46:23.765: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 13 19:46:23.775: INFO: Going to poll 10.200.131.149 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  May 13 19:46:23.778: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.131.149 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7354 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:46:23.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:46:23.778: INFO: ExecWithOptions: Clientset creation
  May 13 19:46:23.778: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/pod-network-test-7354/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.200.131.149+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0513 19:46:24.731755      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:46:24.900: INFO: Found all 1 expected endpoints: [netserver-0]
  May 13 19:46:24.900: INFO: Going to poll 10.200.5.28 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  May 13 19:46:24.903: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.5.28 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7354 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 13 19:46:24.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  May 13 19:46:24.914: INFO: ExecWithOptions: Clientset creation
  May 13 19:46:24.914: INFO: ExecWithOptions: execute(POST https://10.32.0.1:443/api/v1/namespaces/pod-network-test-7354/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.200.5.28+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0513 19:46:25.733483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:46:26.011: INFO: Found all 1 expected endpoints: [netserver-1]
  May 13 19:46:26.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7354" for this suite. @ 05/13/23 19:46:26.023
• [26.515 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 05/13/23 19:46:26.053
  May 13 19:46:26.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:46:26.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:46:26.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:46:26.093
  STEP: Creating configMap with name projected-configmap-test-volume-fd23e681-eeb1-4a6e-aa0b-d1e13657b2cb @ 05/13/23 19:46:26.099
  STEP: Creating a pod to test consume configMaps @ 05/13/23 19:46:26.11
  E0513 19:46:26.733821      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:27.748916      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:46:28.132
  May 13 19:46:28.133: INFO: Trying to get logs from node worker00 pod pod-projected-configmaps-a791fa40-e830-49b8-88bf-50034e1c179a container agnhost-container: <nil>
  STEP: delete the pod @ 05/13/23 19:46:28.136
  May 13 19:46:28.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6164" for this suite. @ 05/13/23 19:46:28.147
• [2.097 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/13/23 19:46:28.151
  May 13 19:46:28.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 19:46:28.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:46:28.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:46:28.164
  STEP: Create set of pods @ 05/13/23 19:46:28.165
  May 13 19:46:28.169: INFO: created test-pod-1
  May 13 19:46:28.175: INFO: created test-pod-2
  May 13 19:46:28.186: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/13/23 19:46:28.186
  E0513 19:46:28.754887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:29.755172      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 05/13/23 19:46:30.226
  May 13 19:46:30.229: INFO: Pod quantity 3 is different from expected quantity 0
  E0513 19:46:30.759372      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:46:31.233: INFO: Pod quantity 3 is different from expected quantity 0
  E0513 19:46:31.761268      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:46:32.233: INFO: Pod quantity 3 is different from expected quantity 0
  E0513 19:46:32.761712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:46:33.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3694" for this suite. @ 05/13/23 19:46:33.234
• [5.087 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 05/13/23 19:46:33.239
  May 13 19:46:33.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 19:46:33.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:46:33.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:46:33.252
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:46:33.254
  E0513 19:46:33.762413      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:34.762596      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:46:35.267
  May 13 19:46:35.268: INFO: Trying to get logs from node worker00 pod downwardapi-volume-0ff4b54c-1110-484a-a8a1-d2a30d35c5bb container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:46:35.272
  May 13 19:46:35.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9620" for this suite. @ 05/13/23 19:46:35.285
• [2.049 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 05/13/23 19:46:35.288
  May 13 19:46:35.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename statefulset @ 05/13/23 19:46:35.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:46:35.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:46:35.299
  STEP: Creating service test in namespace statefulset-2685 @ 05/13/23 19:46:35.301
  STEP: Creating statefulset ss in namespace statefulset-2685 @ 05/13/23 19:46:35.309
  May 13 19:46:35.319: INFO: Found 0 stateful pods, waiting for 1
  E0513 19:46:35.763590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:36.763947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:37.764156      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:38.764235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:39.768111      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:40.768368      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:41.768364      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:42.768567      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:43.770434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:44.770722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:46:45.322: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/13/23 19:46:45.325
  STEP: Getting /status @ 05/13/23 19:46:45.338
  May 13 19:46:45.342: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/13/23 19:46:45.342
  May 13 19:46:45.347: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/13/23 19:46:45.347
  May 13 19:46:45.349: INFO: Observed &StatefulSet event: ADDED
  May 13 19:46:45.349: INFO: Found Statefulset ss in namespace statefulset-2685 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 13 19:46:45.349: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/13/23 19:46:45.349
  May 13 19:46:45.349: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 13 19:46:45.353: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/13/23 19:46:45.353
  May 13 19:46:45.354: INFO: Observed &StatefulSet event: ADDED
  May 13 19:46:45.354: INFO: Deleting all statefulset in ns statefulset-2685
  May 13 19:46:45.356: INFO: Scaling statefulset ss to 0
  E0513 19:46:45.771047      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:46.771137      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:47.771385      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:48.771520      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:49.772558      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:50.774273      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:51.778462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:52.778607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:53.779872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:54.780658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:46:55.365: INFO: Waiting for statefulset status.replicas updated to 0
  May 13 19:46:55.368: INFO: Deleting statefulset ss
  May 13 19:46:55.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2685" for this suite. @ 05/13/23 19:46:55.389
• [20.103 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 05/13/23 19:46:55.393
  May 13 19:46:55.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename security-context-test @ 05/13/23 19:46:55.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:46:55.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:46:55.409
  E0513 19:46:55.781210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:56.782199      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:57.782483      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:46:58.782564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:46:59.430: INFO: Got logs for pod "busybox-privileged-false-4d56133a-7c61-4115-a42b-34eedadd10b5": "ip: RTNETLINK answers: Operation not permitted\n"
  May 13 19:46:59.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-1955" for this suite. @ 05/13/23 19:46:59.437
• [4.062 seconds]
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/13/23 19:46:59.455
  May 13 19:46:59.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/13/23 19:46:59.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:46:59.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:46:59.469
  STEP: Creating 50 configmaps @ 05/13/23 19:46:59.473
  STEP: Creating RC which spawns configmap-volume pods @ 05/13/23 19:46:59.712
  E0513 19:46:59.786214      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:46:59.844: INFO: Pod name wrapped-volume-race-fb7c5da5-3c29-4f14-a619-affa0b897d7a: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/13/23 19:46:59.844
  E0513 19:47:00.786540      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:01.787165      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 05/13/23 19:47:01.895
  May 13 19:47:01.903: INFO: Pod name wrapped-volume-race-7aa019ca-eec7-44e0-8739-df497d4275dd: Found 0 pods out of 5
  E0513 19:47:02.788118      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:03.788694      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:04.800925      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:05.801004      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:06.801707      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:47:06.911: INFO: Pod name wrapped-volume-race-7aa019ca-eec7-44e0-8739-df497d4275dd: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/13/23 19:47:06.911
  STEP: Creating RC which spawns configmap-volume pods @ 05/13/23 19:47:06.924
  May 13 19:47:06.945: INFO: Pod name wrapped-volume-race-70f843ca-fb30-419b-b405-5d640fabc3f9: Found 0 pods out of 5
  E0513 19:47:07.802380      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:08.802977      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:09.803281      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:10.803620      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:11.803784      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:47:11.953: INFO: Pod name wrapped-volume-race-70f843ca-fb30-419b-b405-5d640fabc3f9: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/13/23 19:47:11.953
  May 13 19:47:11.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-70f843ca-fb30-419b-b405-5d640fabc3f9 in namespace emptydir-wrapper-4820, will wait for the garbage collector to delete the pods @ 05/13/23 19:47:11.965
  May 13 19:47:12.023: INFO: Deleting ReplicationController wrapped-volume-race-70f843ca-fb30-419b-b405-5d640fabc3f9 took: 5.279225ms
  May 13 19:47:12.124: INFO: Terminating ReplicationController wrapped-volume-race-70f843ca-fb30-419b-b405-5d640fabc3f9 pods took: 100.713139ms
  E0513 19:47:12.806580      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-7aa019ca-eec7-44e0-8739-df497d4275dd in namespace emptydir-wrapper-4820, will wait for the garbage collector to delete the pods @ 05/13/23 19:47:13.426
  May 13 19:47:13.488: INFO: Deleting ReplicationController wrapped-volume-race-7aa019ca-eec7-44e0-8739-df497d4275dd took: 8.8856ms
  May 13 19:47:13.590: INFO: Terminating ReplicationController wrapped-volume-race-7aa019ca-eec7-44e0-8739-df497d4275dd pods took: 101.498101ms
  E0513 19:47:13.806699      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:14.807808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-fb7c5da5-3c29-4f14-a619-affa0b897d7a in namespace emptydir-wrapper-4820, will wait for the garbage collector to delete the pods @ 05/13/23 19:47:14.991
  May 13 19:47:15.049: INFO: Deleting ReplicationController wrapped-volume-race-fb7c5da5-3c29-4f14-a619-affa0b897d7a took: 5.544267ms
  May 13 19:47:15.150: INFO: Terminating ReplicationController wrapped-volume-race-fb7c5da5-3c29-4f14-a619-affa0b897d7a pods took: 100.305933ms
  E0513 19:47:15.809007      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:16.809639      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 05/13/23 19:47:16.951
  STEP: Destroying namespace "emptydir-wrapper-4820" for this suite. @ 05/13/23 19:47:17.127
• [17.678 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 05/13/23 19:47:17.134
  May 13 19:47:17.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename namespaces @ 05/13/23 19:47:17.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:17.155
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:17.158
  STEP: Creating a test namespace @ 05/13/23 19:47:17.161
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:17.175
  STEP: Creating a service in the namespace @ 05/13/23 19:47:17.183
  STEP: Deleting the namespace @ 05/13/23 19:47:17.194
  STEP: Waiting for the namespace to be removed. @ 05/13/23 19:47:17.202
  E0513 19:47:17.809853      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:18.812689      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:19.813071      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:20.813110      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:21.813473      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:22.813956      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/13/23 19:47:23.205
  STEP: Verifying there is no service in the namespace @ 05/13/23 19:47:23.21
  May 13 19:47:23.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3357" for this suite. @ 05/13/23 19:47:23.216
  STEP: Destroying namespace "nsdeletetest-8135" for this suite. @ 05/13/23 19:47:23.22
  May 13 19:47:23.222: INFO: Namespace nsdeletetest-8135 was already deleted
  STEP: Destroying namespace "nsdeletetest-620" for this suite. @ 05/13/23 19:47:23.222
• [6.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 05/13/23 19:47:23.227
  May 13 19:47:23.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename endpointslice @ 05/13/23 19:47:23.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:23.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:23.24
  STEP: getting /apis @ 05/13/23 19:47:23.243
  STEP: getting /apis/discovery.k8s.io @ 05/13/23 19:47:23.245
  STEP: getting /apis/discovery.k8s.iov1 @ 05/13/23 19:47:23.247
  STEP: creating @ 05/13/23 19:47:23.247
  STEP: getting @ 05/13/23 19:47:23.259
  STEP: listing @ 05/13/23 19:47:23.262
  STEP: watching @ 05/13/23 19:47:23.265
  May 13 19:47:23.265: INFO: starting watch
  STEP: cluster-wide listing @ 05/13/23 19:47:23.266
  STEP: cluster-wide watching @ 05/13/23 19:47:23.268
  May 13 19:47:23.268: INFO: starting watch
  STEP: patching @ 05/13/23 19:47:23.269
  STEP: updating @ 05/13/23 19:47:23.273
  May 13 19:47:23.279: INFO: waiting for watch events with expected annotations
  May 13 19:47:23.279: INFO: saw patched and updated annotations
  STEP: deleting @ 05/13/23 19:47:23.279
  STEP: deleting a collection @ 05/13/23 19:47:23.286
  May 13 19:47:23.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2699" for this suite. @ 05/13/23 19:47:23.296
• [0.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 05/13/23 19:47:23.3
  May 13 19:47:23.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/13/23 19:47:23.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:23.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:23.313
  STEP: creating @ 05/13/23 19:47:23.316
  STEP: getting @ 05/13/23 19:47:23.329
  STEP: listing in namespace @ 05/13/23 19:47:23.333
  STEP: patching @ 05/13/23 19:47:23.341
  STEP: deleting @ 05/13/23 19:47:23.348
  May 13 19:47:23.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-4956" for this suite. @ 05/13/23 19:47:23.359
• [0.062 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 05/13/23 19:47:23.362
  May 13 19:47:23.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename security-context @ 05/13/23 19:47:23.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:23.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:23.374
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/13/23 19:47:23.375
  E0513 19:47:23.814266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:24.815960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:25.817349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:26.817553      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:47:27.406
  May 13 19:47:27.412: INFO: Trying to get logs from node worker00 pod security-context-722231d7-0786-46be-a0eb-13c1fd3dae6c container test-container: <nil>
  STEP: delete the pod @ 05/13/23 19:47:27.425
  May 13 19:47:27.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-7535" for this suite. @ 05/13/23 19:47:27.448
• [4.094 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/13/23 19:47:27.461
  May 13 19:47:27.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename pods @ 05/13/23 19:47:27.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:27.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:27.494
  STEP: creating the pod @ 05/13/23 19:47:27.5
  STEP: submitting the pod to kubernetes @ 05/13/23 19:47:27.5
  STEP: verifying QOS class is set on the pod @ 05/13/23 19:47:27.514
  May 13 19:47:27.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-221" for this suite. @ 05/13/23 19:47:27.527
• [0.079 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 05/13/23 19:47:27.539
  May 13 19:47:27.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename field-validation @ 05/13/23 19:47:27.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:27.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:27.569
  STEP: apply creating a deployment @ 05/13/23 19:47:27.572
  May 13 19:47:27.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6067" for this suite. @ 05/13/23 19:47:27.594
• [0.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 05/13/23 19:47:27.605
  May 13 19:47:27.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename downward-api @ 05/13/23 19:47:27.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:27.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:27.632
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:47:27.637
  E0513 19:47:27.818609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:28.819521      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:29.820171      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:30.820827      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:47:31.666
  May 13 19:47:31.670: INFO: Trying to get logs from node worker00 pod downwardapi-volume-f436e859-8822-498f-9509-f165d842a727 container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:47:31.679
  May 13 19:47:31.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9191" for this suite. @ 05/13/23 19:47:31.712
• [4.115 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 05/13/23 19:47:31.721
  May 13 19:47:31.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/13/23 19:47:31.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:31.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:31.75
  STEP: getting /apis @ 05/13/23 19:47:31.755
  STEP: getting /apis/storage.k8s.io @ 05/13/23 19:47:31.762
  STEP: getting /apis/storage.k8s.io/v1 @ 05/13/23 19:47:31.764
  STEP: creating @ 05/13/23 19:47:31.767
  STEP: watching @ 05/13/23 19:47:31.783
  May 13 19:47:31.783: INFO: starting watch
  STEP: getting @ 05/13/23 19:47:31.796
  STEP: listing in namespace @ 05/13/23 19:47:31.8
  STEP: listing across namespaces @ 05/13/23 19:47:31.804
  STEP: patching @ 05/13/23 19:47:31.809
  STEP: updating @ 05/13/23 19:47:31.827
  E0513 19:47:31.843000      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:47:31.904: INFO: waiting for watch events with expected annotations in namespace
  May 13 19:47:31.905: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/13/23 19:47:31.905
  STEP: deleting a collection @ 05/13/23 19:47:31.929
  May 13 19:47:31.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-7876" for this suite. @ 05/13/23 19:47:31.949
• [0.237 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 05/13/23 19:47:31.958
  May 13 19:47:31.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replicaset @ 05/13/23 19:47:31.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:31.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:31.985
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/13/23 19:47:31.992
  E0513 19:47:32.844101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:33.845710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 05/13/23 19:47:34.024
  STEP: Then the orphan pod is adopted @ 05/13/23 19:47:34.046
  E0513 19:47:34.846884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 05/13/23 19:47:35.063
  May 13 19:47:35.067: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/13/23 19:47:35.089
  E0513 19:47:35.850942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:47:36.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3518" for this suite. @ 05/13/23 19:47:36.113
• [4.167 seconds]
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 05/13/23 19:47:36.125
  May 13 19:47:36.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-probe @ 05/13/23 19:47:36.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:36.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:36.154
  E0513 19:47:36.855245      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:37.856159      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:38.858973      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:39.859683      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:40.860464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:41.861235      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:42.861340      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:43.861669      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:44.862246      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:45.862465      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:46.863087      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:47.863722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:48.864208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:49.865422      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:50.868619      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:51.868970      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:52.870033      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:53.871236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:54.871451      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:55.873875      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:56.879868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:57.880320      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:47:58.235: INFO: Container started at 2023-05-13 19:47:37 +0000 UTC, pod became ready at 2023-05-13 19:47:56 +0000 UTC
  May 13 19:47:58.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-5041" for this suite. @ 05/13/23 19:47:58.242
• [22.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 05/13/23 19:47:58.269
  May 13 19:47:58.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename replicaset @ 05/13/23 19:47:58.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:47:58.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:47:58.297
  May 13 19:47:58.312: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0513 19:47:58.883074      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:47:59.883582      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:00.883747      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:01.884006      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:02.884259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:03.317: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/13/23 19:48:03.317
  STEP: Scaling up "test-rs" replicaset  @ 05/13/23 19:48:03.317
  May 13 19:48:03.332: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/13/23 19:48:03.332
  W0513 19:48:03.348126      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 13 19:48:03.356: INFO: observed ReplicaSet test-rs in namespace replicaset-1723 with ReadyReplicas 1, AvailableReplicas 1
  May 13 19:48:03.372: INFO: observed ReplicaSet test-rs in namespace replicaset-1723 with ReadyReplicas 1, AvailableReplicas 1
  May 13 19:48:03.400: INFO: observed ReplicaSet test-rs in namespace replicaset-1723 with ReadyReplicas 1, AvailableReplicas 1
  May 13 19:48:03.432: INFO: observed ReplicaSet test-rs in namespace replicaset-1723 with ReadyReplicas 1, AvailableReplicas 1
  E0513 19:48:03.884784      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:04.885548      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:05.137: INFO: observed ReplicaSet test-rs in namespace replicaset-1723 with ReadyReplicas 2, AvailableReplicas 2
  May 13 19:48:05.491: INFO: observed Replicaset test-rs in namespace replicaset-1723 with ReadyReplicas 3 found true
  May 13 19:48:05.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1723" for this suite. @ 05/13/23 19:48:05.497
• [7.237 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 05/13/23 19:48:05.508
  May 13 19:48:05.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/13/23 19:48:05.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:05.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:05.542
  STEP: create the container to handle the HTTPGet hook request. @ 05/13/23 19:48:05.557
  E0513 19:48:05.885836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:06.886128      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/13/23 19:48:07.634
  E0513 19:48:07.886260      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:08.888259      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/13/23 19:48:09.671
  STEP: delete the pod with lifecycle hook @ 05/13/23 19:48:09.702
  E0513 19:48:09.888989      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:10.892313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:11.893318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:12.893913      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:13.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4392" for this suite. @ 05/13/23 19:48:13.752
• [8.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 05/13/23 19:48:13.77
  May 13 19:48:13.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename disruption @ 05/13/23 19:48:13.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:13.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:13.787
  STEP: Waiting for the pdb to be processed @ 05/13/23 19:48:13.794
  E0513 19:48:13.895702      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:14.896665      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 05/13/23 19:48:15.801
  STEP: Waiting for all pods to be running @ 05/13/23 19:48:15.811
  May 13 19:48:15.816: INFO: running pods: 0 < 1
  E0513 19:48:15.897200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:16.897784      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/13/23 19:48:17.82
  STEP: Waiting for the pdb to be processed @ 05/13/23 19:48:17.837
  STEP: Patching PodDisruptionBudget status @ 05/13/23 19:48:17.843
  STEP: Waiting for the pdb to be processed @ 05/13/23 19:48:17.848
  May 13 19:48:17.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4527" for this suite. @ 05/13/23 19:48:17.856
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/13/23 19:48:17.861
  May 13 19:48:17.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename watch @ 05/13/23 19:48:17.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:17.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:17.875
  STEP: creating a new configmap @ 05/13/23 19:48:17.877
  STEP: modifying the configmap once @ 05/13/23 19:48:17.88
  STEP: modifying the configmap a second time @ 05/13/23 19:48:17.885
  STEP: deleting the configmap @ 05/13/23 19:48:17.891
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/13/23 19:48:17.896
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/13/23 19:48:17.897
  E0513 19:48:17.897999      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:17.898: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4834  027c70cf-7ef4-45e4-99d3-b04e3b3c66a4 49438 0 2023-05-13 19:48:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-13 19:48:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 19:48:17.898: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4834  027c70cf-7ef4-45e4-99d3-b04e3b3c66a4 49439 0 2023-05-13 19:48:17 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-13 19:48:17 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 13 19:48:17.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4834" for this suite. @ 05/13/23 19:48:17.901
• [0.043 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 05/13/23 19:48:17.906
  May 13 19:48:17.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 19:48:17.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:17.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:17.921
  STEP: Setting up server cert @ 05/13/23 19:48:17.936
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 19:48:18.408
  STEP: Deploying the webhook pod @ 05/13/23 19:48:18.426
  STEP: Wait for the deployment to be ready @ 05/13/23 19:48:18.435
  May 13 19:48:18.439: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0513 19:48:18.899681      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:19.899862      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 19:48:20.447
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 19:48:20.462
  E0513 19:48:20.900168      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:21.463: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/13/23 19:48:21.468
  STEP: create a configmap that should be updated by the webhook @ 05/13/23 19:48:21.488
  May 13 19:48:21.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3738" for this suite. @ 05/13/23 19:48:21.584
  STEP: Destroying namespace "webhook-markers-1492" for this suite. @ 05/13/23 19:48:21.603
• [3.720 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 05/13/23 19:48:21.627
  May 13 19:48:21.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename secrets @ 05/13/23 19:48:21.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:21.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:21.665
  STEP: creating a secret @ 05/13/23 19:48:21.675
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/13/23 19:48:21.682
  STEP: patching the secret @ 05/13/23 19:48:21.689
  STEP: deleting the secret using a LabelSelector @ 05/13/23 19:48:21.706
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/13/23 19:48:21.717
  May 13 19:48:21.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1598" for this suite. @ 05/13/23 19:48:21.729
• [0.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 05/13/23 19:48:21.741
  May 13 19:48:21.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 19:48:21.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:21.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:21.783
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/13/23 19:48:21.791
  E0513 19:48:21.901395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:22.903063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:23.905834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:24.906208      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:48:25.826
  May 13 19:48:25.828: INFO: Trying to get logs from node worker00 pod pod-2530a9ec-c808-4feb-8e2e-946794742aff container test-container: <nil>
  STEP: delete the pod @ 05/13/23 19:48:25.831
  May 13 19:48:25.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8677" for this suite. @ 05/13/23 19:48:25.853
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 05/13/23 19:48:25.859
  May 13 19:48:25.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename webhook @ 05/13/23 19:48:25.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:25.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:25.87
  STEP: Setting up server cert @ 05/13/23 19:48:25.883
  E0513 19:48:25.906352      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/13/23 19:48:26.223
  STEP: Deploying the webhook pod @ 05/13/23 19:48:26.226
  STEP: Wait for the deployment to be ready @ 05/13/23 19:48:26.234
  May 13 19:48:26.239: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0513 19:48:26.907904      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:27.908484      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/13/23 19:48:28.245
  STEP: Verifying the service has paired with the endpoint @ 05/13/23 19:48:28.254
  E0513 19:48:28.910840      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:29.255: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 13 19:48:29.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1556-crds.webhook.example.com via the AdmissionRegistration API @ 05/13/23 19:48:29.772
  STEP: Creating a custom resource while v1 is storage version @ 05/13/23 19:48:29.8
  E0513 19:48:29.911947      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:30.912720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/13/23 19:48:31.833
  STEP: Patching the custom resource while v2 is storage version @ 05/13/23 19:48:31.851
  May 13 19:48:31.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0513 19:48:31.917221      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-8009" for this suite. @ 05/13/23 19:48:32.518
  STEP: Destroying namespace "webhook-markers-8665" for this suite. @ 05/13/23 19:48:32.536
• [6.685 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 05/13/23 19:48:32.545
  May 13 19:48:32.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 19:48:32.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:32.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:32.573
  STEP: Counting existing ResourceQuota @ 05/13/23 19:48:32.577
  E0513 19:48:32.918531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:33.922819      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:34.928815      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:35.929300      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:36.929451      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/13/23 19:48:37.58
  STEP: Ensuring resource quota status is calculated @ 05/13/23 19:48:37.588
  E0513 19:48:37.929493      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:38.930647      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 05/13/23 19:48:39.591
  STEP: Ensuring resource quota status captures replicaset creation @ 05/13/23 19:48:39.608
  E0513 19:48:39.931789      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:40.932597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 05/13/23 19:48:41.612
  STEP: Ensuring resource quota status released usage @ 05/13/23 19:48:41.616
  E0513 19:48:41.933295      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:42.936210      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:43.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7854" for this suite. @ 05/13/23 19:48:43.622
• [11.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 05/13/23 19:48:43.629
  May 13 19:48:43.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/13/23 19:48:43.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:43.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:43.646
  May 13 19:48:43.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:48:43.938602      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:44.938604      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:45.938830      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:46.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1030" for this suite. @ 05/13/23 19:48:46.734
• [3.121 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 05/13/23 19:48:46.75
  May 13 19:48:46.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename namespaces @ 05/13/23 19:48:46.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:46.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:46.765
  STEP: Creating namespace "e2e-ns-75xlp" @ 05/13/23 19:48:46.767
  May 13 19:48:46.778: INFO: Namespace "e2e-ns-75xlp-4642" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-75xlp-4642" @ 05/13/23 19:48:46.778
  May 13 19:48:46.785: INFO: Namespace "e2e-ns-75xlp-4642" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-75xlp-4642" @ 05/13/23 19:48:46.785
  May 13 19:48:46.790: INFO: Namespace "e2e-ns-75xlp-4642" has []v1.FinalizerName{"kubernetes"}
  May 13 19:48:46.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9275" for this suite. @ 05/13/23 19:48:46.792
  STEP: Destroying namespace "e2e-ns-75xlp-4642" for this suite. @ 05/13/23 19:48:46.795
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 05/13/23 19:48:46.803
  May 13 19:48:46.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename daemonsets @ 05/13/23 19:48:46.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:46.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:46.816
  May 13 19:48:46.827: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/13/23 19:48:46.831
  May 13 19:48:46.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:48:46.835: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/13/23 19:48:46.835
  May 13 19:48:46.852: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:48:46.852: INFO: Node worker01 is running 0 daemon pod, expected 1
  E0513 19:48:46.939570      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:47.856: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:48:47.856: INFO: Node worker01 is running 0 daemon pod, expected 1
  E0513 19:48:47.939928      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:48.856: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:48:48.856: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/13/23 19:48:48.859
  May 13 19:48:48.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:48:48.877: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0513 19:48:48.940545      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:49.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:48:49.880: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/13/23 19:48:49.88
  May 13 19:48:49.895: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:48:49.895: INFO: Node worker01 is running 0 daemon pod, expected 1
  E0513 19:48:49.941139      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:50.898: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:48:50.898: INFO: Node worker01 is running 0 daemon pod, expected 1
  E0513 19:48:50.941768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:51.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:48:51.899: INFO: Node worker01 is running 0 daemon pod, expected 1
  E0513 19:48:51.942992      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:52.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 13 19:48:52.899: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/13/23 19:48:52.903
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3650, will wait for the garbage collector to delete the pods @ 05/13/23 19:48:52.903
  E0513 19:48:52.946530      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:52.970: INFO: Deleting DaemonSet.extensions daemon-set took: 14.188514ms
  May 13 19:48:53.070: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.220443ms
  E0513 19:48:53.947974      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:54.948685      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:48:55.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 13 19:48:55.372: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 13 19:48:55.374: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49931"},"items":null}

  May 13 19:48:55.376: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49931"},"items":null}

  May 13 19:48:55.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3650" for this suite. @ 05/13/23 19:48:55.397
• [8.598 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 05/13/23 19:48:55.401
  May 13 19:48:55.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename gc @ 05/13/23 19:48:55.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:48:55.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:48:55.414
  STEP: create the rc @ 05/13/23 19:48:55.417
  W0513 19:48:55.425345      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0513 19:48:55.949721      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:56.951189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:57.951808      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:58.951980      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:48:59.952151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/13/23 19:49:00.427
  STEP: wait for all pods to be garbage collected @ 05/13/23 19:49:00.432
  E0513 19:49:00.952823      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:01.953661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:02.954152      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:03.954697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:04.955038      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/13/23 19:49:05.441
  E0513 19:49:05.956017      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:06.957201      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:07.957621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:08.961166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:09.961394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:10.963145      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:11.964712      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:12.966437      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:13.967397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:14.968383      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:15.969132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:16.970048      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:17.970298      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:18.971449      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:19.971995      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:20.972686      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:21.973933      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:22.974839      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:23.976093      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:24.976941      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:25.977207      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:26.977392      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:27.979811      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:28.981715      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:29.981879      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:30.982775      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:31.982776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:32.989274      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:33.989754      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:34.990491      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:35.990674      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:36.990779      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:37.991223      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:38.991676      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:39.992722      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:40.993938      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:41.994244      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:42.995101      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:43.995166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:44.997342      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:45.997633      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:46.998593      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:47.999218      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:49.000592      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:50.002821      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:51.003240      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:52.004428      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:53.006587      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:54.007355      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:55.007960      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:56.008887      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:57.011492      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:58.011731      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:49:59.012419      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:00.012806      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:01.016120      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:02.016155      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:03.016189      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:04.016872      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:05.019014      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:50:05.634: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
  May 13 19:50:05.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7774" for this suite. @ 05/13/23 19:50:05.645
• [70.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/13/23 19:50:05.671
  May 13 19:50:05.672: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:50:05.675
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:50:05.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:50:05.705
  STEP: Creating projection with secret that has name projected-secret-test-c122bd1b-006c-441d-86e5-ad94d64219c7 @ 05/13/23 19:50:05.71
  STEP: Creating a pod to test consume secrets @ 05/13/23 19:50:05.717
  E0513 19:50:06.020107      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:07.020267      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:08.021369      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:09.023981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:50:09.751
  May 13 19:50:09.757: INFO: Trying to get logs from node worker00 pod pod-projected-secrets-c8fde1da-eae9-4965-82d6-711058e69d24 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/13/23 19:50:09.765
  May 13 19:50:09.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1898" for this suite. @ 05/13/23 19:50:09.792
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 05/13/23 19:50:09.8
  May 13 19:50:09.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename container-probe @ 05/13/23 19:50:09.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:50:09.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:50:09.826
  STEP: Creating pod test-grpc-e4fef4c8-9a1d-4d18-bb52-f1d8713f35f8 in namespace container-probe-7996 @ 05/13/23 19:50:09.832
  E0513 19:50:10.025037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:11.026237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:50:11.853: INFO: Started pod test-grpc-e4fef4c8-9a1d-4d18-bb52-f1d8713f35f8 in namespace container-probe-7996
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/13/23 19:50:11.853
  May 13 19:50:11.856: INFO: Initial restart count of pod test-grpc-e4fef4c8-9a1d-4d18-bb52-f1d8713f35f8 is 0
  E0513 19:50:12.030037      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:13.032183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:14.035083      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:15.035233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:16.036069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:17.036130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:18.036816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:19.037390      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:20.040052      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:21.040130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:22.041076      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:23.045803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:24.045836      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:25.045991      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:26.046564      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:27.046897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:28.047085      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:29.047565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:30.047911      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:31.048186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:32.048318      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:33.048477      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:34.049855      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:35.063774      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:36.055585      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:37.056569      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:38.057536      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:39.057746      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:40.059058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:41.059255      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:42.061167      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:43.061579      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:44.061737      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:45.062088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:46.062147      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:47.062462      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:48.063768      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:49.064885      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:50.065563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:51.065764      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:52.066828      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:53.067279      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:54.067648      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:55.067982      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:56.069123      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:57.069844      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:58.070204      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:50:59.074735      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:00.075858      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:01.077209      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:02.078803      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:03.079464      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:04.080721      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:05.081091      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:06.108130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:07.108216      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:08.110677      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:09.110845      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:10.110880      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:11.112181      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:12.112450      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:13.113717      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:14.114671      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:15.115927      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:16.116130      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:17.116776      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:51:18.000: INFO: Restart count of pod container-probe-7996/test-grpc-e4fef4c8-9a1d-4d18-bb52-f1d8713f35f8 is now 1 (1m6.143423301s elapsed)
  May 13 19:51:18.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 19:51:18.007
  STEP: Destroying namespace "container-probe-7996" for this suite. @ 05/13/23 19:51:18.037
• [68.259 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 05/13/23 19:51:18.06
  May 13 19:51:18.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename runtimeclass @ 05/13/23 19:51:18.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:51:18.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:51:18.099
  May 13 19:51:18.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0513 19:51:18.117543      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "runtimeclass-2715" for this suite. @ 05/13/23 19:51:18.119
• [0.071 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/13/23 19:51:18.132
  May 13 19:51:18.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename prestop @ 05/13/23 19:51:18.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:51:18.155
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:51:18.164
  STEP: Creating server pod server in namespace prestop-490 @ 05/13/23 19:51:18.174
  STEP: Waiting for pods to come up. @ 05/13/23 19:51:18.19
  E0513 19:51:19.117794      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:20.118598      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-490 @ 05/13/23 19:51:20.212
  E0513 19:51:21.118890      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:22.118998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 05/13/23 19:51:22.236
  E0513 19:51:23.120249      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:24.121013      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:25.123319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:26.127498      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:27.130834      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:51:27.259: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  May 13 19:51:27.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 05/13/23 19:51:27.268
  STEP: Destroying namespace "prestop-490" for this suite. @ 05/13/23 19:51:27.289
• [9.183 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/13/23 19:51:27.316
  May 13 19:51:27.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sched-pred @ 05/13/23 19:51:27.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:51:27.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:51:27.355
  May 13 19:51:27.365: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 13 19:51:27.381: INFO: Waiting for terminating namespaces to be deleted...
  May 13 19:51:27.389: INFO: 
  Logging pods the apiserver thinks is on node worker00 before test
  May 13 19:51:27.400: INFO: etcd-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.402: INFO: 	Container etcd ready: true, restart count 0
  May 13 19:51:27.403: INFO: gobetween-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.403: INFO: 	Container gobetween ready: true, restart count 0
  May 13 19:51:27.404: INFO: kube-apiserver-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.406: INFO: 	Container kube-apiserver ready: true, restart count 0
  May 13 19:51:27.406: INFO: kube-controller-manager-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container kube-controller-manager ready: true, restart count 0
  May 13 19:51:27.407: INFO: kube-proxy-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container kube-proxy ready: true, restart count 0
  May 13 19:51:27.407: INFO: kube-scheduler-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container kube-scheduler ready: true, restart count 0
  May 13 19:51:27.407: INFO: calico-node-xsr47 from networking started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container calico-node ready: true, restart count 0
  May 13 19:51:27.407: INFO: metallb-speaker-v7flx from networking started at 2023-05-13 18:18:58 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container speaker ready: true, restart count 0
  May 13 19:51:27.407: INFO: tester from prestop-490 started at 2023-05-13 19:51:20 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container tester ready: true, restart count 0
  May 13 19:51:27.407: INFO: sonobuoy from sonobuoy started at 2023-05-13 18:11:13 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 13 19:51:27.407: INFO: sonobuoy-e2e-job-dc09634ed87e4d2d from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container e2e ready: true, restart count 0
  May 13 19:51:27.407: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 19:51:27.407: INFO: sonobuoy-systemd-logs-daemon-set-0619921c091243c5-6tddn from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 19:51:27.407: INFO: 	Container systemd-logs ready: true, restart count 0
  May 13 19:51:27.407: INFO: ceph-csi-cephfs-nodeplugin-rhhl2 from storage started at 2023-05-13 18:18:58 +0000 UTC (3 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:51:27.407: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:51:27.407: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:51:27.407: INFO: ceph-csi-rbd-nodeplugin-t44kh from storage started at 2023-05-13 18:18:58 +0000 UTC (3 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:51:27.407: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:51:27.407: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:51:27.407: INFO: ceph-mon-worker00-79cd8cd599-5skt9 from storage started at 2023-05-13 18:18:58 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.407: INFO: 	Container ceph-mon ready: true, restart count 0
  May 13 19:51:27.407: INFO: 
  Logging pods the apiserver thinks is on node worker01 before test
  May 13 19:51:27.428: INFO: coredns-5bf6b5d445-frtlk from kube-system started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container coredns ready: true, restart count 0
  May 13 19:51:27.429: INFO: coredns-5bf6b5d445-wv86k from kube-system started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container coredns ready: true, restart count 0
  May 13 19:51:27.429: INFO: gobetween-worker01 from kube-system started at 2023-05-13 18:05:13 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container gobetween ready: true, restart count 0
  May 13 19:51:27.429: INFO: kube-proxy-worker01 from kube-system started at 2023-05-13 18:05:13 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container kube-proxy ready: true, restart count 0
  May 13 19:51:27.429: INFO: kubernetes-dashboard-67c9dc7898-qhknx from kube-system started at 2023-05-13 18:06:09 +0000 UTC (2 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May 13 19:51:27.429: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May 13 19:51:27.429: INFO: calico-kube-controllers-849bffbf8c-mh9pf from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 13 19:51:27.429: INFO: calico-node-bsrzk from networking started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container calico-node ready: true, restart count 0
  May 13 19:51:27.429: INFO: calico-typha-7f5df955fb-2bg4l from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container calico-typha ready: true, restart count 0
  May 13 19:51:27.429: INFO: metallb-controller-64cc69b5c4-wd4dh from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container controller ready: true, restart count 0
  May 13 19:51:27.429: INFO: metallb-speaker-wntmv from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container speaker ready: true, restart count 0
  May 13 19:51:27.429: INFO: sonobuoy-systemd-logs-daemon-set-0619921c091243c5-wlgqk from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 19:51:27.429: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 19:51:27.429: INFO: 	Container systemd-logs ready: true, restart count 0
  May 13 19:51:27.429: INFO: ceph-csi-cephfs-nodeplugin-c7nqc from storage started at 2023-05-13 18:05:58 +0000 UTC (3 container statuses recorded)
  May 13 19:51:27.430: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:51:27.430: INFO: ceph-csi-cephfs-provisioner-b795fb565-6gcfc from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 19:51:27.430: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:51:27.430: INFO: ceph-csi-cephfs-provisioner-b795fb565-8656n from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 19:51:27.430: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:51:27.430: INFO: ceph-csi-cephfs-provisioner-b795fb565-m982k from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 19:51:27.430: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:51:27.430: INFO: ceph-csi-rbd-nodeplugin-xgqs8 from storage started at 2023-05-13 18:05:58 +0000 UTC (3 container statuses recorded)
  May 13 19:51:27.430: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:51:27.430: INFO: ceph-csi-rbd-provisioner-674549bd88-lkl8l from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 19:51:27.430: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:51:27.430: INFO: ceph-csi-rbd-provisioner-674549bd88-n9pkj from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 19:51:27.430: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:51:27.430: INFO: ceph-csi-rbd-provisioner-674549bd88-nk4kp from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 19:51:27.430: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:51:27.430: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:51:27.431: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:51:27.431: INFO: ceph-mds-worker01-9b6db8bb9-q6w24 from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.431: INFO: 	Container ceph-mds ready: true, restart count 0
  May 13 19:51:27.431: INFO: ceph-mgr-worker01-5bc7cddf9f-pmtvr from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.431: INFO: 	Container ceph-mgr ready: true, restart count 0
  May 13 19:51:27.431: INFO: ceph-osd-worker01-594bc8755f-r5bdz from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.431: INFO: 	Container ceph-osd ready: true, restart count 0
  May 13 19:51:27.431: INFO: ceph-rgw-worker01-64f8846fdb-jzw8c from storage started at 2023-05-13 18:05:59 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.431: INFO: 	Container ceph-rgw ready: true, restart count 0
  May 13 19:51:27.431: INFO: ceph-setup-q5j8d from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.431: INFO: 	Container ceph ready: false, restart count 0
  May 13 19:51:27.431: INFO: snapshot-controller-7dd49c567-9rfhc from storage started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.431: INFO: 	Container snapshot-controller ready: true, restart count 0
  May 13 19:51:27.431: INFO: snapshot-controller-7dd49c567-h5tj7 from storage started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:51:27.431: INFO: 	Container snapshot-controller ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/13/23 19:51:27.431
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.175ecbe339c2b4e5], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] @ 05/13/23 19:51:27.479
  E0513 19:51:28.132621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:51:28.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5697" for this suite. @ 05/13/23 19:51:28.483
• [1.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/13/23 19:51:28.495
  May 13 19:51:28.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename deployment @ 05/13/23 19:51:28.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:51:28.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:51:28.518
  May 13 19:51:28.522: INFO: Creating simple deployment test-new-deployment
  May 13 19:51:28.539: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0513 19:51:29.132816      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:30.133573      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 05/13/23 19:51:30.556
  STEP: updating a scale subresource @ 05/13/23 19:51:30.56
  STEP: verifying the deployment Spec.Replicas was modified @ 05/13/23 19:51:30.58
  STEP: Patch a scale subresource @ 05/13/23 19:51:30.585
  May 13 19:51:30.625: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-9204  5be9ee04-44f6-41fc-8ca6-f115567f6d42 50712 3 2023-05-13 19:51:28 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-13 19:51:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:51:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00312f3f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-13 19:51:30 +0000 UTC,LastTransitionTime:2023-05-13 19:51:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-05-13 19:51:30 +0000 UTC,LastTransitionTime:2023-05-13 19:51:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 13 19:51:30.638: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-9204  40897c9b-f8fa-4c8a-bdb0-af2a6bb914bb 50715 3 2023-05-13 19:51:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 5be9ee04-44f6-41fc-8ca6-f115567f6d42 0xc0042c1df7 0xc0042c1df8}] [] [{kube-controller-manager Update apps/v1 2023-05-13 19:51:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5be9ee04-44f6-41fc-8ca6-f115567f6d42\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:51:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042c1e88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 13 19:51:30.655: INFO: Pod "test-new-deployment-67bd4bf6dc-4s59m" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-4s59m test-new-deployment-67bd4bf6dc- deployment-9204  6ea54f00-bed7-4af5-b626-7e858e74d391 50717 0 2023-05-13 19:51:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 40897c9b-f8fa-4c8a-bdb0-af2a6bb914bb 0xc00312f827 0xc00312f828}] [] [{kube-controller-manager Update v1 2023-05-13 19:51:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40897c9b-f8fa-4c8a-bdb0-af2a6bb914bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k44sv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k44sv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:51:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 19:51:30.655: INFO: Pod "test-new-deployment-67bd4bf6dc-7ksnw" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-7ksnw test-new-deployment-67bd4bf6dc- deployment-9204  b04df8da-bdfb-495b-b1d3-bd178a405f77 50704 0 2023-05-13 19:51:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:e5639e7ed6f3693165a34b0d6a9ae44ce1dc7cda31d05ca19f09fa4b78307478 cni.projectcalico.org/podIP:10.200.131.153/32 cni.projectcalico.org/podIPs:10.200.131.153/32] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 40897c9b-f8fa-4c8a-bdb0-af2a6bb914bb 0xc00312f9a0 0xc00312f9a1}] [] [{kube-controller-manager Update v1 2023-05-13 19:51:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"40897c9b-f8fa-4c8a-bdb0-af2a6bb914bb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-13 19:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-13 19:51:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqnlg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqnlg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:51:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:51:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:51:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:51:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:10.200.131.153,StartTime:2023-05-13 19:51:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 19:51:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4a0062ebf9bafd8018b9cf6d9029c127330917543498c83a86a6b6fc9f0bbec3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.153,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 19:51:30.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9204" for this suite. @ 05/13/23 19:51:30.67
• [2.199 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 05/13/23 19:51:30.696
  May 13 19:51:30.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename emptydir @ 05/13/23 19:51:30.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:51:30.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:51:30.733
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/13/23 19:51:30.74
  E0513 19:51:31.134782      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:32.138998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:33.139706      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:34.139878      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:51:34.769
  May 13 19:51:34.775: INFO: Trying to get logs from node worker00 pod pod-0ea0c317-0e95-49d2-ba7c-dd9fbafa2c72 container test-container: <nil>
  STEP: delete the pod @ 05/13/23 19:51:34.792
  May 13 19:51:34.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2851" for this suite. @ 05/13/23 19:51:34.835
• [4.149 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 05/13/23 19:51:34.846
  May 13 19:51:34.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename deployment @ 05/13/23 19:51:34.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:51:34.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:51:34.879
  STEP: creating a Deployment @ 05/13/23 19:51:34.888
  May 13 19:51:34.889: INFO: Creating simple deployment test-deployment-pgg8s
  May 13 19:51:34.907: INFO: deployment "test-deployment-pgg8s" doesn't have the required revision set
  E0513 19:51:35.140603      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:36.143293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 05/13/23 19:51:36.916
  May 13 19:51:36.919: INFO: Deployment test-deployment-pgg8s has Conditions: [{Available True 2023-05-13 19:51:36 +0000 UTC 2023-05-13 19:51:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-13 19:51:36 +0000 UTC 2023-05-13 19:51:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pgg8s-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/13/23 19:51:36.919
  May 13 19:51:36.936: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 51, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 51, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 13, 19, 51, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 13, 19, 51, 34, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pgg8s-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/13/23 19:51:36.936
  May 13 19:51:36.938: INFO: Observed &Deployment event: ADDED
  May 13 19:51:36.938: INFO: Observed Deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-13 19:51:34 +0000 UTC 2023-05-13 19:51:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pgg8s-5994cf9475"}
  May 13 19:51:36.939: INFO: Observed &Deployment event: MODIFIED
  May 13 19:51:36.939: INFO: Observed Deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-13 19:51:34 +0000 UTC 2023-05-13 19:51:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pgg8s-5994cf9475"}
  May 13 19:51:36.939: INFO: Observed Deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-13 19:51:34 +0000 UTC 2023-05-13 19:51:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 13 19:51:36.939: INFO: Observed &Deployment event: MODIFIED
  May 13 19:51:36.939: INFO: Observed Deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-13 19:51:34 +0000 UTC 2023-05-13 19:51:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 13 19:51:36.939: INFO: Observed Deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-13 19:51:34 +0000 UTC 2023-05-13 19:51:34 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pgg8s-5994cf9475" is progressing.}
  May 13 19:51:36.939: INFO: Observed &Deployment event: MODIFIED
  May 13 19:51:36.939: INFO: Observed Deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-13 19:51:36 +0000 UTC 2023-05-13 19:51:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 13 19:51:36.939: INFO: Observed Deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-13 19:51:36 +0000 UTC 2023-05-13 19:51:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pgg8s-5994cf9475" has successfully progressed.}
  May 13 19:51:36.940: INFO: Observed &Deployment event: MODIFIED
  May 13 19:51:36.940: INFO: Observed Deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-13 19:51:36 +0000 UTC 2023-05-13 19:51:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 13 19:51:36.940: INFO: Observed Deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-13 19:51:36 +0000 UTC 2023-05-13 19:51:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pgg8s-5994cf9475" has successfully progressed.}
  May 13 19:51:36.940: INFO: Found Deployment test-deployment-pgg8s in namespace deployment-9933 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 13 19:51:36.940: INFO: Deployment test-deployment-pgg8s has an updated status
  STEP: patching the Statefulset Status @ 05/13/23 19:51:36.94
  May 13 19:51:36.940: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 13 19:51:36.949: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/13/23 19:51:36.949
  May 13 19:51:36.951: INFO: Observed &Deployment event: ADDED
  May 13 19:51:36.951: INFO: Observed deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-13 19:51:34 +0000 UTC 2023-05-13 19:51:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pgg8s-5994cf9475"}
  May 13 19:51:36.951: INFO: Observed &Deployment event: MODIFIED
  May 13 19:51:36.951: INFO: Observed deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-13 19:51:34 +0000 UTC 2023-05-13 19:51:34 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pgg8s-5994cf9475"}
  May 13 19:51:36.951: INFO: Observed deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-13 19:51:34 +0000 UTC 2023-05-13 19:51:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 13 19:51:36.951: INFO: Observed &Deployment event: MODIFIED
  May 13 19:51:36.951: INFO: Observed deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-13 19:51:34 +0000 UTC 2023-05-13 19:51:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 13 19:51:36.951: INFO: Observed deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-13 19:51:34 +0000 UTC 2023-05-13 19:51:34 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pgg8s-5994cf9475" is progressing.}
  May 13 19:51:36.951: INFO: Observed &Deployment event: MODIFIED
  May 13 19:51:36.951: INFO: Observed deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-13 19:51:36 +0000 UTC 2023-05-13 19:51:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 13 19:51:36.951: INFO: Observed deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-13 19:51:36 +0000 UTC 2023-05-13 19:51:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pgg8s-5994cf9475" has successfully progressed.}
  May 13 19:51:36.951: INFO: Observed &Deployment event: MODIFIED
  May 13 19:51:36.951: INFO: Observed deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-13 19:51:36 +0000 UTC 2023-05-13 19:51:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 13 19:51:36.951: INFO: Observed deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-13 19:51:36 +0000 UTC 2023-05-13 19:51:34 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pgg8s-5994cf9475" has successfully progressed.}
  May 13 19:51:36.951: INFO: Observed deployment test-deployment-pgg8s in namespace deployment-9933 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 13 19:51:36.952: INFO: Observed &Deployment event: MODIFIED
  May 13 19:51:36.952: INFO: Found deployment test-deployment-pgg8s in namespace deployment-9933 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  May 13 19:51:36.952: INFO: Deployment test-deployment-pgg8s has a patched status
  May 13 19:51:36.955: INFO: Deployment "test-deployment-pgg8s":
  &Deployment{ObjectMeta:{test-deployment-pgg8s  deployment-9933  8ae10742-b516-46a2-9737-a0ba8fad2349 50863 1 2023-05-13 19:51:34 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-13 19:51:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-13 19:51:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-13 19:51:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c3eb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-pgg8s-5994cf9475",LastUpdateTime:2023-05-13 19:51:36 +0000 UTC,LastTransitionTime:2023-05-13 19:51:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 13 19:51:36.958: INFO: New ReplicaSet "test-deployment-pgg8s-5994cf9475" of Deployment "test-deployment-pgg8s":
  &ReplicaSet{ObjectMeta:{test-deployment-pgg8s-5994cf9475  deployment-9933  3c2daa11-a2d5-455a-843d-bfc2ad82bdd3 50856 1 2023-05-13 19:51:34 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pgg8s 8ae10742-b516-46a2-9737-a0ba8fad2349 0xc004c3ef60 0xc004c3ef61}] [] [{kube-controller-manager Update apps/v1 2023-05-13 19:51:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8ae10742-b516-46a2-9737-a0ba8fad2349\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-13 19:51:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c3f008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 13 19:51:36.961: INFO: Pod "test-deployment-pgg8s-5994cf9475-8xxsc" is available:
  &Pod{ObjectMeta:{test-deployment-pgg8s-5994cf9475-8xxsc test-deployment-pgg8s-5994cf9475- deployment-9933  97ebb8ce-813d-4b10-b6d2-5e5d06ddeb9d 50855 0 2023-05-13 19:51:34 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[cni.projectcalico.org/containerID:4ddd1569c0f0c3ddec2d86ee2f5427a4c593178f1e0a67a2f91ae2def0c2d75f cni.projectcalico.org/podIP:10.200.131.184/32 cni.projectcalico.org/podIPs:10.200.131.184/32] [{apps/v1 ReplicaSet test-deployment-pgg8s-5994cf9475 3c2daa11-a2d5-455a-843d-bfc2ad82bdd3 0xc0058929e0 0xc0058929e1}] [] [{kube-controller-manager Update v1 2023-05-13 19:51:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c2daa11-a2d5-455a-843d-bfc2ad82bdd3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-13 19:51:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-13 19:51:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.200.131.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrg9q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrg9q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:51:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:51:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:51:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-13 19:51:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.58.100,PodIP:10.200.131.184,StartTime:2023-05-13 19:51:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-13 19:51:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e15e60a68511bfe40039a3bc3ad051328d0bfb7bfb51038848efcd8dc9b3f87a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.184,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 13 19:51:36.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9933" for this suite. @ 05/13/23 19:51:36.966
• [2.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 05/13/23 19:51:36.971
  May 13 19:51:36.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename projected @ 05/13/23 19:51:36.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:51:36.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:51:36.989
  STEP: Creating a pod to test downward API volume plugin @ 05/13/23 19:51:36.992
  E0513 19:51:37.143833      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:38.145166      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:39.145964      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:40.146180      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:41.146606      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:42.147021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/13/23 19:51:43.015
  May 13 19:51:43.018: INFO: Trying to get logs from node worker00 pod downwardapi-volume-6a9bcfc8-e5af-4725-bb89-d4461b0fe4cc container client-container: <nil>
  STEP: delete the pod @ 05/13/23 19:51:43.023
  May 13 19:51:43.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5869" for this suite. @ 05/13/23 19:51:43.04
• [6.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 05/13/23 19:51:43.048
  May 13 19:51:43.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename gc @ 05/13/23 19:51:43.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:51:43.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:51:43.071
  STEP: create the deployment @ 05/13/23 19:51:43.075
  W0513 19:51:43.106656      21 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/13/23 19:51:43.106
  E0513 19:51:43.149063      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 05/13/23 19:51:43.152
  STEP: wait for all rs to be garbage collected @ 05/13/23 19:51:43.169
  STEP: expected 0 rs, got 1 rs @ 05/13/23 19:51:43.185
  STEP: expected 0 pods, got 2 pods @ 05/13/23 19:51:43.194
  STEP: Gathering metrics @ 05/13/23 19:51:43.701
  E0513 19:51:44.149531      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:45.149897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:46.150272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:47.150997      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:48.151104      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:49.151649      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:50.152388      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:51.152732      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:52.153284      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:53.155241      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:54.156341      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:55.157153      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:56.157311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:57.157704      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:58.157940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:51:59.158942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:00.159124      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:01.159507      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:02.161326      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:03.161430      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:04.161572      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:05.161640      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:06.163058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:07.163998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:08.164161      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:09.164894      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:10.165897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:11.166981      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:12.167291      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:13.168183      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:14.169021      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:15.169631      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:16.169942      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:17.171565      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:18.171487      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:19.171590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:20.172035      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:21.173105      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:22.173736      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:23.174078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:24.174227      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:25.176719      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:26.176861      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:27.177388      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:28.180395      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:29.181923      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:30.182689      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:31.183607      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:32.183692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:33.184233      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:34.184525      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:35.185583      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:36.189636      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:37.190664      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:38.190780      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:39.191868      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:40.193851      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:41.194454      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:42.194595      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:43.195807      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:52:43.866: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
  May 13 19:52:43.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6513" for this suite. @ 05/13/23 19:52:43.872
• [60.845 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 05/13/23 19:52:43.895
  May 13 19:52:43.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename resourcequota @ 05/13/23 19:52:43.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:52:43.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:52:43.935
  STEP: Counting existing ResourceQuota @ 05/13/23 19:52:43.942
  E0513 19:52:44.196772      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:45.197002      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:46.197496      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:47.197560      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:48.198773      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/13/23 19:52:48.954
  STEP: Ensuring resource quota status is calculated @ 05/13/23 19:52:48.987
  E0513 19:52:49.199554      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:50.200043      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 05/13/23 19:52:50.992
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/13/23 19:52:51.017
  E0513 19:52:51.200154      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:52.201200      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/13/23 19:52:53.022
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/13/23 19:52:53.027
  STEP: Ensuring a pod cannot update its resource requirements @ 05/13/23 19:52:53.03
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/13/23 19:52:53.035
  E0513 19:52:53.201645      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:54.203050      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/13/23 19:52:55.048
  STEP: Ensuring resource quota status released the pod usage @ 05/13/23 19:52:55.063
  E0513 19:52:55.203294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:56.204086      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:52:57.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5395" for this suite. @ 05/13/23 19:52:57.076
• [13.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 05/13/23 19:52:57.088
  May 13 19:52:57.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename dns @ 05/13/23 19:52:57.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:52:57.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:52:57.113
  STEP: Creating a test externalName service @ 05/13/23 19:52:57.117
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6070.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6070.svc.cluster.local; sleep 1; done
   @ 05/13/23 19:52:57.153
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6070.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6070.svc.cluster.local; sleep 1; done
   @ 05/13/23 19:52:57.153
  STEP: creating a pod to probe DNS @ 05/13/23 19:52:57.154
  STEP: submitting the pod to kubernetes @ 05/13/23 19:52:57.154
  E0513 19:52:57.206590      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:58.206661      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:52:59.207213      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/13/23 19:52:59.237
  STEP: looking for the results for each expected name from probers @ 05/13/23 19:52:59.245
  May 13 19:52:59.258: INFO: DNS probes using dns-test-031ef115-ea25-4ffe-a5af-4a7e926fd23d succeeded

  STEP: changing the externalName to bar.example.com @ 05/13/23 19:52:59.258
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6070.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6070.svc.cluster.local; sleep 1; done
   @ 05/13/23 19:52:59.276
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6070.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6070.svc.cluster.local; sleep 1; done
   @ 05/13/23 19:52:59.276
  STEP: creating a second pod to probe DNS @ 05/13/23 19:52:59.276
  STEP: submitting the pod to kubernetes @ 05/13/23 19:52:59.276
  E0513 19:53:00.207658      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:01.209150      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/13/23 19:53:01.298
  STEP: looking for the results for each expected name from probers @ 05/13/23 19:53:01.304
  May 13 19:53:01.318: INFO: DNS probes using dns-test-a2945d14-d84a-45a3-85b3-4eb619353e39 succeeded

  STEP: changing the service to type=ClusterIP @ 05/13/23 19:53:01.318
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6070.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6070.svc.cluster.local; sleep 1; done
   @ 05/13/23 19:53:01.334
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6070.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6070.svc.cluster.local; sleep 1; done
   @ 05/13/23 19:53:01.334
  STEP: creating a third pod to probe DNS @ 05/13/23 19:53:01.334
  STEP: submitting the pod to kubernetes @ 05/13/23 19:53:01.346
  E0513 19:53:02.209455      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:03.210535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:04.211703      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:05.215472      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:06.216032      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:07.218001      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:08.218061      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:09.218302      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:10.220362      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:11.221757      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:12.222608      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:13.222813      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:14.223755      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:15.224769      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:16.224886      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:17.226700      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/13/23 19:53:17.441
  STEP: looking for the results for each expected name from probers @ 05/13/23 19:53:17.447
  May 13 19:53:17.461: INFO: DNS probes using dns-test-4116924f-bd41-4b7e-98a4-354b125ac5f2 succeeded

  May 13 19:53:17.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/13/23 19:53:17.467
  STEP: deleting the pod @ 05/13/23 19:53:17.484
  STEP: deleting the pod @ 05/13/23 19:53:17.513
  STEP: deleting the test externalName service @ 05/13/23 19:53:17.548
  STEP: Destroying namespace "dns-6070" for this suite. @ 05/13/23 19:53:17.598
• [20.540 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 05/13/23 19:53:17.629
  May 13 19:53:17.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/13/23 19:53:17.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:53:17.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:53:17.679
  STEP: set up a multi version CRD @ 05/13/23 19:53:17.684
  May 13 19:53:17.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  E0513 19:53:18.227237      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:19.229135      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:20.230765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:21.231119      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 05/13/23 19:53:21.92
  STEP: check the unserved version gets removed @ 05/13/23 19:53:21.944
  E0513 19:53:22.233952      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:23.234963      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/13/23 19:53:23.379
  E0513 19:53:24.235719      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:25.235629      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:53:26.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-413" for this suite. @ 05/13/23 19:53:26.117
• [8.504 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 05/13/23 19:53:26.134
  May 13 19:53:26.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename taint-single-pod @ 05/13/23 19:53:26.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:53:26.146
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:53:26.152
  May 13 19:53:26.155: INFO: Waiting up to 1m0s for all nodes to be ready
  E0513 19:53:26.236720      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:27.237715      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:28.237936      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:29.238429      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:30.238880      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:31.239011      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:32.241251      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:33.243278      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:34.245238      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:35.246762      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:36.246897      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:37.248234      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:38.249313      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:39.249471      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:40.250069      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:41.251280      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:42.252019      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:43.252417      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:44.253058      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:45.254679      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:46.255299      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:47.258902      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:48.259215      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:49.259804      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:50.262202      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:51.262272      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:52.263231      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:53.263847      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:54.264158      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:55.266624      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:56.267508      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:57.268513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:58.269908      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:53:59.270046      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:00.270108      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:01.270354      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:02.270915      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:03.271293      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:04.271460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:05.272328      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:06.274882      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:07.276308      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:08.279359      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:09.280133      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:10.280642      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:11.281453      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:12.284445      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:13.284621      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:14.285791      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:15.286088      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:16.286349      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:17.287998      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:18.288979      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:19.289219      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:20.290236      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:21.290876      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:22.292373      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:23.292993      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:24.293871      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:25.294269      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:54:26.193: INFO: Waiting for terminating namespaces to be deleted...
  May 13 19:54:26.201: INFO: Starting informer...
  STEP: Starting pod... @ 05/13/23 19:54:26.202
  E0513 19:54:26.294788      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:54:26.427: INFO: Pod is running on worker00. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/13/23 19:54:26.427
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/13/23 19:54:26.448
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/13/23 19:54:26.456
  May 13 19:54:26.457: INFO: Pod wasn't evicted. Proceeding
  May 13 19:54:26.457: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/13/23 19:54:26.482
  STEP: Waiting some time to make sure that toleration time passed. @ 05/13/23 19:54:26.508
  E0513 19:54:27.297374      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:28.297756      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:29.298186      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:30.298710      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:31.298954      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:32.299481      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:33.348905      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:34.349252      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:35.349729      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:36.350653      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:37.351143      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:38.351831      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:39.352361      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:40.352485      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:41.352822      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:42.353132      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:43.353576      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:44.354728      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:45.355434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:46.356563      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:47.357638      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:48.359125      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:49.359526      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:50.359972      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:51.361162      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:52.361279      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:53.361627      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:54.361630      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:55.363319      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:56.363421      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:57.363709      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:58.364167      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:54:59.365609      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:00.365760      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:01.365772      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:02.368478      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:03.368726      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:04.369692      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:05.370178      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:06.370708      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:07.372697      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:08.374194      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:09.375333      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:10.376023      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:11.376535      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:12.378409      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:13.379095      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:14.380338      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:15.380460      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:16.381550      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:17.382714      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:18.385099      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:19.386117      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:20.386687      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:21.387332      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:22.388378      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:23.388431      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:24.388652      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:25.392397      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:26.391311      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:27.391384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:28.392912      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:29.393384      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:30.395151      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:31.396294      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:32.397394      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:33.397583      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:34.397597      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:35.398434      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:36.400012      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:37.400287      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:38.401408      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:39.401270      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:40.404765      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:41.404928      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:55:41.509: INFO: Pod wasn't evicted. Test successful
  May 13 19:55:41.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-128" for this suite. @ 05/13/23 19:55:41.513
• [135.394 seconds]
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 05/13/23 19:55:41.529
  May 13 19:55:41.529: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename init-container @ 05/13/23 19:55:41.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:55:41.539
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:55:41.543
  STEP: creating the pod @ 05/13/23 19:55:41.546
  May 13 19:55:41.546: INFO: PodSpec: initContainers in spec.initContainers
  E0513 19:55:42.405078      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:43.406610      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:44.407097      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:45.407266      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 13 19:55:45.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8110" for this suite. @ 05/13/23 19:55:45.71
• [4.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 05/13/23 19:55:45.718
  May 13 19:55:45.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename server-version @ 05/13/23 19:55:45.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:55:45.729
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:55:45.732
  STEP: Request ServerVersion @ 05/13/23 19:55:45.734
  STEP: Confirm major version @ 05/13/23 19:55:45.735
  May 13 19:55:45.736: INFO: Major version: 1
  STEP: Confirm minor version @ 05/13/23 19:55:45.736
  May 13 19:55:45.736: INFO: cleanMinorVersion: 27
  May 13 19:55:45.736: INFO: Minor version: 27
  May 13 19:55:45.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-3960" for this suite. @ 05/13/23 19:55:45.738
• [0.025 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/13/23 19:55:45.744
  May 13 19:55:45.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3569800363
  STEP: Building a namespace api object, basename sched-pred @ 05/13/23 19:55:45.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/13/23 19:55:45.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/13/23 19:55:45.766
  May 13 19:55:45.769: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 13 19:55:45.776: INFO: Waiting for terminating namespaces to be deleted...
  May 13 19:55:45.779: INFO: 
  Logging pods the apiserver thinks is on node worker00 before test
  May 13 19:55:45.786: INFO: pod-init-1931b5a0-6ef0-4f11-96db-e09c1af08aab from init-container-8110 started at 2023-05-13 19:55:41 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.786: INFO: 	Container run1 ready: false, restart count 0
  May 13 19:55:45.786: INFO: etcd-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.786: INFO: 	Container etcd ready: true, restart count 0
  May 13 19:55:45.787: INFO: gobetween-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container gobetween ready: true, restart count 0
  May 13 19:55:45.787: INFO: kube-apiserver-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container kube-apiserver ready: true, restart count 0
  May 13 19:55:45.787: INFO: kube-controller-manager-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container kube-controller-manager ready: true, restart count 0
  May 13 19:55:45.787: INFO: kube-proxy-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container kube-proxy ready: true, restart count 0
  May 13 19:55:45.787: INFO: kube-scheduler-worker00 from kube-system started at 2023-05-13 18:05:03 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container kube-scheduler ready: true, restart count 0
  May 13 19:55:45.787: INFO: calico-node-xsr47 from networking started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container calico-node ready: true, restart count 0
  May 13 19:55:45.787: INFO: metallb-speaker-7rzvf from networking started at 2023-05-13 19:54:26 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container speaker ready: true, restart count 0
  May 13 19:55:45.787: INFO: sonobuoy from sonobuoy started at 2023-05-13 18:11:13 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 13 19:55:45.787: INFO: sonobuoy-e2e-job-dc09634ed87e4d2d from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container e2e ready: true, restart count 0
  May 13 19:55:45.787: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 19:55:45.787: INFO: sonobuoy-systemd-logs-daemon-set-0619921c091243c5-6tddn from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 19:55:45.787: INFO: 	Container systemd-logs ready: true, restart count 0
  May 13 19:55:45.787: INFO: ceph-csi-cephfs-nodeplugin-mkrhr from storage started at 2023-05-13 19:54:28 +0000 UTC (3 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:55:45.787: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:55:45.787: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:55:45.787: INFO: ceph-csi-rbd-nodeplugin-qckx2 from storage started at 2023-05-13 19:54:28 +0000 UTC (3 container statuses recorded)
  May 13 19:55:45.787: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:55:45.787: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:55:45.787: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:55:45.788: INFO: ceph-mon-worker00-79cd8cd599-2xbpj from storage started at 2023-05-13 19:54:26 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.788: INFO: 	Container ceph-mon ready: true, restart count 0
  May 13 19:55:45.788: INFO: taint-eviction-4 from taint-single-pod-128 started at 2023-05-13 19:54:26 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.788: INFO: 	Container pause ready: true, restart count 0
  May 13 19:55:45.788: INFO: 
  Logging pods the apiserver thinks is on node worker01 before test
  May 13 19:55:45.796: INFO: coredns-5bf6b5d445-frtlk from kube-system started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.797: INFO: 	Container coredns ready: true, restart count 0
  May 13 19:55:45.797: INFO: coredns-5bf6b5d445-wv86k from kube-system started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.797: INFO: 	Container coredns ready: true, restart count 0
  May 13 19:55:45.797: INFO: gobetween-worker01 from kube-system started at 2023-05-13 18:05:13 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.797: INFO: 	Container gobetween ready: true, restart count 0
  May 13 19:55:45.797: INFO: kube-proxy-worker01 from kube-system started at 2023-05-13 18:05:13 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.797: INFO: 	Container kube-proxy ready: true, restart count 0
  May 13 19:55:45.798: INFO: kubernetes-dashboard-67c9dc7898-qhknx from kube-system started at 2023-05-13 18:06:09 +0000 UTC (2 container statuses recorded)
  May 13 19:55:45.798: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
  May 13 19:55:45.798: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
  May 13 19:55:45.798: INFO: calico-kube-controllers-849bffbf8c-mh9pf from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.798: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 13 19:55:45.798: INFO: calico-node-bsrzk from networking started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.798: INFO: 	Container calico-node ready: true, restart count 0
  May 13 19:55:45.798: INFO: calico-typha-7f5df955fb-2bg4l from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.798: INFO: 	Container calico-typha ready: true, restart count 0
  May 13 19:55:45.798: INFO: metallb-controller-64cc69b5c4-wd4dh from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.798: INFO: 	Container controller ready: true, restart count 0
  May 13 19:55:45.798: INFO: metallb-speaker-wntmv from networking started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.798: INFO: 	Container speaker ready: true, restart count 0
  May 13 19:55:45.798: INFO: sonobuoy-systemd-logs-daemon-set-0619921c091243c5-wlgqk from sonobuoy started at 2023-05-13 18:11:19 +0000 UTC (2 container statuses recorded)
  May 13 19:55:45.799: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container systemd-logs ready: true, restart count 0
  May 13 19:55:45.799: INFO: ceph-csi-cephfs-nodeplugin-c7nqc from storage started at 2023-05-13 18:05:58 +0000 UTC (3 container statuses recorded)
  May 13 19:55:45.799: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:55:45.799: INFO: ceph-csi-cephfs-provisioner-b795fb565-6gcfc from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 19:55:45.799: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:55:45.799: INFO: ceph-csi-cephfs-provisioner-b795fb565-8656n from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 19:55:45.799: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:55:45.799: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:55:45.799: INFO: ceph-csi-cephfs-provisioner-b795fb565-m982k from storage started at 2023-05-13 18:05:58 +0000 UTC (5 container statuses recorded)
  May 13 19:55:45.800: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:55:45.800: INFO: ceph-csi-rbd-nodeplugin-xgqs8 from storage started at 2023-05-13 18:05:58 +0000 UTC (3 container statuses recorded)
  May 13 19:55:45.800: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container driver-registrar ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:55:45.800: INFO: ceph-csi-rbd-provisioner-674549bd88-lkl8l from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 19:55:45.800: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:55:45.800: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:55:45.800: INFO: ceph-csi-rbd-provisioner-674549bd88-n9pkj from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 19:55:45.801: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:55:45.801: INFO: ceph-csi-rbd-provisioner-674549bd88-nk4kp from storage started at 2023-05-13 18:05:58 +0000 UTC (7 container statuses recorded)
  May 13 19:55:45.801: INFO: 	Container csi-attacher ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container csi-rbdplugin ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container csi-rbdplugin-controller ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container csi-resizer ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 13 19:55:45.801: INFO: 	Container liveness-prometheus ready: true, restart count 0
  May 13 19:55:45.801: INFO: ceph-mds-worker01-9b6db8bb9-q6w24 from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.801: INFO: 	Container ceph-mds ready: true, restart count 0
  May 13 19:55:45.801: INFO: ceph-mgr-worker01-5bc7cddf9f-pmtvr from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.802: INFO: 	Container ceph-mgr ready: true, restart count 0
  May 13 19:55:45.802: INFO: ceph-osd-worker01-594bc8755f-r5bdz from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.802: INFO: 	Container ceph-osd ready: true, restart count 0
  May 13 19:55:45.802: INFO: ceph-rgw-worker01-64f8846fdb-jzw8c from storage started at 2023-05-13 18:05:59 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.802: INFO: 	Container ceph-rgw ready: true, restart count 0
  May 13 19:55:45.802: INFO: ceph-setup-q5j8d from storage started at 2023-05-13 18:05:58 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.802: INFO: 	Container ceph ready: false, restart count 0
  May 13 19:55:45.802: INFO: snapshot-controller-7dd49c567-9rfhc from storage started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.802: INFO: 	Container snapshot-controller ready: true, restart count 0
  May 13 19:55:45.802: INFO: snapshot-controller-7dd49c567-h5tj7 from storage started at 2023-05-13 18:06:09 +0000 UTC (1 container statuses recorded)
  May 13 19:55:45.802: INFO: 	Container snapshot-controller ready: true, restart count 0
  STEP: verifying the node has the label node worker00 @ 05/13/23 19:55:45.824
  STEP: verifying the node has the label node worker01 @ 05/13/23 19:55:45.846
  May 13 19:55:45.871: INFO: Pod coredns-5bf6b5d445-frtlk requesting resource cpu=100m on Node worker01
  May 13 19:55:45.871: INFO: Pod coredns-5bf6b5d445-wv86k requesting resource cpu=100m on Node worker01
  May 13 19:55:45.871: INFO: Pod etcd-worker00 requesting resource cpu=0m on Node worker00
  May 13 19:55:45.871: INFO: Pod gobetween-worker00 requesting resource cpu=100m on Node worker00
  May 13 19:55:45.871: INFO: Pod gobetween-worker01 requesting resource cpu=100m on Node worker01
  May 13 19:55:45.871: INFO: Pod kube-apiserver-worker00 requesting resource cpu=250m on Node worker00
  May 13 19:55:45.871: INFO: Pod kube-controller-manager-worker00 requesting resource cpu=200m on Node worker00
  May 13 19:55:45.871: INFO: Pod kube-proxy-worker00 requesting resource cpu=200m on Node worker00
  May 13 19:55:45.871: INFO: Pod kube-proxy-worker01 requesting resource cpu=200m on Node worker01
  May 13 19:55:45.871: INFO: Pod kube-scheduler-worker00 requesting resource cpu=100m on Node worker00
  May 13 19:55:45.871: INFO: Pod kubernetes-dashboard-67c9dc7898-qhknx requesting resource cpu=100m on Node worker01
  May 13 19:55:45.871: INFO: Pod calico-kube-controllers-849bffbf8c-mh9pf requesting resource cpu=0m on Node worker01
  May 13 19:55:45.871: INFO: Pod calico-node-bsrzk requesting resource cpu=250m on Node worker01
  May 13 19:55:45.871: INFO: Pod calico-node-xsr47 requesting resource cpu=250m on Node worker00
  May 13 19:55:45.871: INFO: Pod calico-typha-7f5df955fb-2bg4l requesting resource cpu=0m on Node worker01
  May 13 19:55:45.871: INFO: Pod metallb-controller-64cc69b5c4-wd4dh requesting resource cpu=100m on Node worker01
  May 13 19:55:45.871: INFO: Pod metallb-speaker-7rzvf requesting resource cpu=100m on Node worker00
  May 13 19:55:45.871: INFO: Pod metallb-speaker-wntmv requesting resource cpu=100m on Node worker01
  May 13 19:55:45.871: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker00
  May 13 19:55:45.871: INFO: Pod sonobuoy-e2e-job-dc09634ed87e4d2d requesting resource cpu=0m on Node worker00
  May 13 19:55:45.871: INFO: Pod sonobuoy-systemd-logs-daemon-set-0619921c091243c5-6tddn requesting resource cpu=0m on Node worker00
  May 13 19:55:45.871: INFO: Pod sonobuoy-systemd-logs-daemon-set-0619921c091243c5-wlgqk requesting resource cpu=0m on Node worker01
  May 13 19:55:45.871: INFO: Pod ceph-csi-cephfs-nodeplugin-c7nqc requesting resource cpu=0m on Node worker01
  May 13 19:55:45.871: INFO: Pod ceph-csi-cephfs-nodeplugin-mkrhr requesting resource cpu=0m on Node worker00
  May 13 19:55:45.871: INFO: Pod ceph-csi-cephfs-provisioner-b795fb565-6gcfc requesting resource cpu=0m on Node worker01
  May 13 19:55:45.871: INFO: Pod ceph-csi-cephfs-provisioner-b795fb565-8656n requesting resource cpu=0m on Node worker01
  May 13 19:55:45.871: INFO: Pod ceph-csi-cephfs-provisioner-b795fb565-m982k requesting resource cpu=0m on Node worker01
  May 13 19:55:45.871: INFO: Pod ceph-csi-rbd-nodeplugin-qckx2 requesting resource cpu=0m on Node worker00
  May 13 19:55:45.871: INFO: Pod ceph-csi-rbd-nodeplugin-xgqs8 requesting resource cpu=0m on Node worker01
  May 13 19:55:45.872: INFO: Pod ceph-csi-rbd-provisioner-674549bd88-lkl8l requesting resource cpu=0m on Node worker01
  May 13 19:55:45.872: INFO: Pod ceph-csi-rbd-provisioner-674549bd88-n9pkj requesting resource cpu=0m on Node worker01
  May 13 19:55:45.872: INFO: Pod ceph-csi-rbd-provisioner-674549bd88-nk4kp requesting resource cpu=0m on Node worker01
  May 13 19:55:45.872: INFO: Pod ceph-mds-worker01-9b6db8bb9-q6w24 requesting resource cpu=0m on Node worker01
  May 13 19:55:45.872: INFO: Pod ceph-mgr-worker01-5bc7cddf9f-pmtvr requesting resource cpu=0m on Node worker01
  May 13 19:55:45.872: INFO: Pod ceph-mon-worker00-79cd8cd599-2xbpj requesting resource cpu=0m on Node worker00
  May 13 19:55:45.872: INFO: Pod ceph-osd-worker01-594bc8755f-r5bdz requesting resource cpu=0m on Node worker01
  May 13 19:55:45.872: INFO: Pod ceph-rgw-worker01-64f8846fdb-jzw8c requesting resource cpu=0m on Node worker01
  May 13 19:55:45.872: INFO: Pod snapshot-controller-7dd49c567-9rfhc requesting resource cpu=0m on Node worker01
  May 13 19:55:45.872: INFO: Pod snapshot-controller-7dd49c567-h5tj7 requesting resource cpu=0m on Node worker01
  May 13 19:55:45.872: INFO: Pod taint-eviction-4 requesting resource cpu=0m on Node worker00
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/13/23 19:55:45.872
  May 13 19:55:45.872: INFO: Creating a pod which consumes cpu=1960m on Node worker00
  May 13 19:55:45.895: INFO: Creating a pod which consumes cpu=2065m on Node worker01
  E0513 19:55:46.407513      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0513 19:55:47.408884      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/13/23 19:55:47.919
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2fa271a9-25e5-4183-9994-8cf800cb215c.175ecc1f6514e1b0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8877/filler-pod-2fa271a9-25e5-4183-9994-8cf800cb215c to worker00] @ 05/13/23 19:55:47.921
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2fa271a9-25e5-4183-9994-8cf800cb215c.175ecc1f864e0334], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/13/23 19:55:47.921
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2fa271a9-25e5-4183-9994-8cf800cb215c.175ecc1f8712617d], Reason = [Created], Message = [Created container filler-pod-2fa271a9-25e5-4183-9994-8cf800cb215c] @ 05/13/23 19:55:47.921
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2fa271a9-25e5-4183-9994-8cf800cb215c.175ecc1f8b75701e], Reason = [Started], Message = [Started container filler-pod-2fa271a9-25e5-4183-9994-8cf800cb215c] @ 05/13/23 19:55:47.921
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-df30faed-319a-4637-9488-510b61ba0898.175ecc1f66b0f46a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8877/filler-pod-df30faed-319a-4637-9488-510b61ba0898 to worker01] @ 05/13/23 19:55:47.921
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-df30faed-319a-4637-9488-510b61ba0898.175ecc1f885ed1a9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/13/23 19:55:47.921
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-df30faed-319a-4637-9488-510b61ba0898.175ecc1f894e5754], Reason = [Created], Message = [Created container filler-pod-df30faed-319a-4637-9488-510b61ba0898] @ 05/13/23 19:55:47.921
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-df30faed-319a-4637-9488-510b61ba0898.175ecc1f8dce98a4], Reason = [Started], Message = [Started container filler-pod-df30faed-319a-4637-9488-510b61ba0898] @ 05/13/23 19:55:47.921
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.175ecc1fde4c2335], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] @ 05/13/23 19:55:47.932
  E0513 19:55:48.408940      21 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node worker00 @ 05/13/23 19:55:48.933
  STEP: verifying the node doesn't have the label node @ 05/13/23 19:55:48.942
  STEP: removing the label node off the node worker01 @ 05/13/23 19:55:48.946
  STEP: verifying the node doesn't have the label node @ 05/13/23 19:55:48.954
  May 13 19:55:48.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8877" for this suite. @ 05/13/23 19:55:48.973
• [3.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  May 13 19:55:48.981: INFO: Running AfterSuite actions on node 1
  May 13 19:55:48.981: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.034 seconds]
------------------------------

Ran 378 of 7207 Specs in 6227.618 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h43m47.911614509s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

